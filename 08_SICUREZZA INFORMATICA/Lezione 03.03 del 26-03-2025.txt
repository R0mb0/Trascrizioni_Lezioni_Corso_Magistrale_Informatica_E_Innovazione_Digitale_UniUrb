Ok, allora. Sì. Sì. Sì. Quando parlavamo della proposizione dell'Occario che chiudici vengono approvate nello stesso digest che non so. Allora, sì, l'abbiamo definita la collisione, l'abbiamo definita in questo modo. Ci siamo detti che si ha collisione quando rispetto ad una certa chiave accade questo, no? Ok. E le collisioni comporterebbero una violazione dell'integrità se fossero note all'avversario perché te metti nei panni della solita trasmissione da Alice a Bob che vuole garantire l'integrità. Alice deve mandare un messaggio M0 calcola il tag no? E manda la coppia M0 T a Bob. Se Bob usando una chiave K che condivide con Bob. Ok, quindi T sarebbe il tag calcolato a partire dalla chiave K. se l'avversario conoscesse una collisione quindi sapesse questo potrebbe sostituire no? In trasmissione M0 con M1 e quindi violerebbe l'integrità perché Bob non avrebbe modo di accorgersene dato che T è un tag corretto di M1. Ok? Quindi conoscere una collisione è un modo per l'avversario di violare l'integrità è per questo che abbiamo fatto un discorso relativo a funzioni che devono essere resistenti alle collisioni visto che le usiamo per costruire i MAC ok? E i MAC vogliono garantire e i MAC per essere sicuri devono essere robusti rispetto a quella tag game in cui l'avversario cerca di falsificare una una coppia messaggio tag è chiaro che se l'avversario conosce una collisione se il MAC è costruito a partire da una certa funzione con chiave e l'avversario conosce una collisione rispetto alla chiave K che Alice e Bob utilizzano allora attraverso quella collisione vincerebbe l'attack game contro il MAC costruito a partire da quella funzione questo è il motivo per cui la proprietà che ci interessa per l'UHF che definiscono l'UHF è appunto la resistenza delle collisioni nel caso di HP dell'UHF HP allora evidentemente l'ho spiegato male la volta perché è lo stesso che mi hai posto te nel caso di questo di questo polinomio ok allora HP è un polinomio no lo potrei anche andare a riprenderlo però il punto qual è? il punto è che vediamo la rappresentazione grafica del polinomio HP è scritto da una funzione che ha come rappresentazione grafica la curva di un polinomio dove? esatto esatto diciamo più è alto il grado del polinomio il grado dipende dalla lunghezza del messaggio perché dipende dal numero di blocchi del messaggio allora siccome il polinomio HP viene calcolato in aritmetica modulare siamo in ZP ok dove quindi in realtà del del polinomio ci interessa solo l'intervallo compreso fra 0 e il numero primo P è solamente in questo intervallo che noi scegliamo la chiave ovvero il punto all'interno del quale valutare il polinomio è chiaro che se se P è un numero piccolo diciamo non è super poli allora vuol dire che il dominio delle chiavi è piccolo e polinomialmente limitato e quindi vuol dire che un avversario con risorse polinomiali le prova tutte un po' come per lo stesso motivo per cui a suo tempo quando abbiamo parlato di cifrari a chiavi simmetriche abbiamo sempre detto che il dominio delle chiavi deve essere super poli ok questo come condizione di base per garantire che la probabilità che l'attaccante indovini la chiave deve essere trascurabile ma lo è solamente se il dominio delle chiavi è super poli e qui va il nostro discorso quindi veramente più grande è P maggiore il dominio delle chiavi possibili e questo è il motivo per cui la la robustezza di questo UHF è direttamente proporzionale alle dimensioni di P infatti eravamo arrivati a dire che HP è un L fratto P UHF dove L fratto P è l'epsilon che stabilisce la la robustezza dell'UHF quindi da una parte avere P grande mi garantisce un dominio ampio delle chiavi o poniamo che la curva sia questa questa è la tua curva come si comporta la curva tra 0 e P ok chiaro che fai crescere P consideri una porzione maggiore del dominio del polinomio quindi rendi più difficile per l'avversario provare a dominare qual è la chiave che è stata utilizzata ok quindi P al denominatore è chiaro il ruolo che ha poi appunto dicevamo la forma della curva è determinata dal messaggio perché la tua funzione polinomiale come coefficienti usa i valori del messaggio quindi tu interpreti ogni blocco di M del messaggio come un numero e quel numero rappresenterà il coefficiente di un certo termine ogni blocco associato a un termine di un certo grado ai v blocchi e quindi avrai v più 1 termini nel tuo polinomio quelli che vanno dal grado 0 fino al grado v ok quindi la forma di quella curva è determinata dal messaggio ok e la lunghezza del messaggio dipende da questo L no perché L è la lunghezza massima perché l'ha imposto la condizione che v è compreso tra 0 ed L dove L è limitato polinomialmente ok quindi questo valore che è il grado del tuo polinomio numero di blocchi chiaramente è limitato da questo da questo bound da questo bound qui facendo crescere questo bound qui facendo crescere L di conseguenza aumenta la lunghezza dei messaggi che te puoi prendere in input ma ti ritrovi con delle curve con parecchie oscillazioni più alto il grado maggiore sono i punti di minimo e massimo locali nella tua funzione ma avere parecchi punti di minimo di minimi massimi locali cosa significa significa che preso preso qualunque assintoto orizzontale te hai che il tuo polinomio interseca quella assintoto in parecchi punti e quelli sono tutte collisioni prendiamo questo vuol dire che questa chiave collide con questa che collide con questa che collide con questa e sì sì ok queste sono collisioni che riguardano però questo cosa significa significa che comunque più facile per l'avversario a quella definizione no ok ok no no no qui collisione no no certo chiaro qui collisione inteso non è non è quella di collisione che abbiamo usato per definire la robustezza del dell'UF dell'UHF chiaramente qui ha a che fare col fatto che avendo più chiavi che ti producono lo stesso lo stesso aumenta la probabilità esatto aumenta la probabilità per l'avversario di indovinare magari non la chiave giusta ma una che collide con quella ok e quindi è il discorso di prima l'avversario può fare un tentativo random di indovinare qual è la chiave e se ci sono molte chiavi che collidono sul suo valore è proprio più facile trovare questo ok e facendo troppo questo grazie di dei diversi chiavi che collidono con la la similità si si va la cosa e anche ok è l'incognita è l'incognita del tuo polinomio no cioè se prende un polinomio di banale come questo x al quadrato più 3x più 1 ok questo è il la funzione hp per il messaggio che ha i blocchi che rappresentano la codifica dell'intero 4 del 3 e dell'1 quindi tu hai un messaggio fatto di tre blocchi dove il primo blocco è la codifica binaria del 4 il secondo blocco la codifica binaria del 3 e il terzo blocco la codifica binaria dell'1 quindi questo messaggio qui è associato a questo polinomio qua ok poi l'x è la tua incognita allora il digest il digest di m è il valore di questo polinomio calcolato dove calcolato in k cioè k è l'incognita è è il punto è il punto dove tu è il punto qui dentro è lì è l'elemento del dominio su cui tu calcoli la funzione no quindi se tu scegli questo punto qui e ti vai a calcolare il polinomio in quel punto lì ti salta fuori questo valore qua ok un punto nella curva però è chiaro che se te non sai qual è l'x su cui calcoli il valore del polinomio tu non sai qual è questo valore qua tu conosci la curva come è fatta perché questa la conoscono tutti no perché m viaggia in chiaro quindi sai come fatta la curva ma non sai quale punto della curva ok se comunque il discorso no ha capito l'ambiguità sul discorso della della collisione è semplicemente un discorso legato al fatto che se hai più chiavi che si comportano allo stesso modo e quindi aumenta la probabilità per l'avversario di vincere di vincere la tag game tu qua oggi invece vediamo funzioni hash che a differenza di quelle che abbiamo studiato la volta scorsa non hanno bisogno di una chiave ok quindi funzioni hash senza senza chiave che come vedremo sono anche più problematiche rispetto alle funzioni hash con chiave perché no nelle funzioni hash con chiave c'è un elemento di segretezza che è la chiave ok e quindi l'avversario non è in grado di calcolare l'hash di un messaggio se non conosce la chiave invece nelle funzioni hash senza chiave chiaramente chiunque può calcolare l'hash di qualunque messaggio dato che non c'è bisogno di utilizzare una una chiave da questo punto di vista ok e quindi vedremo che questo solleva delle maggiori problematiche relative alle definizioni di sicurezza della della funzione hash perché comunque le proprietà da garantire sono sempre le stesse in particolare la resistenza alle collisioni ok dove per collisione esattamente come l'altra volta intendiamo una situazione di questo genere no allora diciamo che il CRC32 è un algoritmo che si che calcola un tag a partire dal messaggio però lo calcola applicando una funzione che è facilmente invertibile ok ad esempio non lo so caso classico è la parità la funzione di parità cioè quindi la funzione che ti restituisce uno zero a seconda del numero della quantità di uni che ci sono nel messaggio pari o dispari ok è una funzione banale facilmente invertibile insomma quindi nel nostro caso invece su cui è facile trovare delle collisioni tra l'altro no anzi è estremamente banale trovare collisioni in quel caso quindi nel caso del CRC è utile la funzione che ha di garantire l'integrità perché te hai un messaggio e il tag se i due non coincidono vuol dire che il messaggio è stato alterato durante la trasmissione però chiaramente non è praticabile in un contesto in cui le alterazioni sono malevole cioè è praticabile in un contesto in cui le alterazioni che ti interessano sono gli errori di trasmissione ok può capitare un errore di trasmissione confrontando messaggio e tag se non coincidono se non corrispondono vuol dire che c'è stato un errore di trasmissione e rifiuti il messaggio ok ma in un contesto in cui le alterazioni non avvengono per caso per colpa del mezzo di trasmissivo ma c'è un utente malevolo allora un algoritmo quello non è più utile perché per l'utente malevole è facile trovare collisioni su un algoritmo come quello su cui si basa il CRC32 e quindi può fare la sostituzione di un messaggio come panno di pare quindi non hai la prova di integrità rispetto ad un utente malevolo è chiaro che se prendi in considerazione solo gli errori di trasmissione qual è la probabilità che per un errore di trasmissione un messaggio venga alterato in un altro che ha lo stesso tag è difficile questo è chiaro che se lo fai apposta lo trovi facilmente e il punto nel contesto dell'intervità in ambito di sicurezza è proprio quello cioè rendere difficoltoso ad un avversario l'individuazione di un messaggio alternativo che abbia lo stesso tag ok ma questo poi lo metteremo in evidenza anche nel risultato oggi significa che l'avversario non non deve poter invertire la funzione la funzione hash cioè cosa vuol dire questo vuol dire che se hai il messaggio è facile calcolarne la hash il tag ok e va bene ma se hai il tag deve essere difficile risalire a tutti i possibili messaggi che possono generare quel tag quindi perché è chiaro che se tu potessi invertire la funzione allora il gioco la tag e non vinceresti sempre no perché vedi passare un messaggio vedi passare il suo tag prendi il tag lo inverti e trovi un messaggio alternativo a quello che è stato mandato e fai la sostituzione ok quindi il problema è sempre lo stesso in un contesto legato alla alla sicurezza se vuoi usare funzioni hash a questo scopo devono essere funzioni su cui è difficile trovare delle collisioni quindi funzioni per le quali è difficile intuitivamente a partire dal tag di un messaggio cercare un messaggio alternativo per quello stesso tag ok che siano con chiave o senza chiave il problema è sempre quello evitare che ci possano essere delle collisioni poi è chiaro che le funzioni hash in altri contesti dove quello che ti interessa non ha la sicurezza allora puoi usare funzioni hash qualunque puoi usare anche funzioni che non sono OHF tornando al discorso della settimana della settimana scorsa o anche funzioni che sono non sono resistenti alle collisioni non so se avete studiato nel corso non lo so non ricordo se nel corso di sistema di basi di dati si studiano o nel sistema operativo le tabelle hash seguono per garantire accessi più rapidi alla memoria i record vengono organizzati in bucket in blocchi che corrispondono al loro valore hash così quando devi cercare un record all'interno di una tabella molto grande sai già in quale bucket andare a cercarlo in quel caso se ci sono delle collisioni tutti i record che colliderano li mettono nel nostro bucket quando hai saturato il bucket ci sono delle tecniche per spostarle altrove ma lì il problema non sono le collisioni perché il problema non è la sicurezza il problema di fondo è un accesso efficiente alla memoria senza dover che so scandire sequenzialmente tutti i bucket di una tabella o quasi se lo scopo se lo scopo è l'efficienza delle collisioni te ne freghi ci saranno le gestisci e lì ci sono tanti modi di gestirle ma l'obiettivo è sempre quello di garantire l'efficienza cioè l'accesso più rapido possibile nel nostro caso l'obiettivo il tema non è quello dell'efficienza l'obiettivo è la sicurezza ovvero garantire che un avversario non possa cambiare il messaggio taggato col suo valore hash mettiamola così con un altro messaggio che abbia lo stesso valore hash perché se potesse fare una cosa del genere allora violerebbe l'integrità non sarebbe l'attack game contro il MAC ok quindi per evitare che l'avversario possa vincere l'attack game contro il MAC abbiamo bisogno di funzioni hash resistenti le collisioni perché è quello il problema che l'avversario deve risolvere se vuole vincere l'attack game contro il MAC ok ripeto che si tratti di funzioni hash con o senza chiave cambia nulla nel senso che in ogni caso il problema di fondo è la resistenza alle collisioni in questo senso qua è chiaro che se parliamo di funzioni hash con chiave quindi la collisione ce l'hai in un caso di questo genere c'è un elemento di segretezza che è la chiave che rende più complicato il compito per l'avversario l'avversario non sa su quale rispetto a quale chiave deve cercare la collisione ok quindi questo in questo caso rispetto a questo qua tra virgolette è più semplice perché perché l'avversario non ha necessità di ragionare su quale chiave usano Alice e Bob dato che Alice e Bob non usano una chiave ma semplicemente usano una funzione hash senza chiave e calcolano il tag in qualche modo che dipende vedremo come calcolano il tag in qualche modo che dipende dal valore hash del messaggio ma perché è più semplice il lavoro per l'avversario da questo punto di vista perché questa cosa questa ricerca qui l'avversario la può fare offline visto che la chiave non è un elemento no nello scambio tra Alice e Bob l'avversario si può mettere lì in maniera asincrona rispetto a quello che fanno Alice e Bob a tutto il tempo che vuole a disposizione e comincia a cercare delle collisioni e se ne trova una la funzione hash non serve più a niente perché diventa sicura ok quindi ripeto il fatto che le funzioni hash siano o meno resistenti alle collisioni dipende dal contesto applicativo le tabelle hash che si usano appunto per la gestione del file system lì la collisione non è un problema perché il tema non è la sicurezza il tema è l'accesso e quindi le performance se invece le funzioni hash le dovete usare in un contesto abbastanza tipico che è quello di garantire l'integrità di che ne so di alcuni file critici del sistema operativo o alcune librerie di sistema quindi file critici dal punto di vista anche della sicurezza però di solito questi questi file critici spesso volentieri vengono memorizzati in aree di memoria che sono ridolli proprio per garantire che che non vengano alterati perché questi non possono non devono essere alterati e spesso volentieri chiaramente non c'è a disposizione una quantità di memoria ridolli a sufficienza per ospitare tutti questi questi file allora di solito quello che si fa è si memorizzano i file critici in un'area di memoria normale non ad accesso ridolli mentre invece nella memoria ad accesso ridolli ci metti l'hash del file ok quindi è chiaro che chiunque può alterare il file ma se tu alteri il file invalidi la corrispondenza col valore hash che è memorizzato nell'area ridolli ok però questa cosa funziona se chi va ad alterare il file non riesce a trovare una collisione rispetto all'hash che è memorizzato nell'area ridolli ok quindi qui la resistenza alla collisione è importante quindi dipende sempre dal contesto applicativo ok le funzioni hash come vedremo una delle prime applicazioni che useremo sarà quella di accorciare i messaggi che vanno taggati che è l'approccio che abbiamo già visto poi un paio di lezioni fa l'hash then prf ovvero un meccanismo per cui è un messaggio lungo da taggare e che cosa fai ci sono diversi modi per creare un tag a partire da un messaggio molto lungo uno di questi è applicare in cascata un prf a ogni blocco del messaggio come si fa in fstella in fcbc e poi si fa la cifratura finale no l'encrypted mac che abbiamo visto un paio di lezioni fa un altro modo banale è appunto quello di prendere il messaggio calcolarne l'hash e poi applicare il mac sul hash che sicuramente ha una dimensione ridotta rispetto a quella del messaggio originale ok comunque le funzioni hash senza chiave hanno tanti diversi domini applicativi che esulano anche dall'ambito della cyber security però noi li usiamo sempre comunque se e solo se continuano a rispettare quelle stesse proprietà che abbiamo visto settimana scorsa nel caso delle funzioni hash con chiave quindi in particolare la resistenza alle collisioni perché altrimenti non perdiamo la capacità di garantire l'integrità ok questo un po' è l'idea ok e rivediamo un po' gli stessi concetti che abbiamo visto settimana scorsa eliminando il fatto che non c'è più una chiave in ballo ok però le definizioni sono sempre le stesse quindi in particolare una funzione hash senza chiave è una funzione che va da un dominio dei dei messaggi M grande ha un codominio dei dei digest possibili dove la cardinalità di M come al solito è tipicamente molto più grande rispetto alla cardinalità di T e quindi c'è il problema delle collisioni e l'attack game analogo a quello che abbiamo visto l'altra volta è quello che in cui l'obiettivo dell'attaccante appunto è quello di trovare due messaggi che convivono vedete qui l'attack game è più semplice rispetto a quello che abbiamo visto l'altra volta perché l'altra volta nell'attack game un ruolo ce l'ha anche il challenger perché il ruolo ce l'ha anche il challenger perché il challenger sceglie la chiave da usare per calcolare il digest di un messaggio qui la chiave non c'è quindi il ruolo del challenger è inesistente tant'è che nell'attack game l'unico agente in ballo è l'avversario no che senza dover interagire con un challenger quindi offline quello che fa è cercare se ci riesce due messaggi che collidano e il suo vantaggio è la probabilità di riuscirci ok da qui capiamo il fatto che la situazione è un po' più delicata rispetto all'altra volta perché il compito dell'avversario è facilitato dal fatto che non ha bisogno di interagire con il challenger dato che non c'è una chiave ok si dice si dice al solito che la nostra funzione H è resistente alle collisioni se il vantaggio dell'avversario è trascurabile per tutti gli avversari efficienti quindi vuol dire che un avversario che ha risorse di calcolo limitate polinomialmente ha una probabilità molto vicina al zero una probabilità trascurabile di trovare una collisione quindi trovare due messaggi che collidono sullo stesso digest ok definire funzioni resistente alle collisioni in questo senso qua è complesso no? come potete immaginare proprio perché ripeto l'attacco viene condotto offline dall'avversario prendete una funzione hash che si usa in ambito cyber security da 15 anni a questa parte vuol dire che negli ultimi 15 anni no? sono stati fatti tentativi di trovare collisioni e non sono state ancora trovate che è una cosa ben diversa no? dal garantire che durante l'esecuzione di uno scambio di messaggio di Alice a Bob con una certa chiave l'avversario non riesce a trovare una collisione rispetto a quella chiave c'è una bella differenza no? tra tra scenari di questo genere ok? quindi è piuttosto più complicato definire funzioni hash senza chiave resistenti alle collisioni l'applicazione la prima applicazione che ci viene in mente delle funzioni hash senza chiave appunto è quella che dicevo prima ovvero applicare la funzione hash a un messaggio che deve essere taggato prima di applicare la funzione di segnatura del mac ok? quindi la funzione di segnatura del mac invece che essere applicata a M viene applicata al digest di M ok? quindi voi prendete il messaggio questo come dicevo prima ci permette di poter usare dei mac anche su messaggi lunghi a piacere perché tanto preliminarmente la lunghezza del messaggio viene ricondotta a quella del suo digest applicando la funzione hash poi il mac si applica con la sua chiave si applica al digest del messaggio ok? questo è un modo poi vedremo che nella pratica le cose non si fanno proprio così perché non è estremamente efficiente combinare funzioni hash con i mac con i prf in questa maniera qua però a livello teorico questa costruzione è sicura se H è resistente alle collisioni posto naturalmente di usare un maxicuro perché H deve essere per usare questa costruzione qui perché H deve essere resistente alle collisioni se non lo fosse e torniamo al problema di prima perché se H non fosse resistente alle collisioni allora l'avversario potrebbe trovare due messaggi che collidero sullo stesso digest ma se due messaggi collidero sullo stesso digest vuol dire che avranno lo stesso tag rispetto al mac perché il mac viene applicato sul digest non sul messaggio e quindi perderemo e quindi perderemo di nuovo l'integrità ok ora perché è particolarmente complesso definire funzioni HASH resistente delle collisioni vi faccio un esempio che deriva da un lemma da un risultato di calcolo delle probabilità legato appunto alla probabilità di avere delle collisioni e il lemma prende il teorema prende il nome di paradosso del compleanno perché mette in evidenza il fatto che avere delle collisioni è molto più semplice dal punto di vista statistico rispetto a quello che noi siamo portati a immaginare a pensare nel nome del paradosso del compleanno perché è particolarmente intuitivo interpretare questo risultato in un contesto pratico che adesso vi vado a raccontare immaginate qui il teorema dice che dovete avere un insieme n di dimensione n piccolo no e k diverse variabili casuali distribuite uniforme che seguono distribuzioni uniformi su quelli insieme m cosa vuol dire questo facciamo un esempio pratico adesso qui siamo in pochi immaginiamo di avere un'aula con una trentina di persone m quella m grande sono i giorni dell'anno ok quindi quell'n piccolo è 365 35 le k variabili casuali siete voi quindi ognuno di voi è una variabile casuale ognuno di voi è una variabile casuale che come valore assume che cosa il giorno del proprio compleanno quindi un valore compreso per 1 365 ok per semplicità stiamo assumendo visto che se non ci sono altri criteri particolari stiamo assumendo che se noi prendiamo 30 persone a caso ok la data del compleanno di ciascuno sarà distribuita in maniera sarà una variabile casuale distribuita in maniera uniforme tra i 365 giorni dell'anno visto che possiamo presumere astrettamente che non ci siano date più probabili di altre ok quindi possiamo assumere che se prendiamo una persona a caso la data del suo compleanno è rappresentata da una variabile casuale con distribuzione di probabilità uniforme perché tutti i giorni sono i più probabili per rappresentare il compleanno di quella persona quindi x1 x2 xk sono le persone siete voi ognuno con il suo compleanno che seguirà la distribuzione di probabilità uniforme visto che le persone non sono scelte con un criterio particolare i giorni dell'anno non sono non seguono distribuzioni significativamente diverse rispetto a quella uniforme per quanto riguarda i periodi delle possibili nascite il terreno cosa dice dice appunto che preso M grande cioè i 365 giorni dell'anno e prese K diverse variabili casuali distribuite uniformemente quindi prese un campione di K persone qual è la probabilità che in quel campione ci siano due persone che assumono lo stesso valore cioè che compiono gli anni nello stesso giorno e il calcolo si può dimostrare pari al valore di quell'equazione 1 meno E alla meno K per K meno 1 fra 2 N che approssimativamente è una quantità maggiore o uguale del minimo tra quei due valori è un'approssimazione ok quindi in pratica questo teorema vi dice qual è la probabilità che due elementi di un campione di dimensione K assumano lo stesso valore quando il range di valori possibili è determinato dall'insieme M che è di cambio di Rital ok nel nostro esempio preso una classe di 30 persone la probabilità che nella classe ci siano due persone che compiono gli anni lo stesso giorno è data da quella formula approssimata da quel valore ok perché questo teorema ci è utile nel nostro contesto perché sta descrivendo che cosa sta descrivendo la probabilità delle collisioni il fatto che due persone compiono gli anni nello stesso giorno per noi è una collisione ok se fate due conti usando la la versione approssimata abbastanza quindi n piccolo è 365 k è il numero di persone se volete che la probabilità sia almeno il 50% vi basta una classe di 27 28 persone una tipica classe di scuola quindi in una qualunque classe di prima elementare c'è una probabilità molto vicina al 50% che ci siano due bambini che compiono gli anni questo giorno ok che tutto sommato uno non se lo aspetta uno sospetta che questa probabilità sia molto più bassa ok invece no c'è un teorema che ce lo fa calcolare in maniera puntuale poi ripeto nella realtà le cose non vanno esattamente così perché l'ipotesi di questo teorema è che le variabili casuale seguono distribuzioni uniforme quindi vuol dire che ogni giorno dell'anno ha la stessa probabilità vuol dire che prendete le nascite in Italia nel 2024 vorrebbe dire che in ogni giorno del 2024 è nato lo stesso numero di persone allora vorrebbe dire che la distribuzione di probabilità è uniforme quindi se così fossero le cose allora se voi prendete un campione di 30 nati 28 nati nel 2024 avreste il 50% circa di probabilità di dentro di averne due che compiono gli anni dei nostri giorni le cose non stanno esattamente così nella pratica non è che tutti i giorni dell'anno sono i più probabili per tutta una serie di motivi però resta valido il discorso del teorema che nel nostro caso invece si applica perché la distribuzione uniforme è esattamente l'ipotesi che facciamo sempre noi per quanto riguarda le variabili casuali coinvolte nei nostri meccanismi ok quindi è un teorema che si può applicare anche nel nel nostro contesto in particolare nel caso delle funzioni HESH e il lemma che vedete dopo fa due conti su quelle che sono le caratteristiche che l'esperimento condotto dall'avversario dovrebbe avere per garantirgli una probabilità circa del 50% di trovare una collisione no ok perché il teorema ci dice sotto quali condizioni di fatto noi troviamo una collisione con una certa probabilità che è questa approssimativamente questa quantità qua ok ma questo è esattamente l'obiettivo che ha l'avversario no trovare trovare una collisione allora andiamo a fare due conti basati sulle caratteristiche delle funzioni HESH per vedere quanti esperimenti l'avversario deve fare perché questo se vogliamo corrisponde al numero degli esperimenti no nel nostro esempio l'esperimento consiste nel prendere 30 persone ok nate in un certo anno e vedere se ce ne sono due che compiono gli anni nel caso dell'attack game dell'avversario contro la funzione HESH l'esperimento consiste nel prendere K messaggi ok e vedere se due di questi messaggi hanno lo stesso HESH chiaro l'analogia no allora sotto quali condizioni applicando il teorema quindi questo risultato qua sotto quali condizioni l'avversario ha una probabilità di circa del 50% di trovare due messaggi che collidono quanto deve essere grande questo K quanti esperimenti deve fare l'avversario per trovare una collisione allora nel caso delle funzioni HESH abbiamo detto che questo è il dominio dei messaggi ok questo è il codominio dei digest quindi il codominio dei digest va a coincidere con questo elemento qua del teorema del paradosso del compleanno ok sappiamo anche che di solito questo è molto più grande di questo ok facciamo facciamo un'ipotesi 100 volte più grande ok ora se l'avversario fa un numero di esperimenti approssimativamente uguale alla radice di n che è la cardinalità dello spazio dei digest allora la sua probabilità di trovare una collisione è il 50% che è ok ok ok cioè se l'avversario ha la possibilità di fare un numero di esperimenti pari almeno alla radice della cardinalità dello spazio dei digest la probabilità di trovare una collisione è pari al 50%. Quindi di fatto vince il game, vince l'attaggame. Quindi non solo, quando ragioniamo sulle funzioni hash, questo cosa ci dice? Ci dice che non ci basta ragionare sulle dimensioni di questo codominio, perché già la radice quadrata delle dimensioni del codominio rappresenta una quantità di esperimenti sufficiente per l'avversario per vincere il gioco. Quindi questo cosa significa? Significa che la cardinalità di T deve essere super poli. Se vogliamo escludere che l'avversario abbia una probabilità non trascurabile di trovare una collisione, ma deve essere super poli in termini, se vogliamo, più stringenti rispetto a quanto abbiamo sempre detto in passato per quanto riguarda, ad esempio, lo spazio delle chiavi per ricifrare i simmetrici. Ad esempio, nel caso di AS, se vi ricordate, abbiamo detto che una lunghezza delle chiavi di 128 bit è sufficiente, perché 2 alla 128 è grande, è super poli, e quindi la probabilità per l'avversario di indovinare la chiave è negligible. Nel caso delle funzioni hash, 128 non basta, quindi lo spazio dei digest di 2 alla 128 non basta, perché è vero che è super poli, però questo lemma qui ci dice che già prendendo un numero che è la radice dello spazio dei digest, quindi riducendolo di parecchio, l'avversario ha comunque una probabilità ben più che non traspurabile, ha probabilità al 50% di trovare una collisione. Quindi dobbiamo partire da quantità estremamente più grandi, se vogliamo garantire che l'avversario non abbia la possibilità di condurre un numero di esperimenti sufficiente, perché, ripeto, nella tag game contro la funzione hash, l'avversario, per provincia della tag game, non è che deve avere il 50% di probabilità di trovare una collisione, deve avere una probabilità non trascurabile, quindi anche molto piccola, ma comunque diversa da 0. quindi questo significa che in realtà per avere una probabilità, che ne so, faccio un caso pratico, lo 0,1% di trovare una collisione, e questo è non traspurabile, e quindi gli farebbe vincere la tag game, gli basta, facendo le dovute proporzioni, un numero di esperimenti molto più piccolo di questo, perché con questo numero di esperimenti il suo vantaggio è 0,5, in realtà noi da 0,5 dobbiamo portarlo quasi a 0, quindi vuol dire che noi dobbiamo ragionare su una cardinalità di questo insieme estremamente grande, se vogliamo che riducendola rimaniamo comunque super poli, perché l'idea qual è? L'idea è che il numero degli esperimenti che l'avversario deve fare per trovare una collisione con una certa probabilità deve essere talmente grande che un avversario efficiente non ce la fa. Sì? Eh, eh, vabbè, lo vediamo. Nella pratica, per quanto riguarda le funzioni di hash, lo spazio dei digest deve essere quantomeno questo, quantomeno. Quindi lo spazio dei digest almeno 2 alla 256, infatti vedete che è parecchi ordini di grandezza più grande rispetto a 2 alla 128, nel caso ad esempio dei cifrari simmetrici. Quindi i digest delle funzioni hash non possono essere lunghi quanto le chiavi di cifrari simmetrici, tanto per fare un esempio. Deve essere lunghi almeno il doppio. Ok? I messaggi, hai voglia? qui stiamo parlando di tag di 256 bit. Stiamo parlando di digest, comunque i plaintext possono essere di qualunque lunghezzo, come ho detto prima. Se prendi plaintext di un mega, quanti diversi plaintext di un mega ci sono? 2 è elevato alla un milione, che è una quantità neanche immaginabile, quindi è abbastanza scontato, insomma, che altro che superpoling, il spazio dei plaintext è incalcolabile. Quindi tutto sto discorso può arrivare a dire che bisogna essere ancora più cauti con le funzioni di hash rispetto a quanto abbiamo fatto per quanto riguarda invece i cifrali. Quindi al giorno d'oggi una lunghezza minima per i digest, quindi per gli elementi dell'economia della funzione hash deve essere quanto meno di 256 bit. ora, come si costruiscono le funzioni hash senza chiave chiaramente con la proprietà di essere resistente alle collisioni? Come dicevo prima, non è facile, proprio per i motivi che abbiamo detto. esiste una costruzione standard dovuta a due tizi, Merkel e Damgard, che hanno proposto un paradigma che semplifica un po' il problema. Ok? In che modo semplifica il problema? Beh, loro dicono guarda, se c'è una funzione hash, chiamiamola h piccolo, e in gergo viene chiamata funzione di compressione, ok? Che riesce a calcolare il digest in maniera sicura per plaintext di piccole dimensioni e come vedremo è relativamente facile fare questo, cioè definire definire funzioni hash resistenti alle collisioni che lavorano su plaintext piccoli, piccoli relativamente, come dicevo prima, che ne so, plaintext di 256 bit o poco più, ok? Perché già abbiamo visto che 256 è sufficiente per avere il digest appartenente a un codominio sufficientemente grande da eludere l'attacco di prima dell'avversario. Quindi, posto che sia possibile costruire facilmente funzioni hash che lavorano su plaintext piccoli, e questo come vedremo è possibile, è relativamente facile, Merkel e Amgar dicono che a partire da una qualunque funzione di compressione h sicura, in questo senso, è sempre possibile, usando un meccanismo che loro propongono, costruire una funzione hash altrettanto sicura che però lavora su plaintext di qualunque dimensione. È chiaro l'idea, è chiaro l'approccio. Cioè, loro dicono è facile costruire una funzione hash resistente alle collisioni che lavora su plaintext piccoli. Benissimo, perché ce ne sono tante, dopo ve ne farò vedere qualcosa. Benissimo. Allora, noi che cosa facciamo? Noi facciamo vedere che esiste un modo, sfruttando la funzione di compressione qualunque essa sia, per costruire una funzione hash capace di calcolare il digest di qualunque messaggio, grande quanto vogliamo, con lo stesso livello di robustezza della funzione di compressione. Ok? E questo è il paradigma di fatto più utilizzato in letteratura. dopo vi farò vedere un esempio. Come funziona il loro paradigma, il loro metodo? È un metodo che si ispira, è molto simile, se vi ricordate, ai PRF, no? Se vi ricordate, anche i PRF hanno questo problema. Lavorano su blocchi di piccole dimensioni. Allora, se io devo calcolare, applicare un PRF a un messaggio lungo, che cosa faccio? Ciclicamente, in qualche modo, prendo il PRF e lo applico a ogni singolo blocco del messaggio. F stella, FCBC o XORASH, erano due modi di fare questa cosa qua. Il paradigma di Marker & Darman è molto simile. Cioè, loro cosa fanno? Prendono il... Immaginiamo che di avere un messaggio M di grandi dimensioni. Ok? Loro spezzettano in tanti blocchi, che sono questi, tutti di uguali dimensioni e tutti di dimensioni tali da poter essere dati in pasto alla funzione di compressione, alla funzione H piccolo. Ok? Dopodiché, all'interno di un ciclo, loro non fanno altro che applicare iterativamente la funzione hash a ciascuno di questi blocchi, combinando in cascata i risultati per ottenere il digest finale. Ok? L'idea qual è? Vi rappresento quello che succede nel ciclo proprio dal punto di vista iterativo. Loro partono da un digest iniziale. Ok? Che è sempre lo stesso. È una costante. Qualunque sia il messaggio di cui calcolare l'hash, il digest di partenza è sempre quello e viene chiamato initial rule. Ogni funzione hash avrà il suo initial rule ed è il nostro T0. Il T0 è il digest iniziale. Poi all'interno di un ciclo che cosa succede? Succede che io prendo il digest attuale, quindi inizialmente T0, prendo il prossimo blocco del plaintext, li concateno e li do in pasto alla funzione di compressione H. Ok? È questa operazione che vedete qua. La funzione di compressione per come è definita vedete che cosa fa? prende in ingresso un elemento del dominio X, nel nostro caso sarebbe questo, questo appartiene a X, ok? Prende un elemento invece che è una sequenza di L bit, no? Ma che è questo? Perché questo appartiene a 0,1 all'L dato che il plaintext è stato spezzato in blocchi di L bit ciascuno. Quindi la funzione, ripeto, la funzione di compressione vuoi in ingresso un elemento di questo dominio e un elemento di questo dominio e l'elemento di questo dominio è il tag l'elemento di questo è il blocco e cosa restituisce? Restituisce un nuovo elemento di X che sarà chiameremo T1 che è il nuovo digest. Sì, vanno concatenati perché la funzione di compressione vuole T0 vuole la coppia T0 M1 Sì, sì, infatti H prende alla fine H prende in ingresso una stringa che è lunga quanto la lunghezza di un elemento di X più altri L bit. Non è scelto, è una costante. È uguale ogni algoritmo, ogni funzione di hash avrà il suo. Non è random. Poi questa operazione viene reiterata perché adesso T1 insieme a M2 verrà data di nuovo in pasto la funzione di compressione in cui il risultato sarà T2 e questa cosa va avanti finché non avete esaurito tutti i blocchi del plaintext. Siccome di blocchi ne avete S, dove S chiaramente dipende da quante grande M, se voi prendete la lunghezza di M diviso L che è la dimensione di ciascun blocchettino ottenete S. Quindi questo ciclo for è reiterato S volte e l'ultimo tag che è l'ultimo digest che è valore che ottenete sarà TDS e TDS chi è? È il digest di M di M grande. Com'è? No, ah, questo L qui? Sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, sì, è la lunghezza, quella L grande e la lunghezza di M, quindi sì, scusami, qui ho messo M per intendere la lunghezza di M non sarebbe L grande. quindi vedete, alla fine, usando questo paradigma, applicando questo ciclo a qualunque plaintext che sta qui dentro, otterrete comunque un digest della stessa dimensione del digest che restituisce la funzione di compressione. ripeto, perché Merkel e Damgard propongono questo paradigma? Perché, come vedremo tra poco, costruire funzioni hash come questa su dominio relativamente piccolo è facile, mentre invece non lo sarebbe farlo direttamente su messaggi di grande dimensione. Quindi, si va a pescare una qualunque funzione di compressione sicura, si applica a questo ciclo ed ecco che si ottiene una nuova funzione che è esatto. La cosa bella è che la robustezza di H grande coincide con la robustezza di H piccolo. Quindi vuol dire che l'esecuzione di questo ciclo non ha vulnerabilità ulteriori rispetto a quelle che di per sé avrebbe H piccolo. è molto simile ad altre costruzioni di questo genere che abbiamo visto applicate ai PRF. Il principio alla fine è analogo. Un esempio classico di funzione o per meglio dire famiglia perché in realtà sono tante funzioni HASH che si basano sul paradigma di Merckx e Damgard è la funzione di famiglie SHA SHA è l'acronimo di Secure HASH Algorithm che è probabilmente la famiglia di funzioni HASH più usate in ambito sicurezza ed è lo standard proposto dal NIST ed è una implementazione del meccanismo che abbiamo appena visto dove L piccolo è 512 bit quindi il plaintext che vogliamo taggare viene suddiviso in blocchettini di 512 bit che gli mi hanno questa dimensione mentre invece i tag hanno come dimensione 256 bit che è il valore minimo di cui parlavamo prima per poter essere robusti rispetto al lemma del paradosso del compleanno ok quindi vuol dire che la funzione di compressione su cui vi dirò dopo qual è nel caso di SHA cosa prende in input prende l'abbiamo visto prima prende in input un tag e un blocco quindi prende in input una sequenza che è di 512 più 256 bit ok e l'initial value di SHA 256 come abbiamo detto prima è una costante ed è un numero di 256 bit espresso in binario non mi ci stava nella slide perché sono appunto 256 valori mentre invece in esa decimale mi ci stava quindi questa è la rappresentazione esa decimale del valore iniziale a 256 bit che rappresenta t0 ok il teorema conferma quello che ho detto prima ovvero la la costruzione di Merkel e Damgaard produce una funzione hash che ha lo stesso livello di sicurezza della funzione di compressione su cui si basa ha la stessa resistenza alle collisioni come dicevo prima costruire una funzione di compressione resistente alle collisioni è relativamente semplice rispetto rispetto a farlo direttamente su messaggi di lunghezza variabile e il motivo è che si può la costruzione della funzione di compressione si può basare sui block cipher ok e i block cipher lo sappiamo già lavorano su blocchi di piccole dimensioni questo è il motivo il modo tipico di costruire funzioni di compressione consiste nel combinare block cipher con loxor il primo esempio in questo senso è dato da questa funzione di compressione dove è la funzione di cifratura di un block cipher qui si può usare qualunque block cipher sicuro ad esempio a come funziona la funzione di compressione allora se la funzione di compressione lavora sulla concatenazione di due elementi come abbiamo visto prima no ok quindi per questo motivo che qui ci ho messo la coppia x e y sono i due elementi che concatenati formano l'input per la funzione di compressione e come avviene il calcolo del digest beh si prende uno dei due elementi in questo caso il secondo e lo si usa come chiave per cifrare il primo è quello che succede qui dentro vedete y viene usato come chiave per cifrare x immaginate che il block cipher che si usa è a s ok e se torniamo all'esempio di prima sì la risposta è sì perché il primo elemento è il digest secondo il messaggio quindi vuol dire che stai usando il messaggio per cifrare il digest ok e però il risultato non legge in xorra con il digest sono tutti elementi noti l'input di h sono tutti elementi noti perché il il primo digest è una costante lo conoscono tutti un messaggio lo vedono tutti quindi i calcoli li può fare chiunque no perché il calcolo del digest lo può fare chiunque perché non c'è nessun elemento diciamo confidenziale nel calcolo del digest di di di di un certo di un certo input perché tanto a noi quello che interessa è è la la la non invertibilità che ci permetterebbe di trovare delle collisioni ci sono varianti di questa che più o meno sono analoghe dove cambia cambia il secondo operando del del deboxor e compatibilmente con il teorema del paradosso del compleanno che abbiamo visto prima l'avversario per avere una probabilità non trascurabile di individuare una una collisione dovrebbe testare un numero di messaggi pari alla radice della cardinalità di links che è lo spazio dei digest mi ricordo ma se i digest sono di 256 bit esplorare 2 alla 256 diversi diversi messaggi è un'operazione che un avversario efficiente non riesce a fare in tempo polinomiale tant'è che ad esempio su SHA 256 non sono note delle collisioni al momento perché ripeto l'obiettivo è quello definire una funzione su cui sia difficile trovare collisione sì sì sì sì l'avversario che prende un campione di messaggi e calcola il digest del campione di messaggi sperando di trovare due che hanno lo stesso lo stesso lo stesso lo stesso digest qui vedete allargato il discorso relativo alla famiglia di funzioni hash che usano esattamente usano l'intera famiglia di funzioni hash usa il paradigma di merkel e damgard quindi usa questa costruzione qua ok e usa come funzione hash questa come funzionare di compressione usa la prima che abbiamo visto qui ok dove è la funzione di cifratura di un cifrario simmetrico particolare che di cui non abbiamo mai parlato che si chiama shakal che è un cifrario che è stato inventato appositamente per quando è stato definito shakal tutto qua la famiglia shakal su base su queste due costruzioni lì vedete un po' di cronostoria di evoluzione della famiglia shakal di funzioni hash famiglia che si è evoluta nel tempo semplicemente andando ad agire sulle dimensioni dell'input cioè della funzione di compressione quindi aumentando lo spazio dei digest ad esempio shakal come abbiamo visto prima shakal 256 usa digest di 256 bit e lavora su blocchi di 512 tutte le funzioni della famiglia shakal lavorano su blocchi di 512 bit cambia la dimensione del digest ok il primo shakal lavorava con digest di 160 bit ok è nato nel 95 e nel giro di poco più di 20 anni è stata trovata una collisione però ci sono comunque voluti più di 20 anni per trovare la prima collisione perché lì lo spazio era di 2 alla 160 diversi diversi digest attualmente 2 alla 63 è il costo operazionale più efficiente che bisogna pagare per trovare una collisione su shakal per questo motivo poi è stato proposto nell'arco del tempo altre evoluzioni c'è shakal 256 c'è anche shakal 512 però al momento shakal 256 rappresenta ancora la soglia minima di garanzia prima della famiglia shak un antesignano se vogliamo di di di funzioni hash era md5 è stata una delle primissime funzioni hash resistente alle collisioni che però lavorava su digest di 128 bit che non è più sufficiente ok e anche nel caso del md5 da giro di pochi anni sono trovati delle collisioni addirittura con un algoritmo di complessità varia 2 alla 16 come sforzo computazionale non è una di prezioni da fare si trovano collisioni su md5 quindi oramai md5 con un pc dello portatile di oggi si riesce a violare insomma si trovano banalmente delle collisioni su md5 e ciò nonostante ci sono ancora delle applicazioni di software che usano md5 come funziona hash oramai lo standard invece è da scia 256 in su ok più raramente scia 112 perché comunque è un po' più inefficiente come come tempi di esecuzione naturalmente proprio perché lavora su digest più più grandi ok ci siamo quindi usando funzioni come sha in teoria noi potremmo applicare questo schema qua per costruire un mac sapendo che il primissimo mac che abbiamo proposto consisteva nell'usare un prf brutalmente solo il problema qual era il problema è che i prf lavorano su input di piccole dimensioni e quindi non si può usare un prf di per sé per realizzare un mac e da lì è venuto fuori tutti gli discorsi che abbiamo fatto sulla sulla composizione di prf quindi f stelle fcbc compagnia bella è venuto fuori poi il discorso delle funzioni hash con chiave per avere il paradigma di composizione prf o hf e qui qualcosa di analogo si potrebbe pensare componendo ad esempio sha con un prf quindi voi prendete il messaggio lungo quanto vi pare gli applicate sha tirate fuori il digest del messaggio che è piccolino su quel digest applicato il prf ed ecco che avete il vostro mac usando la costruzione hash prf è una costruzione però poco efficiente perché combinare combinare una funzione hash come sha è in cascata con un prf come ad esempio può essere as è abbastanza inefficiente e quindi si sono studiati dei modi alternativi di costruire direttamente dei prf usando come unica primitiva la funzione hash quindi modo un modo diverso di costruire prf in passato abbiamo visto che un prf sicuro ce l'abbiamo usando un block cipher sicuro come AS128 adesso vediamo come si può costruire un prf sicuro pulando solo e esclusivamente come primitiva crittografica una funzione hash resistente alle collisioni e la costruzione più semplice da questo punto di vista che darà come risultato appunto un prf sicuro e quindi un mac sicuro perché abbiamo visto all'inizio abbiamo detto fin dall'inizio un prf sicuro è un mac sicuro consiste nell'usare quest'idea qui la Tukinest che molto semplicemente da qui vediamo l'efficienza realizza il prf usando due volte in cascata una funzione hash resistente alle collisioni come si definisce nel nostro prf la chiave del prf si divide in due una prima porzione k1 una seconda porzione k2 questo immaginate che sia il messaggio lungo a piacere di cui volete calcolare il su cui volete applicare il prf e la costruzione è questa si prende la prima porzione della chiave del prf concatenato col messaggio e se ne calcola l'ash il risultato lo concatenate con la seconda porzione della chiave e ricalcolate di nuovo l'ash questa costruzione dà luogo ad un mac perché ripeto l'idea originale per implementare i mac consiste nell'usare brutalmente un prf sicuro e questa idea è implementata in quello che letteratura è diventato uno standard e si chiama hmac hmac è usato diffusamente un po' in tutti i protocolli per la trasmissione dei dati su internet lì ne ho citati alcuni che sono quelli che studieremo più avanti tutti quanti usano usano hmac come mac sicuro per taggare i messaggi garantire l'interità e h può essere ad esempio shadow 256 quindi ripeto qual è l'idea del nostro prf prendo la chiave del prf e la spazza metà poi la prima metà della chiave concatenata con il messaggio ne calcolo il digest il risultato lo concateno con la seconda metà della chiave e ricalcolo di nuovo il digest il risultato finale è l'output del nostro prf e definito in questo modo il prf è sicuro quindi vuol dire che il risultato del prf è impredicibile se io non conosco la chiave rispetto rispetto a tutti i mac che abbiamo visto in passato è una costruzione abbastanza efficiente perché richiede di usare la stessa primitiva due volte vi ricordate invece l'encrypted mac che faceva applicava tante volte un prf su ogni blocco del messaggio applicavo un'ultima volta per cifrare l'operazione da fare era costo elevato o anche la composizione prf o hf o hf sono un po' più inefficienti da implementare rispetto a alle funzioni esce senza chiave quindi questo è il motivo per cui questo tipo di costruzione del mac è piuttosto è piuttosto diffusa sì sì allora qui si poteva metterla in realtà perché poi in realtà l'ho messa l'ho usata qua fammi vedere mi ricordo come input sì lì l'ho messa qua l'ho messa no la funzione di compressione applicata ai due elementi quindi avrei potuto metterla anche qua perché la concatenazione a seconda dei contesti delle volte ci sono diversi modi di rappresentare la concatenazione o sovrapponendo in sequenza i messaggi da concatenare senza usare dei simboli come ho fatto lì a volte si usa la doppia barra verticale per esprimere la concatenazione a volte si usa la virgola non esiste una notazione standard univoca per esprimere la evitare di usare la virgola secondo me è meglio perché fa confusione rispetto a quando invece avete una funzione su più argomenti sì sì sì probabilmente probabilmente dovrei cambiare qua dovrei cambiare qui ma perché di testo da dove l'ha copiato usava la virgola due perché esatto perché qui c'è la virgola perché HP è definito in maniera tale da prendere due elementi però a livello pratico è come prendere la concatenazione ok quindi qui la notazione della virgola è coerente con la signature di H è coerente però se ci pensi dal punto di vista della realizzazione pratica alla fine per come per come lavora la funzione di compressione ma è che poi dopo lo vedi qui nella definizione i due i due argomenti vengono manipolati in questa in questa maniera qua qua non ho usato la virgola perché perché perché questo lo dobbiamo interpretare come il messaggio il plain text che viene dato in pasto alla funzione H grande appunto no perché perché vedi H grande di nuovo andiamo a vedere la signature di H grande la signature di H grande la signature di H grande è questa cioè in input prende un oggetto un elemento che ha una sequenza binaria di L bit ok e quindi là non ho usato la virgola per esplicitare il fatto che sono due stringhe che con catenate formano quell'unica stringa che va in input ad H grande quindi sono notazioni che vengono usate coerentemente con la definizione della funzione però alla fine se ci pensi a livello implementativo è un po' la stessa cosa no no no infatti è per quello che non ho chiaramente lì non ho usato la virgola non ho usato la virgola perché sarebbe stato sbagliato mettere la virgola lì avrei potuto usare la doppia barra verticale per rendere ancora più spicito il fatto che è una concatenazione però questo è coerente col fatto che la funzione H grande prende una stringa che in questo caso è il risultato della concatenazione di due elementi uno segreto e l'altro no perché K2 è segreto qui infatti il se guardi gli argomenti di F F gli argomenti ne prende sempre due una chiave e il messaggio no e infatti il primo argomento l'ho chiuso tra parentesi no per esplicitare il fatto che la chiave che stiamo dando in input al PRF è la coppia K1 K2 che però da un punto di vista pratico possiamo vedere come la concatenazione di K1 e K2 è un'unica chiave che noi spezziamo a metà e la prima metà la chiamiamo K1 e la seconda metà la chiamiamo K2 ok questo è l'idea però sì bisogna sempre fare attenzione al tipo delle funzioni per trovare la giusta corrispondenza con l'interpretazione e vi faccio vedere come funziona HMAC nella pratica la cui la cui implementazione di base è questa no la funzione H applicata due volte dove H grande esce a 256 ok quindi le peculiarità di HMAC stanno nel modo in cui vengono determinate K1 e K2 fondamentalmente ok in HMAC i blocchi di messaggi quelli del paradigma di Marker e Dagmar sono di un genericamente di B-byte dove B è una costante ok dipende dalle implementazioni no quindi questi blocchettini qua i vari MI sono di sono di una lunghezza pari a un parametro che viene stabilito di implementazione di implementazione di HMA che è quel B grande ok poi invece K1 e K2 che io prima genericamente vi ho detto sono la prima metà e la seconda metà della chiave non sono proprio così sono calcolati in maniera un po' diversa nel senso che comunque partiamo dal fatto che c'è un'unica chiave la chiave di integrità chiamiamola così condivisa da Alice e Bob in HMA non è che banalmente la chiave viene spezzata a metà per generare K1 e K2 ma K1 e K2 vengono derivate dalla chiave K attraverso l'applicazione di due maschere Inksor queste due maschere vengono chiamate inner pad e outer pad quindi K1 è l'oxor della chiave K con l'inner pad K2 è l'oxor della chiave K con l'outer pad i due pad sono costanti e sono quei due valori che mi interessa il byte 00110110 ripetuto di grande volte e l'altro per l'inner pad e l'altro per l'outer pad quindi ci sono due maschere costanti predefinite per HMAC che vengono applicate alla chiave K per derivare K1 e K2 dopodiché K1 e K2 si usano come abbiamo visto qui e H grande è shadow 156 ok questo è il modo in cui funziona di di base HMAC poi ripeto parametri che dipendono da implementazione da implementazione sono il valore di B grande cioè quanto sono grandi blocchettini e poi in certi casi HMAC può essere volendo basato su shadow 512 piuttosto che shadow 256 anche qui dipende da bigrand l'altra cosa interessante di cui parleremo domani è che HMAC è la base rappresenta la barri allora HMAC lo dobbiamo vedere come un MAC che preso un messaggio e presa una chiave vi tira fuori un tag ok a partire da quella chiave ed è un MAC sicuro quindi un avversario non è in grado di falsificare coppie messaggio tag per una certa chiave HMAC nella pratica viene molto utilizzato in particolare per realizzare un'altra primitiva di cui volevo accennarvi qualcosa ma a questo punto lo cuore domani che è HKDF che è molto utile nelle applicazioni pratiche perché perché HKDF fa una cosa molto interessante ovvero HKDF prende in ingresso un segreto che può essere una password può essere una chiave condivisa da due agenti comunque una stringa di lunghezza limitata ok e produce in output una sequenza più o meno lunga piacere segreta a sua volta cioè in pratica HKDF fa quello che fanno i PRG perché a partire da un segreto vi costruisce una sequenza di segreti perché è molto utile nella pratica è molto utile nella pratica perché spesso l'interi quello che succede è che due agenti condividono un unico segreto no una password condividono una chiave e nient'altro però magari quei due agenti devono essere coinvolti in più protocolli devono essere coinvolti in più istanze di uno stesso protocollo e quindi magari hanno bisogno di dialogare tra di loro basandosi su diverse chiavi e come fanno i due agenti a condividere tante diverse chiavi quando in realtà ne condividono una sola è un problema di cui abbiamo già parlato in passato e la cui soluzione poteva essere quella di usare il PRF HKDF è un modo estremamente efficiente perché si basa su HMAC per produrre una sequenza di segreti a partire da un unico segreto quindi l'idea che ne so è che se due agenti hanno bisogno di condividere 100 chiavi perché gli servono 100 chiavi per realizzare un certo protocollo in realtà ne condividono una sola loro prendono quell'unica chiave che condividono gli applicano HKDF ottengono una stringa molto lunga che loro spezzano in 100 porzioni e ciascuna di queste porzioni sarà una delle chiavi di cui hanno bisogno quindi HKDF si utilizza per generare una sequenza di sottochiavi a partire da un'unica chiave all'uso di primitive diverse esatto perché questo stesso problema l'avevamo già visto quando abbiamo introdotto i PRF anche lì avevamo fatto un esempio applicativo ho una sola chiave come faccio ti hanno i qualità antiappi con PRF con un input su un input fisso a partire dalla chiave e quella era un'astrazione questa è un'implementazione di quello stesso principio però un'implementazione che si basa su una primitiva completamente diversa rispetto ai PRF perché si basa su HMAC che a sua volta si basa su funzioni hash piuttosto che su PRF però alla fine le cose tornano a essere equivalenti perché perché c'è un teorema che dice che questa costruzione qua questa doppia applicazione di una funzione hash fatta in questo modo è un PRF e se H è resistente alle collisioni il PRF è sicuro quindi di fatto è la stessa idea che avevamo visto un'altra volta implementata in un modo particolare questo è domani vi faccio vedere come funziona HKDF HKDF ripeto è molto usata nella pratica ad esempio HKDF per fare un esempio applicativo di strumento dove veniva usato vi ricordate le app di contact tracing che si utilizzavano durante il covid magari qualcuno di voi l'ha anche installato sullo smartphone qualcuno qualcuno no quelle app lì che avevano un sacco di problemi di sicurezza da gestire confidenzialità integrità autenticazione proprio perché comunque era un'app che facevano dialogare dispositivi vicini tra di loro scambiandosi dei dati e lì problemi di privacy se ce ne erano quanti ne volete per risolvere tutti i problemi di privacy di non tracciabilità perché lì il problema fondamentale era garantire la non tracciabilità e per garantire la non tracciabilità tutte le comunicazioni che avvenivano tra dispositivi vicini ma anche tra dispositivi e server centrale perché a un certo punto il dispositivo doveva trasmettere col server centrale per comunicargli con chi era entrato in contatto quindi erano tutte comunicazioni che avvenivano sulla base di una quantità di chiavi enorme le chiavi addirittura cambiavano la frequenza di una volta ogni 10 minuti dato per dire questo era fondamentale perché io entro in contatto con lei dopo 10 minuti entro in contatto con un'altra persona questi due contatti non sono linkabili e quindi non sono tracciabili perché? perché il primo si basa sulla chiave il secondo si basa su un'altra questa era l'idea l'idea di base questo permetteva di far sì che certe informazioni non potessero essere collegate correlate perché di fatto la chiave cambiava continuamente quindi si fa a cambiare chiave così spesso si usava nelle implementazioni si usava hkdf per generare a partire da una chiave che il dispositivo condivideva col server centrale gli permetteva di costruire quindi condividere una sequenza lunga di chiavi da utilizzare poi negli scambi di prossimità magari se ci sarà tempo ve lo faccio anche vedere come funzionava però questo è un esempio dove è pratico dove è importante avere a disposizione tante chiavi per far funzionare un protocollo e come si fa generare queste chiavi il modo più efficiente per farlo è anche il capo di acce a partire da un'unica chiave condivisa va bene direi che ci possiamo fermare qui questo discorso lo continuiamo domani invece oggi pomeriggio torniamo sul tema del penetration testing e cominciamo a vedere qualche esempio pratico di quel tipo lì volevo che può uscoin mi passare c'è corra dopo che