Dunque, la tua ultima questione del conto è di analizzare dal punto di vista delle condizioni di requisiti di sicurezza le funzionalità tipiche dei classificatori basati su machine learning. In realtà questo discorso si apete anche al deep learning. Vado velocemente, sanno un po' le motivazioni che stanno dietro l'utilizzo di questo genere di tecniche per l'analisi dei dati, per l'analisi della predizione anche dell'andamento dei dati. E vado direttamente a rivedere un attimo quelli che sono i concetti di base che stanno dietro tecniche come machine learning, per capire quali potrebbero essere, quali sono poi nel mondo reale, le potenziali vulnerabilità dietro l'utilizzo di questo tipo di algoritmi. di questo tipo di strumento, scusate, in cui scopo è quello di fondamentalmente imparare a analizzare determinate serie di dati senza avere a disposizione un algoritmo, un programma esplicito che lo faccia, ma solo attraverso la costruzione di un modello a partire da dati che sono già noti. che sono già noti. L'esempio di riferimento su cui si fa vedere, o su cui tra poco vedremo una semplificazione di quello che è il comportamento tipico di questi modelli durante la fase del prognamento, consiste nel considerare come input una coppia di dati con x e y e x an input e y è l'output corrispondente. che vedete, non conosciamo la funzione matematica che mappa x e y, perché se la conoscessimo non avremmo bisogno di un modello e non avremmo bisogno dell'utilizzo di tecniche di machine learning, basterebbe implementare, scrivere un programma che descrive il comportamento di f, dopodiché vediamo x e non ci diamo x. Noi però questa f non avremmo bisogno, per tanti motivi, perché è una funzione estremamente complicata, perché le feature, le variabili che penso non sono numerevoli, non sapevamo qual è il legame tra queste e il modo che viene calcolato l'output, insomma per tutta una serie dei motivi, non siamo capaci di costruire, non siamo capaci di costruire f e quindi abbiamo bisogno di un modello che approssima f e che a partire dall'input x ci predica quello che dovrebbe essere l'output di rispondere a x. Ok? Chiaramente il modello è tanto più preciso quanto più l'output che fornisce, o, in questo caso, è vicino alla y reangulo. Ora, nemmo andare, lo sapete meglio di me, queste cose magari le avete viste anche da riflesso, nemmo andare a costruire un modello che riesca a predire con la precisione esiderata l'output y, l'idea appunto è quella di addestrare un modello, quindi partire da una base, da un modello in cui tutti i vari parametri che poi adesso dobbiamo vedere sono configurati in una certa maniera, e addestrarla, cosa significa? Significa fornire tutta una serie di input di cui conosciamo già l'output corretto, vedere cosa risentisce il modello e, a seconda degli errori che il modello fa nell'accursionare l'output, potete intervenire con delle correzioni sui parametri e andare avanti in questo modo fino a quando la differenza tra l'output che spara il modello e poi l'oreale non sono sufficientemente vicine, cioè la loro differenza è stata per essere visible in qualche modo. e l'aggiustamento serve una serie di correzioni che si basano, come vedremo, sulla variazione dei valori di certi parametri guidata dal valore che viene fatto durante le fasi di addestramento. vi faccio vedere nella linea di principio del carizzo di base, ditemi se è molto diverso da quello che si effettuati, perché poi alcuni di questi ingredienti che vengono fuori più avanti permettendo di evidenza a quelli che potrebbero essere delle vulnerabilità rispetto al proprietà di sicurezza. in questa data vi vedete uno schema generale di come funziona a partire dal valore degli input in ingresso, quindi questi x1, x2 e così via, sono diversi di x2, i valori vengono forniti in ingresso, questi input vengono combinati linearmente utilizzando dei pesi, con lo scopo di ottenere un valore scalare, che poi viene sommato a un termine lato, il cui risultato viene dato in pasto a quella che poi viene da un'infonzione di efflas. sottofondo vedete un esempio tipico di funzione di archivazione che fondamentalmente scarta i risultati mediagradi e restituisce solo quelli positivi. l'idea è appunto di cominciare a mostrare il modello rappresentato dalla configurazione dei pesi e vedere quanto dista l'output che il modello restituisce rispetto a quello atteso che conosciamo già rispetto agli input che abbiamo in ingresso. e la differenza tra l'output atteso e quello che viene determinato nel modello può essere calcolato da diverse dimensioni, quella che vedete in gioco è un esempio di funzione, la cosiddetta loss function che dipende da che cosa, dipende da tutti i palami con gioco, quindi dipende dal vettore x dei valori input, dipende dall'y atteso, cioè il risultato vero, dipende dal vettore dei pesi u doppio, dal terminato b, quindi tutti quanti gli ingerenti che partecipano un po' al capo di o. ok? e questo loss function che vedete qui non è altro che una somma pesata del quadrato delle differenze tra i valori attesi, i y e il risultato prodotto dal modello, ok? ovvero quella sequenza di operazioni che abbiamo messo in evidenza a partire dalla combinazione di input rispetto ai pesi fino ad arrivare all'applicazione della funzione di attivazione. ok? quindi quella somma pesata rappresenta in qualche modo l'errore che è stato fatto nel modello nel predire l'anct tutto associato a x rispetto al valore atteso che sarebbe appunto y. l'idea qual è? l'idea per aggiustare il modello consiste nel aggiustare i pesi di una quantità che dipende dalla loss function chiaramente definata rispetto ai vari feature come vedete ogni feature ha un suo peso e quindi ogni peso viene modificato di uno fattore che dipende dalla derivata di questa funzione rispetto alla feature di riferimento. infatti la variazione la vedete qua giù in basso il nuovo valore del peso, un certo peso, è dato dal vecchio valore meno un fattore mu che viene moltiplicato per appunto la derivata dello loss function rispetto a quella particolare di rischio. molto costante che in qualche modo vuole rappresentare la tendenza della furba con cui le soluzioni del generale che possa un modello convergono verso le soluzioni corrette questo è un po' ho cercato di rassumere in due minuti come funzionano i modelli di machine learning classici il cui obiettivo appunto è quello di costruire un modello predittivo ragionevole poi vedremo che questa questa cosa qua geometrica che non so se avete mai visto basso io che fin qui sono tutte cose di meno male avete già più o meno visto poi vedremo quali sono gli elementi qua dentro che in qualche modo possono prestare il fianco a qualche tipo di di vulnerabilità e quindi distrarre il modello utilizzando un algoritmo simile a quello che abbiamo appena schematizzato l'idea appunto è quella di costruire un modello rappresentativo di quella che è una di fatto quella che è una distribuzione che associa gli input X ai corrispondenti output dopodiché questo modello può essere usato e validato ovviamente la validazione consiste nel prendere un set di campioni diversi da quelli che sono stati usando per l'addestramento e si va a vedere rispetto a questi campioni come si può andare nel modello ma in questo caso naturalmente assomigliamo di manoscere il risultato corretto e vediamo quanto è preciso il modello che può essere valutato da risparmi diversi meriti quindi hai una cropezza, una precisione con diverse tipologie di score per questo ci interessa meno e una volta che l'abbiamo validato il modello che può essere utilizzato per fare trasformazione informazioni che è il contesto specifico su cui ci concentreremo noi ma può essere usato anche per altri scopi in generale nuovi dati e particolari e così via in generale nuovi dati e particolari e così via in funzione di caposso da essere tante quindi tornando a quello che dicevo all'inizio l'idea del problema di base è che abbiamo una serie di variabili di linguaggio abbiamo una serie di variabili di linguaggio che assumono determinati valori di solito in questo contesto il numero di questi variabili è strumento elevato può essere strumento elevato sappiamo che esiste una funzione che combina i valori di questi variabili e che deve restituire un risultato che lega appunto il input al dato questa funzione noi non la conosciamo non siamo in caso di costruire perché è troppo perché è troppo complessa ok quindi apprendere come deve essere per una funzione X è un po' un po' difficile questo è il momento del motivo per cui dando tecniche basate sul machine learning si preferisce cercare di bypassare questo problema questo è il momento del che non ricalca il comportamento di una funzione che non conosciamo ma che cerca di simulare questo e questo è il momento questo è il momento del modello costruito come abbiamo visto prima proprio quello di cercare appunto di simulare in una maniera più precisa possibile quindi il comportamento di una funzione che non conosciamo questo è un po' è l'idea è l'idea è l'idea problemi di costruzione diciamo almeno dal punto di vista teorico sono piuttosto vecchi sono molto datati nell'ambito dell'analisi matematica sono i cosiddetti problemi di pack learning ovvero problemi in cui obiettivo è quello appunto di riuscire attraverso una metodologia iterativa ad apprendere in maniera approssimata quindi mobilistica il comportamento di una certa funzione ed è un po' quello che si si riscontra nei modelli basati su machine learning che avete studiato il modo in cui vengono validati dal punto di vista dell'efficacia è molto simile a questa definizione dove si dice che l'errore che un modello H fa è pari a che cosa? la probabilità che per un qualunque input ci sia una certa differenza tra cosa restituisce il modello rispetto a quegli input e cosa restituirebbe la funzione che non conosciamo rispetto a quello stesso rispetto a quello stesso input e ciò che ci aspettiamo per misurare l'efficacia del nostro modello è che questo errore sia negligible ok con negligible otteniamo un concetto molto simile a quello che abbiamo studiato nella definizione di la taggate per i qualità di sicurezza Cosa vuol dire in questo contesto? Che la probabilità che l'errore sia oltre una certa soglia dovrebbe essere sotto un delta tipo la ricerca, che è una evidenzione molto simile rispetto a quella che abbiamo visto nella definizione delle possibilità di sicurezza. Tant'è che effettivamente c'è un po' di ortogonalità tra quelli che sono gli obiettivi di un modello della fine e gli obiettivi di uno schema fotografico. Perché in entrambi casi l'obiettivo è massimizzare o minimizzare una certa probabilità. Nel caso del machine learning vogliamo minimizzare la probabilità di avere un certo tipo di errori. Nell'ambito degli schemi elettografici e le proprietà di sicurezza si basano su quale obiettivo? Qual è l'ultimo modo di rendere negligible, rendere mascurabile, minimizzare la probabilità di un avversario, riesca ad apprendere qualche foglio. Quindi vedete l'obiettivo, l'obiettivo sono apposta, nel machine learning vogliamo, l'obiettivo è apprendere, avere un modello che si abbia appreso come si comporta una certa funzione, appreso con un margine di errore che si è negli esibili. Nell'ambito di esibili è che succede esattamente il contrario, cioè l'obiettivo è un nostro schema a far sì che l'avversario non apprenda, con un margine di errore, cioè con una capacità di apprendimento, quindi il resto da scuola. Quindi i due obiettivi sono l'uno e l'altro, nei due ordini. Tant'è che effettivamente c'è un contrasto dal punto di vista teorico tra la fotografia e il machine learning, perché se noi prendiamo una definizione di Pac-Learn, cos'è che può essere apprenduto attraverso metodi tra cui ad esempio il machine learning. C'è tutta una teoria in letteratura su quelli che sono i problemi che sono fattorebili, cioè problemi che sordiscono questa condizione di efficacia, e rispetto a determinati recensi, chiaramente ci aspettiamo che apprendimento richieda un tempo con i nonni altri. Esattamente come nell'anglese della fotografia, il discorso della probabilità di crisi contro un avversario di apprendere qualche cosa fa comunque riferimento ad avversari efficienti, cioè quali sono sempre i nonni altri. Quindi questo bound legato ai divini di complessità c'è sempre il band di gas. Comunque c'è la letteratura, ci sono tutta una serie di studi che stabiliscono quali sono le caratteristiche di generale di problemi che siano Pac-Learn, cioè problemi per i quali è possibile costruire un modello che soddisfi questa condizione di efficacia. E questo ci dà una nuova idea di quali problemi, ad esempio, possiamo cercare di affrontare, di risolvere, di usare un tecnico di scendere. E per come sono definiti le proprietà di sicurezza e quindi i schemi criptografici che sordiscono le proprietà di sicurezza, va da sé, in realtà le definizioni che ci stanno sotto, che i problemi sottostanti le proprietà di sicurezza delle schemi criptografici, come ad esempio violare la confidenzialità, l'obiettivo dell'avversario è questo, violare la confidenzialità di un plaintext cifrato usando una chiave non nota all'avversario, scelta in maniera randocula. Tanto per fare un esempio, non abbiamo parlato della sicurezza di sicurezza, per il valore la lunga probabilità di sicurezza. Quelli sono tutti i problemi nel contesto degli schemi che abbiamo introdotto, e di quali abbiamo dimostrato la lunga probabilità, a dimostrare che i problemi schemi vanno da una probabilità. Equivale a dire che quei problemi di riduzione sono a Puck-Levoli. Quindi questo taglio una testa, diciamo, a ogni genere di dubbio sul fatto, come mi è stato chiesto più volte, negli anni passati, durante il corso, ogni o qualunque io mostravo quantomeno i risultati, come le dimostrazioni del fatto che, ad esempio, la S128, che arrabbisce la sicurezza sanitaria, o è a CPI sicuro, perché c'è la dimostrazione del fatto, dice che l'avversario, quando mi è da un baseball, mi dice un certo attacco di EMA, allora la domanda che mi viene fatta è va bene. Ma se io adesso ho un modello di deep learning, con tanti plaintext, di cui fornesco del satratax per le corrispondenti, per fare l'investramento, e se adesso ho un modello in questa maniera qua, poi mi posso aspettare che quando lo vado a verificare, se vi do un cypertext di cui non conosci il plaintext, lui mi indovino il plaintext anche se non conosce la biasa, c'è un'altra parola. La domanda è, posso avversionare un modello fornendogli delle coppie plaintext e satrata, senza barri la biasa, addestrarlo nel rispetto di requisiti di efficienza, quindi rispettandola nel tempo buono di anni, poi posso aspettare che questo modello mi pari quella che è la relazione tra plaintext e satratax e mi riesca poi a disperare il satratax senza conoscere la biasa, ma è un esito, no? La risposta è no, perché quelli sono problemi che non sono pac-verbal, quindi sono problemi che non sono risultibili attraverso i modelli di apprezzamento, proprio per come sono definiti gli schemi che implementano le imprese. Quindi il problema di decifrare un site-product o risalire a vari proprietà del plaintext senza conoscere la variabile è un problema che non è pac-verso, proprio per i rimotti in cui sono rifiuti gli schemi di cifratura e decifratura, perché hai dimostrato dal punto di vista teorico che tutto nemmeno si fa semantiche, valgono delle proprietà che contraticono invece le proprietà che devono soddisfare il modello di apprendimento. Quindi chiudo questa parentesi, quindi non si può, non si potrà mai, adattare il modello di learning, quanto con presso sia, che fa, che tanno così, quindi risale al plaintext senza conoscere le chiave. Ok? Quindi, è interessante anche perché sapere quali sono i problemi di PacLab, con quelli che non lo sono, ci dice, ci dà dei suggerimenti su quali problemi possiamo usare per definire il nuovo schema fotografico o quali problemi possiamo usare invece per possiamo risolvere, ad esempio, utilizzando il modello di apprendimento. Prendete, faccio un esempio, il problema della fattorizzazione dei numeri primi che hanno dei problemi alla base dell'RSA o il problema dei gruppi ciclici che sta a base di l'UF di Edmund, ok? Quindi il problema del modifico discreto, e le dicendo, quelli sono problemi che non sono PacLabo. E proprio perché non sono PacLabo, cioè non posso costruire un modello che risolve quel tipo di problemi, vuol dire che quelli sono i problemi adatti per essere usati all'interno di primitivi e di trattori. Ok? Chiudo questa parente sull'analogia tra modelli di apprendimento e primitivi e di trattori. Quello secondo me che voglio considerare in questa lezione è stabilire fino a che punto noi ci possiamo affidare i modelli di trattori. che vedo. Sotto diversi punti di vista, no? Uno dei punti di vista riguarda sicuramente l'affidabilità dei risultati, ok? Che è un qualcosa con cui noi non ci occuperemo. C'è tutta la letteratura che in qualche modo riguarda il problema di stabilire quanto accurati sono i risultati prodotti da un modello. Ok? Chiaramente questo non è un problema che ha compato con la sicurezza A che fa la cicatrizia per non con la loro sicurezza. Così come un altro problema legato alla stabilità consiste nella interpretabilità dei risultati. Ok? Ottengo risultati precisi? Sì, va bene, ok. Come devo interpretare questi risultati? In che misura questi risultati dipendono dai diversi valori di input? Spiegare perché un modello di aggredimento complesso su dei clienti estremamente complessi ha un certo risultato piuttosto che un altro e quindi descrivere quello che è una relazione di un modello addirittura a diciamo individuato che ha l'input e l'output chiaramente è un qualcosa che aiuta a migliorare la percezione di effettibilità dei risultati prodotti del modello che può succedere. E in qualche modo vedremo che questo è legato anche alla sicurezza di questo tipo di sistema. perché capire che misura ogni feature dell'input impatta sull'output che viene prodotto è anche un modo come vedremo per capire che margini ci sono da parte di un avversario di compromettare ad esempio gli esitati di un umano di questo tipo poi vedremo questo cosa significa nel metà però diciamo che questi sono aspetti automodali che sono molto interessanti c'è tanta letteratura di riferimento su questo ma che sono un po' laterali rispetto alla cyber security avvicinandoci un po' invece a quelle che sono questioni che hanno molto a che fare con la sistema che fa sicuramente una delle criticità importanti che stanno dietro utilizzando di modelli di apprendimento riguardano la privacy e parallelamente alla privacy anche la confidenzialità delle informazioni che vengono utilizzate per fare training dei modelli ok molto spesso certi modelli vengono addestrati utilizzando informazioni sensibili che non devono essere le le le pubbliche per per preservarli la confidenzialità ma anche per preservarli accarsi di chi ha fornito quei dati ok vi faccio un esempio classico si fa un largo uso di modelli predictivi classificatori che dice un ambito sanitario perché perché si utilizzano fare diagnostica questi modelli sono stati amministrati utilizzando che cosa utilizzando l'informazione della pazienza di reali ok spesso una stragrande svarza dei casi si tratta di informazioni sensibili e si tratta anche di informazioni che non vogliono non devono essere in nessun modo compromesse questo cosa significa e ci sono attacchi un dettato che ho questo significa che ad esempio utilizzando interrogando il modello magari io non ho accesso al train insan ma ho accesso al modello risultante dall'utilizzo un certo train insan interagendo col modello a volte è possibile riuscire a risalire a quelli che sono i dati che sono stati utilizzati per apprestare e non ho sconso quando poi che hanno riuscito a dare delle chavi in delle ADI perché era stato allenato su modello e questo è l'indigità non no no no questo è un altro però sì il concetto è abbastanza semplice sono dei paper che raccontano gli attacchi di questo genere che fanno vedere come in ambito li ho visti in ambito sanitario ma anche in ambito finanziario vengono molto utilizzati anche dei modelli per dire a un esempio che sono mesi precedenti modelli predittivi per stabilire se un presto deve essere concesso oppure no sulle base caratteristiche del cliente che hanno visto anche lì l'arcastramento di questi modelli viene fatto su banche reali e dall'analisi non so se non so se non vedo nel dettaglio però dall'analisi dell'interazione con il modello si è riuscito in molti casi a risalire all'identità di cura delle persone che hanno fatto i dati per i set ok per fare un esempio quindi una delle problematiche che in tal senso consiste proprio nel preservare la privacy del training set ma non solo la privacy e la confidenzialità ma una caratteristica molto importante è anche l'integrità del training set una verso siccome questi modelli vengono usati per ad esempio per dire fare delle stritte torniamo ad esempio di prestito bancario ok è chiaro che se noi garantiamo l'integrità del training set cosa potrebbe succedere potrebbe succedere che l'amversario fa injection nel training set dei dati felici il cui effetto è quello di alterare il modello e indurlo ad dare delle risposte che siano favorevoli agli scopi dell'avversario è chiaro che se voi alterate il training set cambia il modello e non cambia il risultato questo produsio magari l'avversario dovrebbe farlo semplicemente per ricoltare voi il solo di tirizzi del modello che vuole compromettere la funzionalità dei servizi che si basano su questi modelli quindi un altro dei problemi che è importante affrontare un modo il modello del provimento consiste nel garantire le classiche proprietà di sicurezza che abbiamo visto e studiato rispetto al training set quindi garantire la confidenzialità del training set e garantirne l'interità in modo tale da di riflesso avere la garanzia che un modello che poi viene utilizzato sia un modello tra virgolette corretto cioè addestrato su dati corretti non alterati di cui deve essere garantita la confidenzialità e quindi la base su questi aspetti non mi darò degli esempi ma comunque se siete interessati ad approfondire il tema dell'utilità e della confidenzialità nell'ambito del vaccino di riferimenti le soluzioni accontate in questo contesto consistono di solito nell'integrale all'interno del processo di addestramento che abbiamo visto prima strumenti retografici ok questo è l'idea sono sempre ricordato dove abbiamo stato nel corso tecnico come monomorfismo che sono alcune tecniche che consentono di produrre a partire dal plaintext che hanno le stesse proprietà di cui analisi fatte su ciphertext rispecchiano i risultati dell'analisi fatte sui corrispondenti plaintext e questo è esattamente quello che vorre in un gruppo che sto succedendo un altro esempio di probabilità che è quello di cui voglio parlare un pochino più detenatamente oggi consiste nell'adversario al machine learning in cosa consiste l'adversario al machine learning è un filore di ricerca della sicurezza dei modelli di apprendimento che è nato una decina di anni fa e che consiste in un'attivazione di interità non tanto del training set che è che è anche un po' meno diverso ma degli input e l'idea qual è? l'idea è quella di obiettivo dell'avversario di intervenire sull'input quindi immaginiamo che il modello è corretto quindi in rispetto al training set in confidenziale di apprendimento garantito è tutto perfetto ok l'avversario è che cosa fa quindi non puoi intervenire sul modello non puoi intervenire sul training set interviene sul input in che maniera interviene su input con delle modifiche delle piccole violazioni di identità nel modello che però seppure piccole sono tali da influenzare drasticamente il risultato fornito dal modello vedremo cosa significa a seconda dei contesti il primo paper di cui vi porgo un esempio che ha messo in evidenza questo tipo di attacco di Seghetti nel 2013 è dal punto di farò vedere che manipolando l'input in qualche modo è possibile produrre una variazione significativa nel comportamento e quindi nella condizione condita dalla ragione vi farò un esempio dei classificatori per produrci sempre a qualcosa di omogeneo di diversi anni vi faccio vedere subito un esempio questo è un andamento nel corso degli anni da quando il primo lavoro di Seghetti c'è stata un'esplosione combinatoria con i scientifici nell'ambito dell'avversario al machine learning questo è l'esempio costruire quello che viene chiamato avversario ad esempio quindi un controesempio di artefatto dall'avversario che in qualche modo ha il la predizione fatta dal nostro modello qui l'esempio è quello della classificazione di mazul quindi non vogliamo avere un moderno machine bear tutto quello che viene usano qualunque tipo di tecnica sono tutte moderabili all'avversario machine tutte quelle conoscente hanno tutto questo tipo di problematiche immaginate di avere un modello destrato per riconoscere l'immagine quindi immaginate di avere questa qui sulla sinistra un immagine non alterata dall'avversario che viene cassificata come parola con una certa accuratezza dal nostro modello del 47% non è non è tanto ma comunque è un valore significativo sulla destra vedete quella che apparentemente è la stessa immagine ma è l'immagine originale qui è stata applicata una maschera e può andare ad alterare leggermente il valore in questo caso di ciascun pixel quindi anche se a occhio nudo voi non lo notate tutti i pixel immaginate sulla destra sono leggermente diversi rispetto a quella originale però la differenza non è percepibile da l'uomo umano e anche dal punto di vista al di là dell'interpretazione umana anche dal punto di vista matematico la differenza tra gli input tra le feature dell'immagine di sinistra e le feature dell'immagine di destra è comunque un'antipariato natural molto piccola ok nonostante il classificatore viene dotto ad una classificazione di abbanda vedete che la classificazione cambia appunto quello che prima veniva classificato come panda con un 57% di conflienza ora viene classificato come una scimmia con un 59% di conflienza ok e così via poi vi potete divertire andare a cercare in letteratura se questo vi ho preso da te per le potete trovare quali voi volete di esempi di questo genere perché anche in questo caso l'automus viene classificato come scorsa dal classificatore ma anche in questo caso la tecnica di di perturba dell'input è la stessa ogni pixel viene eventualmente perturbato ok si può scegliere che cosa vogliamo appare lì c'è la maestria di polo sì ci sono ci sono tecniche che vi farò vedere un esempio permettono di stabilire in cosa volete che venga classificata in un masino originale e di conseguenza intervenire modificandola appositamente per ottenere il risultato quindi vi farò vedere un esempio proprio in questo senso ok ora i primi adversari dell'esempio di consideratura a parte che per parecchio tempo sono stato considerato un fatto curioso di cui non si capivano le motivazioni e le origini ma comunque venivano considerati all'inizio perlomeno degli esempi così artificiose senza alcuna larita pratica che al di là del gioco divertente di ingannare un classificatore modificando un'immagine digitalo però se vogliamo pensare a delle conseguenze pratiche di questo tipo di vulnerabilità anche a me è difficile che ci vengono in mezzo perché voglio dire non è un ascoltato dipende dal contesto ok dipende dal contesto però immaginare il contesto dove abbiamo un modello che ha un'immagine la deve la deve classificare interviene un'immagine che trova la maschera di perturbazione adatta per trasformare quell'immagine in qualcosa che viene classificato diversamente è difficile da pensare che per reale che possa possano migliere col tempo però gli avversari ad esempio sono devoluti e sono diventati qualcosa di pratico ok tant'è adesso vi faccio vedere un esempio che due o tre anni fa si è arrivati a realizzare attacco di questo genere e reali non basati sulla manipolazione di un file di città ok in questo paper di di unico di ricercatori di Sbrigo ha fabbricato fisicamente un paio di occhiali generati generati usando trasformatori quindi sfruttando massimi learning per ingannare tutti massimi learning questa è idea hanno fabbricato fisicamente un paio di occhiali che sono quelli che vede lui è Luce Bauer è uno degli autori del lavoro e quelli sono gli occhiali dei grani che li ha indossati e e l'effetto di questi occhiali indossati è un po' simile all'effetto della perturbazione digitale fatta a livello dei pixel del mouse digitale con una differenza mentre in questi primi esempi l'obiettivo era modificare in maniera impercettibile il valore di ogni feature in questo caso di ogni pixel ok Bauer qua usa un approccio diverso come un contesto reale come cavolo faccio a modificare in maniera impercettibile il valore di ogni feature non è praticabile immaginatevi una situazione in cui c'è l'ingresso cavolo di una banca che è protetto da una telecamera che fa entrare solo le persone che li conoscono di per cui c'è un senderino dietro sotto che fa il conoscimento dei volti c'è solo oramai più del dubbio di dispositivi che funzionano in questo senso qua l'ultimo che l'ho visto la settimana scorsa era un negozio all'interno di uno stadio in America dove entro nel negozio e vengono preconosciuti all'ingresso se sono registrati su un portale dove è registrata l'unità della persona il volo o del numero di carta di pedico loro entro nel negozio sono un telecamera che si conosce prendono quello che vogliono una negozio e poi se ne mandano e passano dalla cassa perché all'uscita la stessa telecamera riprende riprende quello che hanno preso dagli scaffali e il pagamento è buono senza parlare di dispositivi di riconoscimento passato che sono ormai i presenti in tutti gli incroci nel centro di docchio in altre città del medio ambiente insomma sono dispositivi quelli che oramai si utilizzano abbastanza comunemente ora pensare di ingannare uno di quei sistemi con un attacco di questo genere è il pensato perché nel mondo reale che cosa significa applicare un filtro perturbativo di questo genere non è chiaro come implementare una cosa di questo genere mentre invece l'approccio di Bauer è togliale Bauer dice invece di perturbare in maniera impercettibile ogni singola feature andiamo a perturbare in maniera significativa un piccolo insieme edificio perché questa è una pausa su cui possiamo avere il controllo e infatti è l'effetto di quegli occhiali l'effetto di quegli occhiali non è modificare in maniera impercettibile l'intera immagine catturata dalla videocamera ma consiste nell'andare a perturbare in maniera significativa una piccola area di un'immagine di quegli occhi per cui l'idea è che il nostro Bauer senza occhiali viene riconosciuto per quello che è di fronte alla telecamera nel momento in cui indosso gli occhiali viene misclassified e la foto sotto rappresenta la persona che il modello ritiene di avere visto ok questo esempio come ha detto lui hanno detto che dovevano vedere lei nel senso che doveva essere riconosciuta lei no no allora lui si è presentato davanti alla telecamera con gli occhiali quindi in teoria il modello doveva riconoscere lui il modello doveva dire davanti alla porta c'è Luce Bauer Luce Bauer ha autorizzato vedete apriamo la porta la porta si ha ok quindi lui presentandosi i modelli occhiali veniva riconosciuto come non mi ricordo la mia decisione però quindi veniva riconosciuto il server è stato abdestrato al suo interno con un certo numero di intenti di voti tra cui quello di Bauer e quello dell'attrice quindi il modello era stato prestato per riconoscere queste due persone più tutta un'altra serie di persone il problema è che quando lui si è presentato con gli occhiali il modello ha detto questa immagine non è quella di Uso Tauro ma quella di dell'attrice e nell'articolo loro spiegano come hanno creato l'occhialo questo lavoro è interessante proprio perché fa diventare l'interversale l'occhialo qualcosa di praticabile rispetto rendere praticabile un attacco di questo genere non si sa come perché se si fa un contesto reale ad applicare un filtro che va a perturbare in maniera impercettibile tutti quanti pixel del base digitale che non è ancora stata creata invece in questo caso qua diventa praticabile poi vi farò vedere qualche altro esempio lo stesso approccio lo stesso approccio è stato usato nel corso degli anni in numeri di contesti diversi ad esempio questo riguarda la classificazione dei malware mi pare sì l'antico del 2016 sì la classificazione dei malware praticamente in tutti i domini applicativi in cui si utilizzasse un modello produttivo per classificare un input che sia un management che sia un find che sia un anglo un video o un logogeno di input ebbene in quel contesto utilizzando le stesse tecniche le stesse idee del lavoro di slides se vedi è possibile posturire avversari dei zempi qui in questo lavoro gli autori mostrano che un malware che viene riconosciuto come tale da un intrusion prevention system che utilizza strumenti di machine learning che non mai ce ne sono tanti che usano i machine learning per fare la classificazione e la nostra tradizione dei malware quindi un malware che con il 91% di conferenza veniva riconosciuto come tale perturbato cosa vuol dire perturbato se volete vedere il paper cosa fanno gli autori vanno a inserire nel codice del malware dei commenti i punti strategici o aggiungono delle variabili o delle linee di codice che non hanno nessun effetto sull'esecuzione di malware come ad esempio ho messo un scritto che da qualche parte fa passare un decimo di croc senza fare nulla quindi modifiche che a livello sintattico impattano sul file ma che a livello sintattico non influenzano sul risultato dell'esecuzione ebbene con queste perturbazioni vedete che col 100% di confidenza lo stesso file viene classificato come un'emiglia di un malware quindi in questo modo il petrocoso di erci si illustrava che le stesse tecniche che funzionavano con la classificazione degli imbrani in realtà funzionavano anche in tanti altri contesti perché il problema non è il domenico applicativo lo vedremo poi il problema sta nel nel modello predittivo e non sta neanche nel modo in cui viene costruito poi lo capiremo attraverso un'interprecazione di altra di umano e tra l'altro in molti contesti applicativi si utilizzano i classificatori di Amazon per fare non si può dire perché i classificatori di un masino storicamente sono tutti i primi e quindi sono quelli più efficienti sono quelli su cui si è lavorato di più per cui in tanti altri contesti in cui è necessario fare una classificazione piuttosto che costruire un classificatore che lavora sull'input originale si preferisce prendere l'input originale trasformato in un masino e poi adestrare un classificatore per l'inmasino l'isente tipico è sempre quello dei malware uno degli approcci che si trova in letteratura su cui ho anche scritto un articolo l'anno scorso con un vostro collega consiste nel prendere il per caso la la classificazione malware non malware delle app che si prendeva il codice dell'applicazione che non è altro che una lunga stringa di bit e interpretare quella lunga stringa di bit con un'immagine quindi andare a riempire i pixel di un template con valori che sono presi dalla stringa di bit che era presente il codice nella cazza salta fuori lo schema che viene interpretato come un'immagine si addestra un classificatore per immagini usando i risultati di queste conversioni e poi si impara l'addestramento su un training set sappiamo dove sono malevoli e quindi la righe e come stiamo sono tecniche che riescono a classificare il malware per un divenuto di un'immagine che superano tranquillamente il 90% o il 60% però c'è questo problema qua capire i vari modelli che abbiamo usato in questo modo se sono diciamo vulnerabili agli attacchi di questo di questo tipo qua questo è un altro esempio con un altro contesto applicativo quello di fare l'audio in alto vedete la traccia audio e il il il il il vocale originale ok in mezzo vedete la perturbazione che è stata applicata la traccia audio e in basso vedete la nuova traccia che è pressoché identica a quella originale perché comunque la traccia è stata minimale ma vedete anche che il modello di learning come interpreta l'audio rispetto a quello originale cioè travesa completamente il messaggio in cima ok sempre sempre sulla falsa lì anche l'accrocio usato da Bauer ci sono altri tentativi di successo di attacchi di questo genere che è un po' come la storia degli occhiali in Bauer no utilizzando transformer si generano in questo caso degli sticker ok che vengono fisicamente applicati dove volete questa è la foto di un tavolo con una banana sotto un tavolo ok che dal modello addestrato viene riconosciuta come banana attaccando lo sticker sul tavolo diciamo la banana ecco che vedete che il classificatore sbaglia completamente non la riconosce più come banana ma la riconosce come tostapano ok qui vedete che sebrate gli esempi che hanno un altro fatto perché produrre usando un certo alcolismo uno sticker è attaccato in giro dove ad esempio intervengono modelli di questo genere può fare dei danni e questo è un altro esempio lo stesso tipo di attacco è stato realizzato con successo contro la Nova LX della testa che ha un sistema di video automatico molto evoluto basato ad esempio sul riconoscimento dei cartelli stradali per cui se c'è un limite di velocità la la testa che sono iniziati sulla sempre non è non supera quel limite qui è stato fatto lo stesso giocchino lo stesso approccio seguito qui è stato seguito qua questo quello che vedete qui al centro è il cartello stradale modificato è stato attaccato uno sticker che allunga di qualche centimetro la barretta del 3 ok vediamo se riesco a farvi vedere il video un po' allora dobbiamo fare dividerlo un po' tutto diciamo Grazie. Grazie. Il riferimento al cartello, avete visto, hanno piazzato il cartello adulterato, la macchina è partita, il software della macchina ha visto il cartello, però nonostante questo vedete che ha continuato a crescere. E la cosa interessante di questo attacco è che è stato un attacco di black box. Nessuno conosce il modello di apprendimento usato dal classificatore implementato da Alters. È una macchina. Uno sarebbe portato a pensare, ma per costruire un attversario ad esempio, io ho chiuso di vedere come è fatto il classificatore. Non è necessario. Non è necessario perché Alters.ale ad esempio sono abbastanza cross-cutting, nel senso che gli stessi Alters.ale ad esempio tendono a funzionare anche contro modelli diversi. Quindi voi potete costruire, ma non è stato un esempio, ma in un white box attack nei confronti di un modello che conoscete, lo stesso versare ad esempio a volte funziona anche contro modelli diversi che sono delle backbox. Qui la Tesla la risolta dicendo che tutto ha un ruolo di software che diventa la Tesla, la risolta di fatto il giornale, il fix-up, il marketing, il marketing, il marketing, il marketing, il marketing, il marketing. È difficile di fare questo tipo di problematica. Questo tipo di problematica viene risolta in maniera definitiva. Non lo dico io, lo dice la letteratura, adesso vi faccio vedere un paio di esempi, ma è pieno la letteratura di esempi di questo genere. Questo è un paper di qualche anno fa che descrive degli attacchi contro i video, classificazione di video. Queste sono le conclusioni del paper che appunto dice che sono riusciti a implementare l'attacco rispetto a due diversi dataset di video. Uno contenente video di attività course grade, video di attività di l'Università di Lone. Vi fa esempio una persona che si sta truccando rispetto a una che sta giocando a bullying. Ma anche nei confronti di dataset contenenti video con differenze meno sostanziali. Mi pare che l'esempio fosse fatto rispetto a dei video dove c'è la persona che simula, non so se avete presenti i segnali che fanno all'interno negli anni degli aeroporti, ci sono operatori aeroportuali che dicono al pilota dell'aereo che mandano a rifare, non spostarsi, quindi usano delle parete e fanno dei gesti per guidare il pilota. Sono andato a certi video di questo segmento, ma anche in quel caso sono riusciti a implementare l'attacco. In un video dove, non so, veniva fatto il bacio a sinistro, il bacio a destra, veniva fatto un gesto con una mano, un posticulato, comunque, video con differenze minimali. In tutti i casi, con una percentuale di successo solo oltre il 80%, gli attacchi sono riusciti ad andare a un fine, in segura misclassification dell'interpretazione del video. Questa è la conclusione. Non più tardi di un anno dopo, non più tardi di un anno dopo, infatti vedete, dal 2019 al 2019, esce l'altro articolo che dice abbiamo risolto il problema. Dice abbiamo studiato un meccanismo di difesa che si deve essere robusto rispetto agli attacchi discursi nell'articolo precedente. Abbiamo fatto una serie di esperimenti che ci garantiscono l'accuratezza di questi meccanismi di difesa rispetto, abbiamo sottolineato, agli attacchi nordi. Ok? Quindi, il risultato di questo paper è che rispetto agli attacchi già noti, il loro meccanismo di difesa funziona. E rispetto ad versare un esempio non noti, come si comportano? A non lo sappiamo. Non abbiamo una dimostrazione del fatto che il nostro meccanismo di difesa è dimostrabilmente robusto rispetto a ogni tentativo dell'essere. esempio. Quindi, se ragioniamo un po' su questo esempio, in letteratura succede un po' questo. Si trova un attacco, si trova un avversario ad esempio, si studia la contromisura che però finisce rispetto a quello, dopodiché si deve fare un altro e ilude la misura, la contromisura e si va avanti così. Senza soluzione di continuità. A seconda del tipo di adversare un'avversario di design in polla, diversi elementi di difesa possono essere più o meno efficaci. Ok? in qualche misura falliscono dietro. Ci sono le cripte di difesa che sono ristri rispetto a una buona santa di design che sono, ad esempio, di dire, stradillo, ma si possono costruire secondo la stessa modalità e secondo la stessa approccia. Cambia l'approccio e così. Ok? Quindi, proprio, questa è l'idea. Ma come si costruiscono gli avversari, ad esempio, qual è l'obiettivo dell'attaccante e qual è l'obiettivo invece del difessore? Allora, l'obiettivo dell'attaccante è un po' questo qua. No? Abbiamo un input e abbiamo x e abbiamo un modello A. Ok? L'obiettivo dell'attaccante è trovare, se esiste, un altro input x' simile a quello originale di D è una funzione di distanza che stabilisce matematicamente qual è la differenza tra x e x' e astroiano. Quindi, l'obiettivo dell'attaccante è trovare un x' simile a x che però venga classificato dal modello A in una classe diversa rispetto a quella che viene a segnare x. Questo è l'obiettivo dell'attaccante. Ovviamente, l'obiettivo del difessore è quello di rendere robusto A, ma non va a evitare la misclassification rispetto all'inbutta che sono le giardine di diversi tra di loro. E adesso vi farò vedere che è impossibile. Che la misclassification di input di diversi tra di loro non è un problema di solito. quello che si può cercare di fare è rendere difficile per l'attaccante trovare l'x' che induce la misclassification. Ma non possiamo impedire che ci sia. Non possiamo costruire un modello che impedisce che ci sia un x' simile a x che viene classificato diversamente. che è un'attaccante che è un'attaccante che è un'attaccante che è un'attaccante che aveva seguito servirei nel suo lavoro originale era un'approccio abbastanza intuitivo ed era un approccio ortogolare rispetto a quello dell'addestramento. Come funziona l'addestramento che abbiamo l'addestramento viene dato un modello calcola un risultato poi applicando la loss function verifica qual è la differenza cioè il risultato che ha trovato e quello reale e in maniera back-core cosa vado a fare vado ad aggiustare i pesi in una quantità che dipende dall'errore che è stato calcolato famoso credente. Ok? Bene per costruire l'adversario ex-z o l'ex-z gli fa una cosa simile solo che ragione in maniera back-core ovvero abbiamo già i parametri perché il modello è già costruito e io non devo intervenire il training in base all'errore ha giusto i parametri ok? L'avversario invece cosa fa? Ha il modello ha i parametri e allora bisogna prova parli da un punto prova a modificarlo e osserva il risultato della classificazione ok? E a seconda del risultato della classificazione sono ancora nella stessa classe di x ma ancora non sono riuscito e così via cosa fa? Aggiuti in base all'errore che osservano il risultato aggiusta in maniera backward 20 quindi durante il training in base all'errore si aggiustano i parametri chi fa l'attacco ragione in maniera ma in base all'errore che osserva non aggiusta i parametri ma giusta ok? Usando sempre il gradiente come come riferimento ok? Qui in formule vedete un po' quello che succede no? x è l'input p è la perturbazione in maniera tale che sia piccola uno generale di soldi hanno un infilcio di una bonificata di una quantità impercettibile e si va a vedere perturbando l'input quindi si va perché perturbando l'input leggermente si va a vedere che effetto ha la perturbazione sui calcoli che fa il modello ok? sulla base del dato prodotto dal modello e quindi si va di nuovo a applicare la nostra mano su un video dicendo si calca leggeramente e che cosa si fa? si va a modificare come diceva l'addestratore cioè va a prendere l'input che ha usato e aggiusta la perturbazione di una quantità pari alla prelevato su una mente e la mano di espasso quindi vedete che l'approccio di chi è a destra l'approccio di chi è a destra di costruire l'attacco esattamente lo stesso solo che durante l'addestramento l'aggiustamento viene fatto sul parametri del modello nel creare l'attacco l'aggiustamento viene fatto sul valore del input e anche lì si fa per tentativi ma anche durante l'addestramento si fa per tentativi da un input poi guardo l'errore aggiusto da un input da un errore aggiusto e qua succede un altro prendo un input perturbato vedo cosa sta fuori c'è un errore aggiusto aggiusto la perturbazione e vado avanti fino a quando non ottengo una perturbazione che mi fa cambiare classe questo è un po' è l'idea ok ora come dicevo le domande da corso per il risultato sono tante la risposta è quasi sempre la risposta si possono trovare delle soluzioni delle toppe adattati specifici ma la soluzione definitiva è ancora lontana da venire e vi faccio vedere un po' il motivo il motivo è di altura geometrica ci sono i risultati che salgono agli anni 70 che ci dicono formalmente che l'adversario è l'esempio esistono e non solo in lì ok e c'è un lavoro interessante che fa questo parallelismo tra i passificatori che si comportano i risultati della geometria che risalgono sui quant'anni fa queste sono le ipotesi di questo lavoro che può adattare anche in altri contesti quindi immaginiamo di avere input con un feature quindi abbiamo input con un feature m classi che rappresentano i possibili gli output le foto delle persone che devono essere riusciute o gli animali che devono essere riusciuti e così via l'attacco è di tipo white box perché è fatto conoscendo il modello ma ripeto come questi sono trasversali se deve operare anche gli altri modelli e lì e qui l'idea è che abbiamo in input io ho un x che viene classificato in c1 ok adesso ho un'immagine che viene classificata in un altro ok dopodiché io attaccante ho un target il target potrebbe essere un'immagine immagine di uno stupo che viene classificata con uno stupo ok e il quindi questo è l'immagine che voglio perturbare classificatemi in modo un rappresentativo di un'altra classe che è quella che io voglio ottenere perturbando l'output che cos'è dell'algoritmo che vi accenderò è un un punto un possibile input dell'immagine che ha una certa distanza da quella originale quindi è una perturbazione di quella originale la distanza tra l'input originale x e quello z che ho trovato è l'input di k loro usano come funzioni di distanza l'input quindi l'output è un'immagine vicina a quella originale che però appartiene alla stessa classe del target x ok cioè tornando all'esempio di prima ho il masone ho l'immagine che viene classificata come io voglio che venga classificata come struttura quindi a che partengono ad un'altra classe ok quello che l'algoritmo tira fuori è una perturbazione dell'immagine dell'autobus che verrà classificata come struttura questo è il modo in cui funzionano l'angelo come poi loro che loro propongono e come funziona questo sfrutta dei risultati che risalgono ripeto agli anni anni 70 risultati di geometria degli iperpiani ok perché degli iperpiani dipende da quanti sono difficili dipendono scusate dipende da quante sono le classi voi avete immaginiamo diciamo una cosa ma non immaginiamo di avere due facebook ok le possiamo visualizzare come gli assi in x e y no la combinazione degli fici all'individuo dei punti nel piano ora a seconda del input nel secondo di il piano il risultato deve essere una classe c nuova classe c perché la classificazione del binano come nel caso di malware è un malware come un un dal punto di vista geometrico è come se il piano dei punti che descrivono le coordinate fosse tagliato da una retta da qualche parte che divide il piano in due aree per cui tutti i punti che stanno in un'area sono classificati come malware e tutti i punti che stanno in un'altra area sono classificati come bene quindi questo è il caso di l'acqua ora se le feature sono tre la rappresentazione geometrica non è più un piano uno spazio ok se le classi sono due mi basta ancora un piano per tagliare lo spazio in due zone se le feature diventano n non abbiamo più uno spazio per dimensione ma abbiamo uno spazio a n dimensione ok se le classi sono n non abbiamo più semplicemente dei piani che tagliano l'iperspazio non abbiamo degli iperpiani che tagliano l'iperspazio questa che vedete qui è la rappresentazione di piatta perché un iperpiano tagliato da un un iperspazio tagliato da l'iperspazio quindi questa è la rappresentazione di piatta queste rette che vedete sono i tagli che iperpiano hanno tutto spazio e questi tagli udividono delle armi delle zone che rappresentano le classi ok e quindi l'iperspazio n dimensione non è un numero delle feature che è tagliato da un numero di iperpiani che dipende dal numero delle classi per calcolare la classe corrispondente a un set feature in una notazione matriciale avremo l'espressione del tipo n x p x appunto il vettore del vettore del vettore dei vettori di milioni esattamente come si fa per addestramento ok e ogni cella individuata dal taglio di iperpiano di questo iperspazio è un vettore caratteristico che è descritto da quelli che sono i segni i valori che hanno tutti quanti gli input che sono mappati in quella cella tutti gli input mappati in quella cella sono caratterizzati dal fatto di avere lo stesso segno in ogni in ogni in ogni feature c'è un terreno che dice quante sono le cellule il numero di cellule è una formula quindi combinatoria rispetto al numero degli iperpiani ma la cosa interessante di questo studio geometrico è che c'è un georema che ha un effetto immediato sul problema di attraversare l'esempio perché questo teorema che cosa dice questo teorema dice che prendete un iperspazio tagliate da iperpiani ipermio sono iperpassi ok definito dalla matrice di questo tipo mx di b è esattamente quello che succede con l'addestramento di ipermio quindi c'è un'analogia forte questi risultati seguenti secondo me succede a destra lo tra le maschile di ipermio e poi dice teorema prendete una coppia qualunque di celle di aree identificate le varie classi ok ogni celle definita dal vettore dei segni mi dicevo prima e teorema dice come condizione sufficiente per potersi muovere da un qualunque x della cella c1 a un qualche y della cella c2 cambiando al più k coordinante delle varie feature ebbene perché si possa fare questo ci deve essere una combinazione lineante di al più k delle varie feature quali quelle che fanno parte del quelle che rispettano come segni il vettore dei segni del nostro c2 ok c'è teorema adesso questo nella sostanza dice che per potersi muovere da un punto di la cella a un punto di la cella attraverso una sequenza minimale di variazioni quindi attraverso un numero minimale di perturbazioni di x quindi teorema dice bene prefi la cella qualunque da un punto della prima cella voi vi potete spostare a un punto di una altra cella modificando quel punto in maniera in che maniera cambiando al massimo k un valore di k feature purché esiste una combinazione lineare fatta in un certo modo ok e qua sotto vedete quelle che sono le condizioni di esistenza di queste combinazioni lineari a seconda dei valori di k a seconda di quante feature voi volete andare modificando questo è un problema di esistenza quindi questo termine sta dicendo che sotto certe condizioni esiste sempre un modo di perturbare leggermente un valore facendolo diventare un altro a lui vicino che però appartiene ad un'altra classe quale quella che volete voi ok questa è l'idea che è queste condizioni qua la prima dice se k uguale a 1 vuol dire se io voglio cambiare una sola feature ok una condizione sufficiente per poterlo fare è che n sia maggiore di 2 n se invece volete cambiare due feature una condizione sufficiente per poterlo fare è che n sia maggiore della radiosi di 2 alla m fatto n e così via l'ultima ragione dell'inizio cioè se n è più grande di n un certo rapporto allora voi avete la garanzia che esiste se il modo di perturbare un certo x che può parte come ce l'ha facendolo diventare un punto che però appartiene ad un altro e in un altro ok e queste condizioni qua se ci fate caso sono abbastanza quasi sempre spettacolo nell'ambito del machine n che rappresenta il numero d'edificio n il numero di caso il sono quindi sono condizioni che vanno che sempre quindi questo genere che cosa sta dicendo che indipendentemente di fatto che riguarda la geometria che ci sta sotto quindi mi sta dicendo che indipendentemente per cui voi avete costruito un modello in ogni caso vostro modello taglia in base a come voi calcolate questa condizioni in ogni caso non la costruite in altri caso se voi la costruite l'effetto qual è? è fatto quello di tagliare un iperspazio con degli iperpiani che vivono gli spazi in celle indipendentemente dalla tecnica che potete usare il risultato è questo e se il risultato è questo c'è un teorema che dice che preso un punto qualunque sotto certe condizioni che valgono praticamente sempre lo potete modificare in un generale intervenendo su una due o più feature e farlo diventare un punto che però parte è esattamente la definizione dell'versario di esempio solo che è un risultato di 50 anni fa ad un punto di base di datore di sistema questa è una cosa forte ed è questo che fa ritenere che l'inversario di esempio sono cose di inevitabili proprio perché è un problema fondazionale vi faccio vedere beh questo è l'algoritmo ma non ve lo commento vi faccio vedere però il risultato della applicazione dell'algoritmo che ci dimostra che effettivamente nella pratica otteniamo esattamente quello che dice l'algoritmo ad esempio qui l'XX è l'immagine di questa ok le fissure sono i pixel quindi sono tanti non mi ricordo a queste immagini a quanti pixel sono come le classi sono 10 perché sono i numeri da 0 a 9 l'obiettivo del classificatore è prendere l'immagine e classificare il meno numero che è compreso con 0 e 2 ok quindi rientriamo nelle ipotesi del teorema dove l appunto sono i pixel m sono vari 10 non mi ricordo che tipo di modello è stato costruito ne hanno provato diversi e l'idea qual è l'idea è che sfruttando il teorema e l'algoritmo che ci sta sotto il teorema cosa dice dice che scelta una classe il nostro x è 7 appartiene alla classe scelta una qualunque la classe nel vostro ripospazio la classe con 0 il teorema dice che esiste sempre un modo per perturbar l'input c'è un vostro immagino nel 7 perturbarla leggermente fino a far diventare un qualcosa altro che viene classificato nella classe che volete voi e qui ci sono i 10 esempi cioè come devo perturbar il 7 per farlo classificare come fosse un 0 come devo perturbar l'immagine del 7 per farla classificare come un 0 e così via per farvi vedere la perturbazione perché la perturbazione è possibile cambiare il livello di diviso non più di 11 pixel quindi 11 è il valore di k del teorema ok e l'algoritmo che è descritto qui vi costruisce quella famosa combinazione lineare che descrive il teorema quindi con k uguale a 11 l'algoritmo trova la perturbazione di non più di 11 feature per input quindi non più di 11 pixel dell'immagine del 7 i pixel rossi non mi ricordo stanno a significare che sono un po' più scuri rispetto all'originale sono stati perturbati rendendole un po' più scuri i pixel verdi invece sono un po' più chiari rispetto all'originale solo che se vi faccio vedere la perturbazione non la notava quindi vi ha fatto di imparare il possedente quindi perturbando 11 pixel in un certo modo in quale modo sulla base di quella famosa combinazione lineare che descrive il teorema quindi perturbando 11 pixel nell'immagine del 7 voi i brogiati il cittadino il possedificatore è il tour a riconoscere l'immagine come lo 0 o quello di 2 o quello di 2 quindi potete trasformare potete creare un versore un esempio rispetto a qualunque classico ok è chiaro che questo è un po' un toglie di esempio perché si basa su delle ipotesi di partenza che sono queste un attacco whitebox usando la distanza di aiming quindi tutti i modelli hanno queste funzioni qua però l'idea di base è sempre la stessa e l'idea è che fondamentalmente questo tipo di attacchi che mi hanno appurato sono che ci sono delle condizioni di esistenza anche di una ragione di attacchi questo può andare a l'idea quindi diciamo che ultimamente ci siamo asportati l'idea non è più quella di cercare di produrre una tecnica di apprendimento che sia indulnerabile adversare la santa della santa perché pare che questo non sia possibile l'idea è diventata un po' più quella di cercare di costruire sistemi che utilizzano machine learning dove noi la sicurezza dobbiamo garantire il sistema noi diciamo che è il modello vulnerabile quindi non possiamo costruire un modello vulnerabile ma possiamo rendere vulnerabile il sistema che lo so vulnerabile in realtà è da rendere difficoltoso per l'avversario riuscire a implementare l'attacco sul modello ok o altre finali di ricerca invece quando la direzione viene difficile non ci sono grossi di attacchi non rendere difficile per l'avversario rendere difficile dal punto di vista computazionale rendere difficile questo calcolo qui il calcolo della combinazione lineare che mi permette di trasformare x in qualcosa che appartiene alla stessa situazione quindi un grosso risultato sarebbe riuscire a far sì che un avversario efficiente non sia in grado di fare investitato quindi l'avversario ad esempio non c'è ma l'obiettivo diventa rendere complesso trovarlo l'idea poi gli ultimissimi studi che mi è capitato di vedere che sono anche divertenti da questo punto di vista perché sono studi che cercano di verificare se questo tipo di vulnerabilità di cui soffrono i classificatori non è semplicemente di naturale ma è di naturale biologica e quindi c'è il tentativo di verificare se sotto certe condizioni anche l'occhio umano soffre di aggressare la genocchia oppure no quindi immagino che non è un caldo limite perché la perturbazione qui è fuori però all'interno di questo articolo cosa fanno gli autori e tra questi ci sono il paper not good friend no cioè sono tra i crittori più famoso all'anno all'interno di questo paper cosa fanno fanno vedere hanno fatto un esperimento in cui fanno vedere che un essere umano che osserva l'immagine per pochi istanti perché perché pochi istanti perché facendo osservare l'immagine a nessun ma per parecchi secondi subentrano di meccanismi legati all'interpretazione dell'immagine quindi risparlgono fuori la persona risparlgono la crescita del soggetto e quindi subentrano degli aspetti che non hanno nulla a che fare con il di conoscimento visivo ma che fanno entrare in gioco altri meccanismi mentre invece sottoponendo l'immagine per pochi istanti di una persona l'idea quella di vedere come la persona ricapita quella immagine sente il coinvolgimento bias come mi hanno detto nella nostra ricastanza e si è visto che certe perturbazioni che rappresentano la messa dell'esempio per i classificatori hanno sempre avuto qualche affetto anche sul riconoscimento fanno le persone le stesse immagini che fanno risposte per un grande riconoscimento tra i nuovi e i due di un essere umano che può osservare le immagini per una frazione di subito pare che ci sia una con emazione tra l'errore che fa il classificatore e l'errore che fa l'essere umano ed è interessante la conclusione perché è quello che diciamo anche noi poc'anzi cioè se noi arrivassimo a capire che il cervello può essere ingannato da avversario ad esempio quindi questo significa che se ne so come essere umano diciamo che io per cui non devo soffrire la anche il classificatore basato su machine learning e quindi forse vale la pena fare uno shift un focus sulla sicurezza di questi sistemi cercando di progettare modelli passando dal tentativo di progettare modelli robusti ad avversari l'exemple forse è una cosa che si può fare spostandosi verso il tentativo di progettare invece sistemi che siano sicuri e che usano all'interno modelli di machine quindi creare un wrap attorno a questi modelli di sicuri è chiaro che in certi contesti questo fattibile in contesti diciamo real time diventa più complicato riuscire a confinare le vulnerabilità di un modello di riconoscimento è un pochino più complicato ad esempio questa è una grossa sfida per i sistemi intelligenti che si montano in vende auto in autoglu quello classico contesto real time precedenza dove il sistema di riconoscimento deve prendere decisioni un po' distanti ed è costretto a farlo in un ambiente che non è controllato perché la strada di tutti i input ambientali che l'auto prende l'ingresso tipicamente che possono essere fuori contro del sistema non son filtrati qualche motivo quindi forse quello è più complicato dove cercare sede un argomento che può essere più interessante ancora un o cosa qua cioè quando vai in testo, questo è tutto. Anche se adesso si può pensare anche in queste delle storie va bene. Con questo è tutto. Allora, oggi è l'ultima lezione. Quindi, non vedo le lezioni del chat, perché non me la fa vedere? Ah, ok. Quindi, cosa vuol dire? Vuol dire che io che lascio a fare l'esame in una sessione stiva, in uno dei tre appelli della sessione stiva, magari contattatemi prima per concordare insieme a me che tipo di progetto intendo fare, magari vi tenete un po' argomento per non essere sicuri quanto approfondirlo, o se invece è sufficiente. Quindi, magari, ci sentiamo, vi scrivete, o se vi vedete molti e di persona, così sapete già a prima che quella conta fa una cosa che è ragionevole. dopo di che, come si rivolge l'esame, Mario l'ho già detto, mi sono che l'esame è in due parti. La prima parte in cui mi raccontate come se fosse un seminario il tema del vostro del vostro progetto, io apprezzo molto quando gli studenti vengono con una presentazione che è un'opera PowerPoint. Ok? Quindi, ho fatto una prima elezione in cui fate vedere come avete fatto. 15 minuti, una presentazione non più lunga che i 15 minuti. dopodiché invece possiamo volare su quelle parti del programma che non riguardano il tema del progetto. Il programma possono rivolgere le sezioni e il progetto riguarderà una delle macro sezioni, 5 sezioni, quindi se il progetto riguarda una di quelle sezioni lo sapete già che non ha le foto sulle altre. In genere faccio due, tre rimando al massimo. L'altra cosa importante da tenere la mente è che per quanto riguarda le domande che io ho assolutamente siccome ci sono parecchi contenuti molto dettagliari, sono dei scrittori di dettaglio, sono più cose che sono abbastanza complesse. Il mio obiettivo non è che voi imparate le domande queste cose qua. Il mio obiettivo è che voi ve li capite, le sapete presentare, le sapete interpretare nella giusta maniera. di delle volte, per quanto riguarda gli argomenti un pochino più dipendati, quello che faccio è vedere lo slide, una di quelle che ho usato in un'azione, che ne so, può essere una slide che spiega lo schema di firma digitale di Fiat e Cianida. a me non interessa, se io vi chiedo lo schema di firma digitale, a me non interessa non ciò, perché quindi scrive un po' di classico, però io potrei farmi la slide che presenta lo schema e vi potrei chiedere di spiega come mi è che mi sono in un po' di invece per le parti più discorsibili e quelle che ci mi chiedo cosa è uno schema cosa è un'altra slide per il CIP al sicuro e siccome la cosa ripetere di senza poco invece voglio sapere come funziona quando c'è un tipo particolare un protocollo del pentaglio che lo faccio vedere e voi me lo raccontate come se ve lo doveste spiegare se lo conosco io e io valuterò la nostra capacità di aver capito largamente di sapere questo in cui funziona in cui funziona un orale che è un progetto un orale o le domande da durare un o di un o di un o di progetto è un progetto che noi diciamo o creiamo un progetto come un time game e allora come all'inizio del corso avete voi potete scegliere tra un progetto pratico per un esempio di un testo o anche la verifica dei protocolli se non mi piace con la verifica formale dei protocolli prende uno dei tour di cui ho parlato ieri ce ne sono semplici per suggerire io volete un protocollo e vedete il progetto di giocato quindi cosa riesce a un concetto pratico oppure una tesina teolica una piccola tesi di una decina di pagine anche perché se volete fare una presentazione di 15 minuti non è che per interlocutare che sa di cosa non c'erano le decilie nessun quattro di ora quindi è come una discussione della tese di l'anami quindi non più di una decina di raggiuntare in quel caso ho sceglito il tema di quelli che abbiamo toccato solo marginalmente e l'ho confondito non so che è piaciuto il tema relativo al città mi fate la tesina su come funziona a senza salle o non questo è il prego sempre venire in questo che vuole che viene ad esempio di come funziona il micro sa la seconda come funziona l'nfc il contact l'espegment di adesso so ti ho siete автомобilide la scrittografia. Però non ho idea di cosa provare, non dite, ma adesso io propongo di, che ne so, a fare una tesina su uno schema di sembratura quanti, ok? Una cosa che mi ho comunque una variante di quelli che hanno la scollezione, perché non sia la stessa cosa che la scollezione, ci deve essere un'operteguimento. Anche se non lo dovete vedere in cari, l'eterno è, individuiamo l'area e poi propongono te, non lo vediamo. Ci sono le due, non ho la principale per fare un progetto. Ok. Ancora un po'.