Allora, oggi pomeriggio vediamo un po' come funzionano i protocolli di identificazione. Quindi sulla base di quello che abbiamo detto stamattina, nei protocolli di identificazione abbiamo un prover e un verifier. Il complico del prover è dimostrare la propria identità al verifier. Il prover ha a sua disposizione quella che chiameremo chiave segreta e il verifier ha a sua disposizione quella che chiameremo chiave di verifica. La prima serve al prover per dimostrare la propria identità al verifier che farà la verifica usando la verification key. Le applicazioni che sono tante, ne abbiamo menzionate alcune stamattina, così come sono tanti gli aspetti da prendere in considerazione rispetto a diversi criteri di classificazione. I protocolli che vedremo rientrano in diverse categorie, quindi con chiave di verifica che può essere pubblica o segreta, handshaking, stateless o stateful, identificazione one side o mutua. I protocolli che vedremo sono robusti rispetto a diverse categorie di attaccanti, lavoreremo in maniera incrementale, quindi partiremo dagli attacchi più semplici, evolvendo verso attacchi più complessi, partendo chiaramente dal caso degli attacchi diretti, quelli in cui l'avversario prova direttamente a interagire con il verifier. Poi passeremo agli attacchi di tipo voice dropping, dove l'avversario prima di interagire con il verifier ascolta il risultato di sessioni precedenti, e infine gli attacchi attivi, dove l'avversario può impersonare sia il verifier che il prover. Non prenderemo per il momento in considerazione gli attacchi tipo man in the middle. Formalmente uno schema di identificazione, un protocollo di identificazione, non si definisce nella maniera che vediamo qua. Abbiamo tre algoritmi che costituiscono il protocollo. Il primo algoritmo serve per generare le chiavi, quella segreta e quella di verifica. L'algoritmo, quello che qui viene chiamato G, è un algoritmo probabilistico, perché naturalmente la scelta delle chiavi, della coppia, è randomica. assumeremo che la chiave segreta viene data al prover, la chiave di verifica viene data al verifier. Quindi SK è la chiave che usa il prover, VK è la chiave che usa il verifier. Poi abbiamo il protocollo che si divide in due algoritmi, quello seguito dal prover e quello seguito dal verifier. Ok? Quindi P qui indica il protocollo seguito dal prover, che come input utilizza la secret key. Viceversa abbiamo l'altro algoritmo, che è quello seguito dal verifier, che si chiama V, ed è un algoritmo che in input invece prende la verification key. Ok? Poi, essendo un protocollo, prevederà un handshake, quindi uno scambio di messaggi, tra prover e verifier, ma questo dipende naturalmente dal protocollo specifico. Questa diciamo che è una descrizione standard, ad alto livello, di quelle che devono essere le caratteristiche generali di un protocollo di identificazione. Al solito abbiamo la proprietà di correttezza, che dice che qualunque sia la coppia di chiavi, che è stata scelta randomicamente dall'algoritmo di generazione. Se P utilizza in input la chiave segreta e esegue correttamente il proprio algoritmo, e dall'altra parte V usa come input la chiave di verifica e esegue la sua parte dell'algoritmo, allora il risultato finale dell'esecuzione del protocollo sarà che con probabilità 1 il verifier accetta l'identificazione del prover. Ok? Chiaro che se il prover dovesse usare una chiave diversa, il verifier rifiuterà la prova, sempre con probabilità 1. L'idea è sempre questa. Quindi questo è lo schema generale. Quindi ora andremo a vedere alcuni esempi di protocolli che seguono questo schema qua. Partiamo da quello più semplice che possiamo pensare, e che è un classico protocollo di identificazione basato su password. Quindi nella sua versione più semplice in assoluto. Quindi uno schema in cui l'algoritmo G non deve far altro che scegliere randomicamente una password. La password rappresenta la secret key, mentre il digest della password, quindi il risultato dell'applicazione di una funzione hash, rappresenterà la verification key. Ok? Il prover ha la secret key, il verifier ha la verification key. Ok? Quindi questa è la situazione iniziale. Dopodiché, a ogni istanza del protocollo, quindi ogni qualvolta il prover si deve identificare col verifier, quello che deve fare semplicemente è inviare la password al verifier. Ok? Quindi estremamente banale come protocollo. Ok? Chiaramente il verifier accetterà l'identificazione se l'hash della password che riceve dal prover coincide con la verification key. Ok? Ok? Motivo per cui la verification key non è direttamente la password, ma è il suo digest, non è teorico, la motivazione non è teorica, ma di natura pratica, perché questo l'abbiamo già detto anche in altre occasioni. immaginate il verifier che abbia un repository di tutte le password. Se questo repository dovesse essere violato, tutte le password verrebbero esposte in chiaro. Questo tipo di situazione viene evitata se invece delle password appunto il verifier memorizza i digest delle password. Ok? Questo protocollo è talmente semplice da essere, come vedremo, robusto solo rispetto a attacchi diretti. E ora lo andiamo a dimostrare definendo prima l'attack game, poi la nostrazione di sicurezza rispetto ad attacchi diretti e finire con un teorema che ci dirà che questo protocollo di identificazione è sicuro rispetto agli attacchi diretti. L'attack game è molto semplice. L'avversario, come vedete, conosce solo la verification key. Ok? No, scusate. Ho detto, ma cavolata. Il challenger esegue l'algoritmo di generazione delle due chiavi, la chiave segreta e la chiave di verifica. La chiave segreta è nota al prover, la chiave di verifica è nota al verifier, l'avversario non sa nulla. Ok? L'idea è che l'avversario, senza osservare precedenti sessioni, l'esecuzione del protocollo, perché l'attacco è di tipo diretto, quindi l'avversario non è che ascolta l'esecuzione del protocollo, quindi l'avversario prova direttamente a interagire con il verifier. Ok? Quindi il suo scopo è quello di provare a convincere il verifier di conoscere la password fondamentalmente. Ok? Questa è l'idea. Il vantaggio che l'avversario ha è pari alla probabilità che il verifier accetti la sua identificazione. Però, ripeto, dato che l'avversario non ha modo di ascoltare precedenti esecuzioni del protocollo, non ha idea di quale possa essere la password, e quindi l'unica cosa che può fare, verosimilmente, è tentare un attacco a forza brutta, di cui parleremo poi tra poco rispetto alla sua efficacia. Quindi, non avendo informazioni a propria disposizione, la probabilità che l'avversario ha di vincere questo tag game, di fatto consiste nell'indovinare, pare la probabilità di indovinare la password. Questa è l'idea. Il protocollo si dice essere sicuro rispetto ad un attacco diretto di questo genere, se il vantaggio dell'avversario in questo tag game è negligible per tutti gli avversari efficienti. Quindi, ripeto, un tag game in cui l'avversario non sa nulla, non conosce nulla, l'unica cosa che può fare è interagire direttamente con il verifier tentando di convincerlo. Ma l'interazione con il verifier può seguire solo ed esclusivamente il protocollo. E il protocollo che cosa dice? Il protocollo dice che il verifier attende di ricevere la password dal prover. Quindi l'unica cosa che può fare l'attaccante è provare a indovinare la password, inviandola al verifier nella speranza che questo lo accetti. Questa un po' è l'idea. Il teorema dice che il protocollo di identificazione è sicuro contro questo tipo d'attacco, assumendo che la funzione hash sia non invertibile. Per il solito discorso di prima, poniamo il caso che l'attaccante acceda al repository del verifier e quindi abbia accesso alla verification key, che è un digest. Chiaramente se la funzione hash che è stata utilizzata è one way, non invertibile, non c'è nessun modo dal digest di risalire alla password. E quindi l'avversario non avrebbe modo di indovinare la password anche conoscendo il suo digest. Questo un po' è l'idea. Ripeto, è una versione preliminare abbastanza banale perché, come poi vedremo, se invece di un attacco diretto prendiamo in considerazione un attacco di tipo reverse dropping, il protocollo viene subito violato. Perché in un attacco di tipo reverse dropping l'avversario che cosa fa? Ascolta una precedente sessione tra prover e verifier, durante la quale il prover che cosa fa? Il prover manda la password al verifier. Quindi a quel punto l'avversario non deve fare altro che sniffare il canale, intercettare, ascoltare la password e poi riutilizzarla. Chiaramente supponendo che la trasmissione della password venga in chiaro su un canale non protetto, poi chiaramente vedremo come evitare questo tipo di situazione utilizzando ad esempio una VPN. Ma prima di passare agli attacchi tipo reverse dropping, facciamo una piccola analisi sull'efficacia pratica degli attacchi di tipo diretto. Ok? Quindi in uno scenario reale immaginiamo di avere a che fare con un attaccante che voglia indovinare la password. Ok? Un attaccante che non ha modo di ascoltare il canale e quindi di intercettare comunicazioni tra il prover e il verifier. Quindi l'unica cosa che può fare è tentare appunto di indovinare la password. Ad esempio adottando un attacco a forza bruta. Ok? Come poi succede nella pratica. Gli attacchi a forza bruta sono attacchi offline perché non hanno necessità di interazioni dirette o esecuzioni del protocollo se assumiamo che l'attaccante abbia accesso alle verification key. Ok? Facciamo questa ipotesi. L'attaccante ha accesso al repository del verifier e quindi sniffa le verification key. Ok? Quindi per indovinare a quel punto, per indovinare la password di un certo prover, non deve fare altro che provare con un attacco a forza bruta se riesce a trovare una password in cui digest corrisponde con quello memorizzato nel repository del verifier. Ma per fare questa cosa non ha bisogno di interagire col verifier. Quindi è un attacco offline che non richiede nessun handshake. E l'attacco banalmente è rappresentato da quel pseudocodice che si basa sull'utilizzo di un dizionario. Gli avversari dal suo punto di vista di solito cosa fanno? Usano dei dizionari di possibili password. Ok? E una a una le provano tutte. Quindi per ogni parola, per ogni password candidata del dizionario, l'avversario calcola il digest della parola e verifica se coincide con una delle verification key che è riuscito a sniffare dal repository del verifier. Ok? Se la password è compresa nel dizionario usato dall'attaccante, questa verrà provata. Questo è l'idea. Ok? Addirittura gli attacchi offline basati sul dizionario si possono diciamo precostruire, nel senso che prima ancora che l'avversario abbia accesso al repository del verifier, può crearsi a priori una tabella di look-up che associa ogni possibile password di un dizionario di riferimento al suo valore hash. Questo li posso calcolare a priori. Dopodiché quando vado a fare l'attacco vero e proprio sul repository del verifier, non devo fare altro che un banale confronto tra i valori che ho precalcolato e quelli presenti nel repository. Quindi il fatto che ci possa essere una fase di preprocessing rende questo tipo di attacchi ancora più pericolosi da questo punto di vista. È chiaro che la robustezza della password sta tutta nel fatto di non essere presente nel dizionario di riferimento che utilizza l'attaccante. Ok? Al giorno d'oggi password di lunghezza fino anche a 12 passacaratteri costruite solo ed esclusivamente usando lettere dell'alfabeto, ok? Senza caratteri speciali, bene, fanno parte di dizionari che gli attaccanti riescono a esplorare in tempo polinomiale. Ok? Tenendo conto del fatto che, ripeto, nei casi più comuni è stato fatto per processing e quindi spesso e volentieri non... Il cerchio consiste banalmente nel fare un confronto tra valori che sono già stati calcolati e le verification key dei repository del verifier. Quindi quanto più la password è debole, tanto più è probabile che faccia parte di dizionari su cui è già stato fatto preprocessing. Ok? Oramai, ripeto, su password di lunghezza sotto i 12 caratteri fatte solo ed esclusivamente di simboli dell'alfabeto sono già disponibili tabelle di preprocessing che mappano queste potenziali password nei valori hash corrispondenti rispetto a diverse tipologie di funzioni hash. tutta la famiglia SHA piuttosto che altro. Quindi traccare password deboli è qualcosa di estremamente rapido se la password è effettivamente debole. Questo è un motivo per cui la lunghezza delle password diventa sempre maggiore così come si amplia sempre di più la disponibilità di simboli da utilizzare nella scelta delle password così come si complicano sempre di più anche le regole di combinazione di questi simboli. Quindi non solo lettere, maiuscole, minuscole ma anche numeri, caratteri speciali e così via. il motivo di queste complicazioni nelle regole di definizione delle password sono questi cioè legati al fatto che per situazioni banali esistono già i dizionari che mappano stringhe di caratteri in valori hash corrispondenti. al di là del fatto che la password non sia sempre se da indovinare adesso vi faccio vedere anche un esempio delle password cioè più di diventare una frase tipo oggi sono stato sì sì sì sì sì sì è una frase che non dovrebbe essere molto più sicura eh ma il motivo è quello il motivo è ampliare lo spazio delle password candidati quindi fare riferimento a dizionari sempre più sempre più ampi no non lo vogliamo tradurre questo è un sito che viene aggiornato annualmente e che contiene la classifica delle password più comuni utilizzate sul web come fanno a saperlo? usano diverse fonti soprattutto di intelligence soprattutto legate a episodi in cui le password vengono crackate vengono pubblicate ogni qualvolta accade questo diciamo che loro raccolgono tutte le informazioni nella tira le password che sono crackate e stilano la classifica no? c'è da pochi anni ci sono da sei anni questa cosa qui vedi la metodologia quando fanno ricerche analizzano database che provengono da diverse fonti incluso dark web relativi ad attacchi documentati che sono stati effettuati con successo quindi non è che vanno a caccare il password per vedere quali sono ok? e ha diffuso un po' in tutto il mondo e la classifica che che poi stilano è aggiornata annualmente e qui vedete no? ad esempio il fatto che hanno documentato 3 milioni di password della password 123456 ok? un milione e sei della password 123456789 e così via al quarto posto c'è la prima password non numerica che è password ok? poi querte 123 e così via trovate tutte le varianti con il conteggio di quante e se ci pensate sommandole tutte saltano fuori milioni di password in giro per il web banali e qui vedete riportato il tempo necessario per un avversario per caccare quella password no? sono tutte sotto un secondo ok? ma ripeto perché questo? il motivo poi a un certo punto poi aumenta no? più o meno della trentesima in avanti ci vogliono ci vogliono secondi o minuti o qualcosa di più per per per craccarle ma in ogni caso non ci si pensa però anche per craccare password apparentemente non banali ci vuole comunque un tempo assai ragionevole In tre ore si cracca la trentunesima password per frequenza che apparentemente sembra randomica, anche se è stata trovata 90.000 volte. 1G, 2W, 3, 4R, che se guardate la disposizione di quelle lettere, di quei numeri sulla tastiera, è abbastanza facile da riprodurre, il motivo per cui è così frequente probabilmente è proprio quello. Digitando i caratteri della tastiera in un certo ordine vi saltano fuori queste password qua. E il motivo per cui è così facile craccare password anche apparentemente complesse, ripeto, deriva dal fatto che esistono i dizionari su cui gli attaccanti hanno fatto pre-processing. Sì, chiedo scusa a Giorgio, mi sono dimenticato di condividere, lo faccio subito, di condividere la pagina web. Allora, andiamo un attimo. Cambiamo, muoto di visione. Ok. Grazie a Giorgio di avermelo segnalato. Il sito che volevo condividere era questo, che è un repository che classifica le password più comunemente craccate sul web. E la classifica di cui parlavo prima, ma che vedi solo adesso, è questa qua. Ok, quindi password relativamente semplici, sono craccate in meno di un secondo. Nonostante questo, in giro per il web ce ne sono milioni, ma anche adottando password via via leggermente più complesse, comunque i tempi per l'avversario rimangono più che ragionevoli. siamo arrivati qua a commentare la trentunesima, che richiede tre ore. Qui possiamo andare avanti, fino ad arrivare a vedere tutte 200, ma vedete che i tempi di esecuzione sono comunque sempre sotto le tre ore. Non si capisce perché Justin si è usato 27.000 volte, ci sono tante persone che si chiamano così, però infatti vedete che spesso Minecraft è piuttosto interessante come scelta, anche Jordan 23 e così via. Vi potete divertire a guardare quanto tempo ci vuole a craccare, ma ripeto, i tempi di esecuzione sono grossomodo gli stessi per password di lunghezza attorno ai 6-8 caratteri, senza utilizzare caratteri speciali, punteggiatura o elementi che amplino il dizionario. Torniamo alle slide. Il motivo per cui i tempi sono così rapidi, ripeto, dipendono dal fatto che l'attacco al dizionario è innanzitutto offline, non richiede nessuna interazione con il verify, quindi gli attaccanti possono eseguirlo quando vogliono, con tutto il tempo a disposizione, e soprattutto possono fare pre-processing, ovvero possono calcolare l'hash, gli dizionari scelti a priori, e quindi questo fa sì che i possibili dizionari si possano ampliare col tempo sempre più, e quindi questo significa che al tempo stesso bisogna periodicamente complicare le modalità di scelta e soprattutto la lunghezza della password. Ci sono dei metodi per cercare di ridurre la probabilità di successo dell'attaccante nell'eseguire attacchi di scenario offline, ve ne faccio vedere un paio. Uno di questi, abbastanza basico, consiste nel complicare leggermente la verification key. Ok? La verification key, vedete, non è semplicemente il digest della password, ma è il digest della password combinata con un'informazione di natura randomica, che in gergo viene chiamata sale. Questo salt è un valore randomico che viene scelto per, diciamo, introdurre entropia, e quindi determinare una verification key che non dipende esclusivamente dalla password. L'idea è che, anche se io uso una password debole, siccome poi questa viene combinata con il sale, ecco che il digest diventa imprevedibile e non più associabile direttamente alla password debole. Questa è un po' l'idea. Ok? Il protocollo rimane lo stesso, vedete, il prover in ogni caso deve inviare la password al verifier, il quale combina la password con il sale e calcola l'hash di questo nuovo valore e lo confronta con la verification key. Ok? Naturalmente il sale deve essere noto al verifier, che altrimenti non potrebbe fare questo tipo di calcolo, e quindi di solito viene pubblicato nello stesso repository che contiene il digest, quindi che contiene il risultato dell'applicazione della forza Nash. Se date un'occhiata alla tabella degli account degli utenti di MySQL, ovviamente se avete i privilegi di accesso dell'account amministratore di MySQL, c'è una tabella che contiene i dati personali degli utenti, compreso il valore hash della password e compreso il sale, in certi casi, da dove viene utilizzato. L'obiettivo del sale è di fatto rendere vano il lavoro di preprocessing che l'avversario fa sul dizionario, perché l'idea che abbiamo detto prima è che il preprocessing aiuta l'avversario perché prima ancora di interagire con il verifier, l'avversario per i fatti suoi, si può costruire sulla base del dizionario che ha scelto tutti quanti i valori hash e quindi procedere direttamente col confronto con i valori che riesce a sniffare dai repository del verifier. Ma questo funziona, è ragionevole, se non si fa uso del sale. È inutile fare preprocessing se si fa uso del sale perché il digest in quel caso non dipende più solo dalla password, ma dipende anche dalla sala. Quindi è inutile che l'avversario faccia preprocessing per calcolare il digest di tutte le password possibili. Quindi, ripeto, l'uso del sale rende vano il preprocessing negli attacchi offline al dizionario delle password. Non risolve tutti i problemi perché comunque, in ogni caso, l'attacco, nel momento in cui l'avversario riesce a sniffare i repository del verifier, si può condurre lo stesso. Nel momento in cui l'attaccante ha accesso al repository del verifier, dove ci sono le verification key, li troverà sia il digest che il sale. Allora, a quel punto, si metterà a fare tutti i calcoli sulla base del dizionario scelto. Però, chiaramente, non ha più a disposizione il tempo di preprocessing. Per questo motivo, un'estensione del metodo basato sul sale introduce un ulteriore elemento che viene chiamato pepe. Anche il pepe, come il sale, è un valore randomico e viene usato nello stesso modo. Cioè, viene combinato con la password e anche col sale, se c'è pure questo, prima di calcolare il digest. Ok? Infatti, vedete che la verification key che cosa comprende? Comprende l'informazione relativa al sale e comprende il digest di password sale pepe. Ok? Notate una cosa, il pepe non viene salvato da nessuna parte. Ok? Quando viene creata la password per l'utente, si sceglie un pepe, lo si usa per calcolare il digest e poi lo si cancella. Ok? Quindi, nemmeno il verifier conosce più il pepe, il valore del pepe. Questo cosa significa? Significa che quando l'approver si vuole autenticare e quindi invia la password al verifier, il verifier che cosa deve fare? Per verificare la password deve calcolare il digest di password sale e pepe per poter fare il confronto con il valore memorizzato nel proprio repository. Ora, la password l'ha appena ricevuta dal prover. Il sale è pubblicato nel repository, manca il pepe. E quindi, come fa un verifier? L'unica cosa che può fare è provare tutti i valori possibili per il pepe. Di solito il pepe ha un range, viene preso da un range di valori sufficientemente piccolo da consentire al verifier in tempo polinomiale di provarli tutti. Ok? Tanto per rendere le cose pratiche, tipicamente il pepe è un valore a 12 bit. Ok? In tempo polinomiale, in tempo polinomiale significa in pochi secondi, perché non è che possiamo fare aspettare il prover chissà quanto tempo. In pochi secondi il prover, il verifier, scusate, può provare tutti i pepe possibili, fino a trovare se c'è quello che è corretto usare nel calcolare il digest di password e sale e pepe. Ok? Perché è questa cosa? Che di fatto rallenta il verifier, no? Perché, però non solo rallenta il verifier, ma rallenta anche l'avversario. Ora, un avversario che non conosce la verification key, perché non ha ancora avuto accesso al repository, nella pratica non ha alcuna possibilità, anche provando tutte le password deboli, di indovinare allo stesso tempo sia il sale che il pepe e quindi, perché dovrebbe fare un calcolo che è più che esponenziale, perché per ogni sale possibile dovrebbe provare tutti i pepe possibili. No, questo è praticamente impossibile. Quindi, l'avversario non ha alcuna possibilità da questo punto di vista e le sue possibilità si riducono molto anche se l'avversario riesce ad accedere ai repository del verifier, perché è vero che imparerebbe il valore del sale, ma non troverebbe il valore del pepe. quindi, di nuovo, cosa dovrebbe fare l'avversario per implementare il dictionary attack rispetto ai digest che ha trovato nel repository del verifier? Dovrebbe provare tutte le password del dizionario, ma per ciascuna di queste dovrebbe provare tutti i pepe possibili, il che, di nuovo, renderebbe più che superpoli la quantità di stringhe da provare e quindi l'attacco di fatto non avrebbe più effetto dal punto di vista pratico. Ok? Quindi, introdurre un pepe che poi viene cancellato di fatto fa sì che l'attacco offline nel dizionario diventa impraticabile anche qualora la password sia debole. Naturalmente l'utente non ha nessuna conoscenza né del sale né del pepe. L'utente conosce solo la password. L'informazione sul sale è nota e gestita solo al verifier, quella del pepe è nota solo in fase di creazione della password poi viene addirittura cancellata, quindi sono tutte elementi aggiunti che non hanno alcun impatto sul non hanno alcun impatto pratico sul prover. Ok? Quindi queste sono tutte soluzioni che si possono adottare per irrobustire il protocollo banale che abbiamo visto rispetto a attacchi diretti. Ok? Ci tengo a sottolineare che sebbene il protocollo che abbiamo visto sia estremamente banale rappresenta nella realtà una delle soluzioni più utilizzate in assoluto di fatto con qualche accortezza che riguarda il canale su cui viene trasmessa la password. Però per il resto il protocollo è quello e il motivo è che è venuta alla sua semplicità naturalmente. Quindi queste sono le tre versioni del primo protocollo di identificazione. Di solito le funzioni che si utilizzano sono quelle della famiglia SHA SHA 2 o SHA 3 SHA 1 non si usa più anche perché è stata attaccata già più di 10 anni fa da questo punto di vista. Ci sono tanti esempi di attacchi al dizionario che hanno avuto successo dopo che gli attaccanti hanno avuto accesso ai repository dei verifier sono un sacco di più semplici di quanto possa sembrare riuscire a penetrare un sistema e fare informazioni relative a questi repository. Più o meno tutte le multinazionali sono state vittime di attacchi di questo genere. Quindi ho riportato uno abbastanza eclatante come dimensioni perché il successo LinkedIn più di 10 anni fa ha perso le credenziali di 15 milioni di utenti e quasi tutte le password sono state recuperate. Prima sono stati recuperati. Prima sono stati rifatti i digest delle password che non avevano sale. Adesso oramai il sale lo usano tutto. E poi con l'attacchio al dizionario la maggior parte delle password sono state recuperate. Ci sono altri modi insieme all'uso del sale delle pepe per cercare di rallentare l'attacco al dizionario offline condotto dall'avversario. Uno di questi consiste nel rallentare l'esecuzione della tempo di esecuzione della funzione hash. In molti casi ad esempio la verification key non è direttamente l'hash della password ma è l'hash dell'hash della password. Quindi si appena due volte la funzione hash. Ad esempio immesso quello si fa così. SHA 256 si applica due volte ma perché si applica due volte? Non per aumentare il livello di sicurezza del digest che è sempre quello. Se tu la applichi una volta due o tre il digest è comunque scorrelato rispetto alla password la non invertibilità è comunque garantita. L'obiettivo vero è quello di aumentare i tempi di esecuzione. Aumentare i tempi di esecuzione rallenta appunto l'avversario perché rende più complesso il compito dell'attacco basato sulla ricerca forza brutta. Dopodiché ci sono altre varianti il problema delle password è un problema annoso perché è uno perché la scelta della password dipende dall'utente umano e l'utente umano è sempre la vera deboli della catena. L'utente umano se può non cambia mai la password perché dimenticarle ricordare una nuova è uno sforzo e quindi bisogna cercare di tentativi che si sono fatte nelle cose dell'anno sono sempre stati mirati a se è possibile escludere la responsabilità dell'utente attraverso approcci alternativi o in altri casi forzare obbligare l'utente ad adottare delle strategie che comunque lo riparassero da questo tipo di attacchi quindi obbligare un rinnovo a frequenza delle password allungarne la lunghezza introdurre delle regole sulla tipologia e la combinazione di caratteri che si devono usare aggiungendo doppio tripli fattori di autenticazione quindi uno basato magari il primo basato su password il secondo basato su fattori biometrici come impronta o cose di questo genere e via dicendo fattori basati su one time password e cose di questo genere qualcuno di questi lo vedremo nell'evoluzione dei protocolli di identificazione nessuna di queste soluzioni è quella definitiva perché poi problematiche vulnerabilità emergono di continuo quindi diciamo che i protocolli di identificazione rappresentano tuttora uno degli annali deboli di tutta la kill chain ok uno degli annali più deboli di tutta la kill chain ora come vi ho appena detto il protocollo che vi ho appena descritto è robusto rispetto a attacchi diretti ma non può essere robusto rispetto ad attacchi di tipo ives dropping il motivo abbiamo detto è assolutamente banale un attaccante che ha la possibilità di ascoltare una precedente sessione del protocollo tra un certo prover e il verifier banalmente sniffa la password che il prover trasmette al verifier e poi la rintilizza l'attac game su cui si basa questo comportamento lo vedete qui è abbastanza semplice l'unica differenza rispetto a prima è che l'avversario prima di interagire col verifier ascolta un certo numero di sessioni legittime tra prover e verifier ok dopodiché prima o poi l'avversario tenterà di impersonare il prover in una sessione avviata col verifier legittimo e chiaramente vince se la verifier accetta la sua identificazione il vantaggio dell'avversario si definisce nel solito modo così come il non non di sicurezza rispetto a questo tipo di attacco ok in particolare si dice si dice essere sicuro se non si fanno assunzioni particolari sulla chiave di verifica si dice invece debolmente sicuro se si impone che la chiave di verifica debba essere mantenuta segreta ok quindi sicuro se la chiave di verifica può anche essere pubblica debolmente sicuro se la chiave di verifica deve essere segreta ripeto rispetto a questo tag game è chiaro che il protocollo che abbiamo visto è insicuro basta una sessione osservata dall'avversario per sniffare la password e riutilizzarla subito dopo per identificarsi con il verifier al posto del prover legittimo ok quindi bisogna modificare il il protocollo di base che abbiamo visto ok allora qual è la debolezza nel protocollo di base che abbiamo visto al di là del fatto che la password viene trasmessa in chiaro però vorrei che fosse importante notare che la robustezza del protocollo non può dipendere dalla dalla protezione del canale mi spiego anche se voi prendete il protocollo di prima e lo realizzate su una VPN ok e quindi anche se il prover trasmette la password sul verifier questa viaggia criptata no perché la cosa è una VPN questo non mette a riparo il sistema da potenziali attacchi più sofisticati da parte di un attaccante che magari riesce a intercettare il pacchetto che contiene la password non è così difficile questo e magari di utilizzarlo quindi ci sono ci sono tipologie di attacco di cui parleremo che rendono complesso lo scenario per cui non si può dire semplicemente va bene apre una VPN e trasmetto la password in chiaro di sopra bisogna stare attenti questo lo vedremo però l'altro elemento di debolezza di questo tipo di protocollo è che la password è sempre la stessa quindi la uso una volta l'avversario la sniffa l'avversario la riutilizza e vince la tag game il motivo è che per ogni sessione del protocollo la password non cambia mai ok quindi una soluzione consiste nel cambiare la password a ogni sessione ok ogni volta che eseguite il protocollo viene usata una secret key diversa in questo modo anche se l'avversario sniffa sessioni del protocollo le informazioni che recupera non gli serviranno nulla per una futura sessione perché perché la password tanto sarà diversa e quindi osservare le vecchie sessioni non gli da nessuna informazione utile per per violare il il protocollo di identificazione uno dei protocolli che si basa su questa idea è il cosiddetto hash based one time password one time perché ogni password viene usato una volta sola hash based perché come vedremo si basa sull'uso di di funzioni randomiche per determinare il valore della password un particolare di funzioni hash la definizione di questo schema generale poi le implementazioni dipendono appunto dalle primitive iptografiche che si usano si basa sull'uso di un prf quindi di una pseudo random function l'idea qual è? allora l'idea è che come al solito l'algoritmo di generazione della coppia di chiavi sceglie in maniera randomica una coppia che però vedete è un po' più sofisticata rispetto a prima allora la la chiave segreta allora l'algoritmo l'algoritmo che cosa fa? sceglie randomicamente una chiave quindi k è una chiave random ed è una chiave che fa parte del dominio delle chiavi del prf ok quindi l'algoritmo g sceglie una chiave k randomica e inizializza la chiave segreta la secret key in questa maniera quindi la secret key inizialmente è la coppia k 0 0 come vedremo è il valore di un contatore che è appunto inizializzato a 0 la chiave di verifica è vedete uguale alla secret key quindi verification key e secret key coincidono questo significa tra le altre cose che questo schema può essere solo debolmente sicuro perché se la verification key è uguale alla secret key allora vuol dire che anche la verification key deve essere mantenuta segreta ok quindi la prima coppia di chiavi è questa ok ogni sessione di esecuzione del protocollo di identificazione che cosa succede beh se questa è la chiave segreta la coppia k virgola i dove i è un intero generico inizialmente i è uguale a 0 il prover che cosa fa il prover calcola questo prf cioè applica il prff a l'input i e usando k come chiave ok quindi in pratica il prover prende la chiave segreta usa il primo elemento come chiave del prf usa il secondo elemento come input del prf r risultato chiamiamo r il risultato ok dopodiché per la prossima sessione fa avanzare il contatore no ora che cosa fa il prover per la sessione corrente trasmette r al verifier ok il verifier che cosa fa beh il verifier ha una chiave di verifica che è uguale alla secret key che ha usato il prover quindi può fare lo stesso calcolo che ha fatto il prover cioè può calcolare questa quantità e confrontarla con il valore r che ha ricevuto dal prover se i due coincidono l'identificazione viene accettata se non coincidono l'identificazione viene rifiutata dopodiché anche il verifier incrementa il contatore per la prossima sessione ok quindi vedete che anche se la chiave k è sempre quella non cambia mai a ogni sessione cos'è che cambia cambia l'input dato al prf no ora ora da montare ma che dice che se il prf che noi usiamo è sicuro cosa vuol dire sicuro vuol dire che l'output del prf è impredicibile anche se io conosco l'input posto che la chiave k sia segreta ok quindi se il prf è sicuro perché tanto dobbiamo assumere che l'avversario conosca l'input l'input è un contatore l'avversario che fa niffing delle sessioni le conta sa esattamente a ogni sessione il valore del contatore quindi sa esattamente qual è l'input che verrà dato in posto al prf però non conosce k e non conoscendo k non può sapere qual è l'r che viene calcolato r cambierà a ogni sessione perché a ogni sessione cambia il valore del contatore quindi anche se l'avversario niffa tutte le r delle sessioni passate questa informazione non gli è per niente utile per capire quale sarà la r della prossima sessione ok posto naturalmente che il prf sia sicuro ci sono altre due condizioni che riguardano i dominio di riferimento ovvero questo n qui che è il dominio del contatore deve essere super poli e condizione analoga il codominio del prf cioè l'insieme di riferimento per i valori di r anche questo deve essere super poli perché queste due quantità devono essere super poli notate che il prf lavora in z n quindi cosa vuol dire vuol dire che i calcoli vengono fatti in z n no se n non fosse abbastanza grande e voi i contatori li usate tutti poi ricomincia da capo quindi questo cosa significa significa che a un certo punto voi userete come valore di r un valore già usato in precedenza no l'avversario conta a modo di capire quando questo succederà ok e quindi potrebbe vincere l'attag game se nel super poli questo problema non ce l'abbiamo un discorso analogo vale per questo questo il codominio del prf deve essere super poli perché perché se l'insieme dei possibili r non fosse super poli la probabilità per l'avversario di indovinare a caso r sarebbe più che negligible quindi vincerebbe l'attag game ok quindi anche la probabilità che l'avversario ha di indovinare r per il prop la prossima sessione deve essere negligible quindi sotto queste condizioni il nostro HOTP è un protocollo di identificazione sicuro debolmente sicuro debolmente perché perché anche la verification key deve essere segreta dato che coincide con la con la secret key ok implementazioni pratiche di HOTP si basano su come PRF specifico si basano su HMAC ricordate HMAC basato su shadow 156 quindi quel MAC che se vi ricordate prendeva il messaggio da taggare lo combinava con la prima metà della chiave e calcolava l'ESH il risultato lo combinava con la seconda metà della chiave e ricalcolava l'ESH questo era il modo in cui funzionava HMAC quindi usando due volte una funzione HESH che è SHA256 in questo caso ok quindi usando come PRF specifico HMAC SHA256 si hanno molte implementazioni pratiche di HOTP i telecomandi i telecomandi delle automobili per lo più si basano su HOTP ok quindi dalla dalla fabbrica l'automobile esce accoppiata con una chiave no cosa significa questo significa che sia nella chiave sia la chiave che è automobile sono inizializzate con la stessa coppia K 0 ok dove K è la chiave di fabbrica di quella particolare automobile con quella particolare chiave dopodiché ogni volta che voi schiacciate il pulsante viene eseguito questo protocollo qui e ogni volta che schiacciate il pulsante si aumenta il contatore per la prossima volta questa è l'idea ma nella maggior parte dei dispositivi IoT e quindi in contesti wireless o anche bluetooth dove avete un telecomando che deve comandare un dispositivo lo stesso discorso vale per il tempo vale anche per i telecomandi dei garage o dei cancelli ad apertura automatica quindi tutti i contesti dove si ha a che fare con dispositivi con scarse risorse di calcolo perché non è che si possa inserire chissà quale tipo di risforzo computazionale intendo di questi dispositivi si usano senza il mio come questo ok dove basta implementare HMAC 3156 perché avete già il vostro protocollo di identificazione ok in questo modo vedete ogni ogni sessione di identificazione cambia la chiave perché perché cambia il valore del contatore cioè cambia l'input per il PRF la chiave del PRF invece è sempre quella stabilita a priori è chiaro? ok ora una volta sistemi come questo adesso è da un po' di tempo che non mi capita più di vederli però una volta anche anche il sistema di autenticazione di qualche sito di home banking funzionava con un token fisico che veniva consegnato al cliente mi dica quando dice sì sì sì lì c'era un pulsante da schiacciare ogni volta appariva era chiaramente una versione l'idea però era la stessa dell'accolo TP con la differenza che lì ogni volta che schiacciate il pulsante appariva un pin qualcosa che era quello che bisognava poi introdurre come secondo fattore di autenticazione l'idea è sempre la stessa esattamente quella solo che lì dell'assoluto utente che la inserisce a mano però il modo in cui viene calcolato quel valore è esattamente come aveva appena visto ed è un modo che viene applicato contemporaneamente all'interno del token e lato server dove il verifier deve fare la prova però il tipo di calcolo è sempre quello di HOTP che abbiamo appena visto ci sono varianti di HOTP una di queste cerca di rendere ancora più invulnerabile il sistema riducendo il tempo che l'attaccante ha a disposizione per cercare di indovinare quale sarà la prossima chiave di sessione questo tempo nell'HOTP è variabile dipende da quanto tempo passa tra una sessione e l'altra e quindi è un tempo che dipende dall'utente se voi fate l'identificazione oggi e la prossima identificazione la fate fra un mese vuol dire che l'attaccante ha un mese di tempo per provare a craccare la prossima chiave di identificazione ok se invece voi fate identificazioni molto frequenti il tempo di disposizione dell'avversario si riduce allora un'idea può essere cercare di fissare questo tempo indipendentemente dal comportamento dell'utente come si fa questa cosa beh stabilendo una finestra temporale fissa che rappresenta la durata la divinità di ogni di ogni chiave di sessione ok dopodiché se la finestra temporale passa anche se io non l'ho sfruttata anche se l'utente non l'ha usata in ogni caso la chiave cambia lo stesso ok questa variante per del nome di time based otp time based proprio perché a intervalli fissi di tempo il contatore che abbiamo visto prima scatta automaticamente nella versione originale il contatore viene incrementato solo dopo ogni utilizzo invece nella versione time based il contatore scatta a intervalli fissi di tempo quindi che ne so ogni 10 minuti ogni ora una volta al giorno ok chiaramente il conteggio temporale si riazzera nel momento stesso in cui l'utente utilizza quel particolare valore di chiave quindi ogni volta che io la utilizzo cambia la chiave e si riazzera il tempo poi se l'intera finestra temporale passa senza che io lo riutilizzi più la chiave cambia autonomamente da sola il contatore viene incrementato il problema di questo tipo di soluzione è che dispositivo del prover il token il telecomando quello che è e dispositivo del verifier l'automobile il sistema di controllo del rapporto di ingresso il server del sito web o quant'altro sono disallineati non sono collegati a un clock centrale quindi ci può essere un disallineamento temporale di qualche natura tra i due dispositivi e per questo motivo o si scelgono finestre temporali sufficientemente ampie oppure per approssimare scusate per ribassare un po' il meccanismo di verifica il verifier non si limita a verificare la chiave della finestra temporale corrente ma se la verifica non dovesse avere successo allora prova a verificare anche le chiavi delle finestre temporali contigue ok perché proprio per tenere conto del fatto che il clock del token potrebbe essere disallineato con quello del verifier ok ok ok ok un'altra variante molto utilizzata è quella dei protocolli cosiddetti ESCII anche questi sono molto utilizzati in contesti analoghi a quelli che ho citato prima come funziona il protocollo ESCII si basa sempre su funzione hash e funziona nel seguente modo andiamo a vedere allora vediamo la definizione si prende una funzione hash qualunque SHA256 ad esempio dopodiché l'algoritmo G di generazione delle chiavi che cosa fa sceglie randomicamente una chiave un po' come nell'HOTP e analogamente a quello che accade nell'HOTP definisce come chiave segreta una coppia chiave contatore solo che nel caso dell'HOTP la coppia era K,0 perché il contatore partiva da 0 qui invece vedremo il contatore non è crescente ma è decrescente quindi il suo valore iniziale è N dove N è un valore molto grande ma comunque come vedremo tra poco limitato polinialmente quindi la chiave segreta è la coppia K chiave segreta virgola contatore un intero naturale che verrà fatto decrescere come vedremo la chiave di verifica invece e qui vedete la novità rispetto a HOTP non è la stessa chiave segreta ma è un'altra informazione calcolata nel seguente modo ovvero si applica N più 1 volte la funzione hash su K dove H alla N più 1 di K è uguale a H di H di H di K N più 1 volte ok quindi la novità di questo protocollo rispetto al precedente è che chiave segreta e chiave di verifica sono diverse tra di loro il motivo per tenerle diverse vogliamo essere sicuri non solo debolmente sicuri ok perché quando la chiave di verifica è uguale alla chiave segreta allora è necessario che la chiave di verifica venga mantenuta segreta anche lei maggiori sono le informazioni che voi dovete tenere segrete maggiori sono i punti di vulnerabilità perché maggiore è la superficie d'attacco per un attaccante ok in questo caso la chiave di verifica non solo è diversa dalla chiave segreta ma addirittura potrebbe anche essere pubblica quindi potrebbe anche essere resa nota all'attaccante quindi questo riduce la superficie d'attacco per un potenziale attaccante ora andiamo a vedere come funziona l'identificazione quindi chiaro come viene inizializzato il protocollo la chiave segreta è la coppia k,n la chiave di verifica è l'applicazione n più 1 volte di una certa la funzione hash sulla chiave k ok a ogni sessione di identificazione che cosa succede allora immaginiamo che a un certo punto la chiave segreta sia questa ok k,i il prover che cosa fa il prover calcola h alla i di k cioè applica i volte la funzione hash su k ok quindi ripeto se la chiave segreta è la coppia k,i allora il prover cosa deve fare deve calcolare questa quantità applicare un numero di volte pari a questo contatore la funzione hash che è stata scelta a partire dall'hash di k ok dopodiché capiremo per quale motivo viene decrementato il contatore ora il valore così calcolato r viene trasmesso al verifier ok quindi il verifier riceve r quindi riceve questa quantità e come lo deve confrontare con la verification key quanto vale la verification key pensate all'esecuzione di questo protocollo la prima volta ok la prima volta quindi inizialmente la chiave segreta è questa quindi vuol dire che la prima volta che il prover si identifica cosa manda al al verifier manda questa quantità ok ora la chiave di verifica qual è la chiave di verifica è questa no quindi per verificare l'identità il verifier che cosa deve fare deve semplicemente prendere il valore che riceve dal prover applicargli una volta di più la funzione hash e confrontarlo con la chiave di verifica se i due coincidono l'identificazione è successo ok la volta dopo che cosa succede beh andiamo a vedere come vengono aggiornate le chiavi il prover decrementa i ma anche il verifier fa qualcosa perché anche anche il verifier cambia la propria chiave di verifica e la cambia in questo modo usa come prossima chiave di verifica il valore che ha appena ricevuto torniamo al nostro esempio questo è quello che succede la prima volta ok dopo questa identificazione abbiamo detto che il prover decrementa i ok quindi vuol dire che la prossima volta il prover cosa trasmette trasmette questo ok perché perché ha decrementato n è diventato n-1 quindi la prossima volta che si vuole identificare il prover trasmette questo il verifier che aveva questa come chiave di verifica deve cambiare la chiave di verifica e usare che cosa il valore che ha ricevuto dal prover ma il valore che ha ricevuto dal prover è questo è h alla n di k no questo è il valore che ha ricevuto quindi la prossima volta il prover userà questo valore il verifier userà questo e dovrà fare la stessa cosa che ha fatto prima cioè quando riceve questo gli applica h una volta in più e lo verificherà rispetto alla verification key e questa cosa va avanti e la volta successiva ancora di nuovo il prover avrà decrementato il contatore che diventerà n-2 e la nuova chiave di verifica diventerà h alla n-1 k quindi l'idea generale è che ogni volta che si verifica che si esegue una sessione di identificazione abbiamo che la chiave di verifica sarà qualcosa di questo tipo mentre invece ciò che il prover trasmette sarà qualcosa di questo tipo no? per cui ogni volta il il il verifier non deve fare altro che prendere il valore ricevuto dal dal prover applicarmi una volta di più la funzione hash e fare il confronto con la verification key ok? perché questo protocollo è sicuro posto che la funzione hash sia one way sia non invertibile mettetevi nei panni di un avversario che ascolta e vede osserva quello che succede ok? anche se l'avversario osserva un certo numero di sessioni no? prendete l'iesima quella che abbiamo appena descritto qui sa che la chiave di verifica è h alla i di k non conosci k naturalmente perché k non è noto è segreto lo conosce solo il prover quindi sa solo che l'avversario il nostro avversario sa solamente che questa è la chiave di verifica ok? ora per indovinare questa quantità qui dovrebbe invertire la funzione hash se l'avversario riuscisse a invertire la funzione hash il risultato dell'inversione della funzione hash sarebbe il valore che deve usare per identificarsi la prossima volta ma se la funzione hash non è invertibile l'avversario ha una probabilità negligible di riuscirci ok quindi il vantaggio di questo schema rispetto a HOTP è che lo dice il teorema risulta essere sicuro rispetto all'attacco di tipo i vest-dropping non solo semplicemente debolmente sicuro ma risulta sicuro proprio perché la verification key è diversa dalla secret key e addirittura può essere pubblica quindi conoscere la verification key non darebbe nessun vantaggio all'avversario ok quindi in certi contesti dove si teme che l'avversario possa penetrare il sistema del verifier e quindi accedere alle verification key è preferibile usare un protocollo di questo tipo qui piuttosto che l'HOTP si deve ricominciare da capo con un nuovo con un nuovo con un nuovo K perché quando arrivi a zero è finito tra l'altro questo se vogliamo forse è uno dei punti deboli di questo protocollo perché N non può essere super poli perché perché io devo fare questo calcolo no devo fare questi calcoli qui se fosse super poli vorrebbe dire che il prover in tempo polinomiale non è in grado di calcolare di calcolare R quindi N deve essere un valore polinomiale che consente al prover di calcolare in tempi ragionevoli l'R da inviare al verifier ok questo significa che dopo un certo numero di identificazioni la chiave espira e quindi bisogna riazzerare tutto cosa vuol dire riazzerare tutto vuol dire scegliere un nuovo una nuova K che potrebbe non essere banale far condividere una nuova K al dispositivo del prover e quello del verifier infatti tipicamente quello che succede è lato verifier è più semplice lato prover questa cosa qui spesso coincide con dover cambiare il token fisicamente se il prover ha un token fisico come può essere un telecomando insomma per identificarsi ok l'altra cosa scomoda è che in certi contesti non è applicabile l'SQ protocol per quale motivo beh allora il motivo è che si basa sulla funzione hash che ha un dominio un codominio questo X che naturalmente deve essere super poli ok deve essere super poli perché la probabilità per l'avversario di dovinare a caso il digest il valore di R deve essere negligible ora se siamo in un contesto in cui R deve essere che ne so digitato da tastiera perché questo è il sistema di identificazione che può usare un utente per autenticarsi online su un sito di home banking tanto per fare un esempio quanto può essere grande R beh dato che X deve essere super poli noi lo sappiamo che nel caso delle funzioni hash un valore radonevole per la dimensione delle stringhe di dominico dominio si aggira come minimo dai 128 a 256 bit in su ok ma questo tradotto in caratteri da tastiera significa all'incirca 22 caratteri no una stringa a 128 bit coincide con più o meno 22 caratteri dalla tastiera che significa dire che usando un metodo di questo tipo qui l'utente dovrebbe digitare una sequenza di 22 caratteri la tastiera per trasmettere il valore di che rappresenta l'identificazione all'atto verifier quindi chiaramente se c'è un fattore umano di mezzo un metodo del genere non è pratico non è usabile questo metodo funziona se è tutto automatizzato quindi se tutto si ha luogo senza diciamo la partecipazione attiva dell'utente umano che deve star lì magari a inserire a inserire questi valori però al di là di questi casi particolari nella stragrande maggioranza degli scenari diciamo pienamente automatizzati come può essere il controllo di un'apertura tramite telecomando o l'unico di token digitali dicendo diciamo che il protocollo SQ o l'HOTP sono le due soluzioni che vanno per la maggiore in contesti dove è sufficiente la weak security allora ci si basa sull'HOTP in contesti dove invece non basta la weak security perché perché magari i repository dei verifier è considerato vulnerabile allora si usa SQ quindi questo significa che in contesti online è meglio usare SQ cioè là dove il verifier è un'applicazione online chiaramente se invece il verifier si trova in un sistema embedded non collegato a rete o cose di questo genere pensate a un'automobile a meno che non sia smart allora in quel caso è sufficiente utilizzare HOTP proprio perché voglio dire pensate al contesto di un'automobile se l'avversario riesce a violare il repository del verifier sull'automobile allora probabilmente non ha bisogno di essere il comando del proprietario dell'automobile per rubare la macchina ok quindi sarebbe uno sforzo maggiore rispetto all'necessario purtroppo nessuno di questi la prossima volta vedremo quali sono le soluzioni a questo problema nessuno di questi protocolli che vi ho raccontato è sicuro rispetto ad attacchi attivi gli attacchi attivi sono quelli in cui l'avversario può impersonare a sua scelta verifier o anche il prover ok finora abbiamo visto scenari in cui il massimo che poteva fare l'avversario era ascoltare sessioni tra prover e verifier e poi provare impersonare il prover negli attacchi attivi lo abbiamo detto anche stamattina fishing è un esempio negli attacchi attivi l'avversario può provare a impersonare il verifier dialogare col prover quindi indurre il prover a eseguire una sessione del protocollo col finto verifier raccogliere informazioni dal prover e usarle successivamente per identificarsi col verifier originale questa è la strategia che potrebbe seguire l'avversario e che è raccontata in questo tag game ok infatti vedete cosa dice questo tag game dice che l'avversario può impersonare il verifier in una sessione col prover una o più sessioni col prover dopodiché prima o poi l'avversario proverà a impersonare il prover col verifier originale magari sfruttando le informazioni ottenute precedentemente dal prover ok rispetto a questo tag game la nozione di sicurezza è definita sempre nello stesso nel solito modo quindi un protocollo di identificazione è sicuro rispetto ad attacchi attivi se il vantaggio dell'avversario in questo tag game è ascurabile rispetto a tutti gli avversari efficienti la nozione di sicurezza debole è sempre la solita la conclusione osservando questo tag game è che ripeto nessuno dei protocolli visti finora può essere sicuro rispetto a questo tag game qual è il motivo il motivo è prendiamo l'ultimo di quelli che abbiamo visto ma le caratteristiche degli altri sono analoghe ora se il se l'attaccante riesce a impersonare il verifier quindi riesce a convincere il prover di essere il verifier e quindi riesce a convincere il prover a iniziare una sessione cosa fa il prover prover prende la chiave segreta valida in quel momento e che è valida anche la chiave corrispondente di verifica è valida per il verifier legittimo quindi usa la chiave segreta attuale calcola questa quantità e la trasmette all'avversario ok perché ritiene che l'avversario sia il verifier legittimo a quel punto l'avversario può chiudere la sessione non interessa andare avanti perché tanto il suo scopo qual era? il suo scopo era farsi calcolare questa quantità dal prover ora che l'avversario conosce questa quantità vi ricordo che a lato verifier la la verification key è questa no? ok questa è la verification key a lato verifier quando questa è la secret key a lato a lato a lato prover ok quindi ripeto l'avversario fa finta di essere verifier il prover gli manda questo r quindi l'avversario impara quanto vale questa quantità prima non era in grado di calcolarsela chiude la sessione col prover e fa finta a questo punto di essere il prover con il verifier con quello legittimo e cosa fa? gli manda esattamente la il valore che il verifier accetterà ok quindi in pratica con questa strategia l'avversario convince il prover a inviargli la chiave utile per identificarsi nella prossima sessione col verifier legittimo ok dopodiché usa questa informazione per identificarsi col verifier chiaro? spera che non sia cambiato allora il verifier non lo sa che il prover ha avviato una sessione con l'avversario come? rispetto al reverse dropping chiaro rispetto al reverse dropping nell' reverse dropping l'avversario può impersonare solo il prover quindi può interagire col verifier ok cercando di dovinare qual è la chiave da usare però non ha nessuna informazione utile dalle voi qui l'avversario che cosa l'avversario di tipo reverse dropping cosa fa? ascolta quello che succede qui esatto esatto tutto quello che ascolta sessione dopo sessione è inutile per le sessioni successive perché per ogni successione futura quello che succede è che cambia il contatore e quindi qui non c'è nulla che gli possa servire per le prossime sessioni invece nell'attacco attivo l'avversario cosa fa? prima finge di essere il verifier col prover il verifier non lo sa quello che sta succedendo quindi voi congelate una situazione in cui questa è la chiave del prover e questa è la chiave del verifier ok ora che cosa succede? lui si vuole identificare usando il prover si vuole identificare usando la chiave ki però invece di farlo col verifier lo fa con l'avversario che magari lo ha convinto con un attacco di tipo phishing ok e quindi che cosa fa il prover? manda questa informazione che è quella che serve per identificazione all'avversario l'avversario la raccoglie e poi la usa col verifier legittimo che l'accetterà ok proprio perché lui non lo sa cosa è successo nel frattempo lui il suo contatore è sempre IP1 e quindi identificherà l'avversario come se fosse il prover ok questo vale per il protocollo SQ ma vale anche per tutti i precedenti vale anche per HOTP è questo discorso o TOTP non cambia assolutamente assolutamente nulla quindi tutti i protocolli che abbiamo visto finora sono vulnerabili rispetto ad attacchi attivi ok è chiaro che stiamo parlando di attacchi sofisticati perché stiamo parlando di attacchi dove l'avversario può impersonare il verifier ok quindi convincerlo ad essere il verifier non è una cosa banale e può impersonare il prover nell'interazione con il verifier però non è banale ma non è anche impossibile l'abbiamo visto negli esercizi di penetration testing cose di questo genere non o male sono all'ordine del giorno quindi non è così impossibile il motivo fondamentale per cui tutti questi protocolli che abbiamo visto finora sono vulnerabili ad attacchi attivi consistono nel fatto che gli handshake di questi protocolli sono unidirezionali ok cioè in tutti i casi che abbiamo visto è il prover che trasmette qualcosa al verifier non è mai il contrario cioè è un handshake unidirezionale il prover manda delle informazioni verifier le verifica questo consente all'avversario di mettersi in ascolto di quello che gli manda il prover e nulla più no per eludere gli attacchi attivi bisogna studiare protocolli basati su handshake dove non solo il prover manda informazioni ma anche il verifier deve rispondere al prover con delle informazioni questo come vedremo eluderà l'attacco attivo dell'avversario che abbiamo descritto ma questo lo vedremo la prossima la prossima settimana ok grazie a tutti