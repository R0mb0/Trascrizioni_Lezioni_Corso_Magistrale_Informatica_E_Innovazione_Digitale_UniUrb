Ok, allora cominciamo. Ieri avevamo concluso parlando di HMAC e HKDS, in particolare torniamo un attimo indietro per ricordare che HMAC è un Mac che usa funzioni hash come primitive più caratteristiche. Grafiche di riferimento. L'idea che è molto simile a quella che abbiamo già visto anche in passato, usando altre primitive, è quella di praticamente definire un PRF come MAC, quindi con chiave e messaggio da taggare, e definire in questo caso la costruzione applicando due volte la funzione hash. La prima volta su una porzione della chiave concatenata con il messaggio, la seconda volta sul risultato di questa operazione concatenato con un'altra porzione della chiave. In questa maniera abbiamo un PRF sicuro sotto l'ipotesi che H sia una funzione hash resistente alle collisioni. Abbiamo visto come è implementato nella pratica HMAC, in particolare come le due porzioni di chiave K1 e K2 derivano da K. quello che invece è detto anche che come funziona hash utilizza SHA256, rimane da vedere come è implementato invece HKDF, cioè quella costruzione che come vedremo sfruttando HMAC, consente a partire da una singola chiave condivisa di generare una sequenza di sottochiavi. Quindi in linea di principio qualcosa di simile a quello che accade con i PRG. Allora HKDF funziona come definito in questo slide. sfrutta HMAC che come abbiamo visto poc'anzi si basa fondamentalmente sull'utilizzo di SHA256 e come anticipato HKDF che cosa fa? Prende in input una chiave condivisa, un segreto condiviso che qui viene chiamato S e questo è il segreto condiviso dalle parti, più altre informazioni aggiuntive. Queste informazioni aggiuntive sono quello che in gergo viene chiamato sale, generalmente un valore casuale, non segreto ovviamente. E altri metadati, questo parametro info rappresenta ulteriori metadati che ad esempio potrebbero essere utilizzati per descrivere in che modo l'output di HKDF deve essere spezzato in sottochiavi. Questo è un esempio tipico. Quindi HKDF prende in ingresso queste informazioni e restituirà una stringa di una certa lunghezza L grande che rappresenterà la sequenza di bit che poi potremo andare a spezzettare in sottochiavi che le parti potranno utilizzare. HKDF si distingue in due fasi. Una prima fase cosiddetta di estrazione ed una seconda fase di espansione. La prima fase di estrazione consiste nell'utilizzo una volta di HMAC per generare un tag. Il tag come viene generato? Usando il sale, che è l'informazione casuale di partenza, come chiave. Ok? E usando il S, cioè il messaggio segreto, scusate, l'informazione segreta, la chiave condivisa, come messaggio da tagare. Ok? E il risultato è appunto un tag, un tag T, che è diciamo il nostro punto di partenza per la generazione della stringa pseudo casuale che rappresenterà la sequenza di sottochiavi, quindi il risultato finale. Ok? Dopodiché nella fase di espansione T viene elaborato un classico cicloform, come ne abbiamo visti tanti. e il suo obiettivo è quello appunto di generare una sequenza di byte che concatenate insieme, vedremo, rappresentano un output di HKDF. Ok? Il numero di cicli che vengono eseguiti è pari a questa quantità Q, che non è altro che il rapporto tra la lunghezza finale dell'output che vogliamo ottenere e la lunghezza dei digest della funzione hash. Ok? Perché il punto è che a ogni round del ciclo 4 verrà generato un digest. E l'idea è che alla fine questi digest andranno concatenati per formare la sequenza di lunghezza alle grande. Quindi per sapere quanti round dobbiamo fare basta dividere la lunghezza dell'output finale per la dimensione del digest. Ok? E dopodiché nel ciclo 4 che cosa succede? Si generano tanti digest in sequenza. In che maniera? Si parte dal primo digest che di default è una stringa vuota, fate solo di 0. Questo è il primo digest. Dopodiché vedete che a ogni step non si fa altro che applicare HMAC per generare un nuovo digest. E qui vedete ogni nuovo digest in che misura dipende dal digest precedente, del round precedente. Perché vedete che per tirare fuori il nuovo digest HMAC viene applicato a che cosa? Viene applicato usando T come chiave e questo è invariante a ogni round. La chiave di HMAC che è sempre T. Ma T è segreto. Perché è segreto? Perché T l'abbiamo ottenuto al passo di estrazione applicando HMAC all'informazione segreta S. Ok? Dopodiché l'input a ogni round del ciclo 4 di HMAC cos'è? È la concatenazione di queste informazioni. Dove? Queste informazioni che cosa sono? Sono il digest del round precedente. Ok? I metadati. E un contatore. Un contatore di round. Chiaramente il contatore di round sarà diverso a ogni round e questo dà un'ulteriore garanzia del fatto che l'input dato in passo da HMAC cambierà di round in round. E questa cosa va avanti finché non abbiamo generato il numero di digest che serve per avere una sequenza di lunghezza, almeno L grande. Questa sequenza rappresenta una stringa pseudorandom che è spettata opportunamente, ce lo diranno i metadati info, rappresenteranno la sequenza di sottochiavi di cui i due agenti che hanno eseguito HHPDF hanno bisogno. Quindi in altre parole, la sequenza pseudorandom non è altro che la concatenazione di tanti digest. Dove ogni digest in qualche modo dipende dal digest precedente nel interno del ciclo, più altre informazioni. E a ogni round come chiave per HMAC viene usato T. Qualche idea sul motivo per cui viene usato T invece che usare direttamente S? Potremmo usare anche direttamente S, no? Invece che qua dentro, invece che T, come chiave di HMAC, usiamo direttamente S e siamo a posto. Però perché non viene usato direttamente S ma viene usato un T che è il digest di S rispetto a un certo sale? Perché è proprio il terro sale che è un valore random? È un valore random, quindi sicuramente questo contribuisce a... Potremmo usare direttamente S per generare qua dentro la sequenza di sottochiavi. Il motivo non è legato alla segretezza, il motivo è legato alla particità. Perché se voi usate direttamente qua dentro, al posto di T, usate direttamente S, vuol dire che Alice e Bob, a partire dal segreto S che condividono, possono generare una ed una sola sequenza di sottochiavi. Perché a quel punto HKDF sarebbe deterministico, no? Ora, una volta che hanno esaurito quella sequenza di sottochiavi, se ne hanno bisogno di altre, come fanno? Ok? Quello è il problema. Dovrebbero ricominciare da capo con un nuovo segreto condiviso che però non hanno. Ok? Invece in questa maniera, basta cambiare il sale, di volta in volta, ed ecco che Alice e Bob hanno l'opportunità di creare ogni volta una sequenza di sottochiavi sempre diversa, ogni volta che ne hanno bisogno, sempre a partire da esse, senza necessità di cambiare il segreto condiviso. Perché tanto ogni volta che ne hanno necessità, cosa fanno? Scelgono un sale, in maniera randomica, questo sale se lo possono scambiare senza tenerlo segreto, non ha importanza, ciò che conta è che è essere male segreto, e quindi cambiando il sale di volta in volta, cambia T, e cambiando T cambierà il risultato del cicloforo. Quindi la motivazione di Natura Cacca. Ok? Quindi come dicevamo ieri, HKDF è estremamente efficiente, perché vedete che le operazioni che si fanno all'interno del cicloforo sono solo ed esclusivamente applicazioni di SHA256. Ogni volta che viene invocato HMAC, vuol dire che stiamo usando SHA due volte. E questo è estremamente efficiente. E in questo modo permettiamo ad Alice e Bob di condividere un numero a piacere di sottochiavi, ogni volta che ne hanno necessità. Per chiudere il discorso sulle primitive di HESHING, vi ho fatto vedere l'idea basata sulla costruzione di Merkel ed Upgard e sullo sfruttamento delle funzioni di compressione, piccole ma efficienti, facile da calcolare. C'è un appoggio completamente diverso che ha preso piede negli ultimi standard, diciamo, e che è completamente diverso dalla costruzione di Merkel ed Upgard. Quindi è una costruzione completamente nativa. Quindi non sfrutta funzioni hash di compressione, di base, da comporre, ma utilizza delle operazioni atomiche di base che come al solito, tanto per cambiare, un po' come in tutti i meccanismi che abbiamo visto fino ad oggi, si basano su due operazioni elementari che sono l'Oxor e la permutazione. Quindi sono i due ingredienti che come al solito si utilizzano. Qual è l'idea di questa nuova costruzione che si chiama sponge? L'idea di usare, appunto, come primitive, l'Oxor, è una funzione di permutazione. Ok? Funzione di permutazione di questo tipo qua. Quindi è una funzione pi che permuta blocchi di n bit, in blocchi di n bit. Quindi è una funzione obiettiva. Ok? Ogni blocco viene mappato in un solo blocco. E ogni blocco viene mappato una volta solo. N è, questo N qui, la lunghezza dei blocchi, è la somma di due parametri che vanno decisi a priori, di cui significato lo commenteremo poi. Questi due parametri si chiamano rate, R, e capacity, C. Ok? Il significato di questi due parametri, come vedremo, è quello di rappresentare da una parte l'efficienza, ovvero il parametro rate, R, è direttamente proporzionale all'efficienza. Quindi, se scegliamo R alto, vuol dire che vogliamo privilegiare la velocità di esecuzione nel calcolo del digest. Ok? L'altro parametro, C, la capacity, invece, è un indice di sicurezza, di robustezza della funzione hash, ovvero, più grande è C, più la funzione hash è sicura, ovvero resistente alle collisioni. Ok? Ora, dato che in genere N è fissato, mentre invece R e C sono i due parametri su cui possiamo giocare, e dato che N è la somma di R e C, cosa vuol dire? Vuol dire che a seconda di come scegliamo R e C, tenderemo a privilegiare un aspetto piuttosto che l'altro, quindi c'è una sorta di trade off tra performance e sicurezza, come poi è anche abbastanza intuitivo, è chiaro che se vogliamo che la nostra funzione hash sia molto performante, probabilmente dobbiamo rinunciare a qualcosa in termini di sicurezza e viceversa, se la vogliamo molto sicura, allora sarà anche meno performante. E vedremo in che modo questi due parametri effettivamente giocano su questi due aspetti, però già dal fatto che la costante N è la somma dei due, ci suggerisce il fatto che i due sono in controtendenza l'uno rispetto all'altro, se aumenta uno cala l'altro inevitabilmente. L'input per questo tipo di costruzione al solito è un messaggio, una stringa di un certo numero, di al più un certo numero di bit, L grande, e l'output invece è il digest di una certa funzione hash che poi adesso andremo a definire attraverso, come al solito, un ciclo. Quindi quel v piccolo è la lunghezza del nostro digest. Ok? Ora, la costruzione del sponge si basa principalmente su una fase che viene detta di assorbimento. Poi come vedremo opzionalmente ci sarà una seconda, una seconda fase. Nella fase di assorbimento, come al solito, in tanti altri casi, il messaggio in input viene spezzato in blocchi. Ok? ogni blocco ha una lunghezza pari al rate. Ok? Ok? E questo fatto ci dà già un indizio del perché il rate è un indice legato alle performance. Perché è chiaro che se R è molto grande otterremo meno blocchi. E siccome, come poi adesso dovremo vedere, il ciclo for che definisce il comportamento della funzione hash esegue tanti round, quanti sono i blocchi? ogni round lavorerà su un blocco, questa è un po' l'idea. È chiaro che se abbiamo pochi blocchi di grandi dimensioni, faremo meno calcoli rispetto ad avere tanti blocchi di piccole dimensioni. Perché poi all'interno di ogni round le operazioni da fare sono per le stesse. Quindi al crescere di R cala il numero dei round da eseguire e quindi l'algoritmo è più veloce. Quindi dividendo il messaggio M in blocchi di R dimensioni immaginiamo di ritrovarci questo quantitativo di blocchi, S blocchi di R bit ciascuno. E come vedremo calcoleremo il digest all'interno di un ciclo che va da 1S. Quindi ogni round lavoreremo su un diverso blocco. Anche qui un po' come abbiamo visto anche per H, K, D, F o per altri cali. Si esegue un ciclo in cui il digest H viene inizializzato con la sequenza di 0 di tutti i 0 ok? E si lavora su sequenze vedete di lunghezza n che è il nostro parametro, la nostra costante che ci stabilisce un po' rispetto a quale contesto stiamo lavorando. Ok? Quindi noi partiamo con un digest la cui lunghezza di n bit tutti a 0. Questo è il digest iniziale. Questo digest come vedete adesso viene manipolato step by step un round alla volta dove ogni round viene combinato in pratica usando le due primitive che abbiamo anticipato con il blocco corrente del messaggio. Ok? Infatti vedete che cosa succede? A ogni round che cosa succede? Succedono due cose. Ok? Prima cosa si prende il blocco corrente ok? Quindi il blocco del round i lo si concatena con una sequenza di c0 dove c è la capacità in questo modo si ottiene un blocco la cui lunghezza è n perché? Perché ogni blocco è di lunghezza r lo concateniamo con c0 quindi la lunghezza totale che otteniamo è r più c che fa n ok? Perché questo? Perché nella seconda operazione che cosa si fa? Si prende il digest corrente h che è una sequenza di n bits lo si lega in xor con il blocco che abbiamo appena costruito e il risultato lo si dà in pasto alla funzione di permutazione che lo trasformerà in un altro blocco ok? Quindi volendo costruire la sequenza noi partiamo da questo digest ok? Prendiamo il primo blocco concatenato con c0 ok? che va in xor con il digest corrente ok? e il risultato lo diamo in pasto alla funzione di permutazione ok? e il risultato sarà il nuovo digest e questa cosa va avanti prendiamo il blocco successivo concatenato con c0 veri facciamo il nostro xor ok? e di nuovo permutiamo otteniamo un nuovo digest la concatenazione è molto simile a quella che abbiamo visto in tanti altri tanti altri casi questa cosa va avanti finché non abbiamo esaurito tutti i blocchi di m grande ok? il risultato finale sarà un digest di lunghezza n ok? ora se la lunghezza attesa che vogliamo alla fine no perché alla fine abbiamo detto ci serve un digest di lunghezza v ok? se v è minore o uguale di n questo lo decide lo si decide a priori no? di quanto lo vogliamo il digest ma v quant'è? vogliamo se v è minore o uguale di n abbiamo già realizzato prendiamo i primi vbit di h e con il nostro digest ok? e questa è la costruzione male dunque Giorgia chiede se la concatenazione di zeri non va a creare problemi con la funzione di permutazione anche se si applica all'oxor e il punto è proprio che si applica all'oxor che aiuta in questo senso dato che applicando l'oxor ogni 0 sta in fondo può diventare 1 0 1 1 a seconda del bit corrispondente del digest attuale poi interviene la permutazione che mescola ulteriormente le carte e quindi diciamo che a ritroso dovrebbe essere impredicibile riuscire a ricostruire questa catena di operazioni nel senso proprio questo insomma quindi questo è quello che succede se invece se invece diciamo che v è più lungo del rispetto all'atteso allora dobbiamo fare una ulteriore operazione ok che è quella di squeezing diciamo no l'operazione di squeezing che cosa fa estrae sceglie rbit alla volta per costruire per costruire il digest e come fa allora i primi rbit li prende da h no vedete quindi il risultato dello step precedente è il nostro digest h no la cui lunghezza è l questo è il risultato dello step precedente adesso che cosa succede io da h prendo i primi rbit no e richiamo la sequenza dei primi rbit la chiamo z1 ok e questi sono i primi rbit poi entro in un ciclo in cui mescono le carte perché vedete quello che succede è questo prendo h e lo permuto in pratica permutare h significa prendere tutti i bitti di h e mescolarli in base alla funzione di permutazione e ogni volta che permuto h di nuovo prendo i primi rbit di h e quindi mi costruisco z2 z3 e così via ok e ognuno di questi blocchettini sono rbit in pratica ognuno di quei blocchettini è una sequenza casuale di bit estratto da h in base a quello che mi dice la funzione di permutazione io non devo fare altro che concatenare tutti questi finché non raggiungo come lunghezza v che è il parametro che avevo stabilito nei dissi mi fermo non appena questa sequenza di blocchettini non raggiunge v come lunghezza facciamo un'ipotesi sciocca se v è più piccolo di r mi basta il primo blocchettino ok cioè mi basta z1 ma z1 chi è? sono i primi rbit del digest quindi in pratica l'idea qual è? se mi bastano i primi rbit del digest prendo quelli e ho finito se non mi bastano prendo il digest lo mescolo e di nuovo prendo i primi rbit che concateno quegli altri che avevo già estratto ho ottenuto ho due rbit mi bastano? se sì mi fermo lì se no riprendo h lo rimescolo un'altra volta ed estraggo di nuovo i primi rbit e vado avanti così finché non ho esaurito la costruzione del digest che mi serve ok ora perché r? prima abbiamo visto che r è un indice di performance perché r fondamentalmente mi stabilisce r mi stabilisce quanto sarà lungo il ciclo fuoco ok quindi più alto r meno round avrò e quindi un indice di performance perché invece c è un indice di sicurezza beh lo capiamo dal passo di squeezing perché h abbiamo appena detto che è la mia sequenza di n bit no? e abbiamo detto che da h prendiamo i primi r gli altri c non li prendiamo ok? ora immaginate che v sia più piccolo di r minore o uguale di r ok? quindi v cade qua dentro quindi vuol dire che il digest finale che noi produciamo sta entro i primi r bit di h ed è quello che viene sparato in output è quello che vede l'avversario perché sarà l'hash l'hash del messaggio originale quindi questo cosa significa? significa che l'avversario non vede l'intero output del passo di assorbimento perché l'output del passo di assorbimento è una sequenza di n bit l'avversario di questa sequenza di n bit al massimo vede i primi r gli altri non li vede ok? e non vedendoli cosa significa? significa che in realtà l'avversario vede solo una parte dell'output di questa procedura e non vedendoli vedendo l'intero output di questa procedura invertirla perché se l'avversario per trovare una collisione dovrebbe invertire il processo di calcolo dell'hash no? cioè dall'output dell'hash deve cercare di risalire ai possibili input ok? ma di questa procedura qui che in una direzione è facile faccio degli xor e faccio delle permutazioni quindi è facile ma nella direzione opposta mettetevi nei panni dell'avversario che dell'output di questa procedura vede solo una piccola porzione vede solo al massimo i primi rbit gli altri c non li vede sono segreti rimangono segreti quindi dovrei è come dire che io dovrei invertire una funzione ho una funzione f di x uguale a y e dovrei riuscire a invertirla risalire a x non conoscendo tutta y ma solo un suo pezzettino è questa è questa l'idea quindi nascondendo c bit e più grande c è tanto più vero questo rendono più complicato il compito all'avversario di tentare di invertire questo processo perché gli metto a disposizione solo i primi rbit è chiaro che per complicarvi il compito maggiormente io dovrei cercare di avere c più grande possibile ma aumentare c significa diminuire r ma diminuire r significa aumentare il numero di round di questo ciclo quindi vedete che è un cane che si morde la cuore quindi se voglio privilegiare la sicurezza devo pagare di più in termini di costi di esecuzione o viceversa se voglio avere funzioni hash particolarmente rapide allora devo rinunciare a qualcosa in termini di sicurezza ok è il motivo proprio come dicevamo il fatto che una porzione pari a c dell'output del passo di assorbimento rimane segreta e quindi invertire una funzione di cui io ti faccio vedere su una porzione dell'output diventa ancora più difficile questa è l'idea di questa costruzione la cui originalità sta nella sua flessibilità no perché fissato n è n fisso nelle implementazioni della sponge construction perché perché perché la funzione di permutazione lavora su sequenza di n bit quindi n è una è una costante e tant'è vero che poi come dice qua nell'implementazione tipica di questo che è diventato lo standard di questa di questa costruzione n è pari a 1600 ok e lo standard si chiama SHA-3 quindi contrariamente a quello che uno può pensare se vedete in giro SHA-3 uno pensa ma ecco una funzione nella famiglia SHA e quindi paradigma di Merkel-Dagmard è quello che abbiamo visto ieri no SHA-3 si basa su questa costruzione qua che è completamente diversa e la funzione di permutazione di SHA-3 non ve la faccio vedere è una funzione che si chiama Ketchak è comunque open source per trovare letteratura anche questa l'hanno inventata i belgi all'università di Levin esattamente come era successo con AS e cosa volevo dire no la la novità rispetto agli altri paradigmi è che fissato n siccome r e c sono parametrici uno si può confezionare la sua funzione hash a seconda di quanto vuole privilegiare efficienza piuttosto che sicurezza scegliendo di volta in volta il valore di R o il valore di C che gli fa più comodo quindi questo rende la funzione hash estremamente più possibile perché di fatto vuol dire che siamo noi a decidere quanto renderla sicura ok e poi c'è un teorema che nel caso banale in cui la funzione di squeezing non si fa perché basta la data 1 ovvero il caso in cui la lunghezza del digest che ci serve è minore uguale di R e quindi mi basta il primo blocchetto sotto questa condizione qui è facile dimostrare che se questa è una funzione di permutazione casuale e se queste due quantità sono superpoli quindi di conseguenza non è anche due alla R allora la funzione hash che è costruita in questa maniera è resistente alle collisioni anche qui vedete vedete che rispetto ad altre situazioni dove si usano altre primitive le condizioni sono abbastanza forti ad esempio nella costruzione di Merkel e Damgard basata sulle funzioni di compressione di Davis Mayer lì si usa un bloc cipher alla base della funzione di compressione il bloc cipher deve essere sicuro quindi condizioni di una certa complessità qui non c'è nessuna ipotesi specifica che riguarda la sicurezza degli ingredienti vedete le condizioni dicono che questa deve essere una funzione di permutazione casuale ok è semplicemente che la capacità e il rate devono essere due numeri tali per cui il loro elevamento a potenza è super poli è sufficiente questo quindi niente di complicato e sotto condizioni che non hanno poco niente a che fare con quelle classiche di altre situazioni riusciamo comunque a dimostrare la resistenza alla collisione della funzione costruita in questa maniera qua ok è chiaro che partendo da un n in questo caso partendo da un n di 1600 bit avete tante possibilità di scegliere c in maniera tale che due alla c così come due alla r siano super poli ok se li vedete in maniera opportuna chiaro che se scegliete un c molto piccolo vi salta la sicurezza no quindi non potete spingere troppo no nel nello scegliere dove è r dove è c dentro n non potete non vi potete spingere ai due estremi perché se vi spingete troppo allora quello che succede è che o questo o questo non sono super poli mentre invece entrambi lo devono lo devono essere ok quindi ci sono tanti vantaggi dietro questo tipo di costruzione animativa originale e questo è il motivo per cui negli ultimi anni è diventato uno standard attraverso SHA3 che è forse la funzione hash di ultimissima generazione tra quelle che abbiamo visto ok tant'è che si tende ad esempio una funzione hash come SHA512 si usa raramente si usa poco perché è molto più inefficiente rispetto a SHA256 e se l'obiettivo è quello di aumentare la sicurezza allora si preferisce usare SHA3 a quel punto ok questa è l'idea dietro le funzioni hash senza chiave vi ho fatto vedere le due implementazioni principali prima di chiudere il discorso relativo a questo tipo di funzioni vi faccio vedere un paio di applicazioni ok una molto interessante molto utile perché si usa in tanti contesti ve ne farò vedere uno è l'applicazione quella di cosiddetti MerkelTree MerkelTree sono un modo estremamente efficiente per organizzare informazioni in maniera tale da garantirne l'integrità e sono particolarmente efficienti quando io devo garantire l'integrità di grandi molti dati su cui ho necessità di accedere rapidamente di verificare l'integrità rapidamente ok tant'è che ad esempio i MerkelTree si utilizzano all'interno dei blocchi della blockchain di bitcoin ma poi successivamente anche di tutte le altre per memorizzare in maniera efficiente informazioni relative alle transazioni memorizzate nel blocco ok quello è un esempio tipico di cui vi parlerò perché se ho tempo a un certo punto vi racconterò anche come sono fatte dal punto di vista della sicurezza le blockchain vi farò vedere un esempio di bitcoin ok lo scopo dei MerkelTree è quello di creare una struttura dati attraverso la quale verificare in maniera efficiente l'integrità di un file nel caso dei blocchi della blockchain il file è il blocco i cui record sono le transazioni che vengono memorizzate dentro del blocco quindi immaginate di avere un file fatto di tanti blocchi di tanti record e voi volete garantire l'integrità di questo file e lo volete fare nella maniera più efficiente possibile questo può capitare anche nel caso di un file system immaginate di avere un file di grandi dimensioni un eseguibile un eseguibile critico di sistema operativo e volete avere la garanzia che questo eseguibile in memoria sia integrato ok allora l'idea che avevamo già detto ieri è magari pubblico l'hash in un'area di memoria ridolli e poi cosa faccio quando mi serve quel file ne calcolo l'hash e faccio il confronto questo è un'idea qual è il problema di questa idea il problema di questa idea è che calcolare l'ash di un file di grandi dimensioni costa costa tempo magari non mi serve neanche l'intero file perché magari mi serve solo una sua porzione che ne so se si tratta di una libreria magari l'esecuzione della libreria mi comporta di dover caricare dal file solo un paio di blocchi e quindi mi servirebbe conoscere l'integrità solo di quei due blocchi e non dell'intero file lo stesso discorso vale se ragioniamo sulla blockchain magari mi interessa verificare l'integrità di un record all'interno del blocco della blockchain perché mi interessa una certa transazione non mi interessano le altre allora perché dovrei calcolare l'integrità dell'intero blocco applicando l'esce l'intero file è inefficiente i marker tree mi permettono in maniera capillare verificare l'integrità anche su singole porzioni del file e farlo sempre pagando il medesimo costo indipendentemente da dove si trova il blocco che vogliamo validare ok piuttosto che descrivervi l'algoritmo per la costruzione di un marker tree vi faccio vedere direttamente un esempio perché la rappresentazione grafica è particolarmente semplice e intuitiva ora immagina quindi come dicevamo prima l'idea è quella di garantire l'integrità dei blocchi di un file e di poter verificare l'integrità anche solamente di uno di questi senza dover per forza tirare in ballo gli altri ok quindi immaginate che i blocchi del nostro file siano le foglie di questo albero quindi i nodi etichettati x1 x2 fino a x8 immaginate che siano gli 8 blocchi del nostro file per comodità ragioneremo sempre con un quantitativo di blocchi che è una potenza di 2 ok ci si può ricondurre facilmente questo caso aggiungendo blocchi fittizze e quindi x1 x2 fino a x8 sono i blocchi del nostro del nostro del nostro file il merkel tree è una struttura ad albero che si costruisce a partire da questi blocchi che vengono considerati come diciamo gli elementi costitutivi per la generazione dell'albero in che maniera? beh si comincia semplicemente applicando una funzione hash a ciascun blocco ok quindi le foglie i nodi tondi a livello delle foglie di questo albero rappresentano l'ash del dei blocchi ok dove il risultato del hash lo chiamiamo y2 y3 fino a y8 ok quindi y1 è l'ash di x1 y2 è l'ash di x2 e via dicendo ok a questo punto che cosa faccio? mi ritrovo con 8 hash no che etichettano le foglie del mio albero a questo punto 2 a 2 combino questi hash ok perché devo costruire un albero binario quindi andando bottom up quindi alzandomi di livello l'idea è prendere 2 a 2 quindi prendiamo questi 2 prendiamo questi 2 e così via quindi 2 a 2 prendo gli hash li concateno e calcolo l'ash di questa concatenazione di nuovo ok quindi ad esempio concatenando y1 e y2 e applicando di nuovo la funzione hash mi salta fuori un nuovo digest che chiamiamo y9 in questa maniera dato che lavoro sulle coppie al livello soprastante io avrò dimezzato il numero dei digest ok e questo procedimento io lo porto avanti finché non arrivo alla radice del mio albero quindi finché non arrivo ad avere un unico hash quindi questo significa che ha uno step successivo io prenderò y9 y10 li concateno applico la funzione hash e mi salta fuori y13 quindi mi sto creando una gerarchia di ad albero di valori hash dove se osservate a ogni nodo corrisponde l'hash di che cosa della concatenazione dei valori associati ai due figli questo albero ha un numero di livelli che è pari al logaritmo in base 2 del numero di blocchi quindi se siamo partiti da 8 blocchi vedete che abbiamo 3 livelli ok questa struttura io la salvo in memoria insieme al file poi in realtà se voglio proteggere diciamo questa struttura mi basta salvare in un'area ridolli solamente la sua radice non è necessario salvare ridolli intermedie ma è sufficiente la radice infatti poi come vedremo quando vi racconto come funziona la memorizzazione di queste informazioni nei blocchi della blockchain solamente la radice del merkel tree viene tra virgolette protetta poi vedremo in che modo ok a questo punto come viene utilizzata questa questa struttura per validare l'integrità di un blocco immaginate che a un certo punto di che ne so caricare da disco immaginiamo immaginiamo quindi che questo file sia una libreria una vll immaginiamo che a un certo punto per poterlo eseguire dobbiamo caricare da disco il terzo blocco extra ok e voglio sapere se questo blocco è integro non è stato alterato da un avversario magari che ha fatto breccio del sistema e è andato a modificare la DLL ok complementando degli attacchi di cui parlavamo anche ieri come si opera quindi voi avete questo blocco qui lo dovete validare che cosa fate calcolate il suo hash ok calcolate il suo hash l'idea è quella di ripercorrere il cammino che va da quel blocco fino a la radice rifacendo tutte le operazioni che vi permetteranno di risalire all'ash della radice ok ora per risalire alla radice voi partite da questo blocco ne calcolate l'ash per proseguire lungo il cammino anche avviene questo no ora per risalire che nella radice lungo il cammino che cosa vi serve sapere beh questo y3 ve lo siete calcolati voi perché avevate il blocco questo lo dovete leggere dal Merkel tree quindi leggete il Merkel tree recuperate y4 che è il valore sibling il valore gemello di y3 lo recuperate perché lo recuperate perché concatenando y3 con y4 e poi applicando la funzione hash vi salta fuori questo a questo punto dal Merkel tree recuperate questo fate la stessa operazione calcolate l'ash e se tutto è andato bene dovreste ottenere come risultato finale che cosa? la radice del Merkel tree cioè il valore che ottenete alla fine lo confrontate con la radice del Merkel tree se sono uguali che cosa vuol dire? se sono uguali vuol dire che il blocco da cui siete partiti è il blocco originale non basterebbe verificare il y4 e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e si garantisce l'integrità solo nella radice perché Merkel tree può essere di grandi dimensioni un po' come un po' come l'idea del file allora non posso proteggere l'integrità dell'intera dell'intera struttura ma mi basta garantire l'integrità della sua radice perché a quel punto io se vado a leggere io andrò a leggere questo e questo e poi questo qua che sono i tre i tre sibling che mi servono per risalire dalla radice ora se alla fine dei miei calcoli ottengo questo e io ho la garanzia che quel valore lì è protetto perché si trova in un'area ridondi o per altri motivi allora io ho anche la garanzia del fatto che tutti i valori intermedi che ho letto erano originali erano quelli genuini così come il blocco da cui sono partito se il blocco da cui parto mi dà alla fine un risultato diverso rispetto a quello che sta nella radice o il blocco è stato alterato o è stato alterato in merkel 3 in tutti i casi io non mi posso più fidare dell'integrità del file ok e in questo modo il numero di applicazioni della funzione hash da fare è pari al logaritmo pari al numero di livelli dell'albero quindi il logaritmo in base di l'albero ok in gergo i sibling che mi servono per verificare l'integrità del mio nodo quindi nel nostro esempio y4 9 e 14 vengono chiamati merkelproof perché sono la sequenza di hash che mi permettono di dimostrare che il mio blocco è integro esatto sono quelli che io vado a recuperare dal merkel 3 quando io perché il merkel 3 pubblico a qualche parte quindi io so qual è il cammino che devo seguire e quindi so quali sono i i nodi dell'albero da cui pescare le informazioni che mi servono per ricostruire la prova e questo è un modo estremamente efficiente di gestire l'integrità di file di grandi dimensioni o comunque di file anche se non di grandi dimensioni ma comunque di file da cui devo estrarre solamente alcuni alcuni dati o anche semplicemente un modo per limitare il più possibile la quantità di dati da proteggere perché con questo schema qui come abbiamo appena detto alla fine l'unica informazione che io devo veramente proteggere è il valore della radice quindi lo sforzo che devo fare per garantire l'integrità è ridotto a un unico nodo tutto il resto diventa una conseguenza di quello quindi tornando indietro alla definizione dell'algoritmo qui vedete la descrizione di come si costruisce di come si costruisce il Merkel tree usando una qualunque funzione hash purché come vedremo nei risultati che adesso riassumeremo sia una funzione hash resistente alle collisioni chiaramente quindi come abbiamo visto nel nostro esempio se volete dimostrare che un certo blocco fa parte del file e occupa una certa posizione voi non dovete fare altro che ricostruire il cammino che da quel blocco porta alla radice eseguire tutte le operazioni che coinvolgono i nodi sibling e il numero totale di operazioni che farete sarà pari lo deray in base 2 di n dove n è la dimensione del file e c'è un teorema appunto che ci dice che se la funzione hash che avete usata è resistente alle collisioni allora un avversario non può non è in grado di falsificare una prova in altre parole un avversario non è in grado di convincervi che un blocco fa parte del file quando in realtà questo non è vero perché per convincervi l'avversario dovrebbe costruire una merkel proof falsa che dimostra che quel blocco fa parte del file ma tecnicamente per riuscire a fare questa roba dovrebbe conoscere delle collisioni sulla funzione hash quello è l'unico modo perché dovrebbe sapere che c'è magari un altro blocco diverso da quell'originale che collide con il blocco originale sul hash e quindi da lì davanti avreste una proof ricostruita dalla foglia alla radice che funziona quello è l'unico modo per riuscire a imbrogliare ma se appunto la funzione è di tentare collisioni l'attaccante questa opportunità non ce l'ha il merkel tree lo potete usare anche per dimostrare il contrario cioè che un certo blocco non fa parte del file io vi do un blocco che non fa parte del file e voi potete verificare che effettivamente quel blocco non è parte del file e io che ne so vi dico guarda questo questo è il quinto blocco del file fidati e voi che cosa fate voi non dovete far altro che dimostrare seguendo due cammini che non ci sono voi dovete individuare chi sarebbero i due nodi i due blocchi vicini di quello che vi è stato dichiarato essere un blocco del file ok uno vi dice questo è il quinto blocco del file benissimo allora voi cosa fate voi andate a seguire i cammini relativi al quarto o al sesto ok nel sul merkel tree e seguendo questi cammini sul merkel tree verificate se questi due blocchi sono consecutivi cioè non hanno blocchi tra loro in mezzo e se riuscite a dimostrare questo e lo potete fare allora avrete dimostrato che il blocco che vi è stato spacciato come il blocco del file in realtà non può essere un blocco del file e anche qui c'è un teorema che dice che se la funzione esce resistente alle collisioni un avversario non è in grado di falsificare una dimostrazione convincendovi che un certo blocco non fa parte del merkel tree quando in realtà fa parte del merkel tree quindi l'avversario non può falsificare nessun tipo di dimostrazione non può convincervi né che un blocco fa parte quando in realtà non fa parte né che un blocco non fa parte quando in realtà fa parte ok quindi nessun tipo di falsificazione è producibile se la funzione esce resistente alle collisioni quindi se ad esempio usiamo SHA-256 bene per chiudere facciamo una panoramica di proprietà tipiche delle funzioni hash perché noi fino a noi abbiamo considerato uno solo che è la resistenza alle collisioni ok il cui atta game l'abbiamo visto ieri ed è una atta game che dice che l'avversario vince se trova due messaggi che collidono cioè che hanno lo stesso digest è l'unica proprietà questa no ce ne sono altre due importanti che adesso andiamo a definire e che poi metteremo in relazione l'una con l'altra ok magari alcune di queste vi ricordano proprietà che avete già visto in passato in altri contesti dove si utilizzano le funzioni hash una proprietà è la non invertibilità l'idea non invertibilità è stata semplice lo dice il termine stesso cioè una funzione non invertibile intuitivamente se preso x è facile calcolare f di x ma se invece io vi do y che è l'output della funzione f voi non siete in grado di risalire a x ecco questa è l'idea della non invertibilità che è diverso dalla resistenza alle collisioni è una proprietà diversa la definizione formale dice appunto che una funzione h è non invertibile se prendete l'output della funzione rispetto a un certo input m quindi prendete il digest t ok quindi da m calcolate t e questo è facile lo sappiamo dove m è scelto casualmente bene la definizione dice che è difficile a partire da t trovare un m primo che è la sua immagine che potrebbe essere un m stesso o un m diverso può anche essere diverso perché ci sono le collisioni lo sappiamo quindi in ogni caso quello che dice la definizione è che se io vi do a t è difficile risalire da t a un qualunque messaggio di cui t è il digest quindi come dicevamo prima di fatto la funzione non non riuscite a invertirla difficile che cosa vuol dire difficile lo dobbiamo definire rispetto ad un attack game come al solito lo abbiamo sempre fatto quindi questo è il motivo per cui vedete l'attack game che l'avversario deve cercare di vincere in questa maniera riusciamo a formalizzare il concetto di difficoltà ok e come funziona l'attack game allo solito abbiamo un challenger che sceglie casualmente un messaggio e calcola il suo digest ok dopodiché manda il digest all'avversario ok l'avversario che cosa fa cerca un qualunque messaggio il cui digest sia t quindi in pratica cerca di invertire la funzione hash partendo da t e cercando di risalire a qualunque messaggio che possa averlo generato il vantaggio dell'avversario è la probabilità di riuscirci quindi di trovare questo m' e la proprietà di non invertibilità computazionale ci dice che una funzione h è non invertibile one way se il vantaggio dell'avversario dal solito è trascurabile per tutti gli avversari efficienti è chiaro che il fatto che questa proprietà è diversa rispetto alla resistenza alle collisioni perché qui il compito dell'avversario è dato un digest trovare un messaggio che lo genera nella resistenza alle collisioni invece il compito dell'avversario era cercare due messaggi che collidano infatti nella resistenza alle collisioni non c'è il challenger in ballo perché non c'è nulla che il challenger debba scegliere è l'avversario che deve cercare due messaggi qualunque che collidano qui invece il challenger serve perché il challenger che sceglie di riflesso il digest su cui deve lavorare l'avversario ok quindi sono due proprietà differenti poi vedremo legate come l'altra proprietà interessante per le funzioni hash senza chiave è la cosiddetta resistenza di tipo second frame age il nome è bruttissimo però non ho trovato una traduzione in italiano che abbia un qualche senso l'idea è ancora diversa rispetto alle precedenti ed è questa voi scegliete un qualunque messaggio è molto è molto simile alla proprietà di resistenza alle collisioni perché l'idea è voi scegliete un messaggio qualunque e il compito dell'avversario è trovare un altro messaggio che collide con quello che avete scelto voi allora voi scegliete un messaggio voi il challenger scegliete un messaggio l'obiettivo dell'avversario è trovarne un altro che collide con quello che avete scelto voi infatti vedete l'attack game descrive esattamente quello che ho appena detto il challenger sceglie randomicamente un messaggio e lo fa vedere all'avversario l'avversario ne cerca il compito dell'avversario è cercarne un altro diverso che però collide con quello che ha scelto il challenger e al solito il suo vantaggio è la probabilità di riuscirci e al solito la proprietà in questo caso di second primed resistance dice che H è resistente in questo senso se il vantaggio dell'avversario è trascurabile però è un'avversario difficile ok ora perché queste diverse tre proprietà cerchiamo di calarle in un contesto pratico in un contesto reale poi vedremo qual è il legame tra le tre proprietà quindi abbiamo detto la non invertibilità abbiamo detto questa questa proprietà qui e abbiamo detto la resistenza alle collisioni ok ora rispetto a queste tre definizioni prendiamo in considerazione uno scenario classico poi cerchiamo di capire quale delle tre più si avvicinano al tipo di scenario che ci interessa voi avete al solito avete Alice avete Bob e avete l'avversario che sta in mezzo ok Alice manda un messaggio e il suo tag a Bob ok il tag per garantire l'integrità del del messaggio dopodiché Bob quindi Alice fa la segnatura applica la segnatura M per tirare fuori T quindi usa un Mac di qualche tipo di quelli che abbiamo visto Bob cosa fa? Bob fa la verifica quindi vede M vede T applica l'algoritmo di verifica e vede se effettivamente T è un tag di M se lo è lo accetta e siamo tutti contenti che il messaggio è arrivato in integro a Bob per vincere l'attacca game contro il Mac Yves l'avversario cosa deve cercare di fare? deve cercare di falsificare un tag ok quindi deve riuscire a trovare una coppia M' T' T' che Bob possa accettare ok senza che sia la coppia M' T' sia stata generata da Alice ovviamente quindi se Yves è in grado di generare una coppia M' T' ma è vista prima che però Bob accetta allora vince l'attacca contro il Mac e quindi l'integrità salta ok quindi questo è il compito di riuscire a fare questo giochino ok ora delle tre proprietà che abbiamo visto rispetto a questo scenario quali sono quelle che in qualche modo ci garantiscono che l'avversario non riesce a trovare M' T' primo senza chiave senza chiave sì sì sicuramente la non invertibilità no perché se la funzione hash fosse invertibile l'avversario si sceglie un qualunque T' T' inverte la funzione quindi calcola un M' cui dà gesto T' ecco che si è confezionato la sua coppia falsificata la prende e la manda Bob quindi chiaramente la funzione deve essere non invertibile perché altrimenti l'avversario potrebbe fare sto giochino mi scelgo un qualunque tag lo inverto potrei addirittura prendere lo stesso T invertirlo trovando un M' diverso da M e mandare inoltrare tutto al Bob al Bob arriva M' T e non capisce che l'integrità è saltata ok ma anche questa proprietà qua no è importante perché perché se questa proprietà non vale di nuovo cosa può succedere che l'avversario vede passare M ok se questa proprietà non vale l'avversario è in grado di calcolare un M' diverso da M associato allo stesso T quindi sostituire M con M' e far arrivare a Bob M' T se non vale se questa proprietà ok quindi è ragionevole pensare che la non invertibilità e la second pre-major resistance debbano valere perché altrimenti nell'attack game contro il Mach l'avversario ha vita facile in questo caso particolare qui la resistenza alle collisioni in questo caso specifico è meno rilevante ok perché magari che ne so l'avversario trova una collisione che riguarda M1 ed M2 ok no supponiamo che la funzione non sia resistenza alle collisioni quindi vuol dire l'avversario trova un M1 ed M2 che collidono ma se ne fa qualcosa deve sperare che a un certo punto Alice voglia mandare a Bob o M1 o M2 in modo tale che lui possa sostituirlo con l'altro ok quindi in questo contesto particolare qui la resistenza alle collisioni è meno rilevante mentre invece non sono di più le prime due proprietà qual è la relazione di inclusione fra le tre proprietà adesso andiamo a vedere la relazione di inclusione è questa e spiega il motivo per cui in realtà nella pratica quello che si fa verificare è la resistenza alle collisioni perché la resistenza alle collisioni implica le altre due cioè la proprietà più forte poi c'è la second pre-major resistance che a sua volta implica la non invertibilità quindi queste relazioni di inclusione vi dimostrano ad esempio che non basta mai usare una funzione non invertibile bisogna che sia second pre-major resistance quantomeno per il motivo che dicevamo adesso nell'esempietto dell'attack game contro un muck poi è chiaro che se è resistente alle collisioni è ancora meglio perché questa è la più forte di tutte ed è il motivo per cui se si trovano delle collisioni in una funzione hash quella si prende o la si butta via perché vuol dire che prima o poi può essere bucata in contesti reali ok l'ipotesi ci sono delle ipotesi del teorema non vale in assoluto che dicono appunto che h deve essere una funzione hash stata chiave definita su un dominio che è molto più grande del codominio quanto più grande vedete la relazione il il il dominio il dominio oddio ci ho perso il dominio deve essere maggiore uguale del codominio moltiplicato per una quantità superpolita poi se vi ricordate il codominio a sua volta deve essere superpolita ok sotto queste condizioni vale questa catena di inclusioni però ripeto per l'implementazione dei mac quindi per essere sicuri rispetto all'attack game dei mac che abbiamo visto a suo tempo nella gran maggioranza dei casi basta questa per il motivo che dicevamo prima sull'esempietto ok poi a livello teorico ripeto essendo questa la più forte chi fa criptanalisi cerca collisioni sulle funzioni hash perché sa che se ne trova fa saltare tutte le altre proprietà e questo chiude il discorso alle funzioni hash e anche il discorso relativo ai ai mac quindi l'integrità quindi abbiamo chiuso il discorso su confidenzialità da parte e l'integrità dall'altra poi vi faccio vedere uno schemino riassuntivo sull'integrità così come aveva fatto sull'interno confidenzialità per prima di chiudere però volevo mostrarvi come facciamo spesso un esempio pratico ok e questa volta vado a ripescare un esempio pratico che avevamo già visto in passato che era il commitment vi ricordate l'esempio del commitment che avevamo risolto con il PRG cioè l'esempio era due Alice e Bob che si vogliono mettere d'accordo sul risultato del lancio di una monetina in base al quale decidere che ne so se andare al cinema vedere un film o andare a mangiare una pizza ok solo che se la monetina la lanciamo davanti agli occhi vediamo il risultato se siamo al telefono siamo su web ti lancia la monetina allora lì l'idea è ognuno dei due sceglie un valore random si si scambia questo valore random si fa l'oxor dei due e il risultato è il risultato del lancio della monetina virtuale ok solo che il problema qual era chi per primo comunica all'altro il proprio bit senza naturalmente poter essere fregato dall'altro perché è chiaro che se uno dei due vede il bit dell'altro cambia il proprio per in maniera tale da vincere e poi glielo fa vedere quindi per evitare questo la soluzione che avevamo proposto è basata sul commitment cioè ovvero io non pubblico il mio bit ma pubblico un commitment su quel bit e se dopodiché una volta che ho visto il bit dell'altro io svelo all'altro come scoprire il commitment è come una sorta se vogliamo di di forziere chiuso a chiave all'interno del quale chiudo il mio bit no passo il forziere all'altro quando è ora di svelare il commitment gli do la chiave lui apre il forziere lo apre e trova il bit e quella volta avevamo implementato questa idea del forziere usando il prg avevamo visto che le due proprietà fondamentali l'hiding e il binding l'hiding il fatto che chi riceve il forziere non riesce a dedurre nulla sul suo contenuto il binding il fatto che una volta che ho trovato il forziere non ne posso più cambiare il contenuto ok quindi due proprietà a tutela di entrambi e avevamo visto che il l'hiding valeva in un modo il binding valeva nell'altro cioè uno dei due era valeva incondizionatamente della case del binding l'altro invece valeva computazionalmente riproponiamo lo stesso problema del commitment usando però le funzioni hash non i prg e vedremo che otteniamo un risultato che ha le proprietà opposte cioè l'hiding vale incondizionatamente il binding invece computazionalmente in un contesto leggermente diverso invece dell'altro della monetina virtuale ho preso un altro contesto che è quello delle aste online quelle a busta chiusa sapete come funziona un'asta a busta chiusa ognuno mette la propria offerta in busta ok fa un'unica offerta la chiude in busta dopodiché si aprono le buste di solito vince chi ha fatto l'offerta più alta ok questa è l'idea come implementare questa cosa online quindi garantendo che l'offerta chiusa della busta non sia garantisca l'hiding perché io pubblico la mia busta se voi riuscite a fare che t'analisi della busta e a vedere cosa c'è dentro esatto mettete centesimi più nella vostra busta e vincete l'asta quindi dovete garantire l'hiding ma dobbiamo anche garantire il binding io una volta che ho messo l'offerta in busta l'ho pubblicata non è che quando vi do la chiave per aprire la busta magicamente dentro la busta c'è un valore diverso rispetto a quello che c'è ho messo dentro io quindi deve avere anche il binding un tentativo abbastanza ovvio che può venire in mente di fare per risolvere questo problema consiste nel nel cifrare l'offerta ok quindi uso un cifrario qualunque come commitment cioè se questa è la mia offerta io non pubblico la mia offerta chiaramente ma pubblico la cifratura dell'offerta poi quando è il momento di aprire le buste svelo la chiave e quindi potete dal ciphertext risalire al messaggio qual è il problema questo schema garantisce l'hiding sicuramente perché se io non cifrare sicuro voi analizzandoci non deducete nulla per quanto riguarda m quindi l'hiding vale purtroppo non vale il binding perché non vale il binding perché i blocks alpha comunque cifrari semanticamente sicuri a differenza delle funzioni hash in molti casi sono invertibili quindi non sono one way non sono second major resistant quindi questo cosa vuol dire vuol dire che anche se io ho pubblicato c come cifratura di m con una certa chiave k ho comunque la possibilità di trovare un'altra chiave diversa k' da k tale per cui la decifratura usando k' di c mi dà un m' diverso da m quindi vuol dire che le proprietà che abbiamo visto prima la resistenza alle collisioni la second pre-mails resistono non valgono e se riesco a fare questa qui questa cosa qui posso imbrogliare perché metti che io abbia pubblicato abbia messo in busta l'offerta di mille cifrata con una chiave k trovo e ottengo un certo cifrata trovo un'altra chiave poi magari scopro che l'offerta più alta era stata a 1050 quindi io perdo e io devo provare ad alzarla a 1100 o quel che è se io trovo una chiave diversa da k tale per cui la decifratura del cifrata che io ho pubblicato mi dà come risultato 1100 o 1051 o 1060 quel che è ecco che io vinco quindi il binding non vale qua trovate un esempio di cifrari semanticamente sicuri che non garantirebbero il binding l'esempio guardate come esempio addirittura ho preso un cifrario neanche semanticamente sicuro ma ho preso un cifrario perfetto ideale perché ho preso il one time pad e di conseguenza la stessa cosa vale con il cifrario a flusso ok quindi immaginate di usare il one time pad questa è la vostra offerta scegliete una chiave pubblicate come salvertex loxor di 2 ok a posteriori quando dovete svelare la chiave per decifrare c che cosa fate scegliete un qualunque altro messaggio m primo diverso da m che vi fa comodo per vincere ok poi calcolate a ritroso la chiave che vi serve per associare la m primo che avete scelto a c è facile perché loxor è facilmente invertibile da questo punto di vista voi prendete c fate loxor con la m primo che vi fa comodo ed ecco che ottenete k2 a questo punto invece di pubblicare k1 pubblicate k2 chi fa la decifratura alterrà m primo invece che m ed ecco che riuscite a imbrogliare a vostro piacimento fondamentalmente perché questo giochino lo potete fare per qualunque m primo il vostro ciphertext può essere ricondotto a qualunque plaintext cambiando la chiave ok e quindi non possiamo usare un cifrario semanticamente sicuro idea usiamo la funzione hash ok il commitment funziona come vedete qui questo è il modo in cui si calcola il commitment del messaggio quindi come si mette il messaggio in busta ok cosa si fa si sceglie un nonso quindi casuale valore casuale si concatena il nonso con la vostra offerta e si calcola l'ash ok dopodiché la busta che voi pubblicate è questa il digest che avete ottenuto insieme al nonso quando bisognerà fare la verifica voi non dovete fare altro che svelare la vostra offerta e chi deve fare la verifica semplicemente che cosa fa prende la vostra offerta la concatena con il nonso che era già noto calcola l'ash e fa il confronto tra l'ash calcolato e quello che era stato pubblicato se coincidono m viene accettato quindi molto banalmente domanda secondo voi perché aggiungo un nonso e non faccio semplicemente l'ash del messaggio perché se H soddisfa tutte le proprietà che abbiamo visto prima non è invertibile quindi da C il messaggio M non lo recupero e la verifica la potrei comunque fare nel momento in cui pubblico M per quale motivo aggiungiamo un nonso ricorda che le funzioni hash di solito sono anzi di solito sono sempre esatto le funzioni hash sono deterministiche ok se due persone fanno la stessa offerta le due buste sono uguali no perché il digest sarà lo stesso e quindi ancora prima di aprire le buste noi sappiamo che due persone hanno fatto la stessa offerta invece con l'aggiunta del nonso anche se voi fate la stessa offerta sceglierete due nonso diversi e quindi otterrete due digest diversi ok quindi è quell'elemento random che di fatto rende probabilistico il risultato e noi abbiamo bisogno che sia probabilistico ok però è che lo schema è estremamente banale alla fine dei conti ho usato ho usato una funzione hash non è di più le proprietà l'hiding vale incondizionatamente indipendentemente dal tipo di avversario che prendiamo in considerazione quindi questo significa ok qualunque sia l'avversario la proprietà di hiding vale vale perché per ogni coppia di messaggi diversi tra di loro le distribuzioni di probabilità associate ai digest sono indistinguibili questa è come caratteristica delle funzioni hash quando queste sono resistenti alle collisioni ok chiaramente r deve essere super poli perché di nuovo devo garantire che devo avere una proprietà trascurabile che due agenti scelgano lo stesso nostro ok però posto questo se la funzione non esiste alle collisioni qualunque tipo di avversario non è in grado di distinguere un digest dall'altro e il binding invece vale computazionalmente vale computazionalmente perché io riesco a violare il binding solo se trovo una collisione no perché devo invertire per imbrogliare dovrei invertire il digest dal digest e trovare un messaggio diverso che collida quindi devo trovare una collisione ma se la funzione è resistente alle collisioni la probabilità di trovare una collisione è traspurabile per tutti gli avversari efficienti quindi è una proprietà che vale da un punto di vista computazionale rispetto agli avversari efficienti quindi l'hiding vale rispetto a tutti il binding vale rispetto agli avversari efficienti e la cosa divertente è che rispetto alla soluzione che avevamo visto qualche tempo fa che usavi PRG lì era al contrario l'hiding valeva computazionalmente il binding invece incondizionatamente no e come vi avevo anticipato c'è una sorta di trade off tra hiding e binding per cui non esiste una soluzione che soddisfa incondizionatamente tutte e due le proprietà e quindi dobbiamo privilegiare una alla privilegiare poi chiaro che in ogni caso anche la sicurezza computazionale è più che ragionevole perché si rapporta agli avversari efficienti quindi tutto sommato le cose non vanno poi così male però dal punto di vista teorico è interessante vedere che la soluzione ottima rispetto a entrabile proprietà non esiste va bene direi che possiamo chiuderla qua la prossima volta vi faccio vedere lo schemino reassuntivo perché quando abbiamo finito di parlare della confidenzialità vi avevo fatto vedere lo schema di tutte le primitive che avevamo visto di tutte le costruzioni che avevamo usato per integrità facciamo la stessa cosa quindi vi farò vedere lo schema che riassume tutte le primitive che abbiamo introdotto e le costruzioni che abbiamo usato per garantire l'integrità quindi da una parte abbiamo affrontato il problema della confidenzialità dall'altro quello dell'integrità rimane da metterli insieme e quindi finalmente arriveremo a definire protocolli che contemporaneamente garantiscono integrità e confidenzialità insieme ok va bene quindi ci vediamo la prossima settimana vi skillful propria Grazie a tutti.