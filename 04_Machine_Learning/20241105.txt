benissimo allora buongiorno e bentornati alle lezioni riprendo brevemente una cosa che era rimasta in sospeso su questa su questa figura ho controllato questo in realtà il numero di iterazioni non è il numero di epoche ma prenderlo dal libro dal libro di testo e sono andato un po controllare questo dovrebbero essere addirittura le prime dieci iterazioni della prima epoca quindi in realtà è contrariamente a quello che avevamo pensato questo potenzialmente venendo conto che può essere fonte di confusione allora ho aggiunto un'altra slide che trovate nel nel su blended learning e diciamo così riepilogo brevemente quello che è stato detto questo non è una slide diciamo non è un run reale su un dataset reale ma è un esempio esplicativo ulteriore quindi abbiamo una funzione di costo diciamo un prototipo che può essere scomposta in tre somme come se fossero diciamo tre batch mettiamo così ok e quindi c'è l'ottimizzazione la curva nera è un'ottimizzazione full batch ok quindi l'intero batch viene caricato in memoria la stessa funzione viene minimizzata ipotizzando invece la suddivisione in tre batch quindi con dei mini batch e vedete che qui quello che succede è che durante la prima epoca abbiamo la minimizzazione del primo batch quindi l'aggiornamento del pesi del lettore del gradient descent sul primo sul secondo sul terzo e in generale l'andamento è questo cioè un andamento che durante le varie e alla fine di una epoca vi ricordo abbiamo visto tutto il dataset che poi spesso volentieri viene anche rimescolato casualmente per evitare di andare sempre a prendere nella suddivisione gli stessi campioni come primo come secondo come terzo batch e di solito quello che succede che il processo di convergenza è migliore ok quindi questo è quello che siamo da qui qui non era chiarissimo perché era solo un certo numero di iterazioni relative a un'epoca questo è quello che stava fare probabilmente non era non era così significativo ecco questo già anche se è sintetico nel senso che è stato un grafico creato ad hoc per per spiegare però insomma più spiegativo allora detto questo invece torniamo a la parte riguardante l'apprendimento non supervisionato quindi non super base learning e rivediamo brevemente alcune delle cose che avevamo anticipato ieri abbiamo parlato di riduzione della dimensionalità e è la prima cosa di cui ci occuperemo problema del dell'apprendimento non supervisionato è un problema per certi versi più sfumato perché mentre nell'apprendimento con supervisione è chiaro che cosa abbiamo abbiamo l'input e abbiamo anche la risposta l'output io ho un dataset in cui conosco le etichette quindi so che a partire dalle feature del dataset io voglio costruire un modello che mi permetta di ricavare la risposta la risposta può essere una variabile di regressione può essere una risposta di classificazione un output di regressione ma noi la conosciamo qui non sappiamo di preciso cosa vogliamo fare e quindi anche più difficile la valutazione stessa modello di machine learning non supervisionato dobbiamo anche inventarci da zero come andare a capire quanto è bravo a svolgere quel determinato task perché dobbiamo capire che cosa vuol dire qui siamo partiti per esempio dalla riduzione della dimensionalità poi vedremo il clustering e in ambo i casi non è così banale andare a vedere se quello che ci serve effettivamente quello quello che fa il modello soprattutto per esempio lo vedremo nel clustering allora qui avevamo visto un po di di cose abbiamo parlato di di spanning set di insieme ricoprente o base quindi abbiamo detto un punto lo possiamo rappresentare in un opportuna base tramite una combinazione lineare dicevo tramite un'opportuna combinazione lineare con dei pesi che sono i coefficienti della combinazione lineare dei vettori di quella base quindi qualunque punto lo possiamo rappresentare una base che noi abbiamo scelto ovviamente dobbiamo avere il numero di vettori della base deve essere pari alla dimensione dello spazio in cui ci muoviamo e tutti i vettori di questa base devono essere linearmente indipendenti e allora quel punto possiamo scrivere in maniera compatta il prodotto tra la matrice c che la matrice che contiene tutti i vettori della base ok quindi vi ricordo che c è la matrice che contiene tutti i vettori della base c1 c2 ck v doppio è chiaramente il vettore dei pesi e x è il nostro è il nostro vettore che rappresenta il nostro punto ok poi l'altra cosa che avevamo fatto eravamo arrivati a dire ok se scriviamo il nostro punto lo esprimiamo con questa notazione quello che possiamo fare è andare a costruire una funzione di costo e le loss function sono l'elemento essenziale di qualunque modello di machine learning e quello che possiamo fare è andare a costruire una funzione di costo ad esempio una funzione di costo ai minimi quadrati in cui l'obiettivo è minimizzare la funzione di costo per ricavare w1 w2 wp che sono i vettori dei pesi per ognuno dei punti del dataset di addestramento chiamiamolo a questo punto che minimizza questa quantità che è il costo ai minimi quadrati cioè prendiamo la rappresentazione nel nuovo nel nuovo sistema di riferimento questo è il nostro punto originale vogliamo minimizzare questo cioè per ogni punto idealmente vogliamo che questo sia 0 più 0 più 0 più 0 ok questo abbiamo detto può essere minimizzato tramite qualunque algoritmo di ottimizzazione locale quindi prendete un algoritmo tipo discesa del gradiente e riuscite a minimizzarlo in particolare questa funzione di costo ai minimi quadrati l'abbiamo già incontrata quando abbiamo affrontato i problemi di regressione lineare perché abbiamo visto che era proprio esattamente una funzione di costo in cui quello che cambiava era ovviamente il significato qui avevamo il punto e qui avevamo chiaramente quello che era legato alla al problema della regressione lineare quindi avevamo il prodotto tra x trasposto vi ricordo il vettore di qui qui però è del tutto analogo e in maniera del tutto analoga quella che si può fare è utilizzare come vi dicevo un metodo di scelta del gradiente oppure provare a risolvere in forma chiusa questo è un altro di quei casi perché esattamente poi sempre lo stesso in cui vi dicevo il problema di ottimizzazione ha una forma chiusa analitica che sono le cosiddette equazioni normali cioè quello che si può dimostrare adesso non vi diciamo non ve lo faccio per esteso però di fatto quello che si può fare è una cosa di questo genere se voi andate a prendere questa funzione di costo e la scrivete in questo modo andate a scrivere allora anzitutto qui diciamo quello che viene fatto è ridurlo alla risoluzione di un sistema di equazioni lineari cioè quello a cui si arriva che sono le equazioni normali l'abbiamo visto a suo tempo quando abbiamo fatto le questioni allora vi ricordo che un sistema di equazioni lineari solitamente a per x uguale a b ok dove a è una matrice di coefficienti b è il vettore dei termini noti e x le incontri ok qui quello che fate è semplicemente prendere questo termine diciamo sarebbe a per x ok e questo è il termine b allora minimizzare questo significa andare a minimizzare questa norma per x meno b ok che io a sua volta posso scrivere come a x meno b per per per x meno b ok scrivendolo così quello che si può dimostrare è che voi potete cosa facciamo a questo punto questa è un singolo termine di questa funzione di costo quello che voi potete fare è andare a prendere il gradiente di questa funzione di costo lo mettete uguale a 0 e quello che trovate è che il minimizzatore minimo di tutto questo è ve lo scrivo qua il gradiente della funzione di costo e il g uguale a 0 se e solo se a per t per a per x è uguale b quindi se voi vi scrivete gradiente della funzione di costo e lo ponete uguale a 0 ottenete questa ma questa significa che a per x deve essere uguale a b cioè dovete risolvere un sistema di equazioni lineari questo sistema di equazioni lineari è esattamente la soluzione delle equazioni cosiddette normal equation tutto questo per dirvi che cosa che se avete un problema di questo tipo con un po' di matematica ma adesso non ci interessa neanche più di tanto anche qui entrare nel dettaglio riuscite a costruire un sistema di equazioni lineari vedete questa è la matrice A questo è il vettore delle vostre incognite e questo è il vettore B risolvendo questo si può dimostrare che è la soluzione in forma chiusa tramite le equazioni normali per tutti i punti lo possiamo risolvere lo possiamo risolvere per via numerica e ottenere WP quindi WP star che è l'asterisco viene chiamato a quel punto codifica del punto rispetto alla base specificata da C quindi il problema è ho un punto che vive in uno spazio vettoriale lo voglio trasformare nelle coordinate come un cambio di sistema di riferimento vi dicevo ieri come se io avessi una rotazione ad esempio si fa nei corsi per esempio di fisica ok e lo posso fare utilizzando però qui è interessante lo stesso framework teorico di tutti i modelli di machine learning che abbiamo visto in questo caso quello del metodo della funzione di costo con i minimi quadrati e questa può essere risolta o tramite direttamente un metodo di ottimizzazione locale quindi discesa del gradiente oppure in forma analitica la soluzione è la soluzione di un sistema lineare che a sua volta poi viene risolto per via numerica cioè tipicamente si prende e si mette dentro non è che uno lo risolve a mano perché chiaramente non sarà quasi mai un due a meno che non sia due equazioni in due incognite o tre equazioni in tre incognite ma se abbiamo un sistema molto più grande ovviamente lo risolviamo al calcolatore la combinazione lineare dei vettori della base di ogni punto viene chiamata decodifica ok? mentre il vettore dei pesi viene chiamato codifica quindi come vi dicevo ieri quello a cui siamo arrivati è queste sono le equazioni normali soluzioni e quello che possiamo dire è che abbiamo che xp viene codificato diciamo in coding in wp e a sua volta wp poi viene decodificato come cwp uguale a xp allora uno si potrebbe domandare perché dobbiamo fare questo passaggio lo codifichiamo poi lo ridecodifichiamo perché questo è utile per capire in realtà cosa succede quando anziché passare da uno spazio che ha dimensione n a uno spazio che ha dimensione n cosa succede quando noi andiamo a diminuire la dimensione dello spazio di arrivo cioè quando qui dentro andiamo ad avere una rappresentazione di quel punto con meno feature quindi vi dicevo ieri è un problema che assomiglia per certi versi anche alla compressione e questo è un qualcosa che viene molto fatto diciamo molto spesso in tantissimi modelli di machine learning per diversi motivi perché si guadagna in termini di complessità computazionale cioè gli algoritmi che lavorano su numero di feature inferiori spendono meno tempo in computazione vi permette di fare un'analisi esplorativa dei dati cioè in qualche caso riuscite a diminuire in maniera sensata la dimensionalità fino a due o tre dimensioni che significa che riuscite a creare dei plot dei grafici del dato e questo viene fatto però quello che vedremo in questa in questa diciamo nel seguito di questa lezione sono delle tecniche di riduzione della dimensionalità cosiddette lineari cioè partiamo dal presupposto di andare a rappresentare il punto come combinazione lineare di vettori di una base tenete presente che questo meccanismo encoding andare in uno spazio che viene chiamato anche spazio latente spesso comunque è uno spazio a dimensione ridotta e poi decodifica è anche la base di quelli che vengono chiamati autoencoder non lineari che sono dei modelli di deep learning basati sulle reti neurali che fanno esattamente questa cosa ma fanno una proiezione che non è una proiezione lineare è una proiezione non lineare perché l'obiettivo è costruire una trasformazione del punto dallo spazio a n dimensioni in uno spazio a più bassa dimensione ora quello che noi vedremo è lo facciamo tramite una proiezione lineare cioè una combinazione lineare opportuna tramite i coefficienti di WP secondo una base che noi decidiamo in realtà poi vedremo non necessariamente la dobbiamo decidere a priori ma ne parliamo tra un po' però io posso anche proiettarlo secondo una tecnica che non è una combinazione lineare e questo è quello che fanno poi però su questo diciamo ci torneremo sopra al momento opportuno intanto andiamo avanti un passo alla volta e qui quindi abbiamo rappresentazione di un punto in uno spazio a n dimensioni rappresentazione per a fetta come una sorta di cambio di base fin qui abbiamo visto questo che posso impostare anche come un problema ai minimi quadrati ci siamo fin qui? allora questo è un esempio questo è un esempio in cui abbiamo due dimensioni quindi abbiamo tanti punti in due dimensioni va bene? abbiamo tanti punti in due dimensioni questi punti in due dimensioni sono rappresentati a sinistra nello spazio originale quindi abbiamo due feature e per ogni punto abbiamo uno scatter plot che vi dice la feature x1 per questo punto vale quasi 0 e vale meno 2 per questo punto vale 5 e invece meno 6 virgola qualcosa la feature x2 e così via quindi questo scatter plot vi permette di costruire una rappresentazione del dato che vive in questo spazio a due dimensioni dopodiché vedete in questo spazio a due dimensioni sono state scelte due vettori che sono queste due frecce rosse che rappresentano una nuova base una nuova base vuol dire un nuovo sistema di riferimento quindi la domanda è come faccio a passare dallo spazio in cui è rappresentato questo dato x1 x2 che è lo spazio dove vivono i dati originariamente ha un nuovo spazio in cui io voglio esprimere queste feature come coordinate di questo sistema di riferimento e qui abbiamo due vettori vedete questa c è caratterizzata da un vettore c1 e un vettore c2 ok è il primo vettore c1 quindi un vettore che ha coordinate 2 1 e vedete questo è il coordinate 2 1 con questo vettore qua e poi abbiamo un altro vettore che ha coordinate 1 2 e quest'altro vettore qui va bene quindi questi due vettori qui rappresentati in rosso sono i nuovi vettori che definiscono il mio nuovo sistema di riferimento se io voglio fare questo cambio di coordinate voglio passare quindi da come vi dicevo prima lo che voglio fare è prendere xp e andarlo a rappresentare nel mondo delle coordinate c tramite una combinazione lineare che è dettata dai pesi quindi l'obiettivo è ricavare il vettore dei pesi e questo può essere fatto minimizzando quella funzione ai minimi quadrati che vi ho fatto vedere prima e andando a trovare per ognuno di questi punti un vettore vwp quindi per esempio questo punto avrà un vettore vwp che a sua volta è specificato chiaramente da due parametri perché è un vettore bidimensionale perché in questo caso c è una matrice 2x2 che poi verrà moltiplicata per un vettore 2x1 e il risultato è un vettore 2x1 ok? questo come dimensione ok come faccio a trovare questa coordinata che vale ma meno 5 virgola qualcosa e quest'altra che vale 6 virgola qualcosa la soluzione del problema ai minimi quadrati su tutti i punti e il risultato è che in questo nuovo mondo vedete questi sono i dati originali qui ho i dati codificati ho una nuova rappresentazione perché ognuno di quei punti lo rappresento in un nuovo sistema di coordinate quindi questo è un punto che di qua corrispondeva ad esempio non lo so a quest'altro mettiamo ma non sono sicurissimo probabilmente si ok? ok? e così per tutti i punti ora qui quindi posso rappresentare questo nuovo vettore quindi w1 w2 che sono le coordinate del mio vettore w p quindi questa sarà relativa al punto p rappresentano le coordinate quindi le feature in questo nuovo spazio ok? tutto questo qui che cosa ci dice? ci dice che ovviamente io posso passare poi di nuovo da questo spazio tornare a quello originale facendo cosa? semplicemente wp ce l'ho c ce l'ho faccio c per wp e ritrovo xp chiaro qual è il giro ora qui viviamo in uno spazio a due dimensioni qui viviamo in uno spazio a due dimensioni quindi di fatto non abbiamo ancora operato quella non abbiamo operato quella trasformazione che ci permette di ridurre la dimensionalità che è il nostro obiettivo però intanto stiamo mettendo le basi per poterlo fare ok? e questo può essere fatto sistematicamente ripeto per tutti i punti e prima di passare a questo volevo un attimo fare un'altra considerazione allora qui vi ho condiviso un'animazione in cui vedete fondamentalmente quello che fa questa animazione è andare a prendere sulla sinistra una griglia di punti e ognuno di quei punti viene trasformato secondo questa nuova base che è una base è una base specificata da questi due vettori e vengono trasformati poi ognuno di questi punti per esempio questo caso e questo punto questo viene trasformato in questo altro mondo in cui ha questa coordinata e questo viene fatto per ogni punto viene ripetuto di questa griglia e il risultato è una cosa di questo tipo quindi questo è di fatto un cambio di coordinate va bene tornando ai nostri esempi invece tornando quindi alla slide che stavamo analizzando e adesso vi ricondivido il risultato è questo quindi fin qui abbiamo detto nessun tipo di di compressione del numero delle feature partiamo da una spazza nelle dimensioni siamo in uno spazza nelle dimensioni le cose le cose diciamo si fanno interessanti quando cominciamo a scendere di dimensioni ma per fare questo dobbiamo fare qualche per rendere questo più semplice qualche passo ulteriore in particolare gli esempi che abbiamo visto erano dei vettori che non erano ortogonali tra di loro allora la domanda che si può fare è che cosa succede se imponiamo come basi di avere dei vettori che sono ortogonali tra di loro e hanno anche norma unitaria quindi vettori ortonormali che sono i vettori per esempio dello spazio cartesiano gli assi x y e z se associamo quelli che vengono chiamati versori quindi sono elementi che hanno lunghezza unitaria e che sono ortogonali uno con l'altro allora se io formo una base con dei vettori che hanno questa caratteristica sono in una situazione di questo tipo in cui ho per ognuno di questi vettori c1 c2 ck vale che cosa? se io prendo c con indice i e c con indice j e faccio il prodotto scalare tra questi due vettori questo vale sempre 1 se i è uguale a j perché il prodotto scalare di un vettore con se stesso è la norma del vettore norma di tipo l2 ok? e in questo caso è unitaria se i è diverso da j il prodotto scalare è 0 sono ortogonali ok? quindi se io vado a prendere ad esempio il vettore che c1 1 0 e c2 0 1 vedete che se faccio c1 rasposto per c1 ottengo che cosa? ottengo un vettore che 1 0 per 1 0 che è uguale a 1 che è la norma di questo vettore c1 stessa cosa per c2 ma se vado a fare c1 prodotto scalare c2 questo è uguale a 1 0 che moltiplica 0 1 che è uguale a 0 ok? ok perché sono ortogonali e hanno norma unitaria quindi sono ortonormali ok? si dice ortonormali ora se io costruisco la mia matrice c a partire da vettori che sono ortonormali quello che succede è che valgono queste quindi se vado a mettere la matrice c trasposta e la moltiplico per c quello che ottengo è la matrice identità cioè una matrice che ha tutti uno sulla diagonale ok a questo punto se io vado ad applicare con questa matrice c il procedimento che abbiamo visto prima quindi costruisco la funzione di costo ai minimi quadrati e vado a risolvere quel sistema di equazioni che ci permette di derivare no? le equazioni normali che ci permette di derivare i pesi ottimali quello che ottenete è questa cosa allora i pesi ottimali vi ricordo che il sistema di equazioni da cui siamo partiti è questo ve lo scrivo qua è questa equazione quella che è la soluzione andiamo indietro ve la faccio vedere allora questa soluzione che è questa qui vedete c trasposto c wp uguale c trasposto xp quelle sono le equazioni normali ve l'ho riscritta qua ok? noi abbiamo c abbiamo xp dobbiamo ricavare wp è un sistema di equazioni a per x uguale b ok? sistema di equazioni lineare la cosa bella che se c è costruita da vettori ortonormali guardate un po' che cosa succede che questo che cosa diventa? la matrice identità ok? quindi wp lo possiamo calcolare semplicemente come ct per xp quindi non dobbiamo più risolvere un sistema di equazioni ma semplicemente un prodotto matrice per vettore ci dà il risultato quindi se io impongo che la base di riferimento di arrivo sia una base ortonormale con un semplice prodotto matrice per vettore calcolo il vettore dei pesi ottimale ok? questo vale per tutti i punti del mio dataset tutta quella griglia di punti che avevamo e quindi quello che possiamo fare è scrivere che cosa? che c per wp vi ricordo è uguale a xp che è il nostro il punto da cui siamo partiti noi vogliamo costruire una combinazione lineare tale che il punto lo possiamo rappresentare come una combinazione lineare tramite quei pesi ok? in questa nuova base ma questo significa arrivare a scrivere al posto di wp sostituiamo ct per xp che lo andiamo a mettere qui dentro c per ct per xp uguale a xp questa è la cosiddetta formula dell'autoencoder ok? di autoencoder si parla anche oggigiorno molto come autoencoder in ambito deep learning sono autoencoder non lineari cioè fanno la stessa cosa proiettano su uno spazio più bassa dimensione poi ritornano anche nello spazio a dimensione originale ma lo fanno con delle tecniche non lineari qui invece si muoviamo con trasformazioni che sono lineari sono esprimibili tramite un prodotto matrice per vetture ok domande? quindi di nuovo per ripetere un po' alcuni concetti abbiamo che questo scrivo in verde c trasposto per xp sarebbe wp e sarebbe l'encoding ok? nel momento in cui faccio c per wp quindi c per wp questo sarebbe il decoding ok? va bene quindi io prendo un punto premoltiplico per la matrice c e ottengo la codifica di nuovo moltiplico per c e ottengo la decodifica quindi codifica e decodifica in questo caso sono proprio operazioni inverse che vengono ottenute tramite la matrice c che ha la proprietà di dar luogo alla matrice identità tutto questo viene noto appunto è noto in letteratura come formula dell'autoencoder questo è quest'ultima quest'ultima equazione vado avanti andiamo ok a cosa ci serve tutto questo? allora ci serve chiaramente a formalizzare la possibilità di cambiare base ma come vi dicevo prima le cose interessanti dal nostro punto di vista cominciano a emergere quando rinunciamo alla pretesa di rappresentare i dati in maniera perfetta e quindi utilizziamo queste basi questi spanning set per vedere che cosa succede quando andiamo in uno spazio dove k è minore di n cioè partiamo da uno spazio che ha dimensione n e andiamo in uno spazio che ha dimensione k dove k è minore di n quindi ho dei punti che sono dei vettori numerici che rappresentano un oggetto su cui voglio fare learning in uno spazio quindi con 15 feature e atterro in uno spazio che magari ha 3 feature 2 feature e voglio vedere cosa succede è chiaro che se k è minore di n non possiamo più pretendere di rappresentare i dati in maniera perfetta nello spazio di input quindi quello che prima era un'uguaglianza stretta noi avevamo scritto cwp uguale a xp adesso non può essere più vero non possiamo imporre che sia uguale ma ci accontenteremo che sia il più possibile vicina ok c in questo caso è una matrice lo vedete qui n per k quindi abbiamo n punti e k n righe e k colonne e wp che la codifica vedete è un vettore k per 1 prima era un vettore n per 1 perché mi diceva quali erano le n coordinate del nuovo spazio adesso è un vettore a k componenti quindi ho diminuito il numero delle componenti o l'encoding wp come lo possiamo ricavare rimane vero quello che era prima cioè noi non pretendiamo più che la funzione di costo lo andiamo a minimizzare a zero ma l'andremo a minimizzare in modo che valga questa non è più un'uguaglianza per ogni punto ma sarà una quasi uguaglianza cioè che valga il più possibile e quindi quella funzione di costo ai minimi quadrati la posso usare comunque posso usare anche la soluzione del sistema del primordine come abbiamo fatto prima quindi come per il perfect encoding quello che cambia è che appunto ottengo una proiezione geometrica dei dati su uno sottospazio che ha dimensione k questa proiezione è una proiezione lineare ok perché perché questo è un operatore lineare va bene ed è un'operazione questa che mi fa perdere dell'informazione vi dicevo ieri se io sono in uno spazio tridimensionale ho un punto che sta qui e lo proietto linearmente sul sul pavimento ottengo un punto se prendo quest'altro punto che sta sulla verticale chiaramente mi collassano nello stesso punto ho perdita di informazione ma è inevitabile sono passato da un mondo tridimensionale a un mondo bidimensionale con una proiezione lineare e questo è un esempio di quello che stavamo dicendo noi abbiamo una serie di punti che sono questi evidenziati colorati in nero ok quindi questo questo questo questo questo e così via che vivono in questo spazio che è uno spazio a tre dimensioni dopodiché io voglio trovare il miglior spazio a due dimensioni che mi permetta di proiettare questi punti adesso supponiamo adesso ancora noi non sappiamo qual è il migliore bisogna capire anche questo su questo ci torniamo tra poco però intanto supponiamo di aver identificato questo piano questo che vi ho segnato adesso che è evidenziato in rosso chiaro come il miglior spazio a due dimensioni su cui andare a proiettare quei punti dove migliore cosa vuol dire che mi minimizza la perdita di informazione che in qualche modo mi rende questa disuguaglianza la più vera possibile qui significa che io ho un punto xp e la proiezione lineare me lo proietta su questo piano che è un piano infatti ecco cosa vuol dire una trasformazione lineare che vanno tutti su questo piano questa è la loro proiezione che è la rappresentazione di xp nel nuovo mondo con le coordinate wp però questo nuovo mondo è un mondo che non è più 3d ma è 2d bidimensionale e ogni punto viene rappresentato questo punto wp come con le sue coordinate secondo c1 e c2 dove c1 e c2 sono questi due vettori ok allora io passando da 3 a 2 dimensioni chiaramente perdo dell'informazione ma se questa cosa la faccio bene ne perdo meno possibile chiaramente io posso fare anche l'operazione di decodifica e tornare da questo spazio due dimensioni in uno spazio originale nel momento in cui faccio questa operazione è ovvio che la perdita di informazione la vedo perché non è più una rappresentazione perfetta come prima in cui tornavo cioè andavo da 3 a 3 e quindi non perdevo nulla e quando ritornavo nello spazio originario ritrovavo gli stessi punti di prima qui no qui cosa succede perdo qualcosa perché in questo passaggio io ho perso dell'informazione quindi nel momento in cui voglio tornare nello spazio originale quindi faccio il decoding quindi qui abbiamo encoding qui abbiamo decoding nel momento in cui facciamo il il decoding perdiamo rispetto allo spazio originale alcune informazioni le abbiamo perse nella fase di encoding e quindi a maggior ragione non le ritroviamo nella fase di encoding sarebbe corretto dire e vedete vedete infatti che tutti i punti adesso vivono in uno spazio tridimensionale ma sono punti che erano stati proiettati su quel piano e quindi rimangono su questo piano allora come si trasforma la formula dell'autoencoder in questo caso la formula dell'autoencoder se c1 e c2 sono ortonormali di nuovo abbiamo questa cosa interessante per cui quello che possiamo scrivere e ve lo scrivo qua scriviamo qui allora noi quello che vogliamo è c per wp sia il più possibile simile a xp ok se di nuovo quindi la base l'insieme ricoprente è ortonormale allora quello che possiamo scrivere è che wp lo possiamo scrivere come c trasposto per xp e quindi qui otteniamo c allora scriviamo così c c trasposto xp e circa uguale xp allora qui cosa ho applicato ho applicato di nuovo l'ortonormalità che avevamo visto prima per cui arriviamo a questa che avevi scritto qua questa l'avevamo vista esattamente nella slide precedente scusatemi nella in questa slide qui vedete che se vale l'ortonormalità io posso scrivere no c per c trasposto per xp uguale xp qui era strettamente uguale se perdo informazione perché riduco la dimensione vale sempre se le vasi sono ortonormali il fatto che il prodotto di c trasposto per c è la matrice identità e per cui posso sempre scrivere c per c trasposto per xp ma questo non è più uguale xp è circa uguale è la formula dell'autoencoder lineare quando ho riduzione della dimensionalità quando non ho la riduzione della dimensionalità questo era esattamente uguale l'avevamo visto ok quindi questo è se abbiamo l'ortonormalità dello spanix se c'è l'ortonormalità dello spanix set io posso scrivere questo per intenderci il nostro wp quindi se io fisso una base c1 e c2 posso comunque andare a ricavare la codifica in uno spazio più bassa dimensionalità va bene se la base è orto normale questa codifica ha una forma anche particolarmente semplice che è questa della formula dell'autoencoder cioè wp lo calcolo semplicemente come c trasposto per xp ok la domanda che a questo punto ci possiamo fare è ok fino adesso abbiamo ragionato con matrice c fissata cioè io so quanto valgono c1 e c2 posso andare a vedere se sono ortogonali ortonormali posso cercare una base ortonormale e vedere che cosa succede a quel punto nella mia trasformazione ma la domanda che mi posso anche fare è dire e se io invece non so qual è la base in cui voglio trasformare il mio punto ma voglio anche che quella sia una variabile da cercare cioè che il mio modello deve andare a ottimizzare a identificare posso farlo e la risposta è sì e questo lo vediamo nella slide successiva perché la domanda appunto che ci possiamo fare è dire possiamo anche andare oltre che a imparare l'insieme dei pesi W possiamo andare ad apprendere quello che è un sistema di riferimento opportuno una base uno spanning set questo equivale a dire che cosa che nel momento in cui noi andiamo a analizzare il nostro dataset e quindi cerchiamo di fare un supervised learning noi vogliamo vi ricordo che valga questa in maniera più stretta possibile idealmente dovrebbe essere uguale ma uguale non sarà mai vogliamo che sia il più possibile valida e abbiamo detto beh un modo per cui questo possiamo farlo è costruire una funzione di posto ai minimi quadrati in cui questo termine sia il più vicino possibile a questo per tutti i punti del dataset che sono più grandi ora noi fino adesso abbiamo ragionato con c che gli avevamo dato a noi come input però nulla vieta di dire anche c oltre a wp è una mia incognita io vado a minimizzare a questo punto una funzione di costo che dipende da w1 w2 wp ma anche da c cioè io voglio trovare oltre al vettore di pesi anche la base posso farlo? sì posso farlo perché io posso esattamente costruire questa loss function che è una funzione di costo mini quadrati è una funzione di costo che non è più convessa quindi diciamo ovviamente in generale non convessa c'ha qualche per cui non possiamo più utilizzare le equazioni normali per risolverla quel sistema di equazioni lineare eccetera però possiamo comunque utilizzare un qualunque metodo di ottimizzazione locale come quelli che abbiamo visto discesa del gradiente su tutto e in questo modo andare a imparare a far imparare al sistema non solo i pesi w1 w2 w fino a p ma anche la matrice c cioè il sistema apprende i pesi e la base migliore su cui andare a proiettare quei punti allora vediamo un esempio di piccolo esempio di punti che vivono uno spazio originale con tre dimensioni poi vengono proiettate due dimensioni e poi qui verranno retroproiettate uno spazio per le dimensioni allora questo che è evidenziato qui in rosso leggero che vi ho appena puntato con la freccia verde è il miglior sottospazio bidimensionale per quell'insieme di punti come è stato identificato minimizzando questo e quindi oltre a trovare questo sottospazio che è un sottospazio lineare perché è una proiezione lineare noi troviamo anche la codifica di ognuno di quei punti secondo le due dimensioni c1 e c2 dove c1 e c2 sono questi due assi questa codifica è esattamente questa la rappresentazione di wpp per cui cwpp è la decodifica di quei dati e quei punti staranno tutti su questo piano quando li proietto di nuovo in uno spazio più alto quindi sono passato spazio più bassa dimensionalità torno nello spazio ad alta dimensionalità e trovo questi punti questo è il gioco che si fa però in questo caso abbiamo ulteriormente aggiunto diciamo degli elementi di cui vogliamo trovare il valore perché prima fino a prima avevamo minimizzato la funzione solo in funzione di questi parametri adesso ci abbiamo aggiunto anche questo che è una matrice le cui colonne sono c1 e c2 questo che sono quelli che noi vogliamo trovare e minimizzando questa funzione di costo voi riuscite a trovarli perché riuscite a trovare questi p vettori e anche k altri vettori che sono le colonne di quella matrice in linea di principio questo è quello che è possibile fare e quindi di nuovo questa è la codifica questa è la decodifica e diciamo non è e questo è il sottospazio che è stato appreso a partire dai dati quindi qui vedete si tocca con mano che cosa vuol dire unsupervised learning noi abbiamo dei punti abbiamo una nuvola di punti in tre dimensioni non abbiamo nessuna etichetta per quei punti non è che questo vi dice un cane un gatto oppure è un qualcosa che vale 3,5 3,7 no abbiamo solo la codifica di questi punti secondo tre feature x1 x2 x3 e a partire solo dal dato di input noi costruiamo una trasformazione che ce lo rappresenta in uno spazio a dimensionalità più bassa e riusciamo a trovare a partire solo dai dati quindi nessuna etichetta qual è il miglior sottospazio in cui questi dati possono vivere in modo da minimizzare la perdita di informazione questa è la minimizzazione della perdita di informazione che io codifico in questo modo ma qui non c'è nessuna label nessuna etichetta non è un problema con supervisione è un problema non supervisionale ok bene ok allora qui possiamo qui abbiamo ragionato assumendo che c abbia una forma qualunque cosa succede di nuovo se noi abbiamo visto che se la matrice C prima era costituita da K vettori ortonormali avevamo la possibilità di semplificare quel quel tipo di di scrittura e avevamo visto che se erano normali ortonormali scusatemi valeva questa mi ricordate l'abbiamo vista torno indietro ok WP uguale CT per XP è questa ora andando a scrivere WP qui dentro otteniamo e imponendo che questi siano ortonormali quindi l'ipotesi è che questa sia costruita a partire da vettori ortonormali otteniamo una cosa di questo genere un'espressione di questo genere che è molto interessante perché perché è una funzione di costo in cui ognuno di quei termini non dipende più da WP ma dipende solo da C e forza questa a valere quando viene minimizzata e questa è quella che viene chiamata formula dell'autoencoder lineare ed è quella che viene utilizzata più spesso perché chiaramente è una forma estremamente compatta in cui andiamo a ricavare esclusivamente C poi una volta che abbiamo C ricaviamo WP in questo modo esattamente esattamente esattamente e quindi qui ricordo non abbiamo uno schema di codifica e decodifica noi lo apprendiamo dai dati stessi e il miglior sottospazio andiamo a trovare quella matrice C ok e si può dimostrare che in questo modo troviamo ok i minimi di questa funzione sono sempre ortonormali ma questa è una proprietà matematica che discende da come l'abbiamo costruita va bene ok su questo direi che non c'è altro di aggiungere molto altro da aggiungere se non che tutto questo per cosa può essere usato allora vi dicevo prima può essere usato perché io ho un insieme di punti e voglio passare da una rappresentazione in cui ogni punto sono 10 feature quindi tornate all'esempio in cui facevamo regressione lineare in cui avevamo per esempio l'esempio vi ricordate della casa del costo degli immobili quindi avevamo un appartamento e avevamo il numero di stanza dell'appartamento il numero di metri quadri distanza dal centro e così via supponiamo di avere 10 feature e noi vogliamo trasformare ognuno di quei punti che rappresenta un elemento di un di un per esempio di un appartamento in uno spazio più bassa dimensione quindi magari 5 oppure 3 o 2 perché vogliamo graficarli metterli in un grafico ecco tutto questo serve per fare questo quindi per mettere quella che viene chiamata analisi esplorativa dei dati io posso volerli visualizzare cercare di capire qualcosa di più ok oppure posso voler fare aumentare diciamo l'efficienza computazionale perché tutti i calcoli anziché andarle a fare su vettori da 15 o magari molte più feature lo faccio su vettori da 5 da 3 sì quali modi sono sempre una parte di un'efficienza oppure un'efficienza allora in realtà se faccio una proiezione sono trasformate quindi perdono il significato originale se vogliamo sì non hanno più no non necessariamente se io voglio fare questo discorso che fai tu devo fare quella che viene chiamata feature selection cioè selezionare quelle più informative ma qui invece stiamo trasformando le feature in un nuovo spazio ok per cui però questo può essere comunque utile perché mi fa vedere magari dei raggruppamenti cioè nel nuovo spazio scopro che alcuni stanno in un raggruppamento alcuni punti no esattamente no però può essere comunque utile perché può essere comunque un raggruppamento per cui tu li trasformi e scopri che prima magari erano mescolati cioè non avevi neanche la visibilità e adesso scopri che mettendoli in due o tre dimensioni hanno dei raggruppamenti delle caratteristiche particolari per cui sai che in questo nuovo spazio vedi che per esempio sono distinti allora sai che puoi essere puoi classificarli con una buona è una buona speranza di ottenere un classificatore significativo però quello che dici è corretto cioè bisogna fare distinzione tra la selezione di un subset di feature ci torneremo sopra su questa cosa della selezione di feature è la trasformazione qui stiamo trasformando il dato però ci posso comunque una volta che l'ho trasformato costruire un classificatore ad esempio un regressore capito e lo costruirò in maniera più efficiente e posso anche fare un'analisi comunque esplorativa perché io vado a fare una proiezione e ottengo una trasformazione chiaramente anche degli assi relativi all'efficio ok ah qui c'è un esempio sempre di questo learning dell'autoencoder cioè noi impariamo anche l'autoencoder impariamo la matrice C tramite algoritmo di scesa del gradiente abbiamo un dataset bidimensionale che è questo andiamo a minimizzare la funzione di costa minimi quadrati per passare da due a una dimensione e lui riesce a trovare anche la direzione migliore che è questa codifica i dati lungo in base a questo vettore e questo chiaramente è la decodifica dei dati rispetto a questo sottospazio lineare la differenza tra gli autoencoder gli autoencoder non lineari che sono quelli di cui si sente parlare oggi quando parlate di reti neurali eccetera è che il sottospazio che vanno a ad apprendere non è più lineare è non lineare quindi può essere una curva una varietà lungo con cui lui va a sistemare i punti questo lasciamo da parte adesso per il momento quindi diciamo gli autoencoder non lineari con le tecniche di deep learning di reti neurali fanno una riduzione della dimensionalità non più lineare vanno su sottospazi non lineare ok bene allora questa formula della funzione di costo dell'autoencoder lineare che abbiamo appena ricavato prima diciamo può avere più punti di minimo più minimizzatori ok quindi più valori di c più combinazione di matrici c che vi minimizzano quella funzione nell'analisi statistica dei dati un particolare set di di valore quindi di un valore specifico di quella matrice c quindi un particolare set di vettori lungo le quali noi andiamo a proiettare il il i nostri dati che riveste particolare importanza è quello cosiddetto delle componenti principali e l'insieme delle componenti principali che cos'è è l'insieme dei vettori che definiscono in un sottospazio lungo nel quale noi andiamo a proiettare i nostri punti che ha una caratteristica cioè questa funzione ha più possibili matrici c come soluzione una di queste è quella che viene chiamata componente principale ok le componenti principali sono e questo lo vedremo direi adesso cominciamo ma poi ci andiamo avanti la prossima volta sono quelle che vedremo forniscono le direzioni di più alta varianza del dato cioè cosa vuol dire vuol dire prendiamo questo esempio di punti in uno spazio a due dimensioni quindi abbiamo due feature x1 e x2 vedete che questi punti sono sparpagliati e in queste coordinate x1 e x2 quello che possiamo vedere a occhio è che c'è un'ellisse ideale ok entro la quale questi punti sono distribuiti e i due assi di queste ellisse che sono queste due frecce rosse che sono riportate qui sono degli assi lungo i quali si sviluppa vedete lungo questo asse si sviluppa la maggiore variabilità del dato poi c'è un altro asse ortogonale che rende conto del fatto che c'è una variabilità lungo quest'altra direzione e queste sono le due direzioni ortogonali tra di loro lungo le quali voi accumulate se andate a vedere la maggiore variabilità del dato cosa vuol dire vuol dire che nello spazio x1 x2 non ho la stessa varianza non variano le coordinate dei punti allo stesso modo se io li vado a sommare tra di loro la varianza vi ricordo che prendete la media dei valori dei punti e fate lo scarto rispetto a questa media e lo elevate al quadrato ora se io avessi preso un altro qualunque altra coordinata ortogonale quindi ad esempio questa e questa non avrei avuto la stessa variabilità dei dati questa variabilità può essere scritta in maniera esatta e precisa come la varianza proprio di quei dati ora come faccio a identificare quindi le direzioni ortogonali in questo caso ne ho identificate due se avessi partito da uno spazio a dimensione 10 potevo identificarne 10 9 8 5 fino ad arrivare a 2 o 1 come faccio a identificare le direzioni che hanno la più alta variabilità e sono ortogonali tra di loro questo è quello il problema che risolve l'analisi dei componenti principali che è uno strumento che in statistica è noto da diverso tempo l'analisi statistica già da più di 100 anni ha sviluppato strumenti per l'analisi dei componenti principali la cosa interessante è che machine learning opera su dati ad altissima dimensionalità spesso e volentieri e soprattutto su quantità di dati molto elevata perché dietro ci sono degli algoritmi che permettono di fare questa cosa questa analisi al calcolatore quindi statistica più algoritmi hanno prodotto la pca moderna su dataset massicci e perché è importante questo tipo di analisi perché vi cattura esattamente delle direzioni di uno spazio trasformato che è un sottospazio lineare lungo le quali il vostro dato ha la maggiore variabilità voi prendete questa prendete questa e potete trasformare fare l'encoding di questi punti lungo queste direzioni e ottenete un possibile valore di c che vi mantiene la variabilità del dato più possibile e quindi diciamo in qualche modo vi permette di essere sicuri che quelle trasformazioni anche se andate in uno spazio di più bassa dimensionalità sono significative perché perché le feature variano molto in un qualunque modello di machine learning che cosa succede se avete una feature che non varia che è costante quella feature non è informativa cioè supponiamo di avere ritornare al problema da cui siamo partiti all'inizio del corso devo discriminare le foto delle immagini di un cane da un'immagine di un gatto e vi ricordate abbiamo detto beh possiamo utilizzare il fatto che abbiano le orecchie appuntite possiamo utilizzare la dimensione del naso ok se volessimo costruire un ideale classificatore ma supponiamo di aggiungere una feature che non varia per niente che è costante ok quindi per esempio il numero di zampe 4 il numero di zampe è costante sia per i cani che per i gatti quella è una feature che non è informativa perché varia scusate perché è costante le feature informative sono quelle che variano da un elemento all'altro perché gli danno informazioni ecco perché la PCA spesso e volentieri è un qualcosa che può essere uno strumento utile ok nell'analisi nell'analisi esplorativa ha i suoi anche contro adesso poi vi farò vedere un contro esempio del fatto che questo non è poi sempre non è sempre così utile però diciamo per perlomeno per per quanto riguarda un'ipotesi introduttiva ecco è un buon modo quello di andare a fare un'analisi di un dataset di questo tipo andando a cercare di andare a identificare le direzioni che mantengono la variabilità più alta perché potenzialmente sono quelle più informative allora per poter fare tutto questo bisogna fare un pochino di trasformazioni quindi di scusatemi di analisi di trasformazioni matematico algebriche e queste magari le vediamo la prossima volta però intanto vi anticipo qual è la direzione che andremo a fare cioè le componenti principali vengono derivate dalla formula dell'autoencoder lineare quindi di fatto la PCA così viene chiamata è una soluzione della formula dell'autoencoder lineare quello che ha le basi ortonormali perché vi ricordo che io voglio che queste componenti vengono chiamate componenti principali proprio che cosa sono le basi di quello spazio vettoriale la cosa interessante è che quindi quei minimizzatori possono essere in realtà si può dimostrare non lo faremo che possono essere calcolati come gli autovettori di una matrice particolare che viene ricavata dal dataset cioè voi avete il vostro dataset costruite questa matrice che si chiama matrice di covarianza poi vi dico come ma molto semplicemente è questa dove x sono tutti i vostri punti che avete organizzato in colonna costruite questa matrice di covarianza e poi di questa andate a estrarre gli autovalori e gli autovettori vi ricordate all'inizio del corso abbiamo fatto un breve riepilogo a proposito di autovalori e autovettori quando abbiamo parlato delle matrici essiane eccetera ecco qui è un altro punto in cui gli autovalori e gli autovettori quindi la scomposizione di una matrice quadrata in autovalori e autovettori gioca un ruolo importante perché se voi fate la scomposizione e provate gli autovalori e gli autovettori di questa matrice si può dimostrare che la potete scrivere secondo questo prodotto tra matrici dove qui dentro ci mettete gli autovalori e gli autovettori ve lo faccio vedere la prossima volta e la cosa bella è che la varianza quindi gli autovettori rappresentano le direzioni di massima variabilità del dato sono ortogonali tra di loro e quanto è questa varianza è dettato dagli autovalori ma con questo ci torniamo la prossima volta è utile questo perché a quel punto voi potete calcolare gli autovalori e gli autovettori di questa matrice a partire dai dati se volete se partite da uno spazio a dieci dimensioni e volete le cinque direzioni di massima variabilità estraete gli autovalori e i corrispondenti autovalori che hanno i cinque valori più alti e quelle sono le prime cinque direzioni di massima variabilità cioè l'autovalore che ha magnitudo più ampia è quello che corrisponde alla direzione di massima variabilità cioè se io torno qui indietro questa freccia rossa che ha lunghezza più ampia ha una lunghezza che è proporzionale all'autovalore cioè l'autovalore associa questo è un autovettore di quella matrice di povarianza che viene costruita a partire dai dati ed è l'autovettore a cui è associato l'autovalore più alto questo è l'autovettore a cui viene associato l'autovalore meno alto in questo caso sono due immaginate di avere una dimensione 10 se volessi proiettarla su un spazio bidimensionale prenderei i due autovalori più alti questo è il gioco che si fa la prossima non vediamo tutte le dimostrazioni però ragioniamo un po' su queste cose la prossima volta vediamo un po' meglio questo in questo modo noi otteniamo una proiezione su uno spazio che è significativo perché è una proiezione che minimizza l'errore di ricostruzione e allo stesso tempo vi conserva la maggiore variabilità del dato ok? questo è il gioco che si fa direi che per oggi possiamo fermarci qui non so se ci sono delle domande o se abbiamo visto parecchi concetti quindi provate a riguardarli poi la prossima volta ripartiamo da qui vediamo la PCA dopo che abbiamo visto la PCA passiamo passiamo un po' alla cluster analysis quindi parliamo un po' di accordi di clustering ma questa è solo a parte del nostro supervisionato direi che sarà tutto va bene? ok allora tocchiamo la registrazione e terminiamo prima