Buongiorno, benvenuti a questa nuova lezione del corso. Oggi cominciamo ad affrontare un nuovo argomento dopo aver esaminato quelle che sono le principali tecniche di ottimizzazione matematica e gli algoritmi che queste tecniche si portano dietro. Cominciamo il vero e proprio studio dei modelli di apprendimento automatico e quindi i primi modelli di machine learning occupandoci di problemi di regressione lineare. Quindi siamo, come vedremo presto, nell'ambito dell'apprendimento con supervisione, quindi andremo a risolvere problemi di apprendimento supervisionato e questi problemi di apprendimento supervisionato in particolare prendono la forma di problemi di regressione. Poi affronteremo anche quelli di classificazione. L'altra cosa da notare è l'aggettivo appunto che è lineare, cioè i primi modelli che andremo ad analizzare sono modelli appunto dove ci portiamo dietro un concetto di linearità. Vedremo poi più avanti come man mano questo possa essere poi rimosso per andare verso modelli anche di tipo non lineare. Però ecco, partendo per ordine, quello che possiamo fare è partire da un esempio, un esempio abbastanza semplice in cui facciamo in condivisione una lavagna. Ok. Ok. Per poter scrivere. Allora, andiamo qui. Benissimo. Allora. Supponiamo per, giusto per, ponere un esempio che ci permetta di orientarci prima di andare alle definizioni formali e quindi diciamo di scendere in un dettaglio con appunto un formalismo matematico più rigoroso, però cominciamo da un esempio. Un esempio che è un esempio tra l'altro pratico in cui supponiamo di avere un dataset in cui abbiamo una serie di informazioni riguardanti degli appartamenti, va bene, che possono essere i metri quadri dell'appartamento e magari il corrispettivo prezzo. Quindi diciamo una stima immobiliare e abbiamo una serie di rilevazioni che ci dicono che ha un certo valore in metri quadri dell'immobile e corrisponde un certo prezzo. Quindi abbiamo una superficie di metri quadri e abbiamo un prezzo in euro. Ok. Quindi potremmo avere dei dati del tipo, insomma, un appartamento da 195 metri quadri e un prezzo che può essere in migliaia, scusate, in... questi sono migliaia di euro, quindi chilo euro diciamo. Per tenere i numeri più piccoli nella tabella potrebbe essere un qualcosa del tipo, che ne so, 410.000 euro. Un altro da 148 metri quadri potrebbe essere 250. Un altro da 220 metri quadri potrebbe essere 370, un, ma so, 372 e così. Ovviamente potremmo averne altri che hanno un superficie simile, magari prezzo diverso, eccetera. Supponiamo anche che questi ecco siano relativi chiaramente a una stessa, una medesima per esempio zona. Ok. Spero che già se comincio a spostarmi verso altre città o verso altre zone di certe città i prezzi cambiano anche al pari della parità di superficie, ma è un esempio giusto per farvi capire. Ecco. Cosa succede? Succede che oltre a questo dato potremmo avere anche altri dati, ne parliamo tra poco, ma intanto se già volessimo provare a fare un grafico in cui andiamo a mettere sull'asse delle x la superficie, quindi vediamo che la superficie... e sull'asse delle y il prezzo, quello che potremmo fare è andare a mettere i vari punti sotto forma di quello che viene chiamato anche scatter plot, cioè un grafico di dispersione, in cui ogni punto corrisponde a esattamente una coppia superficie-prezzo. quindi prima di avere una certa superficie, per avere valori bassi, per partire da 50 metri quadri, poi avrei il 100, 150, 200 metri quadri e così via. Adesso in cima ci saranno ovviamente i castelli, qua gli appartamenti in monolocali. Va bene. i prezzi ovviamente seguono un certo andamento, per cui se andiamo a prendere questi tre punti che abbiamo identificato, avremo un qualcosa, per esempio, i prezzi potremmo metterlo da 50, 100, sono sempre in migliaia di euro, 150, 100, 150, 300, 1000 e così via. E chiaramente se volessimo, per esempio, andare a mettere questi tre punti, avremmo il 195 con qualcosa che sta qua, dell'ordine 410, quindi abbastanza alto sarebbe qui, poi avremmo il 150 con 250 sarebbe qua, il 220 con il 372 sarebbe da queste parti e così via. E' chiaro che io, andando ad aumentare i campioni che ho a disposizione, mi ritrovo con una serie di ipotetici punti, una nuvola di punti che sarà distribuita secondo certi valori, ma che grosso modo quello che ci possiamo aspettare è che al crescere della superficie, a parità di altre condizioni, ovviamente cresca il prezzo di un appartamento. Un appartamento di più metri quadri costerà di più. E questo è del tutto ragionevole. Allora, io quello che posso voler fare, posso voler cercare una relazione che mi dica esattamente dati questi, assegnati questi dati, qual è per esempio la miglior retta, dice interpolante, cioè quella che in qualche modo mi è posizionata più vicina ai vari punti. E questo può essere comodo, perché una volta che io abbia deciso questo, che chiaramente è un modello lineare, e qui arriviamo a quello che è l'obiettivo appunto dell'introduzione di questi modelli, io posso volerlo, posso, dicevo, introdurre una legge di indipendenza di questo genere per vari motivi, posso volerlo fare per appunto, per avere un qualcosa che mi sintetizzi in qualche modo questo andamento e che mi possa permettere ad esempio delle previsioni. Quindi, su un punto incognito, fissata una superficie, quale fare una previsione di quello che può essere un ipotetico prezzo. Posso farlo perché mi permette appunto una visualizzazione molto più efficace, vedete come questa linea di punti, tracciando sopra questa retta, immediatamente un'idea di quello che è l'andamento. Ma posso appunto farlo, come vi dicevo prima, per appunto motivi di introdurre uno strumento di tipo previsionale, che è effetto che le vengono chiamate delle predizioni, o delle inferenze anche. Quindi assegnate un valore incognito dell'input, cercare di restituire un valore dell'output. Ora il punto è come fare a costruire questa retta, adesso cercheremo un po' di capire meglio. Vi faccio anche notare una cosa, che fin qui abbiamo ovviamente trattato un problema unidimensionale, nulla vieta di introdurre ulteriori input. Quindi, ad esempio, accanto alla superficie, io potrei voler aggiungere altre colonne. Possono essere per esempio, non so, il numero di stanze. Il numero di stanze può essere 8, 5, 10, eccetera, eccetera. E questo fa sì che, diciamo, abbiamo un ulteriore dato a disposizione in input, da accoppiare a quello che avevamo originalmente e che ci fornisce quindi ulteriori informazioni potenziali. È chiaro che qui il discorso si fa, diciamo, più articolato da un punto di vista geometrico, significa aggiungere un'ulteriore dimensione e quindi cercare non più, ad esempio, una retta, ma cercare un modello lineare che a quel punto sarebbe un piano. Se gli input, questi input di anticipo, che vengono chiamati nel gergo del machine learning, feature, se il numero delle feature cresce, chiaramente diventa 3, andiamo in uno spazio non visualizzabile direttamente secondo la nostra esperienza di senso comune, ma che da un punto di vista matematico, abbiamo già detto più volte, è perfettamente legittimo andare a prendere in considerazione, è uno spazio nidimensionale e a quel punto questi oggetti lineari diventano quelli che vengono chiamati degli iperpiani. Tutto questo può essere reso formalmente più rigoroso andando a prendere in considerazione questo... andando, dicevo, a prendere in considerazione questo formalismo e quindi qui cominciamo a entrare un po' più nel dettaglio, però questo esempio di fatto conteneva già gran parte delle cose che ci servono per avere un'intuizione. Che cos'è una regressione lineare? È essenzialmente un task, un compito, che consiste nel fare quello che viene chiamato un fitting di una figura geometrica, in particolare, siccome la regressione è lineare, il fitting è di una linea. Chiaramente per l'input a una dimensione diventa un piano in due dimensioni e un iperpiano in due dimensioni. Questo fitting che cos'è? È una parola inglese che significa adattare, cioè bisogna adattare questa superficie a un insieme di punti di cui conosciamo le relazioni input-output. Quindi scegliere la retta che in qualche modo si adatta meglio. Io nell'esempio che vi ho fatto con la retta raccione intuitivamente si adattava meglio, ma era un concetto che è ancora chiaramente intuitivo e che dobbiamo rendere ovviamente rigoroso. Lo scopo, come dicevo, è produrre per esempio una linea di tendenza o una curva di tendenza, perché poi vedremo che tutte le cose che diremo e che introdurremo per la regressione lineare ci serviranno poi per con poco per trasformare e riutilizzare gran parte delle soluzioni che introdurremo per le regressioni lineare, anzi non gran parte, riutilizzare le soluzioni in particolare per risolvere i problemi di regressione lineare li potremo trasferire anche nei problemi di regressione non lineare, ovviamente facendo un opportuno diciamo pre-processing, ma di quello parleremo a suo tempo. Quindi l'obiettivo può essere introdurre una linea di tendenza per visualizzare o sintetizzare meglio i dati o semplicemente appunto apprendere un modello per fare delle previsioni su dei valori futuri. Ho un appartamento di 250 metri quadri in base a quei dati, qual è la stima del mio modello sul prezzo di questo appartamento? Ecco, i problemi di regressione rispondono a questo tipo di logica, a questo tipo di domande. quindi, riassumendo, il problema di regressione ha assolutamente una forma che è questa. È esattamente questa che è riportata qua. Abbiamo un insieme di punti. x1 per il quale conosciamo la risposta, quindi diciamo la variabile indipendente y1 che è il nostro output. Vi faccio notare che x1 ovviamente è in generale un vettore, quindi posso avere n feature, quindi input, quindi ci possono essere superficie, numero divani del nostro appartamento, distanza dal centro, potremmo avere altri input che possono essere distanza dai, che ne so, ospedale, centro sanitario, distanza dalle scuole, tasso di criminalità di un determinato quartiere per certe grandi città, eccetera, eccetera. Tutte queste sono delle variabili indipendenti che diventano importanti per fare una previsione di quella che è la variabile indipendente. La stessa cosa posso avere per un altro punto, quindi un altro appartamento, x2, si porta dietro il suo prezzo y2, nel caso dell'esempio, o più in generale un vettore x2 che si porta dietro una sua risposta y2, fino ad arrivare a x con indice p grande, che ha una risposta yp grande, tutto questo può essere sintetizzato in questa notazione in cui abbiamo tutte le coppie xp, yp, dove p è scritto minuscolo e prende valori che vanno da 1 fino a p grande. In generale quello che possiamo vedere è che ogni input quindi è un vettore, lo dicevamo, i vettori per noi saranno di default laddove non specificato diversamente dei vettori colonna, quindi il generico vettore x relativo al punto p esimo sarà un vettore colonna che avrà le componenti x1, p, x2, p, x3, p, fino a x con n, p, dove il virgola p indica che stiamo parlando del p esimo punto del nostro dataset. Questo dataset è quello che poi chiameremo nel seguito anche dataset di addestramento del modello perché chiaramente è il dataset che ci è utile per conoscere tutto quello che possiamo conoscere del mondo che andiamo a modellizzare e quindi per costruire un modello che ci permetta di fornire la risposta nel momento in cui di quel mondo ci domandiamo cosa succede per un valore nuovo incognito che non apparterà a quel punto a quel dataset. cioè se io voglio fare la previsione su un appartamento che non appartiene al dataset ma che ipoteticamente di cui non conosco il prezzo ma che ipoteticamente voglio sapere e di cui invece conosco le caratteristiche quindi di nuovo il numero di vani superficie distanza dal centro eccetera eccetera a quel punto è un punto che non appartiene al mio dataset sul quale io vado a fare una predizione. però in tutto questo dobbiamo anche ancora domandarci come andare a costruire il modello più adatto. Allora intanto quello che possiamo dire è che questo lo devo mettere in modo che eccoci qua spero che l'audio sia ripartita allora come vi dicevo c'è stato un problema con l'audio ripeto brevemente spero che non si sia perso molto però essenzialmente abbiamo il problema di data una nuvola di punti che sono questi specificati qui trovare un modello lineare quindi in questo caso una retta oppure un piano che danno luogo a quello che viene chiamato il miglior fitting cioè tra le infinite ipotesi che noi abbiamo a disposizione quella che si adatta meglio alla nuvola di punti quindi se abbiamo punti diciamo se abbiamo input scalari cioè n uguale a 1 questo significa che siamo in questa situazione in questo grafico a sinistra e quindi questo corrisponde a fare il fitting di una retta di fatto in uno spazio che è uno spazio bidimensionale infatti una retta ha due parametri che sono l'intercetta e la pendetta cioè in generale noi possiamo scrivere una retta con questa notazione y uguale w0 più w1 per x dove w0 e w1 sono due parametri incogniti che rappresentano rispettivamente la pendenza della retta e l'intercetta questa è l'intercetta mentre chiaramente la pendenza è w1 e questa è l'intercetta l'intercetta viene anche chiamata oh vedete che già qui abbiamo la variabile risposta l'abbiamo appunto espressa in funzione dell'input e ovviamente in funzione di questi due parametri questi sono quelli che dovremmo andare a determinare il task a determinare quei due parametri perché ovviamente io potrei avere una retta come questa che è disegnata ma potrei avere infinite opzioni questa questa questa altre che passano per qua quindi diciamo sono chiaramente infinite infinite possibili soluzioni e tra queste noi dobbiamo scegliere quella migliore questi due parametri vengono chiamati chiaramente il parametro w1 è la pendenza della retta il parametro w0 che c'è anche per il caso diciamo n dimensionale che vedremo tra poco viene chiamato peso di bias bias è una parola inglese che significa diciamo può essere tradotta come inclinazione distorsione è un qualcosa che vi sposta in qualche modo il la retta si a parità se noi manteniamo fissa w1 avrei queste infinite serie di rette parallele al variare del bias w0 ed è un qualcosa che diciamo vi sposta quindi il modello di fatto di un valore di un valore costante nel caso n dimensionale questo ovviamente vale per tutti i punti del nostro dataset il caso n dimensionale è un'estensione di retta una cosa che vi faccio notare ecco che qui cosa vogliamo noi vogliamo in realtà che il nostro modello quindi la nostra retta restituisca torniamo all'unidimensionale prima di passare al multidimensionale circa il valore di yp di uscita per i vari punti questo perché perché come vedete qui alcuni punti staranno sulla retta ma ad esempio questo non sta sulla retta non possiamo pretendere che questi punti giacciano perfettamente sulla retta per tanti motivi perché la retta potrebbe essere un modello approssimato della realtà perché semplicemente ci può essere del rumore anche se la retta è il modello corretto nei dati e quindi diciamo quello che possiamo imporre è che ci sia circa questa corrispondenza cioè ognuno di questi punti più o meno stia nei pressi ecco quindi diciamo di questa retta è questo che vuol dire questa notazione ci siamo chiaro ok nel caso di input nidimensionali ovviamente che cosa abbiamo abbiamo sempre il termine di bias e poi abbiamo una combinazione lineare delle x1 x2 fino a xn feature ognuna pesata per w1 w2 wn che sono i pesi e questa è l'estensione di fatto di questo concetto a più dimensioni qui avete il grafico quando si tratta di due dimensioni quando avete due dimensioni avete che y lo vogliamo esprimere con un termine di bias più la combinazione lineare delle due feature x1 e x2 secondo i pesi w1 e w2 questo oggetto è un oggetto bidimensionale che è un piano bidimensionale ma vive chiaramente in tre dimensioni quindi quello che vogliamo è che provare il piano che questo è evidenziato nella figura che meglio descrive in qualche modo quell'insieme di punti al variare dei due input x1 e x2 quindi fissato w1 w2 e w0 abbiamo questo oggetto che appunto ci permette di rappresentare questa nuvola di punti più o meno bene perché alcuni punti come questo stanno al di fuori di questo piano però chiaramente è un modello come tutti i modelli ha le sue limitazioni quindi si tratterà di capire bene di caso in caso quali sono queste limitazioni quali sono invece le capacità espressive come si dice del modello chiaramente quando siamo nello spazio unidimensionale qui abbiamo una retta e il modello appunto è rappresentato da questa retta e qui mi muovo al variare di x su questa retta domande sì quando abbiamo x1 sì questa questa dice soltanto che siamo in un generico punto piesimo quindi puoi anche togliere nel senso che al variare implicitamente tacitamente puoi dire ecco questo è il punto supponiamo che più piccolo varga 4 e che questo sia il punto 4 allora ci stiamo dicendo quali sono le coordinate del punto 4 e poi avrai il punto 4 avrà un suo valore di x1 che è questo il punto 10 avrà questo valore di x1 e 10 starà qua ognuno di quei punti ha un certo valore chiaramente qui qui eravamo in una ovviamente l'esempio sbagliato dovevo andare di qua in realtà nessuno dei due perché se parliamo di x di no del punto 10 ok punto 10 ti do scusa ho fatto confusione allora riepilogo p piccolo è relativo a quale p uguale a 1 uguale a 2 uguale a 3 uguale a 4 uguale a 5 uguale a 6 fino all'ultimo 7 8 9 sono 10 punti ok e questo è l'indice p piccolo x1 è la prima coordinata nel caso della retta ne abbiamo uno solo nel caso dello spazio bidimensionale abbiamo due coordinate quindi avrò x1 x2 nel caso n dimensionale avrò n coordinate ok quindi semplicemente questo indice ci dice quale feature stiamo andando a considerare x1 x2 fino a xn dove n è la dimensione del problema per cui nel caso unidimensionale mi fermo qua nel caso bidimensionale mi fermo qui questo è il mio modello se ho tre dimensioni aggiungerò un termine e così via ecco prima diciamo avevo in realtà tratto un inganno perché ero andato a prendere x10 di qua ma in realtà non c'entra niente x10 semplicemente il fatto che p piccolo può prendere il valore da 1 a 10 però abbiamo p di quelle abbiamo p grande di quelle esattamente cioè p piccolo va da 1 a p grande in questo caso sono 10 punti per cui 2 4 6 8 10 esattamente p piccolo va da 1 a 10 ok allora diciamo sì è importante all'inizio mettere magari andiamo piano poi almeno siamo sicuri di mettere le basi bene ecco anche con le notazioni a proposito di questo qui introduciamo un po' di ulteriori questioni di carattere più formale allora vi faccio notare che nel momento in cui noi andiamo a trattare un input n dimensionale abbiamo bisogno di w1 w2 fino a wn pesi e w0 che è il termine di bias quindi n più 1 variabili parametri da sistemare allora w0 viene chiamato bias w1 w2 wn vengono chiamati pesi feature touching cioè che toccano le feature perché perché se torniamo un attimo indietro vedete che sono gli unici che vengono moltiplicati per quelle che sono le feature questo si vede bene qua il bias non viene mai moltiplicato per le feature mentre tutti gli altri pesi sì quindi prendono il nome di feature touching weight allora l'annotazione che abbiamo appena visto può essere riscritta in forma più compatta in questo modo possiamo andare a considerare il nostro vettore dei pesi ricordo che questo è il vettore dei parametri che noi non conosciamo quindi saranno quelli che noi dovremmo aggiustare tramite il processo di apprendimento noi lo scriviamo questo vettore dei pesi incogniti in generale come un vettore colonna w0 fino a wn quindi questo è un vettore che avrà n più 1 componenti quindi la dimensione sarà n più 1 per 1 quello che facciamo è modifichiamo il vettore di input x quindi qui sempre p piccolo e relativo al generico punto ci stiamo riferendo del nostro dataset in questo modo andiamo a prendere le sue componenti che sono sempre x1 x2 fino a xn e poi ci aggiungiamo in testa questo vettore colonna un 1 questo e indichiamo tutto questo come x con sopra vedete questo piccolo cerchietto questa è una notazione convenzionale che ci permette di introduzione di una formulazione compatta perché a questo punto la nostra combinazione lineare la possiamo esprimere semplicemente tramite il prodotto scalare il prodotto scalare tra questi due vettori x trasposto con il cerchietto del piesimo punto moltiplicato per il vettore vdoppio perché se voi andate a fare questo prodotto scalare il prodotto scalare tra vettori vi ricordo è il prodotto delle componente per componente quindi qui significa fare un prodotto di un vettore colonna trasposto che diventa un vettore riga quindi abbiamo un vettore questo di nuovo ha scusate n di 1 righe quindi abbiamo il vettore trasposto diventa 1 per n più 1 scusate lo scrivo meglio però non si capisce 1 per n più 1 se facciamo l'analisi della dimensionale che viene moltiplicato per un vettore n più 1 per 1 e questo ci dà un vettore 1 per 1 cioè uno scalare e infatti il prodotto scalare restituisce uno scalare cioè facciamo che cosa? un'operazione di questo tipo andiamo a prendere il primo elemento ve lo evidenzio qua il primo elemento del vettore x trasposto che è questo che va a moltiplicare u più 0 cioè ci ritroviamo u più 0 ok? quindi questa cosa qua è uguale a w più poi cosa abbiamo? il secondo elemento che va a moltiplicare w 1 quindi x 1 p per w 1 più x 2 per w 2 più x n per w n che è esattamente l'espressione che abbiamo scritto nella pagina precedente però il fatto di introdurre questo 1 diciamo in testa questo vettore colonna ci permette di esprimerla in maniera molto compatta come il prodotto tra i due vettori a questo punto il generico punto y di risposta al nostro output quello che abbiamo chiamato appunto y p noi vogliamo che sia il più possibile vicino a questo prodotto scalare e nel momento in cui introduciamo questa notazione vediamo immediatamente appunto quale qual è l'incognita del nostro problema perché noi abbiamo il dato nostro che è x e quello lo sappiamo e sappiamo anche y e dobbiamo determinare w ne abbiamo però diversi di questi punti quindi vogliamo determinare il w che è uguale per tutti i punti intendiamoci che però ci permette di far valere questa uguaglianza approssimata il più possibile per tutti i punti del dataset e questo equivale a risolvere poi trovare questo w per risolvere un problema di ottimizzazione ed equivale appunto effettuare quello che è l'addestramento di questo modello a partire dai dati cioè il sistema impara a partire dai dati a costruire i valori di questo modello una volta che abbiamo fatto l'ipotesi abbiamo detto il modello è questo è un modello lineare dobbiamo far apprendere al sistema a partire dai dati una volta che abbiamo fissato la nostra ipotesi che è un modello lineare i suoi parametri come possiamo trovare quindi i parametri di questi per piano l'abbiamo già detto più volte ma lo ripetiamo chiaramente qui perché qui arriviamo finalmente al punto cruciale perché l'abbiamo dato per scontato fino adesso l'esistenza delle funzioni di costo dobbiamo cominciare a costruire delle funzioni di costo che sono un altro ingrediente fondamentale di qualunque progetto di machine learning dobbiamo definire quindi una funzione di costo una funzione di perdita anche viene chiamata come vi ho detto più volte che ci permette di misurare quanto il modello che stiamo costruendo è aderente vicino ai dati e qui diciamo ci sono diverse scelte per il problema di regressione una tra le possibili opzioni a disposizione quella che però è per diversi motivi la più la più utilizzata è la la funzione di costo ai minimi quadrati la funzione di costo ai minimi quadrati che ha una storia lunga anche nella scienza viene utilizzata per tantissimi diciamo il metodo di minimizzazione ai minimi quadrati vengono utilizzati a partire da almeno direi un paio di secoli se non di più l'aveva utilizzata Gauss per calcolare fare delle previsioni sulle orbite di alcuni oggetti celesti pianeti o asteroidi o cometi non mi ricordo quali la differenza qual è? è che noi utilizziamo diciamo questo armamentario teorico in un contesto in cui il numero di feature cresce e può crescere in maniera estremamente consistente allora adesso vi spiego vediamo insieme qual è questa questa funzione ai minimi quadrati ma diciamo intanto la prima cosa di nuovo è ricordarsi che noi vogliamo che questo prodotto che è un numero quindi preso il generico punto questo prodotto sia il più possibile vicino a quella che è il suo output effettivo quello che è l'output effettivo che noi abbiamo nel dataset per questo punto ok per fare questo quello che possiamo fare è introdurre una quantità che chiamiamo g che dipende da w qui è g che ha un indice p p piccolo perché chiaramente stiamo misurando lo scarto del generico punto piesimo il quinto il sesto il decimo punto dei mille punti che ho e come lo misuro questo scarto allora quindi questa è quella che viene chiamata quindi per il motivo che appunto introduciamo la dipendenza da p piccolo viene chiamata funzione di costo point wise cioè relativa al singolo punto è una funzione che dipende in realtà cioè varia con ovviamente quest'indice ma dipende da che cosa dai pesi quello che per noi è fisso è l'input e l'output a rigor di logica dovrebbe in realtà al variare del dataset chiaramente varia anche la funzione di costo ma per noi fissato il dataset questa è la nostra incognita questo e questo sono fissi cioè l'input e l'out in questo caso vedete quello che facciamo semplicemente che cosa facciamo prendiamo la differenza tra quello che ci restituisce il nostro modello e quello che dovrebbe restituirci e le leviamo al quadrato le leviamo al quadrato perché perché noi ci potremmo trovare in delle situazioni in cui prendere la sola differenza in realtà non non ci porta a un risultato corretto perché io posso avere un caso in cui ad esempio sono in una situazione di questo tipo ho questi tre punti e questa retta rappresenta un fit incorretto e qui diciamo se anche prendessi la differenza la differenza sarebbe zero corretto ma se fosse una situazione di questo tipo in cui ho sempre gli stessi tre punti e avessi questo modello e questo modello vedete il contributo di questo punto è zero lo scarto di quest'altro ha questo valore lo scarto di quest'altro ha quest'altro valore uno è positivo uno è negativo se li vado a sommare perché poi a me interessa non vedere solo il singolo punto ma andremo a fare la somma su tutti i punti di questi valori se io vado a prendere un qualcosa che non ho elevato al quadrato ma me lo tengo con segno rischio che mi si compensino valori positivi e negativi questo è un modello chiaramente del tutto sballato rispetto alla mia nuvola di punti ma non me ne accorgo quindi vedete dobbiamo stare attenti a costruire una funzione di costo che sia ragionevole e quindi non possiamo prendere solamente la differenza dobbiamo prendere la differenza al quadrato per evitare di ritrovarsi in una situazione di questo tipo in cui più e meno si vanno a sommare perché quello che noi vogliamo è non solo il singolo punto quindi la funzione point wise sia tale da ridurre questa differenza ma noi vogliamo che questo sia vero simultaneamente per tutti i punti del 27 e allora cosa facciamo prendiamo la media cioè costruiamo la nostra funzione la nostra loss function in questo modo andiamo a fare il contributo delle varie perdite point wise su tutti i punti lo dividiamo per il numero di punti e otteniamo questa e qui sì che se mi ritrovo se avessi scelto la semplice differenza mi sarei ritrovato potenzialmente in una situazione di quel tipo che voglio evitare allora di qui il fatto di avere il quadrato ecco perché si chiama la funzione di costo ai minimi quadrati io voglio minimizzare uno scarto che è uno scarto quadrato tra la risposta del mio modello e l'input ci siamo? sì fin qui non sono cose particolarmente complicate vediamo un po' un esempio che ci permette comunque ulteriormente di andare a chiarirle e ragioniamo un po' sopra allora noi quindi siamo arrivati al punto in cui vogliamo trovare il vettore parametro W che minimizza la funzione che abbiamo chiamato G di W questa funzione in qualche libro di testo forse ve l'ho già detto la potete trovare con L L sta per l'os L minuscolo anziché i parametri W potete trovare un insieme di parametri che viene chiamato il vettore dei parametri che viene chiamato θ ma è sempre la stessa cosa è la nostra funzione di perdita la nostra loss function o cost function che noi vogliamo minimizzare quindi per minimizzare G di W dobbiamo trovare il valore di W che minimizza questa che è la nostra funzione di costo di minimi quadrati il che significa risolvere un problema di ottimizzazione che viene chiamata non vincolata perché W non ci sono vincoli particolari su quali valori può assumere W W può prendere tutti i valori nel campo rm e qui vedete un'esemplificazione abbiamo una serie di punti una nuvola di punti abbiamo questo in rosso che è la retta che rappresenta il fitting di questa nuvola di punti abbiamo per ognuno di questi un certo scarto che corrisponde a questo segmento e se andiamo a vedere la figura di destra il quadrato costruito su questo segmento ha una sua superficie e questa superficie rappresenta il contributo di quel punto alla funzione di costo minimi quadrati perché è lo scarto al quadrato vedete che quest'altro punto ad esempio questo che è più vicino alla retta avrà associato un quadrato chiaramente più piccolo e quindi il contributo di quel punto sarà inferiore quest'altro punto che è molto lontano avrà associato invece particolarmente pronunciato quindi darà un peso maggiore alla funzione di costo è chiaro che io qui su quest'ultimo punto che ho appena tracciato che ho appena evidenziato sono più lontano dal mio modello e quindi il mio modello spiega peggio quel punto è un fitting peggiore per quel punto d'altra parte i modelli non sono esattamente esattamente useremo proprio quei metodi adesso vediamo proprio se non ricordo male c'è proprio un esempio subito qui dietro ancora no ne parliamo sì esattamente cominciamo a parlarne proprio qui perché una volta abbiamo definito la funzione di costo l'andiamo a minimizzare con i metodi che abbiamo visto allora per applicare i metodi che abbiamo visto abbiamo visto metodi di ordine 0 di ordine 1 di ordine 2 ormai sappiamo quali caratteristiche hanno possiamo fare qualche considerazione la prima considerazione è una considerazione che ci riporta a un teorema un teorema di cui che non è particolarmente difficile però non vi do la dimostrazione chi è interessato diciamo può chiedermi qualche qualche riferimento e vi dico dove dove trovarla non è particolarmente non è particolarmente complicata ma non diciamo per il momento la lasciamo la lasciamo da parte e questo teorema è la dimostrazione del fatto che la funzione di costo ai minimi quadrati è convessa per qualunque dataset voi possiate andare a prendere in considerazione e non solo è convessa ma è anche infinitamente differenziabile cioè differenziabile un numero infinito di volte quindi per questo peraltro per qualunque dimensione quindi per qualunque valore di n questo significa una cosa importante perché possiamo applicare qualunque algoritmo di ottimizzazione locale per minimizzare questa funzione quindi non solo ordine 0 ma anche ordine 1 anche ordine 2 perché abbiamo in ogni punto esiste non solo il gradiente ma esistono anche le derivate seconde quindi possiamo anche questo berlessiano utilizzare anche il metodo di newton e l'altra cosa importante è che non solo è una funzione convessa cioè per dimostrare che è una funzione convessa si va a dimostrare che è una funzione quadratica ora che è una funzione quadratica diciamo ci si è intuitivo perché l'abbiamo costruita come il quadrato di qualcosa quindi diciamo lì dietro c'è questo ma la cosa bella è che si può andare proprio a dimostrare nella dimostrazione che è convessa si costruisce questa funzione di costo e si vede che da un punto di vista vettoriale ha esattamente la struttura di quelle funzioni quadratiche che abbiamo visto la settimana scorsa cioè la posso scrivere come A più vi ricordate no? c'era un termine B per W poi c'era il termine con la matrice C WT per C per W vi ricordate? esattamente esattamente esattamente con l'algoritmo di ordine 2 si minimizza in un passo solo ma non solo la cosa la cosa bella è che quella matrice C che governa questa questa nella dimostrazione quello che si arriva si scrive questa funzione si dimostra che ha una forma vettoriale matriciale dell'equazione che diciamo va a mettere in causa appunto questa matrice C quello che si può dimostrare è che la matrice C è positiva semidefinita quindi ha autovalori positivi e a quel punto diciamo ammette è convessa ammette un minimo che è un solo minimo e come dicevi correttamente essendo una funzione convessa e quadratica la possiamo minimizzare con un solo step dell'algoritmo Ninton questo è quello che è equivalente a quello di fatto nella formulazione che vengono chiamate equazioni normali le equazioni normali sono la soluzione con un solo equazione quindi diciamo in forma chiusa in forma analitica di questo problema il problema della regressione lineare è uno forse direi l'unico che incontreremo uno dei pochi problemi in machine learning l'unico che incontreremo che ammette una soluzione in forma chiusa cioè noi possiamo trovare la soluzione il valore di w che minimizza quella funzione di costo ai minimi quadrati quindi la regressione lineare con quella funzione di costo è l'unico caso in cui riusciamo a ricavare la soluzione in forma chiusa cioè anche questa è una dimostrazione che non vi farò ma ve la dirò diciamo diciamo così ve la dico tra un po' adesso vi faccio vedere prima un altro esempio poi vi dico quali sono queste queste vengono chiamate equazioni normali che in realtà è un'equazione matriciale singola e anche di questo diciamo non vi do la dimostrazione per il momento se avanza del tempo la possiamo fare ma di fatto si basa sul presupposto del teorema del fatto che la funzione di costo è convessa ovunque ed è anche una funzione appunto quadratica e quindi la possiamo minimizzare solo passo quindi regressione lineare con i minimi quadrati noi riusciamo a trovare vedrete è un'equazione un'equazione matriciale che vi permette in forma chiusa di ricavare il valore di W doppio ok è equivalente alla risoluzione di un sistema lineare in equazioni e in incognita e questo adesso lo vediamo dopo tra poco però non solo abbiamo a disposizione il metodo del secondo ordine volendo possiamo applicare anche quelli del primordine e quelli del primordine continuano a funzionare bene con tutti ovviamente i discorsi che abbiamo fatto limiti pro e contro che hanno anche i metodi del secondo ordine in particolar modo se n cresce il metodo del secondo ordine può diventare complicato significa risolvere un sistema di equazioni di n in questo caso n per n dove n può crescere molto allora è più conveniente usare il metodo del gradiente il metodo del gradiente ha altre limitazioni l'abbiamo già detto la discesa del gradiente ma insomma valgono tutte quelle considerazioni però è una cosa molto interessante che viene esemplificata ecco in questo esempio questo è un esempio unidimensionale abbiamo la nostra nuvola di punti quindi esattamente tipo quello da cui siamo partiti il prezzo delle case in funzione della superficie e questo è quello che viene riportato nel grafico a sinistra nel grafico A e qui nel mezzo vedete che abbiamo una funzione che è la funzione di costo cioè in funzione qui abbiamo chiaramente il nostro modello che possiamo andare a scrivere come y uguale w0 più w1 per x e e questa è la funzione di costo ai minimi quadrati che abbiamo appena visto in funzione di w0 e w1 al variare di w0 e di w1 avremmo un qualcosa che è questo oggetto che in tre dimensioni è la superficie convessa di quella funzione quadratica lo vediamo da qui ma lo vediamo anche dalle curve di livello che sono queste lissi chiuse e vi ricordate vi ho detto a seconda che siano più o meno schiacciate quello dipende da quali sono i valori della della matrice c della funzione quadratica ma qui vediamo subito che una funzione di costo di questo tipo è questa chiaramente non è una dimostrazione è semplicemente una riprova del fatto che su quel dataset la funzione di costo ai minimi quadrati è una funzione di costo di questo tipo convessa e quadratica quindi è semplicemente una conferma del teorema e allora siccome è convessa proviamo a vedere cosa succede ed è minimizzabile appunto il minimo tramite il metodo di ottimizzazione locale arriviamo anche al minimo globale vediamo cosa succede se ipotizziamo di utilizzare per esempio il metodo di discesa del gradiente in cui fissiamo ad esempio un valore di alfa l'alfa vi ricordo di learning rate no? pari a 0,5 e facciamo 75 passi quindi 75 iterazioni del metodo di discesa del gradiente partendo da un punto arbitrario questa simulazione l'esito di questa simulazione è riportato qui e di nuovo il punto arbitrario sia per esempio questo da qui ci muoviamo verso questa direzione e il primo punto arbitrario corrisponde però chiaramente ogni punto in questo piano questo è un piano in cui abbiamo w0 e w1 quindi ognuna di quelle coppie definisce un parametro un vettore scusatemi dei parametri e quel vettore dei parametri è un modello potenzialmente e corrisponde ad esempio a questo modello e vedete che all'inizio io ho gettato i dadi a caso ad esempio e ovviamente ho pescato un modello che non ha nulla a che vedere con quella che è la mia nuvola di punti però man mano che vado avanti nella discesa del gradiente io mi muovo e vado per esempio verso questo punto e da qui magari vado verso qua ovviamente prenderò la direzione della discesa quindi sempre ortogonale alle linee di livello vi ricordo e mi ritrovo in una situazione in cui il modello vedete comincia a essere questo e questo già è un po' meglio di quell'altro chiaramente perché è un po' meglio perché sto andando verso valori di minimo della funzione ogni iterazione mi abbassa la loss function poi quando mi avvicino al minimo qua e già vedete che ho questa bella retta che comincia ad essere un buon fitting anche visivamente rispetto alla nuvola di punti quando sono arrivato al minimo sono esattamente su questo che sarà il mio modello finale che è quello che minimizza lo scarto quadratico medio cioè la distanza di ognuno di quei punti della mia nuvola di punti da questa retta e questo corrisponde a questo che abbiamo chiamato history plot cost plot il grafico della storia della minimizzazione della nostra funzione di costo questa storia implica che noi scendiamo da un punto iniziale con un certo rate questo 0.5 andiamo vedete molto rapidamente a convergenza se avessimo scelto un learning rate più basso 0.01 vi ricordate che cosa stava a significare questo? che noi avanzavamo di passi più piccoli in effetti vedete che arriviamo a convergere verso il minimo ma ci arriviamo più lentamente che è questa curva in color fucsia qua sopra e qui abbiamo sotto controllo ovviamente la dinamica dell'ottimizzazione che ci porta a ottenere alla fine il nostro modello quando siamo qui siamo nel punto di minimo quindi questo è il punto lo scriviamo qui questo è il punto w0 w1 con un asterisco diciamo che ci porta a identificare quello valore del parametro w0 e del parametro w1 che corrispondono al minimo quindi quello che viene chiamato l'argmin cioè l'argomento che minimizza quella funzione di costo viene bene ok questi sono gli stessi punti di questa slide precedente e sono stati generati a partire da una retta e aggiunto del rumore quindi sono effettivamente dei punti che sono 50 punti se vi andate a contare che si adattano abbastanza bene a questo tipo di modello ok quindi diciamo vedete come introdurre una funzione di costo idonea e cruciale per la riuscita della soluzione del problema però ovviamente insomma i problemi sono sempre dietro l'angolo ulteriori problemi e diciamo un potenziale problema che può venire fuori con la funzione di costo ai minimi quadrati è che appunto essendo una funzione che eleva al quadrato gli errori chiaramente errori più larghi avranno sempre maggior peso quindi diciamo è una è una funzione di costo che enfatizza particolarmente degli scarti ampi perché lì eleva al quadrato e allora questo è un problema nel caso in cui il dataset di addestramento sia particolarmente rumoroso cioè contenga quelli quindi un rumore per cui ci sono quelli che vengono chiamati anche outlier gli outlier sono dei punti che sono al di fuori della statistica del della distribuzione statistica caratteristica del del dataset cosa intendo intendo se noi guardiamo ad esempio questo questa distribuzione dei punti e vedete che questi sono chiaramente dei punti che sono abbastanza allineati su un'ipotetica retta che potrebbe passare per qui ok però c'è questo punto qui che è evidente che al di fuori della distribuzione statistica di questi punti e che rappresenta appunto quello che viene chiamato un outlier e questo outlier complica la questione perché perché il nostro modello essendo questo un outlier determina un certo errore e questo errore rispetto al modello viene amplificato perché viene elevato al quadrato allora che cosa succede che rispetto a questo che sarebbe probabilmente la curva più giusta la retta migliore e lui che cosa fa cerca in qualche modo di piegarne la pendenza verso l'alto perché perché avrebbe su quest'altro modello uno scarto ancora maggiore e siccome lo eleva al quadrato il nostro algoritmo di apprendimento di minimizzazione quello che fa è cercare di minimizzare questo scarto quadrato quindi di piegare verso questo questo punto il modello e però chiaramente se questo è il risultato di un artefatto come questo non lo vogliamo e allora qui bisogna capire se il risultato di un artefatto oppure se effettivamente così perché il data 7 così questo non è mai semplice è un problema diciamo abbastanza complesso quello che si può fare è provare a costruire una funzione di costo che sia un po' meno sensibile a questi a questi possibili valori che possono essere delle fluttuazioni statistiche nel dataset di apprendimento di addestramento scusate e la funzione di costo alle minime deviazioni assolute è una funzione di costo che di fatto viene utilizzata proprio per queste diciamo casistiche in questo caso quindi per coprire questa casistica di dataset rumorose cioè quando siamo abbastanza sicuri che il dataset contenga potenzialmente dei valori spuri quindi degli errori allora è bene non forzare troppo la mano con il metodo dei minimi quadrati e magari utilizzare una funzione di costo alle direzioni assolute che quello che fa è semplicemente andare a prendere anziché lo scarto quadratico lo scarto in valore assoluto tra il valore della funzione sul modello e il valore effettivo dell'output quindi si prende il valore assoluto anziché lo scarto quadratico per ogni punto è che vantaggio al genere prendere lo scarto quadratico piuttosto di dire valore assoluto cioè non si potrebbe prendere sempre l'output assoluto a questo punto sì si potrebbe fare però il valore assoluto se tu ragiona un attimo la funzione valore assoluto in una dimensione che caratteristica ha quindi è giusto è corretto quello che dici a questo punto non mi conviene prendere direttamente la funzione valore assoluto sì però trascuriamo il problema della minimizzazione perché perché non ha più le caratteristiche diverse è ha delle caratteristiche diverse quindi diciamo convessa lo è però non è differenza esattamente cosa c'è lì che se tu pensi alla funzione in una variabile ammette la derivata prima ma la derivata seconda è sempre zero per esempio insomma c'è quel punto di discontinuità quindi insomma introduciamo una serie di questioni che possono rendere un po più complicato il queste considerazioni anzi rimaniamo qua queste così vi scrivo intanto la funzione valore assoluti e poi la funzione di costo e valore assoluti volevo scriverla qua intanto la scriviamo qui nel punto di costo e valore assoluti point wise è chiaramente gp di w allora possiamo scrivere come xp trasposto per w meno y questo xp trasposto lo prendiamo con il cerchietto sopra a indicare sempre che abbiamo il valore l'uno in cima al vettore colonna e poi chiaramente la funzione di costo g di w diventa 1 su p per la somma per p piccolo che va da 1 a p grande dei vari g con p di w ok questa è una funzione che anche questa si può dimostrare che è sempre convessa ok però la derivata seconda è zero praticamente dovunque quindi se la derivata seconda è zero ci facciamo poco perché il metodo di secondo ordine di fatto non lo possiamo utilizzare infatti non è minimizzabile in un solo passo è convessa ma non è più quadratica quindi non è più minimizzabile in un solo passo con le equazioni normali col metodo di newton le equazioni normali che ancora vi ho fatto vedere ve le faccio vedere quando abbiamo terminato il che significa che abbiamo a disposizione solo il metodo di ordine zero se siamo in bassa dimensione o al limite il metodo di ordine 1 discesa del gradiente ok quindi questo il fatto 1 lo può utilizzare però sa che ha a disposizione solo quel tipo di approcci qui abbiamo un esempio di una minimizzazione ecco ve l'ho scritta questo è quello che vi avevo già anticipato nella pagina precedente qui abbiamo la funzione di costo point wise che quindi possiamo esprimere come differenza tra il valore che viene restituito dal nostro modello quindi lo scarto rispetto all'output in valore assoluto questo deve valere simultaneamente per tutti i punti del nostro dataset di addestramento che va da 1 fino a 1000, 10.000 quelli che sono i punti e se io faccio la media quindi di queste funzioni di costo point wise ottengo questa e qui abbiamo un run di discesa del gradiente a partire dal dallo stesso punto scusate che a partire dallo stesso dataset con i due metodi quindi con due funzioni di costo diverse la funzione di costo è minimo in quadrate e la funzione di costo è la deviazione assolute e vedete che abbiamo qui questo outlier quindi qui vediamo l'effetto chiaramente di quello che è l'outlier il modello colorato in nero è il modello ottenuto con la funzione di costo ai minimi quadrati che è ovviamente più spostato verso l'outlier perché cerca di minimizzare la distanza da quella outlier e ovviamente anche dall'insieme di punti quella in viola invece è la funzione di costo della deviazione assolute che vedete è molto più in linea con quello che cercavamo perché? perché diciamo non dà lo stesso peso a questo scarto che avrebbe se quello scarto fosse elevato al quadrato quindi riesce ad adattarsi meglio alle caratteristiche del dataset e qui a destra abbiamo appunto la funzione di costo durante il processo di ottimizzazione in ambedue i casi vedete che sono due curve che vanno a convergenza chiaramente quella alle deviazioni assolute è più bassa perché restituisce dei valori che sono ovviamente più bassi perché è lo scarto in valore assoluto e niente questo insomma vi fa vedere che la scelta della funzione di loss diventa cruciale anche qui per ottenere modelli che potenzialmente sono diversi già solo in questo semplice esempio un'alternativa è la dove uno disponga di metodi per identificare che ce ne sono anche qui questi outlier e ripulire il dataset preventivamente quindi effettuare un filtraggio togliere i punti più numerosi anche qui non è facile ma ripeto qualche modo qualche metodo c'è quindi in qualche modo si può pensare di mettere in piedi un pre-processing di questo tipo e utilizzare direttamente il metodo ai mini quadrati però la deviazione assoluta è del tutto un metodo più che legittimo tanto più che appunto appoggiandoci sul metodo di scesa del gradiente che è il metodo sicuramente più utilizzato come vi dicevo non abbiamo problemi particolari e riusciamo anche qui a scalare appunto in termini di dimensioni in maniera molto agevole benissimo allora come possiamo fare una predizione una volta che abbiamo addestrato il nostro modello abbiamo visto fino adesso no? se torniamo un attimo indietro indipendentemente dalla funzione di costo io arrivo alla fine del processo di addestramento quindi quando sono qui alla centesima interazione con un modello che è arrivato con dei parametri quindi una funzione di costo che è arrivata a convergenza verso il minimo ospicabilmente o un minimo locale comunque e decido a quel punto di congelare il mio modello quello sarà il mio modello da lì in avanti come faccio a utilizzare quel modello per fare una predizione? quindi se arriva il cliente e mi dice ma io voglio sapere qual è la previsione di prezzo per questo appartamento io mi sono costruito il mio modello di regressione e a quel punto quello che faccio è avere il mio insieme il mio vettore che rappresenta possiamo chiamare W star a questo punto che rappresenta l'insieme ottimale dei pesi che minimizzano la mia funzione di costo della regressione e il mio modello è questo è un modello che dipende chiaramente dall'input x dipende dall'insieme dei pesi e sarà il prodotto scalare tra l'input x e il vettore dei pesi ottimali che sarà esattamente questa combinazione lineare più il termine di bias dei costi scusate dei pesi e degli input che è quella da cui siamo partiti all'inizio solo che qui ci ho ho fissato tra le infinite alternative l'insieme di parametri che mi minimizza la mia funzione di costo quindi dato un input lo possiamo chiamare x con 0 l'output corrispondente che sarà la predizione del mio modello sarà ovviamente model di x0,w star sarà y0 quindi sarà il valore che se andiamo sulla rappresentazione unidimensionale di questo grafico se io fisso un certo valore ecco vedete x' ottengo il corrispondente y' se fisso un altro valore che è supponiamo x con 0 lo chiamiamo avrò un valore y0 e così via quindi questo sarà il prezzo previsto per questo valore incognito di superficie dell'appartamento se torniamo all'esempio da cui siamo partiti all'inizio della lezione e quindi qui c'è anche una domanda la prima domanda era come facciamo a effettuare la predizione ok e questa abbiamo risposto c'è un'altra domanda che viene posta in questa slide come possiamo valutare la qualità del modello cioè come possiamo essere sicuri che il modello sia più o meno buono beh diciamo tramite quelle che vengono chiamate delle metriche cioè qualcosa che mi permetta di andare a dire ok ho costruito quel modello lo voglio confrontare per esempio con un altro modello che un'altra persona ha costruito sullo stesso dataset quello che posso fare è andare a vedere che cosa succede introducendo una metrica cioè un qualcosa che mi misura la qualità del modello e anche qui per misurare la qualità di un modello di questo tipo quello che si può fare è fare ricorso a delle metriche che sono delle funzioni simili a quelle che abbiamo utilizzato per costruire il modello nel senso che io come faccio a vedere vado a vedere la bontà di questa retta rispetto a quella nuvola del fitting di questa retta rispetto a quella nuvola di punti posso andare a vedere ad esempio lo scarto quadratico medio che è quello che abbiamo fatto per costruire il modello oppure lo scarto alle deviazioni assolute e lo posso fare andando a valutare un modello secondo una metrica anche se l'ho costruito con un'altra anche se in realtà è del tutto ragionevole tenere queste due cose allineate per cui valutare un modello costruito con lo scarto i minimi quadrati con quello che viene chiamato errore quadratico medio molto semplicemente una volta che ho congelato il mio modello quindi questo vado a vedere per ognuno dei punti del mio dataset di addestramento qual è lo scarto quadratico li vado a sommare vado a dividere per il numero di punti ottengo quello che viene chiamato min squared error per ridurre l'effetto di outlier quello che si può fare è prendere un'altra cosa che si può fare è prendere la radice quadrata di questo valore quindi la radice quadrata dell'MSE viene chiamata RMS dove R sta per root cioè la radice dell'errore quadratico medio oppure in modo del tutto analogo come abbiamo visto prima possiamo costruire valutare una funzione di costo ai minimi valori assoluti e la metrica in base alla quale andiamo a valutare il modello corrispondente che è sempre questo vi ricordo che questo modello lo possiamo costruire come vogliamo con scarto la deviazione assoluta o con funzione di costo scusate ma la deviazione assoluta o funzione di costo ai minimi quadrati e poi lo possiamo anche valutare con MSE oppure MAD che sta per min absolute deviation che è ovviamente questa funzione di costo cioè il mio modello ha un certo scarto da ogni punto YP del dataset in questo caso di addestramento prendo la media e ottengo un numero che è appunto calcolato secondo queste che vengono chiamate metriche di valutazione della qualità del modello bene una cosa che diciamo qui ancora non abbiamo ben chiarito e che perché non ne abbiamo parlato quindi più che chiarito non abbiamo introdotto che fino adesso stiamo lavorando sul dataset di addestramento vi anticipo un concetto che poi vedremo anche più avanti nel seguito del corso è che in realtà quello che si fa nel momento in cui si costruisce il un modello di machine learning è avere a disposizione un dataset che non viene utilizzato tutto per l'addestramento ma quello che si fa è suddividerlo in una percentuale più piccola viene lasciata il percentuale più grande viene lasciato ovviamente per l'addestramento perché vogliamo costruire il nostro modello su più dati possibili però c'è una percentuale che viene scelta al beta raramente può essere 20% 30% dei punti del nostro dataset di campione del nostro dataset che vengono lasciati per effettuare quello che viene chiamato il il testing quindi vengono chiamati test set il test set serve per fare cosa per dire ok io ho costruito un modello voglio vedere come si comporta nella realtà ipoteticamente in realtà fare delle proiezioni di come potrebbe comportarsi nella realtà prima di metterlo al lavoro nella realtà cosa vuol dire? vuol dire che nel momento in cui noi abbiamo costruito un modello vogliamo per esempio mandarlo in produzione prima di fare questo lo vogliamo testare però chiaramente il testarlo ecco io posso andare a calcolare queste metriche sul dataset di addestramento però ci dicono che cosa ci dicono magari appunto abbiamo visto della curva di costo siamo arrivati a convergenza ci danno un valore che sì può essere significativo ma diciamo ci sono un po' di cose che vedrete dovremo dovremo andare a discutere quando introdurremo certe altre questioni ma intuitivamente è chiaro che nel momento in cui io lavoro solo sul dataset di addestramento posso avere dei problemi che non ho visto in quel dataset e allora quello che si fa è appunto isolare dei punti del dataset lasciarli da parte e quei punti non farglieli mai vedere al modello quindi non utilizzarli per il dataset di addestramento e simulare quindi di fatto che quelli siano dei punti che potenzialmente potrai incontrare una volta che è stato addestrato il modello e lì posso andare a calcolare esattamente metriche di questo genere quando vado a vedere che cosa succede sui punti del dataset di testing non più di quello di addestramento che sono punti che il modello non ha ripeto mai visto in questo modo simulo il comportamento che c'è nella realtà anche se poi la realtà spesso può essere diversa da quello che in realtà c'è anche nel dataset di testing però questa è una pratica che viene fatta anche per tutta una serie di altri motivi perché quando parleremo di problemi dell'overfitting insomma si chiariranno meglio va bene? per cui queste metriche servono per valutare il modello su o da dataset di addestramento o anche di testing eventualmente perché è chiaro che nel momento in cui poi vado nella realtà io y non ce l'ho più ma y lo restituisce nel mio modello va bene allora detto questo adesso vediamo un po' di variazione sul tema in linea di principio i punti che noi abbiamo preso in considerazione hanno tutti lo stesso lo stesso peso ma questo diciamo non è sempre detto che sia quello che noi vogliamo e è per questo motivo che qualche volta si parla di modelli di regressione pesata i modelli di regressione pesata servono proprio per attribuire ai diversi punti un'importanza diversa quindi enfatizzare o deenfatizzare diciamo l'importanza di alcuni punti del dataset per motivi diversi allora qui fondamentalmente i motivi sono riportati in queste due slide sono due quelli principali uno è quello riportato da questo esempio in cui ho una serie di punti che per qualche motivo hanno delle ripetizioni cosa voglio dire voglio dire che nel momento in cui si va a fare ad esempio una misura sperimentale questo è una è una replica dell'esperimento del famoso esperimento di Galileo sul sul piano inclinato cui lui aveva misurato i tempi di percorrenza di una sfera fatta scendere sul piano inclinato e aveva fatto vedere che appunto aveva misurato le le porzioni del piano inclinato che erano che venivano percorse da questa sfera le aveva messe in relazione del tempo e aveva visto che appunto c'era una dipendenza era un moto uniformemente accelerato quindi andando a graficare in funzione delle misure del tempo quella che era la porzione l'avete sull'asse delle y di piano inclinato che veniva percorsa dalla dalla dalla sfera si ottengono una serie di valori e siccome sono delle misure sperimentali e alcuni di questi e queste misure sperimentali riportano ad esempio il tempo di percorrenza in una cifra o due cifre decimali dopo la virgola c'è un effetto di quantizzazione cioè se io ripeto le misure più volte e queste misure alcune di queste misure mi finiscono nello stesso valore cioè se io dico 3.65 ottengo quattro volte quella misurazione 3.65 e anche se magari era 3.651 diventa 3.65 oppure poteva essere 3.649 diventa di nuovo 3.65 questo lo vedete qui a livello grafico perché ci sono alcuni punti che sono di dimensione più grande vuol dire più è grande la dimensione più vuol dire che ci sono punti che cadono in quello stesso valore e la stessa cosa accade per questi altri punti qui per questi per questi ora se io volessi tracciare una retta di regressione che passa attraverso questi punti e io questi voglio considerarli come se fossero più repliche dello stesso punto non voglio che vengano contati una volta sola cioè se io ho dei duplicati perché si tratta di duplicati di punti in questo caso voglio che continuo come due punti che hanno lo stesso valore di uscita per fare questo è lo stesso valore di ingresso per fare questo quello che si può fare è una cosa di questo genere si può andare semplicemente a costruire un modello che fa una cosa abbastanza semplice di questa una cosa dicevo abbastanza semplice andiamo a costruire di fatto una funzione di costo che è questa è una funzione di costo per esempio prendiamo l'esempio ai minimi quadrati che fa una cosa di questo genere vai a prendere noi abbiamo il nostro modello ok noi prendiamo il modello di scusate di x1 prendiamo il modello di x1 w che è il prodotto scalare tra x1 trasposto e w non lo sappiamo ci togliamo y1 e lo eleviamo al quadrato poi facciamo la stessa cosa fino ad arrivare per un certo numero di punti il modello di x1 x1 w meno y1 al quadrato dove questo numero di punti è beta 1 dove beta 1 è il numero di ripetizione del primo punto no? sono quei punti ripetuti ho quattro occorrenze vuol dire che beta 1 vale 4 del punto 1 la stessa cosa ripeto per il punto 2 quindi costruisco la mia funzione di costo andando a metterci dentro anche lo scarto quadratico del modello sul punto x2 più anche qui anche qui ce ne avrò un certo numero quindi ce ne avrò model di x2 virgola w meno y2 tutto al quadrato e di questi ne abbiamo non sappiamo quanto beta 2 e così via fino ad arrivare all'ultimo che avrà un certo numero anche lui di ripetizioni quindi il punto pi grande che avrà beta pi ripetizioni di questo tipo allora facendo così noi arriviamo a scrivere una cosa di questo tipo torno un attimo qui arriviamo a scrivere una funzione di costo che è strutturata in questo modo che è questa che vedete è riportata qui andiamo a analizzare insieme ha esattamente la struttura che abbiamo visto prima perché io prima avevo b1 termini del primo tipo b2 del secondo tipo e così via e qui vedete abbiamo partiamo con p che vale 1 abbiamo b1 termini di scarto quadratico quindi ho le 4 se b1 vale 4 4 ripetizioni del primo punto più b2 ripetizioni sul secondo punto b3 ripetizioni sul terzo fino a b p ripetizioni dell'ultimo chiaramente se b1 vale 1 vuol dire che ho una sola ripetizione di quel punto in questo modo lo stesso punto viene pesato per esempio il primo punto quattro volte e non più una sola ed è quello che volevamo per esempio in un'applicazione di questo genere cioè non volevamo che qui risultasse un solo punto in questo punto qui per esempio ma volevamo che sia qui che qui che qui ci fosse un qualcosa che tenesse conto di quello e quindi modifichiamo la funzione di peso in modo da dire quanti punti ho ripetuti 3 4 8 la funzione di peso tiene conto di questo della funzione quella tradizionale della molteplicità abbiamo soltanto esattamente esattamente esattamente esattamente e vi faccio notare che la somma di b1 più b2 più b3 più bp vi dà il numero di punti ok un altro motivo per cui e questo in qualche applicazione può essere un qualcosa che vogliamo vogliamo fare un altro motivo per cui vogliamo introdurre la regressione pesata e quindi utilizzare una funzione di costa che è esattamente questa di nuovo che ha la stessa struttura è un motivo diverso in questo caso non abbiamo molteplicità di punti ma abbiamo qualche motivo di ritenere che su alcuno su qualche punto perché magari le procedure sperimentali con cui abbiamo raccolto i dati sono tali da permetterci appunto un'analisi di questo tipo abbiamo che su qualche punto noi abbiamo in qualche modo maggior un livello di confidenza maggiore del fatto che quel punto sia effettivamente un punto vero ok quindi se noi andiamo a vedere per esempio questo punto rosso in questo caso supponiamo che di questo punto noi siamo particolarmente fiduciosi sicuri che effettivamente rispetto a questa x il valore sia questo di y e se noi facciamo una normale regressione minimi quadrati è chiaro che questo punto è un po' fuori vedete qui è un problema contrario se vogliamo a quello dell'outlier cioè questo in apparenza è un outlier in realtà però noi siamo sicuri che è questo il punto vero allora se facciamo un metodo minimi quadrati normale otteniamo questo tipo di retta ma noi potremmo a quel punto imporre un qualcosa di diverso e dire ma andiamo a distorcere il modello verso quel punto e come facciamo a distorcere il modello verso quel punto e ripeto una sorta di procedimento inverso rispetto a quello che facciamo quando andiamo invece a minimizzare la sensibilità rispetto all'outlier qui vogliamo massimizzarla per massimizzarla possiamo andare a pesare in maniera diversa quel punto cioè andare a dire che ad esempio c'è un punto particolare supponiamo che questo sia il valore di p piccolo uguale a 1 2 sia il terzo punto 3 noi andiamo a associare a tutti gli altri un beta più uguale a 1 e beta 3 ad esempio che ne so 2 quindi ha un peso due volte quello di tutti gli altri punti e il risultato se faccio il training del modello è ad esempio questo cioè la retta mi si sposta verso quel modello se beta 3 anziché valer 2 lo facciamo valere 4 magari otteniamo questo vedete che il modello viene via via pesato fino a che ne so beta 3 adesso sto mettendo dei numeri che in realtà non corrispondono sicuramente a quello che è la realtà ma diciamo poco importa quello che importa il senso la direzione che stiamo prendendo vedete che qui viene a livello grafico evidenziato che questo punto ha maggiore importanza e il modello si sposta verso quel punto fino a far passare proprio la retta per quel punto perché glielo stiamo di fatto imponendo gli stiamo dicendo che in questo caso è 10 volte l'importanza il peso di quel punto nella funzione di costo pesa 10 volte di più di tutti gli altri punti e questo diciamo si fa appunto con un metodo che viene chiamato e che è quello che abbiamo appena descritto che è quello della regressione pesata modifichiamo la funzione di costo introducendo i pesi che nel primo esempio erano frutto della molteplicità questo è frutto del fatto che noi abbiamo più o meno un certo livello di confidenza rispetto a quelli che sono i punti del nostro data allora io direi che abbiamo visto diverse cose oggi quindi diversa carne al fuoco e abbiamo completato di fatto l'introduzione a quelle che sono le cioè i modelli lineari e la regressione lineare quindi diciamo siamo abbastanza su questo argomento andate avanti rimane fuori una cosa che vi ho detto prima e che direi a questo punto vale la pena lasciare per la prossima volta perché altrimenti mettiamo troppe cose tutte insieme che sono queste famose equazioni normali che sono la soluzione in forma analitica della regressione lineare con la funzione di costo ai minimi quadrati quindi un'equazione matriciale di cui non vi farò la dimostrazione ma che vi scriverò la prossima volta ci vuole un attimo di pazienza perché bisogna un attimo definire un po' le quantità in gioco però diciamo vedrete che è un'equazione matriciale abbastanza compatta e insomma direi che la lasciamo per la prossima volta poi nelle slide troverete anche un paio di slide dedicate alla regressione multi output ma direi che anche quelle ne diciamo ne daremo un cenno così per curiosità la prossima volta ma anche quello è un caso molto particolare perché l'output può essere o un singolo valore o un insieme di valori può essere stesso vettoriale ma molto più tipicamente è un solo valore però giusto un cenno ve lo dirò però direi che queste sono un paio di cose che potremo dire anche la prossima lezione e poi iniziare invece i primi classificatori lineari lì ci staremo anche di più e poi sulla regressione torneremo quando affronteremo il problema della non linearità però intanto per la regressione lineare è tutto e ci torneremo anche al laboratorio e vi anticipo anche che la prossima settimana facciamo lezione di laboratorio vorrei farla giovedì prossimo però vi do conferma anche di questo per la in vista della prossima settimana quindi questo comunque ve lo confermerò direi che intanto per oggi se non prego tanto sulla notazione quando scriviamo model stiamo semplicemente dicendo che che vuoi fissare questo esattamente esattamente è il modello che noi abbiamo fissato quello che prima abbiamo per intenderci era xp trasposto con sopra il cerchietto per w star va bene quindi ognuno di questi chiaramente ognuno di queste figure si riferisce a un model diverso quindi all'issima di pesi diversa va bene allora fermiamo intanto la registrazione voi