allora intanto di nuovo bentornati oggi iniziamo dopo aver visto un po di cose sulla selezione problemi di selezione delle feature iniziamo a parlare di torniamo a parlare di feature engineering quindi ingegnerizzazione delle feature però diciamo questo è il modo per introdurre vedrete il concetto di non linearità nei modelli quindi sono delle trasformazioni che come vedremo sono trasformazioni di tipo non lineare che permettono appunto di ci permetteranno di cominciare a fare dei ragionamenti sui modelli non lineari perché tutto quello che abbiamo visto finora in realtà ha riguardato il mondo dei modelli lineari quindi regressione lineare classificatore lineare binario multiclasse ma erano comunque dei modelli che avevano o nel caso della regressione il fitting di una retta di un piano di un iperpiano o nel caso nel caso dei classificatori un confine decisionale che di nuovo era una retta un piano di iperpiano adesso cominciamo a spostare la nostra attenzione verso il mondo del non lineare perché chiaramente qui bisogna fare una premessa non non è che i modelli lineari siano modelli che sono utili da un punto di vista sicuramente teorico perché servono appunto a costruire tutto un bagaglio di nozioni che sono quelle su cui abbiamo lavorato finora e sicuramente vedrete già da oggi sono riutilizzabili in gran parte anche nell'ambito non lineare ma sono anche utili dal punto di vista pratico quindi non dobbiamo far passare il messaggio che il modello lineare è un modello che non ha il suo motivo d'essere che non è di utilizzo pratico eccetera ci sono dei classificatori lineari regressione logistica svm che comunque hanno assolutamente il loro motivo d'essere anche in tante applicazioni pratiche però ci sono anche delle applicazioni in cui effettivamente bisogna andare verso dei modelli non lineari per riuscire a catturare meglio quelle che sono le caratteristiche del sistema che si va ad analizzare e quindi delle applicazioni in cui se si vuole andare con delle prestazioni di accuratezza arrivare a costruire classificatori più accurati bisogna andare verso la non linearità allora un modo in cui si può fare un modo in cui si può costruire un modello di tipo non lineare e attraverso appunto una tecnica di ingegnerizzazione delle feature che però come vedremo in realtà è fattibile da un punto di vista pratico solo in alcuni ridottissimi casi questo sì ecco cioè il punto di partenza il punto di vista da cui oggi iniziamo questo lo studio di questo argomento è un punto di vista che ci serve per introdurre i concetti ma questo sì effettivamente da un punto di vista pratico non è non è non è praticabile in molti casi per cui fare ingegnerizzazione delle feature in maniera non lineare come le introdurremo nella lezione di oggi è un qualcosa che ci serve per capire cosa sono certe caratteristiche dei modelli non lineari ma che di fatto nella pratica non si fa o non si fa più si faceva una volta fino a un po di anni fa ma è come vedrete come vi rendete conto è un approccio estremamente limitato però ci servirà per introdurre dei concetti utili vedrete che poi nel momento questi in cui questi concetti li andiamo a riutilizzare e apriamo un nuovo una nuova prospettiva sul problema lì si chiariranno diverse cose però andiamo per per gradi ecco appunto quello che vi ho riportato qua è che i concetti che introduciamo in questa lezione di oggi sono utili per diversi motivi anche se non hanno un riscontro vedrete pratico poi ve lo dico tra poco immediato si capisce bene perché però andiamo per gradi cominciamo a vedere come può essere costruito un modello di regressione non lineare quindi torniamo al problema della regressione l'idea qui è quella di dire ok io so cos'è un modello lineare quindi so che cos'è un modello che noi ci siamo abituati a scrivere come una combinazione lineare abbiamo il nostro termine di bias e poi abbiamo la nostra combinazione lineare delle nostre feature le feature sono v1 fino a vn scusatemi x1 fino a xn e queste vengono pesate con i pesi v1 fino a vn quelli che abbiamo chiamato feature touching weights i pesi che vanno direttamente a moltiplicare poi le feature quindi se vi ricordate quando abbiamo fatto lo studio della regressione lineare abbiamo detto beh e organizziamo in un vettore dei pesi tutti i pesi quindi il termine di bias e tutti gli altri pesi e rappresentiamo in un vettore colonna tutti i punti del nostro tutte le feature del nostro sample ci mettiamo anche un 1 per cercare di rendere il più compatto possibile la notazione effettivamente la notazione a cui eravamo arrivati era questo un semplice prodotto scalare tra due vettori il vettore x trasposto e il vettore colombio questo era il modello lineare di regressione noi rappresentiamo il nostro i nostri punti ognuno del nostro insieme di punti lo rappresentiamo come una combinazione lineare delle feature in questione e per cercare di fare in modo che il più possibile questo che è il nostro modello sia effettivamente il più possibile simile a quello che è il valore della risposta cioè il valore che noi sappiamo l'approccio è un approccio di tipo con supervisione quindi noi conosciamo tutti gli yp y1 y2 y3 yp grande e noi vogliamo che questa relazione valga il più possibile per tutti questi punti quindi cosa avevamo fatto avevamo costruito una opportuna funzione di costo che era la media di che cosa dello scarto quadratico di ognuno di questi punti del modello dal valore effettivo questa era la formulazione del modello ai minimi quadrati e abbiamo approfittato per fare anche un breve ripasso su questo che era il modello lineare ora vedremo che l'idea come vi dicevo è provare a trasformare delle feature cioè noi abbiamo le nostre feature proviamo a trasformarle con delle trasformazioni di tipo non lineare quindi iniettando della non linearità per ottenere quello che di fatto vedrete un fitting di tipo non lineare dei dati e vediamo che cosa succede il problema qual è ve lo anticipo subito è che io devo avere le idee chiare di quale direzione andare a prendere cioè la trasformazione che io vado a fare è una trasformazione non lineare ma come la scelgo e qui sta un po il problema che vi dicevo di questo approccio perché la posso scegliere se so dove voglio andare in qualche modo a parare ok però prima di andare a vedere che cosa supponiamo di sapere che quali sono i problemi legati a questo a questo tipo di approccio supponiamo di avere le idee chiare di sapere che noi possiamo vogliamo utilizzare un certo tipo di trasformazione quindi una funzione non lineare che chiamiamo f che noi conosciamo a questo punto questa funzione è una funzione non lineare nel nelle feature che significa che può essere vedrete una funzione quadratica può essere una funzione sinusoidale la funzione logistica qualunque funzione che non è una funzione lineare va bene va bene il problema che appunto ce ne sono un'infinità quindi dobbiamo sapere qual è qual è la funzione che noi vogliamo utilizzare detto questo se noi sappiamo scegliere la f e qui sta il vero problema da un punto di vista pratico noi in linea di principio possiamo costruire un modello che dipenderà da x sempre dal nostro punto e dipenderà da un insieme di parametri che chiamiamo teta questo modello ad esempio posso costruirlo in questo modo guardate qui cosa faccio prendo la nostra x la metto dentro la funzione non lineare e poi la moltiplico per un fattore di scala e ci sono un altro valore quindi costruisco un modello che trasforma iniettando della non linearità la nostra le nostre variabili indipendenti e poi le compone in questo modo più in generale questo è il modello più semplice che ci può venire in mente io trasformo il nostro punto x adesso poi vi faccio vedere anche un esempio grafico in cui questo si capisce meglio cosa cosa facciamo però intanto stendiamo un pochino di matematica che ci prepara un pochino di matematica che ci prepara un pochino di matematica qui abbiamo mi dicevo questa che è una combinazione lineare w1 per f di x più ci mettiamo w0 questo significa in generale nulla vieta che io posso dire beh ripeto questo schema con tante funzioni non lineari ognuna si porta dietro un suo peso poi cioè quello che posso voler fare posso anche in linea di principio dire ok cosa succede se faccio se applico una trasformazione non lineare f1 al mio dato poi una trasformazione lineare f2 sempre al mio dato e fino a una trasformazione lineare f con b grande quindi ho b grande trasformazioni lineari e poi le compongo come faccio una somma pesata quindi queste trasformazioni non lineari le compongo linearmente non è l'unico modo possibile di iniettare la non linearità ma sicuramente il modo più diciamo più immediato ci viene in mente una volta che io dispongo di tutte queste trasformazioni le posso andare a comporre insieme in questo modo e questo è interessante perché se io arrivo a scrivere ah vi faccio una cosa che non vi ho detto vi faccio notare adesso noi abbiamo chiamato i parametri teta allora teta che cosa sono? ve l'ho messo già qua allora qua sono i parametri interni alla funzione f anche ma non solo sono anche w0 w1 cioè l'obiettivo dell'apprendimento sarà a quel punto andare ad apprendere questi parametri ma anche i parametri interni alla funzione f cosa intendo con parametri interni? quella è una funzione non lineare avrà una funzione parametrica avrà i suoi parametri e io l'obiettivo sarà quello di andare ad apprendere non solo w0 w1 ma anche tutti i parametri interni a f e se vado nel caso più generale in cui ho la composizione di queste b funzioni io in teta intendo andare a mettere sì w0 w1 w2 fino a wb ma anche tutti i parametri interni a f1 a f2 fino a fb questo poi adesso ripeto si chiariscono meglio questi concetti nel momento in cui cominciamo a vedere degli esempi degli esempi pratici però intanto arriviamo all'equazione finale di questa di questa slide che è presto fatta perché se io comincio ad organizzare questi b più 1 dove l'1 e il w0 pesi in un vettore a colonna che chiamo w che è un vettore in cui ripeto mettiamo w0 w1 fino a wb e se scrivo un altro vettore che chiamo f e ci mettiamo sopra il cerchietto per indicare che il primo elemento di questo vettore a colonna è un 1 e poi vado a mettere f1 di x f2 di x fino a fb di x quello che otteniamo è che il modello che abbiamo scritto qui lo posso scrivere come il prodotto scalare tra il vettore f e il vettore w se voi andate a fare questo prodotto scalare quello che ottenete è esattamente una cosa di questo tipo avete il vostro vettore un vettore riga che è 1 e qui avete w0 w1 wb se voi fate il prodotto scalare tra questi due vettori avete ovviamente che quello che succede è che avete 1 per w0 più f1 per w1 più f2 per w2 più bla bla bla fb per wb che è esattamente questa ok ci siamo allora questo perché è interessante perché se io riesco a scrivere un modello di questo genere a costruire un modello di questo genere dove ripeto stiamo dando per scontato che noi queste cose qui le sappiamo eh cosa sono che tipo di linea di funzioni sono sono delle funzioni non lineare però lasciamo da parte il problema di quali scegliere ammesso che io riesco a costruire ammesso che io riesco a costruire una cosa di questo tipo la cosa interessante qual è è che il modello che ottengo vedete è lineare in f non lo è più chiaramente in x perché l'obiettivo è introdurre delle trasformazioni non lineare su x ma è lineare in f questa è una cosa interessante perché ha delle ripercussioni su come poi io posso andare a implementare quel modello perché io posso riutilizzare gran parte se riesco a scrivere una cosa di questo genere gran parte delle cose che abbiamo visto a proposito del metodo di ottimizzazione perché l'obiettivo qual è è ricavare w e tutti gli altri parametri qui interni cioè io dovrò andare a costruire questo teta ok tramite un'opportuna funzione di costo e quindi il fatto di scriverlo in quel modo ci riporta in un in una cornice che è esattamente quella che abbiamo introdotto a suo tempo quando abbiamo introdotto tutti i metodi di ottimizzazione che valivano per i modelli lineari però quei metodi di ottimizzazione non è che abbiamo detto valgono solo per i modelli lineari se io riesco a costruire una funzione di costo e il fatto vedete che proprio anche a livello matematico qui noi riusciamo a elaborarla esattamente in maniera analoga a quella che avevamo fatto a suo tempo per i modelli lineari vedrete che ci porta a riutilizzare per esempio algoritmi di discesa del gradiente metodi del primo secondo ordine e così via bene allora adesso facciamo un passo in avanti e cominciamo a capire qualche cosa di più di questa di questa di questi argomenti ok allora quello che mi stavo dicendo poco fa era che il nostro obiettivo a questo punto qual è è che questo che rappresenta il risultato del nostro modello e noi vogliamo che sia il più possibile simile a yp dove p è 1 2 3 se ho 2300 punti voglio che y1 y2 fino a y 2300 siano valori che sono il più possibile simili a questo che è il valore che è il valore che mi restituisce il mio modello ci siamo fin qui come posso fare a vincolare a fare in modo che questo avvenga quindi a vincolare che questa struttura che questa scrittura diciamo sia effettiva e che quindi corrispondente alla realtà beh devo di fatto costruirmi una funzione di costo quello che vi dicevo prima cioè riandiamo in un in uno schema che abbiamo già incontrato cioè ci costruiamo una funzione di costo che è una funzione di costo che non è più solamente noi l'avevamo scritta in funzione dei pesi w qui la scriviamo in funzione dei parametri θ dove i parametri θ vi ricordo sono quei pesi della combinazione lineare e tutti i parametri interni alle funzioni ok questa la scrivo come una funzione di costo che di nuovo la somma di tanti termini quanti sono gli elementi che abbiamo all'interno del dataset che chiamiamo più grande faccio la media ok faccio la media perché così è un qualcosa che risulta indipendente dal da quanti punti noi ci abbiamo messo dentro e che cosa facciamo questo è il nostro modello andiamo a misurare lo scarto rispetto alla risposta che noi vogliamo che sappiamo essere quella effettiva del dato quindi queste sono le etichette diciamo del dato prendiamo il valore quadrato il solito motivo di evitare che errori positivi e negativi si compensino e andiamo a minimizzare una cosa di questo genere la differenza rispetto al modello lineare è che qui avevamo x trasposto per w però per il resto è tutto uguale quindi quello che vi dicevo prima una cosa di questo genere chiaramente qui diciamo molte di queste funzioni di costo che potete andare a trovare e ideare non saranno più convesse ok mentre per esempio la funzione di costo ai minimi quadrati avevamo dimostrato che era convessa chiaramente questa queste proprietà le perdiamo però nulla vieta di utilizzare tutto l'armamentario tecnico che abbiamo messo a disposizione no quindi metodi di scesa del gradiente metodi del primo ordine e questo genere per andare a minimizzare questo tipo di funzione qui avete un esempio di che cosa un primo esempio di che cosa significa intanto cosa significa fare regressione non lineare intanto vi faccio vedere qui sulla sinistra di questo grafico abbiamo l'esempio che abbiamo da cui siamo partiti quando abbiamo spiegato la regressione lineare abbiamo una serie di punti e noi facciamo il fitting di questi punti quindi ricaviamo di fatto i parametri di questa retta per cui x trasposto per v doppio è il nostro modello lineare qui facciamo una cosa analoga però attraverso queste funzioni non lineari quello che ricaviamo è un modello che è lineare in f ma non lo è chiaramente più in x perché f ha trasformato x secondo una non linearità e questo fa sì che io riesca a costruire ad esempio una curva di questo genere che rappresenta il miglior fitting per questa serie di punti qui avete un esempio di un grafico che fa esattamente questo tipo di logica applica in cui avete questa distribuzione di punti che sono dei punti reali in nero qui sono punti chiaramente stiamo lavorando in una dimensione dopodiché la curva la retta blu è chiaramente il risultato di un modello lineare e vedete che corrisponde a un grado 1 perché? perché quello che viene fatto è vengono introdotte come funzioni quelle funzioni che vi ho detto che sono funzioni non lineari dei polinomi di grado 1, 2 e così via quindi polinomi di grado 1 di fatto è una retta quindi la trasformazione secondo il polinomi di grado 1 è un modello lineare ma se lo trasformo secondo un grado 3 o un grado 5 ottengo dei polinomi che sono delle curve chiaramente di grado 3 o 5 che rappresentano questi fitting di queste curve colorate che vedete cominciano a adattarsi meglio a quell'insieme di punti l'obiettivo è una cosa di costruire questi modelli non lineari è andare in questa direzione cercare di ottenere dei fitting migliori rispetto al fitting che avremo con il solo modello lineare iniettando come vi dicevo della non linearità iniettare della non linearità significa progettare una trasformazione e dire ok le feature le prendo e le trasformo secondo questa funzione non lineare in questo caso vengono trasformate secondo dei polinomi di grado 3 o di grado 5 e ottenete quelle curve lì poi vi farò vedere se spero di aver tempo anche a laboratorio come si fa però insomma intanto fidatevi su scikit-learn è molto semplice ecco volevo dire questo tra l'intanto fidatevi si può fare ecco cioè questo è un grafico ottenuto proprio da da quel tipo di di logica con con quell'implementazione ok allora per capire un po meglio le questioni che stiamo introducendo e andare verso anche un paio di esempi che ci permettano di fare qualche riflessione ulteriore su questa su questo su questo argomento facciamo un attimo un breve ripieno delle cose che abbiamo detto finora allora l'obiettivo è costruire quello che mi viene chiamato appunto anche in inglese feature mapping cioè noi vogliamo costruire un qualcosa che ci permetta di trasformare le feature quindi abbiamo le nostre feature e le trasformiamo secondo queste funzioni lineari ok il punto vi dicevo è come le scegliamo ecco lì se torno a questa figura che avevamo qua nella slide precedente beh io qui rappresento i punti siamo in una dimensione e mi accorgo che ad esempio una retta può non essere la miglior curva per fare il fitting mentre magari questi altri polinomi di grado 3 o di grado 5 possono essere delle migliori approssimazioni di quell'insieme di punti però me ne accorgo perché siamo in una dimensione quindi io ho questa possibilità fin tanto che ho feature 1 dimensionali o 2 dimensionali perché quando vado al di sopra delle due dimensioni chiaramente riesco nella terza dimensione a visualizzarlo quando ho feature 3, quando 3, più di 2 feature quindi parto da 3 dovrei andare nella quarta, nella quinta e così via dimensione ovviamente non lo riesco a raffigurare quindi la visualizzazione è un qualcosa chiaramente che ci fa comodo che riusciamo a utilizzare però appunto è limitata ed è questo un primo motivo per cui da un punto di vista pratico non è semplice riuscire a ricavare quale tipo di trasformazione è la più adatta qui ci possiamo avere un'idea gli esempi che faremo adesso tra poco ci facciamo un'idea di qual è l'andamento della nuvola di punti migliore magari però chiaramente nella pratica non sarà sempre così vi ho anticipato più volte il machine learning moderno è un machine learning ad alta dimensionalità quindi ci possiamo sicuramente scordare di lavorare in una o due dimensioni con una o due feature e questo rende tutto il discorso di progettare queste trasformazioni F1, F2 fino a FB molto complicato quindi anche solamente decidere quanti devo scegliere di quei termini F1, F2 fino a FB quanto vale quel B? non è semplice ecco non è una domanda alla quale è sempre così semplice a rispondere anzi nella maggior parte dei casi non è semplice a rispondere e questo ha fatto sì che questo che rientra a pieno titolo in quello che viene chiamato ingegnerizzazione delle feature perché noi trasformiamo le feature esattamente come facciamo quando facciamo vi ricordate quando io vi ho parlato dell'ingegnerizzazione delle feature vi ho detto beh ci sono due tecniche per esempio si fa si applica lo standard scaler cioè si fa si vanno a scalare quindi si sottrae la media si divide per la deviazione standard ecco qui siamo nell'ambito sempre dell'ingegnerizzazione cioè delle trasformazioni delle feature però è un ambito che rientra a pieno titolo appunto e vi ho detto all'epoca non è l'unica lo standard scaler quindi fare scalare le feature è una cosa che si fa comunemente per tutti i tipi di problemi però se volete costruire un qualcosa che va ad operare in ambito audio avrete fino a qualche anno fa tutto un insieme di trasformazioni dell'input che erano adatte a far emergere diciamo delle delle proprietà per cui alla fine il il classificatore che costruivate era un classificatore migliore perché le trasformazioni magari vi permettevano di discriminare meglio tra i vari punti se andate in ambito computer vision le trasformazioni sono di altro tipo ecco fino a qualche anno fa e questo ve l'avevo detto appunto anticipato ma ve lo confermo qui a maggior ragione quando si va verso la direzione della non linearità una gran parte del tempo dello sforzo di chi lavorava su questi sistemi per costruire un sistema di machine learning pratico era proprio dedicato a a fare ingegnerizzazione delle feature cioè a fare a cercare di trasformare le feature in modo da adattare il più possibile questa trasformazione al problema sottostante e il problema è che anche diciamo delle non solo la visualizzazione la perdiamo di vista quindi già facciamo fatica a capire qual è il tipo di trasformazione più adatta ma in alcuni casi le trasformazioni stesse possono diventare ad esempio molto difficili da gestire anche da un punto di vista proprio della della dimensione se voi pensate a un per esempio al numero di termini prendiamo l'esempio che vi ho detto prima una trasformazione un polinomio ok un polinomio elevato al cubo ha un numero di termini che è cubico nella dimensione cioè se voi avete tre variabili quello che partiamo da due spostiamo di avere una funzione di due variabili x1 e x2 ok se voglio scrivere un polinomio di grado 3 di questa funzione di due variabili devo cominciare a scrivere avrò il termine noto poi avrò il termine w1x più il termine v2x al quadrato facciamo anziché x1 e x2 facciamo x e y così almeno è uguale è giusto per funzioni di due variabili x e y va bene quindi qui mettiamo x al quadrato più abbiamo il termine cubico poi abbiamo la stessa cosa con y 4y più w5y al quadrato più w6y al cubo più w7 x y al quadrato più vado qui sotto w8 x quadro y più w9 x quadro y quadro ecco sono nove termini se andate a vedere e qui siamo solo in due variabili se volete costruire un un polinomio di grado 3 in generale per una funzione di n variabili dovete ritrovarvi a gestire un numero di termini che è che è cubico nella dimensione quindi n variabili significa n al cubo termini ok quindi anche questo capite bene che se n comincia a diventare grande vuol dire portarsi dietro un sacco di termini quindi c'è anche questa questa questa problematica cioè la alta dimensionalità complica un sacco l'ingegnerizzazione delle feature non lineare la complica perché non riusciamo a visualizzare il problema quindi non riusciamo facilmente a capire quali sono le tipologie di di non linearità che fanno al caso nostro ma non solo anche quando allora lo sapessimo il numero di termini risulta facilmente cioè cresce in maniera da renderlo difficilmente gestibile quindi cresce con una tale facilità nel caso per esempio dei termini polinomiali da risultare estremamente complicato ecco allora tutto questo vi dicevo era quello che succedeva fino a qualche anno fa dopodiché c'è stato un qualcosa in realtà diciamo gli strumenti di cui vi comincio a introdurre qualcosa adesso che sono ad esempio le reti neurali per intenderci le reti neurali sono un modello che vi permette di costruire dei predittori quindi classificatori o regressori di tipo non lineare ma l'aspetto interessante non sono gli unici non sono gli unici modelli ad esempio anche gli alberi sono tutti sono tutti i modelli che erano noti da da diversi anni che però in questi ultimi diciamo dieci anni hanno avuto per diversi motivi una una crescita notevole in abito del loro utilizzo eh ma uno degli aspetti più interessanti è che voi riuscite a costruire ad esempio una rete neurale a progettarla eh direttamente dall'input così come ce l'avete l'input cosiddetto grezzo e di fatto eliminando gran parte dei casi tutta questa parte molto tediosa di ingegnerizzazione delle feature a mano cioè voi non andate a specificare alla rete neurale voglio un polinomio di grado 3 o un polinomio di grado 5 voglio una trasformazione esponenziale voglio no voi vedrete addestrate la rete neurale in modo da aggiustare i suoi parametri e e lo fate senza specificare il tipo di non linearità e questo è di fatto il il clamoroso vantaggio che vi offre uno strumento come questo rispetto a tutto il processo di eh ingegnerizzazione delle feature manuale che è quello che abbiamo cominciato ad introdurre adesso è quello su cui continuiamo a ragionare adesso per alcune slide perché ripeto dobbiamo introdurre un po' di concetti però tenete presente che l'obiettivo poi alla fine sarà sbarazzarsi del dover dire allora voglio approssimare la mia la mia curva anche se sono in una in una dimensione la riesco a vedere devo dirmi ok cos'è è più approssimata eh da un polinomio di grado 3 da un grado 5 da una funzione sinusoidale da una funzione eh seno iperbolico o quello che volete tutto questo lo eliminate se utilizzate un approccio in cui non si fa più ingegnerizzazione delle feature ma si fa lo scrivo meglio ovviamente non questo ma questo questi modelli vedrete sono in grado di fare non hanno più bisogno di feature engineering cioè di ingegnerizzazione di feature ma fanno in automatico quello che viene chiamato feature learning cioè prendono loro le feature necessarie le costruiscono all'interno del modello allora torniamo al nostro problema della regressione non lineare vediamo un esempio semplice su cui introduciamo ulteriori dettagli allora supponiamo di avere un insieme di punti che è questo che vedete qui rappresentato qui a sinistra questi punti neri che hanno questo andamento vedete che questo andamento ha un andamento se io lo no avessi quei punti e riuscissi in caso non in dimensionale riesco a plottarli quindi a visualizzarli ho un andamento di tipo come se fosse un'onda ok quindi a questo punto dico se devo costruire un modello che fa il fitting di quell'insieme di punti una cosa ragionevole è modellizzarlo come un'onda e allora qui si ha senso dire introduco io una trasformazione che ad esempio è un'onda sinusoidale quindi una funzione sinusoidale quindi la mia f di x vedete che a x ho tolto il il il grassetto quindi non è più un vettore perché siamo in una dimensione e lo modellizzo come il seno di w0 più x w1 dove w0 e w1 sono due parametri sono quelli che prima vi dicevo sono i parametri interni ok e qual è il prerequisito e qui si capisce bene quello che stavamo dicendo prima cioè io qui devo uno visualizzare il poter visualizzare l'insieme di dati e ok ce la facciamo perché sono in una dimensione due devo capire e qui l'occhio umano ovviamente l'esperto oppure il fatto che io conosco bene il dominio di applicazione di cui stiamo parlando allora dico ok sì guardate che qui c'è un andamento con un andamento di tipo sinusoidale dietro ma capite bene che questo è un qualcosa che ci un lusso che ci possiamo permettere molte poche volte in tante applicazioni pratiche cioè non di meno ci è utile ripeto da un punto di vista generale per introdurre una serie di concetti perché a questo punto il mio modello lo vado a scrivere esattamente in questo modo vedete lo posso scrivere come w2 più w3 per f di x a questo punto w2 e w3 sono diciamo i parametri non più interni potremmo chiamare esterni e teta chiaramente è un qualcosa in cui è l'unione dei parametri interni e dei parametri esterni ok ed è l'insieme dei parametri del mio modello cioè io se voglio cercare di costruire il mio obiettivo è costruire un modello per cui alla fine la mia risposta sia questa sinusoide nella figura rappresentata in verde che vi ho appena evidenziato in arancione il mio obiettivo sarà andare ad apprendere un vettore teta a questo punto possiamo andare a scrivere che sarà composta da w0 w1 w2 e w0 l'ipotesi di partenza è che io sono in grado di identificare la mia funzione f che come vi dicevo è molto complicato però spesso e volentieri però se lo riesco a fare che cosa succede? che mi posso costruire un mio modello che a questo punto sarà un modello che è uguale a f con sopra il cerchietto prodotto di x scusatemi prodotto scalare per w w se voi andate a prendere questo vettore dei pesi w e ci andate a mettere dentro tutti i pesi e fate il prodotto scalare con f ritrovate esattamente questa espressione e la cosa interessante vi dicevo che questa espressione è lineare in f infatti se voi andate a costruire un grafico in cui andate a mettere per ognuno di questi punti andate a costruire la sua trasformazione secondo la funzione f e l'andate a mettere come nuovo asse delle x e poi andate a mettere il corrispondente y quello che ottenete è una figura di tipo questa riportata a destra e qui vedete una cosa che ci possiamo aspettare cioè che la dipendenza di y rispetto a f è una dipendenza lineare cioè il modello è lineare rispetto ad f non lo è chiaramente rispetto ad x ci siamo? allora questo diciamo si traduce nel fatto quello che si dice che il modello è lineare nello spazio trasformato cosa intendiamo lo spazio trasformato? è lo spazio dei punti e l'insieme dei punti che vengono trasformati secondo quel mapping secondo quella trasformazione non lineare io prendo un qualunque punto che sta nello spazio originale e lo trasformo secondo f cioè prendo il seno di w0 più x w1 quello avrà un certo valore e la y sarà il valore corrispondente la y rimane sempre la stessa chiaramente lo faccio per tutti i punti e ottengo un grafico di questo genere e vedete che l'andamento è un fitting lineare nello spazio trasformato sì cioè è lineare nello spazio trasformato se voi lo guardate e lo vedete anche da qua se voi prendete se questo è il vostro vettore di input quella è una regressione lineare ma non è il vostro vettore di input il vostro vettore di input non è f è x chiaramente però io una volta che lo trasformo lo posso trattare come se fosse una regressione lineare da qui in avanti però è ovvio che non lo sarà cioè di fatto ho trasformato x in modo da ci ho messo soprapposto una sinusoide l'ho trasformato sopra una sinusoide e l'obiettivo è ricavare u0 u1 u2 u3 in modo che mi si presenta un domani una volta che io addestro il mio il mio modello in modo da minimizzare lo scarto quadratico tra tutti questi punti e questa curva qua il che significa minimizzare lo scarto quadratico tra questi equivalente me lo costruisco alla fine fisso un valore di teta ben preciso e se mi arriva un altro punto ad esempio che ha questo valore mi chiedo scusatemi pardon ovviamente se mi arriva qui un ulteriore punto ok lo chiamiamo x con 0 e mi chiedo qual è il corrispondente valore di y 0 il mio modello mi darà subito la risposta se mi arriva un altro punto qui il mio modello mi dà quest'altra risposta questo lo chiamo x con 1 se voglio se voglio sapere se voglio sapere qual è la risposta del mio modello nel punto x con 2 il mio modello mi dirà y2 come fa? eh lui l'abbiamo addestrato abbiamo minimizzato lo scarto quadratico medio ha trovato w0 w1 w2 w3 che minimizzano quell'errore quadratico medio però il presupposto è che io ho costruito io questa non linearità è qui che c'è l'ingegnerizzazione manuale delle feature perché io ho riconosciuto che quello è un andamento di tipo ondoso sinusoidale dove l'ho detto bene le forme d'onda sinusoidali si adattano bene a questo tipo di discorso e questo non è sempre facile anzi nella maggior parte dei casi come vi dicevo è un problema non da poco ok qui c'è un altro esempio simile questo è un esempio che non so se se volendo possiamo anche ricollegarlo al mondo della fisica nel senso che è di fatto un po' la stilizzazione del di un esperimento famoso quello di Galileo sulla caduta dei gravi lui quando ha fatto i primi esperimenti sulla caduta dei gravi che in realtà non faceva cadere poi tra l'altro dall'alto perché un peso ma lo faceva rotolare delle sfere su un piano inclinato perché era più semplice misurare i tempi perché aveva chiaramente non aveva dei cronometri molto affidabili usava degli orologi ad acqua e faceva rotolare quindi queste sfere su un piano inclinato e andava a misurare i tempi di percorrenza di questa sfera sul piano inclinato ed era equivalente a far cadere un peso diciamo dall'alto andare a misurare i tempi di percorrenza sulla verticale siccome poi si tratta questo lo sappiamo dopo che appunto lui ha cominciato a formalizzare questo problema e a modellizzarlo si tratta di un moto uniformemente accelerato significa che abbiamo una dipendenza quadratica dello spazio percorso dal tempo cioè se il tempo raddoppia lo spazio diventa quattro volte tanto ok quindi lui che cosa ha fatto è andato a misurare i tempi di percorrenza su questa rampa e ha cominciato a costruirsi dei punti per ogni tempo di percorrenza uno spazio percorso altro tempo di percorrenza uno spazio percorso altro tempo di percorrenza uno spazio percorso e così via abbiamo questi punti in nero ora se abbiamo questi punti in nero se sappiamo adesso che sappiamo le leggi del moto uniformemente accelerato sappiamo che c'è una relazione di tipo quadratica all'epoca non si sapeva chiaramente però ha cominciato lui ovviamente misurando questi tempi a dire che la relazione era di quel tipo volendo andare a modellizzare una cosa di questo genere se voi avete un insieme di punti di questo tipo vedete che a occhio potete dire beh questo per esempio è una parabola una parabola vuol dire che se io voglio costruire quindi un modello quadratico di non linearità chiaramente la relazione tra y e x è non lineare se io voglio costruire un modello di tipo quadratico quello che posso fare è introdurre due termini perché in generale il modello quadratico c'ha sì il termine quadratico ma c'ha anche sotto un termine lineare potenzialmente quindi due funzioni di cui la prima è lineare la seconda è non lineare e costruire un modello che è questo qui che dipende da x ma dipende anche dall'insieme di parametri teta che è una combinazione lineare di f1 e di f2 dove f1 ripeto è il termine lineare ed f2 è il termine quadratico e in più eventualmente io potrei in questo caso non ci sono dei parametri interni ok perché ho x e x quadro ma volendo potrei mettere qui dentro ulteriori parametri che fanno parte dell'insieme dei parametri che devo andare ad apprendere come faccio ad apprendere l'insieme dei parametri che in questo caso è semplicemente W0 W1 W2 possiamo fare il solito procedimento normalizziamo i punti togliendogli la media dividendo deviazione standard perché rende il tutto il processo di ottimizzazione anche più semplice perché vi ricordo che le curve di livello sono più arrotondate eccetera eccetera minimizziamo la funzione di costo ai minimi quadrati che possiamo costruire per esempio tramite il metodo di scesa del gradiente e questo lo sappiamo fare e quello che otteniamo è questa bella curva questa curva è la curva che ottengo se io vado a mettere qui dentro mi domando non vado a mettere qual è il punto rispetto al punto x0 la risposta del mio modello e la risposta del mio modello è questa prendo un altro punto questa è la risposta del modello questa è la risposta questa è la risposta quindi per tanti punti che non fanno parte più del mio training set ma saranno eventuali punti quindi di un test set la risposta sarà questa chiaro? qui l'analogo del discorso che abbiamo fatto prima nella funzione trasformazione con la funzione seno e nella figura di destra solo che prima avevamo una sola funzione non lineare qui ne abbiamo due f1 e f2 però quello che potete fare è prendere ogni punto del vostro dataset e andarlo a trasformare secondo f1 e secondo f2 una volta che l'avete trasformato secondo f1 e secondo f2 quello che potete andare a fare è andare a vedere all'incrocio tra f1 e f2 quanto vale la y se lo fate per tutti i punti del vostro dataset vi accorgete che questi punti stanno in questo caso su questo piano e questo è corretto perché di nuovo questo modello voi lo potete scrivere come f di x per v2 e vedete che questo è un modello lineare nello spazio trasformato cioè se io prendo il vettore f1 di x e f2 di x rispetto a questo vettore questo modello è lineare quindi nello spazio trasformato è lineare chiaramente non lo è nello spazio originale e il punto di nuovo è che l'ho costruita io la trasformazione perché perché so che si tratta dell'esperimento di Galileo oppure semplicemente guardando mi accorgo che è un andamento che assomiglia a una parabola e quindi comincio a provare comincio a provare delle trasformazioni che siano ragionevoli faccio l'addestramento del modello e trovo i pesi che meglio si adattano a quell'andamento non domande siamo ok allora se ci siamo facciamo un passo avanti e andiamo a vedere anche il problema della classificazione quindi questo è come in generale può funzionare quindi la trasformazione in modo da iniettare della non linearità in un modello di regressione in modo da trasformare quel modello in un modello non lineare adesso vediamo lo stesso approccio alla classificazione per semplicità ci concentriamo sulla classificazione di tipo binario la cosa può essere estesa anche al caso diciamo multiclasse perché uno poi può costruire sempre l'equivalente di un di un classificatore multiclasse a partire da tanti classificatori binari esattamente con la regola di fusione per cui partiamo dal classificatore non lineare binario che sarà per noi appunto il riferimento e analizzeremo questo ah una cosa che non vi ho detto è che come esattamente esiste una regressione multi output per il caso lineare quindi una regressione in cui non abbiamo un solo valore di y come nel caso dell'esperimento di Galileo che ci diceva un solo valore della y ma vogliamo di avere più valori di y per un determinato valore di x la stessa cosa possiamo fare quindi possiamo fare regressione multi output di tipo non lineare si complicano le notazioni la matematica ma in linea di principio è possibile anche quello veniamo invece al caso della classificazione non lineare ripeto partiamo cioè partiamo e in realtà analizzeremo solo il caso binario e e anche qui diciamo come nel caso della regressione vedrete che quello facciamo è partire da alcuni esempi a bassa dimensionalità perché l'analisi visiva ci semplifica ci permette di catturare delle dipendenze non lineari che altrimenti risulterebbero difficili da da immaginare e qui facciamo un breve riepilogo di alcune cose che abbiamo visto e anche qui è l'occasione per fare un ripasso a proposito del dei modelli lineari la cornicola che qui appunto ho chiamato cornice lineare che cosa che cosa prevedeva prevedeva la costruzione di un modello che dipendeva chiaramente da x e da w che era il prodotto ottenuto come il prodotto scalare tra un vettore x che è il vettore del vostro dato con tutte le vostre feature e un vettore dei pesi dopodiché voi avevate nel caso appunto dell'apprendimento con supervisione ad ogni punto associate delle etichette queste etichette nel caso di classificazione binaria sono o meno 1 o più 1 valeva la stessa cosa per 0 1 eccetera per qualunque valore numerico lo abbiamo detto ed evidenziato la cosa interessante è che se prendiamo appunto meno 1 e 1 quello che ottenete è che se fate il prodotto scalare x t per vw lo mettete a 0 avete l'equazione di che cosa di un insieme di punti questa è una forma funzionale lineare che sono o una retta o un piano un iperpiano a seconda delle dimensioni e che è il luogo dei punti in cui il classificatore non può prendere nessuna decisione e quel luogo di punti è di nuovo appunto una retta oppure un piano quindi è per quello che si chiama classificatore lineare e allora a quel punto decidete di attribuire alla y la classe più 1 oppure la classe meno 1 a seconda del segno che ha il valore di questo prodotto scalare se è positivo classe 1 se è negativo classe meno 1 se quel prodotto scalare è 0 vuol dire che sono esattamente sul confine decisionale non posso prendere nessuna decisione va bene? come faccio a costruire il mio modello cioè a fissare questi pesi mi costruisco una funzione di costo abbiamo visto la funzione di costo di tipo cross entropy o quella che abbiamo chiamato softmax che è questa se andate a vedere gli appunti sul classificatore lineare vedete che avete la somma di tanti termini quanti sono i punti del vostro dataset di addestramento diviso per il numero di questi punti e prendete il logaritmo di 1 più e elevato alla meno yp per xp trasposto v per questo l'abbiamo introdotta a suo tempo e di fatto potete appunto continuare a ragionare su questa per le cose che andiamo a vedere adesso che sono appunto come iniettare della non linearità qui dentro a partire da questo modello lineare come faccio? beh faccio una cosa analoga a quella che abbiamo visto per la regressione mi domando cosa succede nel momento in cui comincio a trasformare con una funzione non lineare i miei valori di x e allora supponiamo di avere non una sola trasformazione a disposizione ma volerne trasformare voler trasformare questo valore di x secondo una prima funzione f1 funzione non lineare quindi sì no così no x alla terza x alla quinta cioè polinomi di grado terzo o quinto eccetera una seconda funzione f2 fino a una funzione fb dove b non è specificato quanto vale può valere 5 10 non lo sappiamo negli esempi semplici che abbiamo visto il seno aveva una sola funzione nel caso dell'esperimento di Galileo avevamo due funzioni non lineare quindi b valeva 1 o 2 ma in generale è non specificato a questo punto vedete che di nuovo se io scrivo questa combinazione lineare di termini significa che questo modello lo posso esprimere in questo modo come f trasposto per w dove di nuovo vi ricordo che f è un vettore costruito con un 1 in prima posizione poi f1 f2 fino a fb chiaramente di x ok quindi di nuovo esprimete il modello come un modello che è lineare rispetto allo spazio trasformato ma non lo è più chiaramente rispetto al valore di x originale e però la cosa interessante è che una volta che avete questo prodotto scalare potete andare a vedere il segno questo segno può essere maggiore o minore di 0 e se è maggiore di 0 dite classe 1 se è minore di 0 dite classe meno 1 se fosse uguale a 0 vuol dire che siete sul confine decisionale e la cosa interessante è che nel momento in cui ho questa scrittura io posso esattamente seguendo tutti i passaggi che noi abbiamo utilizzato per costruire una funzione di costo di tipo softmax nel caso dei classificatori lineari ripercorrere quegli stessi passaggi e andare a costruire una funzione di costo che dipende da teta dove teta ripeto è l'insieme dei parametri sia diciamo esterni che interni quelli esterni sono appunto questi w0 w1 w2 fino a wb ma qua dentro ci sono dei parametri che qui non sono esplicitati in questa scrittura ma dipendono da quali sono le funzioni non lineari che io vado a metterli dentro ricordatevi il seno il seno c'era dentro w0 più w1 per x posso costruire però vi dicevo tornando un attimo alla funzione di costo una funzione di costo che è esattamente quella di prima esattamente quella che avevamo costruito per i modelli lineari perché lo schema è esattamente lo stesso quello che cambia non è esattamente la stessa nel senso che cambia un fattore molto importante è questo cioè qui avevamo vi ricordate x trasposto qui non ho più x trasposto ma ho la sua trasformazione non lineare quindi diciamo costruisco un modello lineare nel mondo trasformato secondo le funzioni non lineari però per il resto posso ripercorrere esattamente lo stesso schema quindi costruire funzioni di costo di tipo cross entropy di tipo soft max via via quelle che avevamo visto a suo tempo e la cosa bella è che se voi fate una cosa di questo tipo un'operazione di questo tipo ecco qui a sinistra avete quello che avevamo visto a suo tempo il vostro classificatore lineare binario ha come confine decisionale una bella retta qui abbiamo una nuvola di punti in due dimensioni il confine decisionale era questo e e quindi x trasposto w uguale a 0 vi dava questa retta se fate la trasformazione con queste trasformazioni arbitrarie quello che vi viene è magari un confine decisionale che è quest'altra retta qui a destra che vi sto evidenziando che chiaramente non è una retta quest'altra curva ovviamente che non è una retta volevo dire e quello è il luogo dei punti però che trovate facendo f trasposto per w uguale a 0 e vedete che trasformate un classificatore lineare in un classificatore che non è più lineare perché il luogo dei punti in cui non può prendere decisioni che il confine decisionale non è più una retta non è più un piano se siamo in più dimensioni non è più un iper piano qui è una retta e uno spazio a due dimensioni sarebbe stato un piano e uno spazio con tre feature e un iper piano e uno spazio con un numero di feature maggiori va bene va bene anche questo ripeto è molto interessante perché vi permette di costruire che cosa vi permette di costruire questo modello seguendo la stessa struttura poi di risoluzione che avevamo utilizzato per i modelli lineari una volta che avete impostato la funzione di costo avete deciso quali sono le trasformazioni non lineari voi applicate per esempio la discesa del gradiente trovate i parametri e avete risolto il problema il punto di nuovo cruciale è che questi qui queste trasformazioni chi me le dice quanto valgono sono quelle trasformazioni che vengono costruite a mano a seconda del problema che uno ha frutto della conoscenza e quindi diciamo del fatto che viene distillata una conoscenza è un sapere di magari diversi anni relativi a dominio specifico e questo cosa accadeva nel mondo pre diciamo reti neurali le reti neurali non sono l'unico modo per costruire in maniera automatica questi tipi di sistemi poi ve lo vedremo ne parleremo nel corso della prossima lezione le prossime direzioni però sono sicuramente probabilmente l'esempio più famoso ok allora vediamo un po' più in dettaglio cosa succede no? con con i classificatori di tipo non lineare beh anzitutto partiamo dall'esempio qui a sinistra avete un esempio di classificatore lineare che diciamo è quello da cui siamo partiti quando all'epoca abbiamo abbiamo costruito i primi classificatori lineari abbiamo questa distribuzione di punti quello che abbiamo fatto abbiamo costruito un classificatore lineare l'abbiamo fatto il cui risultato abbiamo fatto passare attraverso la funzione logistica quindi quello che abbiamo fatto è questa linea tratteggiata rappresenta una relazione lineare tra y e x se prendiamo questa combinazione lineare w0 più x w1 che è il nostro modello lineare e la facciamo passare attraverso la funzione ad esempio tangente iperbolica otteniamo scusatemi questa curva nera che vi sto evidenziando dopodiché tutti i punti in cui w0 più x w1 sono minori di 0 sono punti che vengono classificati come appartenenti alla classe blu tutti i punti in cui w0 più x w1 è maggiore di 0 sono questi vengono classificati come appartenenti alla classe rossa e il punto in cui esattamente ho questa transizione per cui l'argomento w0 più x w1 è 0 è il punto che rappresenta il confine decisionale questa è la vista che abbiamo chiamato di regressione questa è la vista da sopra dall'alto che abbiamo chiamato di percetrone in cui il punto in questione che rappresenta il confine decisionale è questo bene allora questo è quello che avevamo fatto quando abbiamo costruito il nostro modello lineare e questo è un modello che questo caso per esempio funziona in maniera più che ragionevole se ci spostiamo in un ipotetico caso in cui abbiamo una distribuzione di punti che è questa che trovate a destra cioè i punti sono questi qui questi cerchi che vi sto evidenziando capite bene che vedete questa distribuzione di punti non riuscirete a modellizzarla correttamente con un classificatore lineare perché il classificatore lineare attraversa per definizione in questo caso l'asse dell'X in un solo punto e qui vedete bene che invece quello che vi servirebbe è un qualcosa costruire un classificatore che incrocia l'asse dell'X in due punti cioè un qualcosa che è non lineare per cui a sinistra del primo incrocio o a destra del secondo incrocio voi avete tutti i punti della classe chiamiamola blu e nel mezzo tra questi due punti avete tutti i punti della classe rossa come fate a costruire una cosa del genere a partire da un modello lineare con il solo modello lineare non ci riuscite però allora uno potrebbe cominciare a dire e vedete questo lo vedete bene anche dall'alto l'ideare sarebbe avere un modello per cui dite ok questi sono due punti che rappresentano il mio confine decisionale qui o qui non so prendere decisione ma di qua sono tutti i punti di una classe di là sono tutti i punti della stessa classe in mezzo ci sono tutti i punti dell'altra ma per costruire una cosa del genere voi dovete dire ok ho bisogno di una curva tratteggiata in questo caso la vedete questa tratteggiata che attraversa l'asse delle x in due punti che curva è? per esempio io posso prendere una parabola allora se prendo una parabola e la vado a disegnare ottengo questa curva tratteggiata e la parabola è w0 più x w1 più x quadro w2 questa è l'equazione di una parabola in una dimensione giusto? ok a questo punto che cosa faccio? se io prendo i valori che mi fornisce questa parabola e li vado a passare all'interno di una funzione sigmoidale come in questo caso la tangente iperbolica che vi ricordo è l'equivalente tra meno 1 e più 1 della funzione sigmoidale è diciamo l'analogo se quindi prendo la tangente iperbolica di questo stesso argomento che è la funzione quadratica ottengo questa curva continua nera che è proprio quella che fa il caso nostro perché è la curva che noi vogliamo per poter dire ok tutti i valori che stanno su tutti i punti che stanno su questa curva che sono minori di 0 per noi saranno punti della classe blu cioè gli assegnerò la classe meno 1 tutti i punti che stanno su questa curva nera che sono maggiori di 0 saranno per noi punti della classe più a questo punto il gioco è fatto vado a costruire ad addestrare un mio tramite un minimizzatore quindi vado a minimizzare una funzione di costo mi devo costruire un'opportuna funzione di costo ad esempio la funzione di costo softmax o la cross entropy e vado a ricavare i parametri w0 w1 w2 che mi minimizzano quella funzione di costo che a loro a sua volta vi ricordo è un proxy cioè è un qualcosa che ci serve per andare a minimizzare l'errore di classificazione non minimizziamo direttamente l'errore di classificazione perché quelle sono delle funzioni che sono difficili da ottimizzare con degli algoritmi perché ci hanno spigoli di discontinuità compagnia bella quindi potremmo usare solamente metodi di ordine 0 ma metodi di ordine 0 l'abbiamo detto cosa succede se andiamo in alta dimensionalità c'è il problema della dimensionalità non riusciamo a minimizzare però con i metodi di ordine 1 2 in questo modo si riesce e riesco a trovare i parametri che minimizzano ad esempio la funzione di costo di tipo softmax e a quel punto auspicabilmente anche il numero di errori di classificazione cosa succede se w0 più x w1 più x4 w2 uguale a 0 sono i punti in cui la parabola interseca l'asse delle x quindi sono questo e questo punto qui non posso dire niente sono esattamente sul confine decisionale il presupposto qual è quindi io riesco a ricavare w0 w1 w2 che mi mette in questa situazione ideale il punto è che da cui siamo partiti è che noi abbiamo avuto un'intuizione perché siamo riusciti a visualizzare il problema e abbiamo detto ok proviamo a metterci dentro una bella funzione quadratica una bella parabola e a quel punto so quanto vale f1 quanto vale f2 e riesco a costruire il tutto ma questo è un caso ideale è un caso in cui voi siete nella dimensione la più bassa possibile perché siete in una dimensione e chiaramente questo non accade quasi mai ripeto il machine learning moderno è ad alta dimensionalità qualunque problema di classificazione che voi incontrate non sarà un problema di classificazione in una o due dimensioni regressione lo stesso ci saranno degli esempi ma sono sicuramente la minoranza e anche se riusciste a visualizzarlo non è mica detto che voi riuscite a capire qual è la funzione non lineare che vi serve in questo caso è chiaro che avete una parabola nell'esempio dell'onda della regressione lì era una sinusoide ma potete avere un andamento che è il più strano possibile e qual è la funzione non lineare che meglio vi cattura a quell'andamento chi lo sa quindi è un problema di non semplice soluzione e il fatto che invece poi ci si riesca in maniera automatica è veramente strabiliante questo insomma ne riparleremo pian piano nel corso della prossima lezione ok qui vi faccio vedere semplicemente l'esempio della slide precedente proprio messo diciamo nero su bianco con dei punti che è un insieme di punti che è questo che sono questi 5 8 14 punti e vedete questo è quello che viene costruito chiaramente è un classificatore di tipo non lineare viene costruito andando a minimizzare a trovare la combinazione di questi tre parametri che minimizza tramite discesa del gradiente questa funzione di costo che è quella di tipo binary softmax e quello che ottenete è questa curva verde che rappresenta appunto il risultato di questo processo quella curva verde vi permette appunto di dire che viene costruita in che modo attribuendo un valore alla x vi permette di dire qual è il valore della corrispondente y poi se state qui chiaramente il valore di risposta sarà questo se siete qui oppure qua avete questo questo muro questo gradino chiaramente perché siete sui due confini decisionali se andate a fare un gioco analogo a quello che abbiamo visto prima per la regressione e che vi avevo anticipato che rimane anche per la classificazione cioè di andare a prendere la x e andare non più nel dominio nello spazio originale ma andate nel dominio trasformato che è un dominio a due componenti perché f1 e f2 e prendete quanto vale f1 di x e f2 di x e poi andate a costruire per ogni coppia f1 e f2 quanto vale la y di quello che otterreste è una figura di questo tipo in cui avete i vari punti che sono distribuiti in questo modo vedete che in questo caso quello che ottenete è qualcosa che ha un singolo punto di transizione e questo è uno no è uno scalino è una figura scalino che è esattamente la figura scalino che voi avreste in un classificatore lineare questo è un doppio scalino quello è un singolo scalino la differenza è che il doppio scalino è chiaramente non lineare il singolo scalino è lineare ma questo è un singolo scalino quindi un modello lineare dove? nel dominio trasformato cioè io ho già applicato le trasformazioni non lineare ok l'ultima l'ultima slide cosa succede se facciamo un ultima slide per oggi insomma delle le elezioni di oggi direi che sì facciamo questa e poi vediamo un attimo scusate sì questo lo vediamo domani cosa succede se ho ad esempio voglio modellizzare un confine decisionale tipo ellittico supponiamo di essere nel caso di avere questa distribuzione di punti qui partiamo da esempio siamo in due dimensioni quindi due feature x1 e x2 e supponiamo di avere una distribuzione di punti in cui avete qui all'interno di questa di questa ellisse tutti i punti di una classe la classe rossa e all'esterno tutti i punti della classe guida qui si vede chiaramente che il miglior classificatore che potete riuscire a costruire è un classificatore che ha questo confine decisionale questa questa ellisse che separa vedete bene queste due categorie di punti è chiaramente un confine decisionale di tipo non lineare voi se provate a separare questo insieme di punti con una retta qualunque fate dei disastri probabilmente da un punto di vista della correttezza della classificazione e allora la domanda che uno si può fare è dire ok come posso iniettare della non linearità a partire da un modello lineare e costruire un confine decisionale di questo tipo e qui ci viene in aiuto la geometria e l'occhio umano la capacità dell'occhio umano di riconoscere che si tratta di un pattern appunto di tipo ellittico allora uno l'equazione dell'ellisso bene o male la sa o la va a chiedere a qualcuno che bene o male di matematica ne sa di più di lui e se ne può venire fuori con una cosa di questo genere perché non proviamo a costruire un modello che è qualcosa del tipo w0 più w1x1 al quadrato più w2x2 al quadrato se qualcuno ha un po' di dimestichezza si ricorda quali sono le equazioni dell'ellisse è una cosa che ci assomiglia ok qui la trasformazione è una trasformazione di che tipo allora abbiamo un primo termine non lineare che prende x1 e lo l va al quadrato un secondo termine non lineare che prende x2 la seconda coordinata e la l va al quadrato e quindi abbiamo una f1 che è questa e una f2 che è questa non ci sono parametri interni per cui l'insieme dei parametri è totalmente definito da w0 w1 e w2 ora se voi prendete questo insieme di punti e andate a fare il tuning quindi l'ottimizzazione di questi pesi minimizzando una funzione di costo come una funzione di costo per esempio del tipo di nuovo binario softmax e utilizzando l'algoritmo di discesa del gradiente quello che ottenete è una cosa di questo tipo ottenete un insieme di punti che se andate a rappresentarli appunto sulla vista diciamo di percetrone dall'alto è esattamente questa curva qua che è un ellis cioè l'insieme di punti in cui f trasposto per w è uguale a 0 vi porta qui sopra l'insieme di punti in cui questo prodotto scalare è maggiore di 0 vi porta qui dentro l'insieme di punti in cui è minore di 0 vi porta qua fuori e questo se lo vedete nella vista cosiddetta di regressione cioè se andate a costruire un grafico tridimensionale in cui mettete su un asse la x1 su un altro asse la x2 e andate a vedere questi punti vi accorgete che questi punti sono chiaramente questi no? questo è meno 1 il livello meno 1 e questo è il livello più 1 ok? e questo è il livello 0 in questi plot tridimensionali cioè laddove il prodotto scalare è uguale a 0 se noi facciamo l'intersezione diciamo con il piano uguale a 0 ottenete vedete questa ellisse e qua avete tutti i punti della classe 1 qua sotto tutti i punti della classe 2 quindi è la stessa rappresentazione cioè da qua se guardo questa curva questa superficie se la guardo dall'alto ottengo questa questa qui la vista di percettrone e la cosa di nuovo che va rimarcata è che se voi fate il plot di tutti questi punti su un grafico che ha f1 ed f2 di x come assi di riferimento otterreste questa distribuzione di punti e se andate a plottare il luogo dei punti in cui f è trasposto per w uguale a 0 in questo spazio di riferimento otterreste questa curva che è effettivamente una retta cioè nello spazio trasformato di nuovo il confine decisionale è un luogo di punti lineari e di nuovo questo lo vedete anche a livello della della della vista di regressione perché vedete che qui avete tutta questa nuvola di punti blu e poi avete uno scalino che vi porta al piano sopra che è la nuvola di punti rossi di nuovo questo è un modello lineare quindi nello spazio trasformato il modello è lineare lo scriviamo meglio che non si vede però nello spazio trasformato come arrivare nello spazio trasformato lo decidiamo noi in questo caso l'abbiamo deciso costruendo l'equazione di un'ellisse alla fine abbiamo visto che il modello era tale per cui da costruire migliore era tale per cui il confine decisionale doveva essere un'ellisse e quindi abbiamo imposto questo tipo di dipendenza però di nuovo se non sono in grado di ridurre e vedere e di visualizzare i miei punti in due dimensioni o in tre al massimo come faccio? Non ci riesco immediatamente non è così semplice oppure se anche ci riesco a visualizzarlo ma ho un andamento per cui i miei punti sono distribuiti in modo tale che il mio confine decisionale anziché essere un'ellisse è una cosa fatta così qual è l'equazione la trasformazione lineare che mi porta a un confine decisionale così non lo sappiamo dobbiamo trovare un'altra strategia di questo cominciamo a parlare nella lezione di domani direi prima ci sono un paio di cose che vediamo sul fatto di come tutti questi discorsi possono essere trasferiti anche al non supervisionato e poi ripartiamo da lì domande? da casa? no tutto tutto chiaro ok va bene allora fermiamo la registrazione la lezione di oggi la trovate come sempre per poi su subvene grafica un'altra Grazie a tutti.