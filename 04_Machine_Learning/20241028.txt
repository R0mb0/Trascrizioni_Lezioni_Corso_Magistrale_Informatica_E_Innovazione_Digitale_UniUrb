intanto bentornati, oggi proseguiamo lo studio delle macchine a vettore di supporto, le svm support vector machine che sono uno strumento abbastanza utilizzato e quindi cerchiamo di capire di capirne un po di più. L'altra volta le abbiamo introdotte e le abbiamo introdotte a partire dal concetto di percettrone con margine o percettrone di margine. Il percettrone con margine è appunto un percettrone in cui noi ci concentriamo su un iperpiano, allora di tutto la premessa è che il dataset deve essere linearmente separabile, quindi separabile tramite appunto un iperpiano e una volta che abbiamo una separabilità lineare abbiamo una zona di buffer che separa appunto l'iperpiano dalla sua versione traslata che tocca il primo punto in una direzione oppure nell'altra. Tutto questo se vi ricordate ci porta a concludere che i punti che possiamo etichettare con label più 1 e quelli con label meno 1 sono punti che stanno rispettivamente sul lato positivo dell'equazione dell'iperpiano traslato che è x trasposto w uguale a più 1, questo a meno del fatto di introdurre una ricalibrazione, una ricalibrazione, cioè una divisione del vettore dei pesi per il valore scalare beta, vi ricordate ve l'ho detto l'altra volta, quindi diciamo senza perdere nulla in generalità qui si può mettere un più 1 e qui un meno 1, anziché un più beta e un meno beta, perché posso dividere tutto per più beta o meno beta e quindi ricondurmi a questo caso. Quindi di qua ho tutti i punti positivi, scusate tutti i punti di classe più 1 e di qua ovviamente tutti i punti di classe meno 1 dal lato negativo di quell'altro confine decisionale. Tutto questo può essere poi tradotto in un sistema di due disequazioni che è quello che è riportato appunto in questo slide, per cui se l'etichetta è più 1 noi vogliamo che il vettore dei pesi sia addestrato, il modello in modo da avere un vettore dei pesi che è caratterizzato da quella disequazione, cioè xp trasposto per w maggiore di 1. Viceversa, se vogliamo che se l'etichetta, scusatemi, è meno 1 vogliamo che il vettore dei pesi sia tale che xp trasposto w è minore o uguale di meno 1 per il maggior numero possibile di punti del nostro dataset, di addestramento chiaramente e poi auspicabilmente anche per quello di testing. Ricordo che ancora qui abbiamo menzionato qualche volta il dataset di test e ci stiamo concentrando sui modelli, sull'addestramento e sulle prestazioni che questi modelli hanno durante l'addestramento. Però non dobbiamo dimenticare, man mano poi che andiamo avanti nel corso, questa cosa verrà fuori diciamo più spesso e più in dettaglio, però non dobbiamo dimenticarci che nel flusso di costruzione di un modello di machine learning, di un sistema di addestramento, abbiamo la fase di addestramento, scusate, di un sistema di apprendimento, abbiamo la fase di addestramento, training e poi abbiamo la fase di test, di verifica. Bene, allora quelle due disuguaglianze possono essere condensate in un'unica disuguaglianza che dà luogo, scusatemi, in un'unica condizione che è questa, che deve essere verificata appunto se non vogliamo commettere alcun errore, quindi imporre il massimo tra 0 e 1 meno yp xp trasposto w, questo deve essere uguale a 0 se non voglio commettere nessun errore su un mio dataset di addestramento. Questo è esattamente da dove deriva, ma deriva dal fatto che noi possiamo andare a prendere in considerazione queste due e in maniera del tutto analoga ripercorrere il ragionamento che abbiamo fatto per il percetrone a suo tempo, dove avevamo al posto dell'1 lo 0. L'unica cosa che cambia è che appunto lì eravamo arrivati alla condizione massimo tra 0 e meno yp xp trasposto w, qui c'è l'1. Fondamentalmente è lo stesso tipo di logica. Per arrivarci però per gradi basta andare a considerare che se noi siamo qui, a partire da queste noi possiamo condensarle in un'unica condizione che è, e questa l'avevamo vista credo la volta scorsa. Chiaro che queste due e questa sono la stessa scrittura equivalente, cioè se yp è uguale a 1 ottengo la prima, se yp è uguale a meno 1 ottengo la seconda. Quindi queste sono equivalenti. Come vi facevo notare prima, nel caso del percetrone, in realtà avevamo la condizione che questo doveva essere maggiore di 0 e questo doveva essere minore di 0. Ok? Nel caso del percetrone. E nel caso del percetrone arrivavamo poi alla quella della slide successiva, condizione della slide successiva, ma adesso ci arriviamo un attimo. Posso cancellare qui intanto? Allora, come dicevo, andiamo una sola e a questo punto siamo qui, qui Torno un attimo al blu. Vi dicevo che se io la specifico correttamente, siccome ve la riscrivo qua, quella della yp xp trasporta, v-doppio deve essere maggiore o uguale a meno. Se voi fate un po' di ragionamenti, ad esempio quando yp è uguale a 1, se yp è uguale a 1, supponiamo che questa quantità sia una quantità pari a 2, ok? Quindi il mio modello mi restituisce 2, ho un valore 2, 2 maggiore o uguale di 1, sono soddisfatto della mia disuguaglianza, ho classificato correttamente, infatti qui viene 1-2, verrebbe che è un valore negativo, non voglio penalizzare la mia scelta, quindi per quel punto mi va bene la predizione che ho fatto. Viceversa, appogliamo che yp sia sempre 1, ma questo anziché restituirmi un valore più 2 mi restituisca un valore, non so, 0,5. 0,5 significa che sono in una condizione in cui 1 per 0,5 chiaramente fa un mezzo che non è maggiore o uguale di 1, ok? E in effetti qui che cosa succede? Che vado a penalizzare il massimo tra 0 e 1-0,5 è chiaramente un numero pari a 0,5, quindi vado a penalizzare leggermente la mia scelta. Se questo addirittura fosse negativo, poniamo, ok? Quindi fosse meno 3, ok? Io a quel punto il penalty che vado ad applicare sarebbe un penalty. 1 più 3, 4, cioè ancora maggiore, cioè più mi discosto, più vado a pagare. Cioè stiamo dicendo al nostro sistema di apprendimento guarda che il valore di W che abbiamo in questo momento è sbagliato. E come glielo diciamo, lo dobbiamo correggere. Come lo correggiamo? Attraverso il... Esattamente. Nel margine, esattamente. E noi glielo diciamo attraverso il processo di apprendimento, cioè durante, per esempio, la discesa del gradiente. Se usiamo la discesa del gradiente. Vi ricordo che nel caso del percettrone, abbiamo visto, avevamo una funzione di costo GP che era del tutto simile, che era il massimo tra 0 e meno yp xp la sposta. Ok, una volta che abbiamo impostato la funzione di costo puntuale, che ripeto, nella condizione ideale, questa deve essere uguale a 0, questa scrittura leggerà sempre con vera se la classificazione è corretta. Chiaramente, una volta che ho la funzione di costo per il punto generico, posso andare a fare la sua media su tutti i punti del dataset, e che ottengo la funzione di costo diciamo globale, che è quella che vogliamo ottimizzare. Di nuovo, qui, se fossi stato nel percettrone, non avrei avuto questo 1. Chiaramente. di nuovo, anche qui possiamo fare un'operazione che abbiamo già incontrato, a proposito, per esempio, appunto, del percettrone, abbiamo sostituito al posto dell'operatore max, della funzione max, una sua versione liscia, una sua versione più facilmente gestibile da un punto di vista dell'ottimizzazione, e quindi quell'operatore l'abbiamo chiamato softmax, avvertendo, attenzione, guardate che, ve lo ridico un'altra volta, non è il softmax che incontrate per esempio nelle reti neurali, è un altro operatore, è un operatore che approssima l'operatore, la funzione max, va bene? E quindi la stessa cosa che abbiamo fatto col percettrone la possiamo far qui e andare a sostituire al posto del massimo tra 0 e quest'altro argomento, il massimo tra due argomenti e vi ricordo e alla s0 più e alla s1, dove s0 e s1 sono i due argomenti in questione. E quindi facendo un'operazione di questo genere, abbiamo proprio quello che vi dicevo, il soft tra 0 e questo secondo argomento che è 1 meno yb xp trasposto vw, lo possiamo scrivere come il logaritmo, forse non l'ho detto, di che cosa? Di e alla s0 più e alla s1, dove s0 è il primo argomento che è 0, quindi e alla 0 fa 1. Qui abbiamo e alla 1 meno yb xp trasposto vw, che è l'approssimazione che abbiamo introdotto. Di nuovo, facendo la media, questo è il singolo punto, se faccio la media su tutti i punti, ottengo la mia brava, scusatemi, pensavo ci fosse scritto qua, ottengo la mia brava funzione di costo, funzione di costo che è chiaramente 1 su p sommatoria per p piccolo che va da 1 a p grande di log di questa cosa. Ecco, scriviamo, siamo sicuri. Quindi lasciare un braccio. questa è la funzione di costo in questione. Questa funzione di costo è una funzione di costo quindi che ci permette di, a partire dal concetto di margine, di andare a costruire appunto un modello che vedrete adesso piano piano si trasforma in un, in quelle che sono le svm, cioè sarà, poi ci porterà le svm. Allora, faccio notare una cosa che noi siamo partiti da imporre un valore 1, cioè andare a mettere un buffer in cui gli iperpiani stavano sul valore 1, gli ho detto può essere rimpiazzato, cioè quello è appunto qualcosa in cui siamo arrivati partendo da un valore beta qualunque, ma volendo possiamo anche prendere un qualunque valore epsilon maggiore di 0 al posto dell'1, cioè qua dove abbiamo questa condizione, questo 1 è del tutto arbitrario, l'avrei potuto sostituire con 0.5, ok? E rinormalizzare tutto rispetto a quello, cioè non cambia nulla, la matematica rimane sempre la stessa e quindi sarei arrivato qui anziché avere un 1 meno epsilon meno avrei avuto. E quindi di nuovo l'approssimazione sop max anziché avere log di 1 più e elevato alla 1 meno yp x trasposto w sarebbe stato epsilon meno yp trasposto x trasposto w doppio. Tutto questo ve lo dico perché? Perché la cosa interessante è che se voi prendete quel valore di epsilon abbastanza piccolo questo esponente può essere trascurato e se lo trascuro ottengo questa espressione e questa espressione l'abbiamo già incontrata, l'abbiamo già vista nel percentuale. quindi vedete che fondamentalmente anche qui stiamo ruotando dietro lo stesso approccio. Esattamente. Esattamente, è proprio questo. Stiamo proprio restringendo il margine e stiamo riconducendo questo modello al modello che abbiamo già incontrato. La cosa bella delle svm è che si riconducono al percettrone ad esempio, quindi fondamentalmente anche alla regressione logistica perché abbiamo detto più volte sono fondamentalmente lo stesso dietro c'è lo stesso concetto però quando il dataset non è linearmente separabile lo vedremo tra poco ma quando il dataset invece è separabile linearmente l'SVM vi trova questo margine il margine maggiore che voi volete cercare cioè vi trova tra tutti gli iperpiani confini decisionali quello che corrisponde al margine maggiore e questo diciamo ci dà un risultato che è una garanzia perché ci ci dice ok abbiamo trovato il modo di separare questi due queste due nuvoli di punti nel modo tramite il margine più ampio possibile questo è il concetto ecco e questo è esemplificato da questa slide l'SVM sono appunto dei modelli di classificatori lineari lasciamo stare il fatto che poi possono essere usate anche per la classificazione non lineare con dei trucchi che insomma che però non abbiamo modo di vedere a lezione e magari ve ne menziono quando faremo l'esercitazione al laboratorio faccio vedere magari al volo un esempio ma non c'è tempo per entrare nel dettaglio di come rendere questi strumenti dei classificatori non lineari la base è un classificatore lineare quindi alla base l'SVM è questo e se noi prendiamo l'esempio in figura vedete qui abbiamo due punti che sono separabili linearmente ma oltre a questi due ne esistono altri infiniti una volta che i dataset sono separabili linearmente lo sono in infiniti modi diversi cioè io potrei avere questo evidenziato in verde questo evidenziato in blu ma potrei avere anche questo come separatore lineare potrei avere questo potrei avere questo allora qual è l'obiettivo dell'SVM? l'obiettivo è andare a dire ok ce n'ho tanti qual è il migliore? da che punto di vista il migliore? allora se noi andiamo a prendere questi due in considerazione l'iperpiano nero e l'iperpiano verde vediamo che quello verde a quello verde corrisponde un margine più piccolo lo vedete perché è il margine d'ampiezza di questa di questa zona di buffer che separa diciamo la versione traslata da una parte dalla versione traslata dalla parte opposta e se la distribuzione fosse questa cioè diciamo cosa intendiamo con la distribuzione? è proprio una distribuzione statistica se questi punti diciamo noi abbiamo un'idea abbastanza buona di come sono distribuiti che significa che io ad esempio posso avere un qualche altro punto incognito un domani che mi si presenta chiaramente come input al mio classificatore ma questo non si discosta molto da come sono distribuiti questi punti in questo spazio beh allora quello con un margine più ampio ci dà maggiori garanzie perché se la distribuzione è questa vuol dire che ad esempio un domani può essere che mi arriva qualcosa qui e va bene può essere che mi arriva qualcosa qui e va bene vedete sempre nella stessa forma posso ipotizzare che magari non c'è nel mio dataset ma qualcosa stia anche qui allora cosa succede a quest'ultimo punto che ho appena aggiunto che mi evidenzo qui ve lo riemo ok che cosa succede succede che questo viene classificato dal classificatore a cui corrisponde l'iperpiano la linea il confine decisionale verde come appartenente alla classe blu e lui lo sbaglia mentre quell'altro vedete no quello nero non lo sbaglia continua a classificarlo correttamente come classificatore come appartenente alla classe dei punti rosso questo perché perché il margine è inferiore su questo è chiaro che se la distribuzione fosse tutt'altra allora lui comincia a spostarsi di qua magari non so se ci fosse qualche altro punto qui uno potrebbe dire a questo punto lo sbaglia il nero ma il verde lo fa correttamente però quest'altro punto che sta qua è molto fuori dalla distribuzione cioè è un punto molto diverso da questa distribuzione qua come? esattamente esattamente quindi questo significherebbe che cosa? che se questo fosse parte reale nella distribuzione vuol dire che io ho mancato nel mio dataset di addestramento molti punti quindi vuol dire che la mia distribuzione è questa allora cambia tutto non è più questo nero il margine il classificatore con il margine migliore e vedete che anche qui è interessante perché si tocca con mano che cosa? il fatto come avere un buon dataset che caratterizzi il più possibile quello che mi può accadere nel mondo reale è fondamentale è importantissimo tant'è che uno dei più grossi problemi che oggigiorno si affronta cioè si cerca di affrontare sicuramente uno dei più grossi problemi che si incontra e che si deve cercare di affrontare è quello che viene chiamato lo shift delle distribuzioni cosa vuol dire? vuol dire noi addestriamo oggigiorno il sistema per un certo tipo di distribuzione statistica dei dati poi per qualche motivo possono essercene tanti tra oggi siamo all'istante di tempo t tra un certo numero di istanti di tempo il mio sistema appunto è entrato in produzione lavora la distribuzione statistica dei dati su cui lavora cambia ed è chiaro che cambiando la distribuzione statistica dei dati ovviamente cambia tutto nel senso che a quel punto sono costretto a mettere in piedi un qualcosa che è quantomeno riaddestrare andare a tenere conto dell'inclusione di questi nuovi punti e così via però adesso questo è un qualcosa che è al di fuori del discorso che stavamo facendo e nel discorso che stavamo facendo invece è la mia distribuzione è ben caratterizzata e allora il margine più ampio di questa distribuzione è quello che mi dà maggiori garanzie ecco il motivo per cui sono stati introdotti le svm cioè più ampia è la distanza migliore la separazione e minore è la possibilità di avere delle classificazioni errate in punti che possono arrivare in futuro a dover essere classificati nel mio sistema questo è il concetto di svm che quindi corrisponde a un classificatore lineare a maggior margine possibile allora come facciamo a identificare quello che ci dà questa garanzia cioè identificare tra tutte le infinite possibili alternative l'iperpiano che corrisponde a un margine massimo si fa con un po' di matematica ed è quella che adesso andremo a a vedere nelle prossime slide qui per ricavare appunto il confine decisionale che corrisponde al margine massimo ritorniamo all'equazione del nostro del nostro iperpiano vi ricordo che il confine decisionale è x trasposto v doppio uguale a zero giusto? questa è l'equazione del nostro iperpiano del confine decisionale qui torniamo per comodità di manipolazione algebrica alla versione non quella compatta in cui avevamo introdotto con la notazione di questo cerchietto l'uno in cima al nostro vettore colonna ma la versione originale in cui abbiamo la componente di bias e quelle che abbiamo chiamato feature touching weights cioè i pesi che effettivamente vanno poi a moltiplicare nelle feature vi ricordate nel modello lineare da cui siamo partiti originariamente proprio per la regressione lineare per esempio per cui omega è l'insieme dei pesi che vanno poi a moltiplicare effettivamente le feature e poi abbiamo in aggiunta il termine di bias questo uguale a zero è una scrittura equivalente del nostro confine decisionale va bene? quello è il nostro piano allora il nostro obiettivo è trovare i parametri omega più b ok che il nostro v doppio in modo tale che la regione che è definita da da che cosa? da x trasposto v doppio oppure b più x trasposto omega uguale a più 1 e b più x trasposto omega uguale a meno 1 al margine più ampio possibile vogliamo massimizzare quella zona di buffer vi ricordo che qua da una parte c'è b più x trasposto omega uguale a più 1 qui c'è b più x trasposto omega uguale a meno 1 in mezzo c'è il confine decisionale noi vogliamo massimizzare tra tutti gli infiniti iperpiani possibili questa distanza quindi tornando alla figura precedente l'iperpiano verde aveva una distanza inferiore allora se andiamo a considerare qui ci sono un paio di premesse da fare questa figura se andiamo a prendere in considerazione questa figura quello che vedrete riusciremo a dimostrare è che questo margine ha questa espressione cioè dipende dall'ampiezza del vettore omega in maniera inversamente proporzionale prima di fare questo dobbiamo fare un paio di passi vedete questo margine è rappresentato da che cosa se noi ci mettiamo in un punto x1 sul primo iperpiano e un punto x2 su quello simmetrico l'ampiezza di questo margine è la distanza tra questi due punti ok e questo è quello che sfrutteremo per arrivare a questa espressione però per arrivare a questo c'è prima un passo precedente che è quello di capire che il vettore omega per come l'abbiamo definito è esattamente questa cosa qua cioè un vettore che ha come direzione una direzione ortogonale all'iperpiano a questi iperpiani allora quindi la prima cosa che andiamo a vedere è come dimostrare che omega è ortogonale al confine decisionale quindi la prima cosa è omega ortogonale al confine decisionale allora per vedere questo prendiamo due punti P e Q che appartengono al confine decisionale quindi questo sia P questo è il punto Q ok allora se stiamo sul confine decisionale vuol dire che vale che cosa che P trasposto per omega deve essere uguale a meno B e anche Q trasposto per omega deve essere uguale a meno B perché siamo sul confine decisionale quindi vale questa al posto del punto X ci sostituite un punto P o un punto Q e quello che ottenete è che P trasposto omega deve essere uguale a meno B e Q trasposto omega deve essere uguale a meno B portate il B di là ok fino a qui niente di particolare da questa che cosa discende da questa discende che se faccio la differenza tra queste due equazioni ottengo che P trasposto meno Q trasposto scalare omega è uguale a meno B meno meno B zero giusto? ho fatto la differenza la prima meno la seconda e ho raccolto omega proprietà distributiva del prodotto scalare ho applicato ok questo significa che il vettore P meno Q chi è il vettore P meno Q? allora siccome P e Q giacciono sul confine decisionale il vettore P meno Q sarà questo come direzione cioè è parallelo al confine decisionale giusto? siete d'accordo con me? che il vettore P meno Q sta su questa retta ok? ma qui allora stiamo scrivendo che cosa? che il prodotto scalare tra un vettore parallelo al confine decisionale quindi questo è parallelo al confine decisionale e il vettore omega il vettore omega è un qualunque vettore parallelo al confine decisionale hanno un prodotto scalare pari a zero questo cosa vuol dire in geometria? vi ricordate? se due vettori hanno prodotto scalare uguale a zero che sono perpendicolari quindi abbiamo appena dimostrato che omega è un vettore perpendicolare al confine decisionale quindi il vettore omega il vettore dei pesi che vanno a moltiplicare le feature in un modello di classificazione lineare è sempre perpendicolare al confine decisionale ok quindi questa dimostrazione ci rende conto del fatto che qui abbiamo scritto che il vettore omega è un vettore normale ortogonale che vuol dire al confine decisionale ok a questo punto se io prendo due punti x1 e x2 che stanno lungo questo vettore normale ok rispettivamente e li prendo all'incrocio dove questo vettore normale la direzione di questo vettore normale interseca i piani positivo e negativo traslati ottengo che il margine che voglio andare a quantificare è proprio la norma di x1 meno x2 cioè questa cosa qua va bene fin qui ci siamo ok e adesso andremo a dimostrare vedete è pari a 2 diviso la norma di omega allora posso cancellare intanto benissimo e adesso andiamo a vedere un po' meglio quello che vi dicevo allora la prima cosa da cui partiamo è questa vi ricordo che deve valere questa e questa quindi che cosa succede se io prendo un punto arriviamo qua b un punto scusatemi x1 e un punto x2 che stanno esattamente qui come vi dicevo quindi che sono ortogonali a quei confini decisionali nel vettore omega ortogonale a quei confini decisionali x1 sta all'incrocio appunto tra il confine decisionale e la retta definita dal vettore omega a quel punto possiamo scrivere che valgono le seguenti b più x1 trasposto per omega è uguale a 1 e b più x2 trasposto per omega è uguale a meno 1 giusto? ok x1 e x2 appartengono a questi confini decisionali a questo punto facciamo di nuovo la prima meno la seconda quindi facciamo 1 meno 2 e 1 meno 2 ci dice che deve essere x1 trasposto per omega meno x2 trasposto per omega uguale a 2 cioè x1 meno x2 tutto trasposto scusate che lo tolgo facciamo così uguale a 2 e quello che trovate qui quindi fino adesso abbiamo preso solamente dei punti che stanno su questi due confini sono la traslazione del confine decisionale e abbiamo visto che vale questa adesso imponiamo il fatto che omega è ortogonale al confine decisionale e vediamo che cosa succede allora siccome abbiamo detto che se ho due vettori a e b prima abbiamo detto che cosa che se sono ortogonali questo prodotto scalare è uguale a 0 adesso vi chiedo cosa succede se sono paralleli sono paralleli massimizziamo il valore del prodotto scalare cioè se vi ricordate anche qui dalla geometria che avete fatto nei corsi di matematica precedenti se avete due vettori paralleli qui ci sarebbe il coseno dell'angolo in questo caso sono paralleli il coseno dell'angolo vale 1 e quindi qui avete che è il prodotto delle magnitud dei due vettori il prodotto scalare ok quindi applichiamo questo qui allora qui che cosa succede che x1 meno x2 è un vettore che sta che è parallelo a omega giusto ok se è parallelo a omega lo posso scrivere come x1 meno x2 norma l2 per omega norma l2 sto applicando questa ok questi sono due vettori paralleli quindi il loro prodotto scalare lo posso scrivere così giusto e questo deve essere uguale a 2 il che significa che la norma del vettore differenza x1 meno x2 deve essere uguale a 2 diviso la norma di omina che è quello che c'è scritto qua abbiamo appena dimostrato questo era quello a cui volevamo arrivare perché volevamo arrivare qua perché ci è utile questo abbiamo fatto questi passaggi matematici perché cosa non dobbiamo perdere di vista quello che è il nostro obiettivo il nostro obiettivo è costruire un classificatore lineare che massimizza il margine ora noi dobbiamo cercare di quantificarlo il margine e dobbiamo quantificarlo in funzione dei parametri che abbiamo a disposizione il mio modello è un modello che dipende da b e da omega e io adesso sono riuscito a quantificare l'ampiezza di quel margine in funzione di omega quindi adesso il mio obiettivo diventa che cosa costruire un classificatore lineare che massimizzi questa quantità ma questa quantità la massimizzo se vado a minimizzare la norma di omega cioè io devo trovare un modello che ha la norma del vettore dei parametri la più piccola possibile e questo adesso abbiamo praticamente siamo arrivati quasi nel senso che dopo vedrete ci sono altre tecnicalità da definire però da affrontare però la definizione del modello è tutta qui perché noi adesso se andiamo a tirare un po' le fila di tutti questi discorsi otteniamo una cosa di questo tipo intanto vabbè questo è quello che abbiamo appena abbiamo appena dimostrato cioè vedete che 2 è uguale a questa quantità e quindi siamo arrivati a dimostrare questo quello che vi ho scritto proprio poco fa e a questo punto andiamo a vi dicevo a tirare un po' le fila di tutti questi ragionamenti allora noi vogliamo trovare un iperpiano che abbia la caratteristica di avere il vettore omega che vi ricordo è perpendicolare a quelli iperpiano stesso e questa deve essere di lunghezza minima cioè dobbiamo minimizzare questa quantità allora lì in realtà vi ho messo la norma al quadrato allora è chiaro che se io minimizzo la norma oppure minimizzo la norma al quadrato ho due formulazioni equivalenti cioè se minimizzo la norma ho il suo valore al quadrato se io cerco il valore di omega che ha norma intensità diciamo minima lo trovo anche se minimizzo quella quantità al quadrato però il vantaggio di introdurre questa quantità al quadrato è da un punto di vista algoritmico perché adesso parliamo di come andare poi a mettere tutto a sistema e questa è non secondaria perché si arriva a una formulazione che da un punto di vista dell'ottimizzazione è assolutamente fondamentale quindi noi minimizziamo questo valore ok quindi vogliamo trovare l'iperpiano che ha il vettore normale di ampiezza minima ampiezza elevata al quadrato perché ci semplifica l'ottimizzazione però allo stesso tempo non dobbiamo dimenticarci di che cosa che ci dobbiamo introdurre i vincoli i vincoli quali sono che di avere un iperpiano che separa perfettamente il dato non possiamo andare a scegliere un iperpiano che casca in modo da non andare a separare perfettamente i dati e questa è quella è la formulazione del problema dell'SVM cosiddetta hard margin cioè a margine diciamo duro perché impone appunto che i dati siano separati perfettamente cioè per intenderci dice io sono qui e non posso andare a scegliere questo come iperpiano perché questo qui lo vado a sbagliare allora una formulazione di questo genere si chiama formulazione hard margin e può essere riscritta da un punto di vista formale in questo modo si tratta di minimizzare la norma del vettore omega elevata al quadrato quindi minimizzare omega in modo tale che questa quantità sia minima e tenere conto anche di b dove oltre che di omega nel fatto che deve essere rispettato il vincolo e il vincolo qual è? sono i vincoli che abbiamo come faccio a dire devo separare perfettamente i dati con quelli per piano sono quei vincoli che abbiamo ricavato abbiamo richiamato già a partire dalla volta scorsa e richiamato all'inizio della lezione il massimo tra 0 e 1 meno yp che moltiplica b più x p trasposto omega deve essere uguale a 0 vi ricordate all'inizio della lezione abbiamo detto questo è vero se classifichiamo correttamente tutti i punti noi li vogliamo classificare correttamente per p che va da 1 fino a p grande questo è questa è la formulazione dell'SVM da un punto di vista proprio formale questo problema è un problema di minimizzazione quindi un problema di ottimizzazione però è diverso dai problemi di ottimizzazione che abbiamo visto finora perché i problemi di ottimizzazione che avevamo visto finora era minimizza questa funzione punto qui c'è un qualcosa di più perché c'è un minimizza questa funzione in cui imponiamo che debbano valere delle altre condizioni questo è quello che nel diciamo nell'ottimizzazione matematica viene chiamato problema di ottimizzazione vincolata cioè introduciamo dei vincoli c'è una funzione di costo che è questa da minimizzare inoltre i parametri da cui dipende questa funzione di costo devono soddisfare una serie di vincoli in questo caso tanti quanti sono i punti dell'assessore ora questi ci sono degli algoritmi di ottimizzazione vincolata che possono essere utilizzati noi non li abbiamo visti perché abbiamo parlato di ottimizzazione non vincolata finora e diciamo quindi è un qualcosa che esula diciamo dai contenuti di questo corso però sappiate che diciamo ci sono tutta una serie di tecniche per poter risolvere questa tipologia di problemi e il fatto di avere introdotto questo quadrato è non secondario ve lo dicevo prima cioè se io avessi preteso di minimizzare direttamente la norma avrei avuto dei problemi da un punto di vista algoritmico invece il fatto di avere questo al quadrato traduce questo in quello che viene chiamato quadratic program è una tipologia di problemi per cui esistono degli algoritmi efficienti che riescono a risolverli anche quando il numero di punti scala abbastanza in alto quindi diciamo è un problema che si riesce a risolvere ma non c'è neanche necessità di andare a ricorrere a queste tecniche di ottimizzazione vincolata perché quello che si fa spesso e volentieri si può affrontare ripeto il problema da questo punto di vista e quindi ha soluzione ma si può anche affrontare il problema utilizzando una tecnica che permette che viene molto fatta anche questa quando si hanno problemi di ottimizzazione vincolata la prima cosa che si fa si cerca di rilassare i vincoli cioè di qualche modo lasciarli da parte cercando appunto di ragionare se questo lasciarli da parte fino a che punto diciamo va a non modificare l'ottimalità della soluzione quello che si fa è arrivare a una formulazione di fatto non vincolata tramite una tecnica che incontreremo altre volte da quella fine del corso che viene chiamata regolarizzazione la incontreremo in contesti diversi ma dietro c'è sempre la stessa idea che viene chiamata regolarizzazione l'idea della regolarizzazione è di aggiungere a una funzione di costo un termine aggiuntivo che viene chiamato termine di regolarizzazione che permette di fare certe cose in questo caso permette di passare da una formulazione hard margin a una formulazione cosiddetta con margine sotto morbido e adesso vediamo e questo margine questa formulazione è una formulazione si riduce a un problema non vincolato quindi un problema di ottimizzazione come quelli che abbiamo visto finora e quindi come tale lo possiamo risolvere ad esempio con metodi di ottimizzazione di ordine 1 di scesa del gradiente su tutti ma anche 2 eventualmente e così via e questo è quello che si fa allora guardate questa questa funzione di costo questa funzione di costo che io vi ho appena proiettato è una funzione di costo che ha due pezzi questo e quest'altro è la somma di due terni giusto? allora quello evidenziato in rosso che cos'è? guardatelo bene vediamo un po' se vi viene in mente è abbastanza evidente perché ce l'avete sopra allora lasciamo stare che c'è l'1 su p che appunto è la media su tutti i punti che cos'è? che cos'è? che cos'è rappresenta? è la condizione che dicevamo prima che deve essere soddisfatta perché i vincoli siano soddisfatti cioè l'imporre questa minimizzare questa parte della funzione se io vado a minimizzare questa vuol dire che mi metto nell'ottica di cercare il punto in cui il più possibile non commetto errori di classificazione esattamente o meglio non tanto per il margine quanto per il fatto che i dati vengano classificati correttamente il margine è nella seconda parte che è quella verde che è questa cioè devo minimizzare la norma del vettore perché guardate che cosa succede io ci ho aggiunto questo termine che lambda è un è un è un iperparametro un parametro ulteriore un numero che viene moltiplicato per e che serve per controllare quanta regolarizzazione vado a imporre adesso ci torniamo sopra su questo concetto però intanto guardiamo che cosa moltiplica questo lambda moltiplica la norma del vettore al quadrato ok quindi intanto vi faccio notare una cosa che non sono più in un problema di tipo vincolato questo è un problema esattamente del tipo che abbiamo visto finora perché io dico se io dico questa è una funzione di costo la devo minimizzare si tratta di minimizzare un qualcosa in cui ho questi parametri b omega che compaiono in questi due termini quindi non è più un problema di tipo vincolato perché non ho un'ulteriore condizione sui vincoli la condizione dei vincoli di soddisfazione dei vincoli da chi è determinata questa è è la parte questa in rosso diciamolo mi evidenzio questa parte vi dicevo in rosso è quella proprio che corrisponde ai vincoli che prima corrispondiva ai vincoli la parte in verde è quella che prima corrispondeva al termine di minimizzazione diretto ora li ho condensati in un'unica in un'unica condizione chiaramente diciamo la matematica qui è diversa e qui il tutto si gioca su quanto vale quel lambda cioè se lambda ipoteticamente proponiamo che lambda vale zero che cosa succede se lambda è zero se lambda è zero e io vado a minimizzare questa funzione vado a cercare semplicemente un iperpiano qualunque che mi soddisfa i vincoli quindi mi va bene un qualunque iperpiano a quel punto gli sto dicendo giusto? e non vado in cerca di quello col margine più ampio se invece comincio ad aumentare lambda io metto dico al mio ottimizzatore beh può essere discenso del gradiente metodo di newton quello che volete beh vai a cercare una configurazione di omega e b tale che questi due termini cominciano un po' a competere tra di loro allora lui cerca di soddisfare la parte in rosso ma cerca di minimizzare anche la parte in verde vuol dire che mi cerca anche tra gli infiniti iperpiani quello che ha un margine un po' più ampio di altri perché? perché minimizza questa quantità vuol dire che va in cerca del margine più ampio se lambda poi diventa cresce via via progressivamente io do sempre più importanza a questo termine significa che a questo punto non mi importa più di andare a minimizzare il primo termine rosso vuol dire che vado in cerca di un qualcosa che massimizza sì il margine ma in realtà alcuni di questi non li vado più a soddisfare e quindi comincio a sbagliare allora trovare il giusto equilibrio tra questi due significa trovare il valore di lambda di quell'iperparametro in realtà solitamente quello che si fa è si prende un valore abbastanza piccolo per lambda 10 alla meno 2 10 alla meno 2 lambda è un iperparametro che io ho introdotto ulteriore è un coefficiente moltiplicativo che mi permette di fare questo gioco che mi regola quanto peso do al margine però sapendo che aumentando lambda rischio di andare a sbagliare quindi che il classificatore non sia più corretto vado a prendere qualcosa che non c'entra niente lambda molto basso significa che io vado a imporre di trovare un classificatore che mi separa bene i punti però non è detta che sia quello a margine più ampio esattamente non a caso viene chiamato iperparametro esattamente come il learning rate e come vi dicevo parte del processo di costruzione del modello machine learning è la scelta degli iperparameteri si devono fare molti esperimenti cercare di capire qual è meglio eccetera ci siamo fin qui allora questo trucco di introdurre vedete questo termine aggiuntivo di regolarizzazione è qualcosa che si trova spesso quindi anche qui si chiama termine di regolarizzazione lambda viene chiamato parametro di regolarizzazione e in questo caso è molto interessante perché perché ci permette di come vi dicevo di trasformare la formula passare dalla formulazione di ottimizzazione non vincolata che è caratteristica del problema hard margin alla formulazione non vincolata che è quella che viene chiamata soft margin ok e qui è interessante perché uno possiamo utilizzare tutte le tecniche che abbiamo visto finora l'ottimizzazione non vincolata sono solitamente più semplici da gestire in questo caso ci va bene perché questo elevare qui al quadrato ci sono degli algoritmi che risolvono questi programmi quadratici in maniera efficiente però non sempre così solitamente tipicamente i problemi di ottimizzazione vincolata sono più complicati da risolvere se non altro perché sono un problema di ottimizzazione non vincolata a cui aggiungiamo delle condizioni in più quindi insomma non è così scontato che si riesca a risolverli l'altra cosa molto interessante oltre al fatto che riusciamo ad utilizzare tutto l'armamentario teorico tecnico e pratico che abbiamo introdotto nelle lezioni sull'ottimizzazione l'altra cosa interessante è che nella formula che è un po' meno scontata è questa nella formulazione hard margin se io quindi vado a lavorare su questo ok che cosa succede che lavorare su quella su quella formulazione significa che io devo minimizzare una quantità imponendo che questi vincoli siano soddisfatti per tutti i mille punti del mio dataset ora che cosa succede se il mio dataset non è per esempio linearmente separabile perché magari ci sono due o tre punti dieci punti quindici punti che diciamo ci siamo quasi molto semplicemente che cosa succede che lui non trova una soluzione in qualche modo si pianta diciamo nel senso che non riesce a soddisfare i vincoli quindi non c'è nessuna combinazione secondo questa formulazione di b e omega che mi soddisfa il problema di ottimizzazione cioè la formulazione hard margin se il problema non è linearmente separabile non riesce a trovare una soluzione la formulazione soft margin invece sì perché non ci sono dei vincoli che devono essere soddisfatti rigorosamente ci sono due termini di minimizzazione allora lui ne trova comunque una soluzione che cosa vuol dire vuol dire che trova un iperpiano che certo in quel caso il margine non c'è perché uno potrebbe dire ok ho dieci punti che ho comunque sbagliato quindi il margine è zero vero però in realtà trovo un iperpiano e se quegli dieci punti magari sono frutto del rumore perché il dataset era rumoroso lui comunque mi trova un iperpiano ragionevole e me lo trova comunque non è più un iperpiano ripeto a massimo margine ma di fatto è un iperpiano e lui lo trova la formulazione hard margin no e quindi questa è un'altra differenza di cui tenere conto molto semplicemente questo lo vedremo quando andiamo al laboratorio se noi imponiamo una formulazione hard margin e il dataset non è linearmente separabile lui non riesce a risolverlo quindi arriva fa questi algoritmi tipicamente imponiamo un certo numero di iterazioni avete visto al disceso del gradiente non trova nulla cioè non riesce a risolvere il problema quindi non lo trova appunto invece di là ci restituisce una soluzione quindi la differenza è proprio questa allora questo approccio qui c'è una sintesi delle cose che vi stavo dicendo allora questo approccio quindi viene chiamato appunto approccio di regolarizzazione il termine lambda è un numero maggiore o uguale di zero solitamente vi dicevo un numero abbastanza piccolo è un numero piccolo che ci permette di più piccolo di uno di mettere abbastanza pressione in modo da poter soddisfare i vincoli che è quello che vogliamo di partenza ma allo stesso tempo diciamo quindi diciamo se è molto piccolo idealmente che si attende a zero diciamo imponiamo solamente i vincoli e allo stesso tempo però ci dice se non è proprio zero beh devi cercare nei limiti del possibile anche di ottenere un qualcosa che mi minimizzi questo termine quindi abbia quel valore del vettore di omega sia piccolo quella norma del vettore di omega il che significa che il margine deve essere ampio e e bene questa versione appunto viene chiamata regolarizzata dalla funzione di costo noi siamo partiti da quella del percetrone del margin percetron viene chiamata svm soft margin e cosa succede se ho dei dati rumorosi ve lo stavo dicendo prima i vincoli della formulazione hard margin potrebbero diventare impossibili da soddisfare mentre quelli della funzione di costo soft margin può essere comunque minimizzata ed è il motivo per cui viene più utilizzata questa formulazione da un punto di vista pratico poi qui volendo avete tante altre variazioni sul tema perché potete utilizzare anziché questa che abbiamo appena visto che è la versione con il massimo tra 0 e 1 o meno la sua versione soft che da un punto di vista dell'ottimizzazione è un pochino più gestibile e quindi anziché il max che ha quello spigolo vi ricordate quando vi ho fatto vedere la funzione a cerniera vi ricordate e soprattutto da una parte la derivata è 0 dall'altra è costante quindi non posso usare derivate se introducete questo che è la sua versione soft max quindi log di e alla 0 più e alla meno yp cioè log di 1 più questo termine questo è una versione regolarizzata che nella formulazione soft margin appunto ha questo termine ed è se l'andate a confrontare con quello che abbiamo visto in lezione scorsa e la lezione precedente ritrovate delle cose che abbiamo incontrato perché se andiamo un po' indietro guardate la forma di questa quindi log di 1 più e alla meno e andate indietro è esattamente questo e tra l'altro quando abbiamo fatto la regressione logistica e adesso qui non c'è nella slide ma aveva esattamente una forma anche lì che era analoga e questo ci dice che cosa beh ci dice una cosa abbastanza semplice che se a questo punto semplice che regressione logistica e per cetrone l'abbiamo visto già a suo tempo abbiamo detto beh i risultati sono praticamente identici ok in questo caso ci mettiamo anche le svm cioè sono sicuramente correlati tra di loro ma se i dataset non sono linearmente separabili i risultati sono del tutto equivalenti se invece i dataset sono separabili linearmente l'svm ha in più questo vantaggio di massimizzare quel margine e questa è un po' la conclusione diciamo di tutto quello che abbiamo visto finora sui classificatori lineari quindi regressione logistica per cetrone per cetrone di margine e formulazione dell'svm e tutto questo può essere può essere sintetizzato nel nel quindi questo significa che la soluzione è fondamentalmente la stessa può essere sintetizzato però questo che vi stavo dicendo da se non sbaglio eccoci qui c'è una slide che che ripiloga un po' questo concetto in maniera lo esemplifica e allora sulla sinistra abbiamo sinistra e destra abbiamo lo stesso lo stesso dataset ok sulla sinistra abbiamo un un classificatore lineare di tipo per cetrone per cetrone per esempio col margine ok quello che però diciamo costruite andando a minimizzare da per esempio da tre punti diversi iniziali col metodo metodo di scesa dei gradienti quello che volete e trovate tre iperpiani se vedete lui li trova questi iperpiani che da un punto di vista del dataset di addestramento lui non sbaglia niente in nessuno di questi tre casi ok però sono chiaramente tre iperpiani che sono diversi da un punto di vista del margine sulla destra avete l'SVM l'SVM in cui avete è stato minimizzato la funzione di costo soft margin con parametro di regolarizzazione vedete 10 alla meno 3 quindi abbastanza piccolo però è sufficiente per farvi trovare tra tutti i possibili infiniti iperpiani quello che corrisponde al margine massimo e questo insomma è un aspetto interessante se non ci fosse stata la condizione di separabilità cioè se io avessi avuto un punto ad esempio non so un punto facciamolo spingiamo di aver avuto un punto blu qui avrei avuto e anche qualche altro punto così non avrei avuto la possibilità di separarlo linearmente chiaramente non avrei trovato nessun nessun nella formulazione al margin nessun tipo di di modello quindi sarei rimasto senza nessun modello la formulazione soft margin mi avrebbe portato comunque a un modello come così pure la regressione logistica o il percettrone e un modello che magari era sì uno di questi ok fondamentalmente sarebbero stati molto simili quindi avrebbe trovato un qualcosa che poteva essere questo adesso vado vado proprio anzi facciamolo in un altro colore avrebbe potuto essere che ne so questo ma adesso ripeto è non mi funziona purtroppo il cambio di colore non so perché vediamo che fosse stato questo non riesco a farlo non lo voglio di questo colore un colore che non c'è già potrebbe essere questo vediamo ok vediamo che fosse stato questo ad esempio ma non è giusto per dargli idea sarebbe stato per tutti e tre i modelli fondamentalmente quello perché non ci sarebbe stata gran differenza a quel punto però la cosa interessante è che ok io avrei sbagliato questo punto nel mio data sedito di allestramento ma avrei il mio modello comunque in tasca quindi se quel punto era frutto del rumore io comunque il mio modello me lo portavo a casa con il modello con la formulazione armargin no l'altra cosa molto interessante che vediamo qui in questa figura è quella che dà il nome a questo modello dell'SVM perché questi due iperpiani che vi ho appena evidenziato ok in fucsia che sono appunto gli iperpiani traslati del confine decisionale laddove diciamo intercettano il primo punto rispettivamente di una classe o dell'altra verso la distribuzione dei punti intersecano appunto i punti del data set qui e qui e questi punti che sono dei vettori perché vi ricordo che sono dei veri e propri vettori in questo caso sono dei vettori a due componenti in generale saranno vettori a n componenti vengono chiamati vettori di supporto cioè sono i punti che intersecano che intercettano diciamo il buffer praticamente i punti che stanno sul confine del buffer vengono chiamati vettori di supporto qui ce ne sono uno per ogni per ogni classe ce ne potrebbero essere più di uno che stanno magari qui o qui ok vengono chiamati vettori di supporto e da qui deriva il nome perché vengono chiamati i vettori di supporto perché in realtà voi se conosceste questi sapendo quali sono i vettori di supporto la cosa interessante è che potreste tranquillamente fare a meno di tutti gli altri punti del dataset una volta che avete quelli avete tutto quello che vi serve per definire la geometria del vostro modello ovviamente non ce l'avete aprivo i vettori di supporto perché li ottenete solo a partire dalla conoscenza del dataset però il motivo del nome è questo ok va bene allora brevemente diciamo qualche altra cosa poi andiamo avanti diciamo su queste su questo anche la prossima volta però diciamo per quanto riguarda intanto il discorso svm e i classificatori binari lineari abbiamo fondamentalmente concluso qui adesso ci sono un paio di argomenti a corredo diciamo di tutto quello che abbiamo visto e iniziamo a dirne alcuni ad entrare un po' nel dettaglio di alcuni di questi poi la prossima andiamo avanti poi passiamo diciamo come come diciamo percorso ad altri argomenti che sono quelli appunto dell'estensione nel caso multiclasse dei concetti che abbiamo visto però prima di arrivare al multiclasse è interessante anche fare alcune digressioni su alcuni aspetti che invece fino adesso non abbiamo tenuto molto in considerazione uno dei quali è la questione delle etichette delle quindi del label in inglese del del del nostro dataset allora dataset noi abbiamo dato per scontato che abbia etichette siamo partiti per esempio per la regressione qui oggi questa non funziona bene scusatemi ecco forse ce la faccio allora vi dicevo per la regressione logistica per esempio siamo partiti da etichette 0 oppure 1 classe 0 classe 1 l'abbiamo esteso classe meno 1 più 1 abbiamo detto è la stessa identica cosa abbiamo la stessa tipologia di classificazione quando utilizziamo delle funzioni di costo tipo cross entropy che è quella da cui siamo partiti o soft max sono essenzialmente la stessa cosa dopodiché che cosa abbiamo fatto abbiamo costruito delle funzioni di costo long error appunto le abbiamo mediate su p abbiamo visto che erano le funzioni di costo convesse eccetera eccetera la domanda che ci si può fare abbiamo detto beh e questo abbiamo detto uno potrebbe farlo anche se anziché avere 0 1 o meno 1 più 1 c'ha più 5 meno 5 per esempio qualunque altra coppia di valori numerici viene in mente però in qualche caso uno si può domandare vabbè ma se è vero che io posso sempre associare a delle etichette categoriche cioè non so il cane e il gatto posso associare delle etichette numeriche cane è 0 e il gatto è 1 è anche vero che implicitamente in questa associazione c'è una caratterizzazione forte cioè che se io associo a 0 il cane e al gatto 1 cane è minore di gatto allora in qualche in qualche tipo di applicazione questo potrebbe non essere diciamo il massimo su qualcosa poi torneremo su qualche esempio anche una lezione a seguire ma per il momento ci basti solamente la domanda dire ma se io ho un etichetta di tipo categorico categorico vuol dire che non c'è associato implicitamente un ordine per cui non dico questo è minore di quest'altro quindi non numerica posso trovare un modo di riutilizzare tutte le cose che abbiamo detto finora per questo tipo di approccio e ripeto ci sono dei casi anche dei motivi per poterlo fare uno potrebbe tranquillamente dire ok no non mi interessa vado avanti con questo però uno potrebbe anche semplicemente domandarselo come diciamo curiosità e dire vabbè che cosa succede se la risposta è affermativa cioè possiamo in realtà riutilizzare le cose che abbiamo visto finora estendendo un pochino la notazione e partendo da appunto quelle che vengono chiamate etichette categoriche e un modo per farlo è che viene utilizzato molto comunemente qui stiamo parlando di etichette categoriche per appunto di etichette categoriche ma vedremo anche ve lo anticipo comunque ve lo segnalo adesso però vedremo qualche esempio anche più avanti direi se non ricordo male comunque ve lo dico adesso la stessa tecnica può essere utilizzata per convertire delle feature categoriche in delle feature che possono essere utilizzate cioè le feature numeriche sono delle feature che vanno a finire in quel vettore X è abbastanza intuitivo se io dico l'appartamento ha 95 metri quadri 5 stanze e dista 4 chilometri dal più vicino ospedale sono tutte feature numeriche ma se io voglio codificare una feature non numerica categorica come ad esempio è l'appartamento in un quartiere a basso tasso di criminalità oppure in un quartiere residenziale particolarmente con un attributo diciamo qualitativo e non numerico come faccio a codificarlo ecco la stessa tecnica che vi sto introducendo adesso viene usata per codificare anche le feature in questo modo esattamente il cane e il gatto stiamo dicendo la stessa cosa io posso dire 0 oppure 1 ma in realtà è un attributo categorico non numerico quindi io o ci associo un valore lo posso fare anche con le feature e dico 0 il quartiere è un quartiere molto tranquillo 1 è un quartiere un po' problematico in realtà quello che si fa più comunemente anziché fare questo tipo di operazione che è un po' una forzatura si utilizza una tecnica che è presente perché è molto utilizzata che si chiama one-hot encoding ok è abbastanza intraducibile encoding sta per codifica codifica one-hot vuol dire che noi accendiamo un elemento alla volta fondamentalmente cioè facciamo che cosa supponiamo di avere un dataset a due classi ad esempio che è questo quindi xp yp quello che facciamo si trasforma l'etichetta 0 1 in dei vettori e questi vettori hanno un solo uno alternativamente acceso quindi ad esempio in maniera di tutto arbitrario in maniera di tutto equivalente potrei dire che se y se il punto ha etichetta 0 gli associo questo vettore che è il vettore 1 0 se y è 1 gli associo quest'altro vettore che è il vettore 0 1 che è chiaramente complementare dove avevo acceso l'uno da una parte lo spengo e accendo l'uno dall'altra se avessi avuto anziché due sole etichette un problema binario un problema alternario avrei fatto un problema di classificazione quindi non più a due classi ma tre classi avrei potuto introdurre un'ulteriore dimensione del vettore dell'etichetta avrei avuto 1 0 0 0 1 0 e 0 0 1 questi sono dei vettori quindi non sono più dei valori numerici ordinati e quindi diciamo da questo punto di vista scompare un po' quella distinzione per cui io associavo al gatto un numero più piccolo di quello che associavo al cane o viceversa quello che volete però ci permettono di con una leggera modifica un po' della matematica di riutilizzare tutte le cose che abbiamo visto finora con questa estensione e questo diciamo si fa in maniera abbastanza agevole la vediamo però direi questa nella lezione di domani insomma oggi di cose ne abbiamo viste abbastanza per cui domani ripartiamo da qua vedrete che insomma facciamo velocemente insomma si riesce con qualche ritocco marginale a utilizzare tutto quello che abbiamo visto finora per in termini per costruire delle funzioni di costo che lavorano con questo tipo di etichette anziché con lo 0 o l'1 o con l'1 o il meno 1 e quindi si arriverà a una formulazione ad esempio che vi anticipo l'equivalente della cross-entropy con etichette di tipo categorico che si chiama cross-entropy categorica va bene però direi che questo la vediamo nella lezione di domani e adesso intanto blocchiamo la registrazione se non ci sono domande su questo esattamente cioè questa è sì sì sì sì questo è molto utilizzato infatti vi ho premesso prima che la codificavo a note in code e la trovate su tantissimi modelli ed è questo cioè e voi ci potete codificare sia l'output quindi le etichette ma anche le feature eventualmente quindi l'input come vi dicevo va bene allora per oggi chiudiamo qua fermiamo la registrazione domani andremo avanti su questo