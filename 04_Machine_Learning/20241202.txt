eccoci qua allora intanto buongiorno a tutti allora iniziamo le ultime lezioni in cui cercheremo di capire qualcosa di più delle reti neurali quindi diciamo è un mondo come ben immaginate estremamente ampio nel quale sarebbero tante cose da dire potrebbe costruire un corso solo chiaramente dedicato all'ambito delle reti neurali e poi non basterebbe neanche oggi giorno le applicazioni diciamo dell'intelligenza artificiale quelle che sono più sotto gli occhi di tutti le più famose sono basate su questa tipologia di modelli di machine learning ovviamente non avremo modo di di vederle di vederle tutte vedremo solamente daremo un'introduzione con una panoramica dell'argomento all'intro un'introduzione che permetta ecco di definire alcuni principi di funzionamento questo sì ecco spero di riuscire a farvi capire alcune alcune caratteristiche di quelle che sono appunto i meccanismi alla base del funzionamento delle reti neurali che poi diciamo ricorrono in maniera trasversale anche in diverse tipologie di queste diverse applicazioni perché poi quello che cambia l'architettura delle varie reti quindi se parliamo di computer vision parliamo di reti neurali di tipo per esempio convoluzionale se parliamo di applicazioni al linguaggio naturale parliamo per esempio di reti neurali che sono in grado di gestire sequenze però poi alla fine quello che c'è dietro è la caratteristica di qual è la struttura di base quindi l'unità di base che vedremo è appunto l'unità che viene chiamata neurone e quello che cambia la configurazione come queste diciamo collegano tra loro più unità di tipo neuronale ma ad esempio alcune caratteristiche come il training rimangono le stesse cioè come addestrare queste strutture indipendentemente dal fatto che abbiano delle architetture diverse e quindi parleremo un po di questi aspetti che le riguardano in maniera direi trasversale allora per fare questo diciamo qui torneremo un attimo su questa slide tra poco prima faccio un breve passo avanti che anche perché la slide questa qui diciamo che vi ho messo come due è sì introduttiva però diremo qualcosa di più diciamo tra poco su questa perché prima andremo a caratterizzare ecco quello che ad esempio è un c'è all'interno di di questi di questi cerchi che cosa rappresentano ognuno di questi rappresenta un'unità neuronale però prima l'andiamo l'andiamo a caratterizzare con l'ausilio di questa slide allora noi abbiamo visto quando abbiamo parlato capitolo delle lezioni dedicato al teacher learning abbiamo parlato di approssimatori universali abbiamo detto le reti neurali sono degli approssimatori universali abbiamo definito un'unità di rete neurale come se riandate a vedere gli appunti di quelle lezioni l'abbiamo definita esattamente in questo modo abbiamo detto un'unità di nette neurale scusate mi è caratterizzata da che cosa dal fatto che prende i suoi input quindi leggiamo x1 x2 e fino a xn come gli input di un dato neurone facciamo una combinazione lineare di questi input ci sommiamo un termine che è un termine di bias e poi lo facciamo passare attraverso nel caso in cui quando ve l'ho spiegato detto la funzione tangenti perbolica ad esempio no se vi ricordate ma mi ho anche detto non necessariamente è solo quella ecco qui c'è tutta la caratteristica di quello che è il un'unità che costituisce una rete neurale se voi pensate ad esempio una cosa che è fatta così e poi avete a livello grafico abbiamo un punto in cui convergono x1 x2 e così via fino a diciamo x con n poi qui abbiamo due elaborazioni che producono l'output di questi x1 xn elementi la prima elaborazione è la combinazione lineare e la andiamo a mettere se non mi c'è sta lì la vado a disegnare qui dentro qui c'è la prima elaborazione e qua c'è una seconda elaborazione che chiameremo 2 allora la prima elaborazione è semplicemente la combinazione lineare la combinazione lineare ho il nostro vettore dei pesi ci sommo un termine di bias e ottengo ok il il risultato combinazione lineare l'abbiamo fatto fino 100 volte fino ad ora nell'ambito del coro secondo elaborazione che fa quell'oggetto è qualcosa di questo tipo prende e fa passare il risultato quindi z in una funzione che può essere facciamo così ad esempio una sigmoide può essere anche eventualmente la tangente iperbolica può essere qualcos'altro questa parte che fa seguito quindi qui abbiamo l'uno è abbiamo detto la combinazione lineare il 2 ecco che questa parte che vi ho appena scritto è la cosiddetta attivazione non lineare attivazione non lineare quindi ogni neurone è composto da una combinazione lineare e una attivazione non lineare del risultato di quella combinazione lineare questo è quello che avevamo visto noi abbiamo quando abbiamo definito il concetto di approssimatore universale abbiamo introdotto l'unità di rete neurale siamo andati a vedere ve l'avevo definita come la tangente iperbolica di che cosa? di u0 più questa sommatoria qua la combinazione lineare dell'ingresso è esattamente quello però vi ho detto guardate che non è l'unica la tangente iperbolica l'unica funzione potete usare anche altre funzioni di attivazione non lineare ad esempio la sigmoide ad esempio altre di cui vi parlerò più avanti ora una struttura di questo tipo viene chiamata nel gerdo delle reti neurali neurone oppure unità di rete neurale ok? è l'elemento base che effettua poi queste operazioni e che composto con altri elementi base vi permette di riuscire a ottenere ad esempio un approssimatore universale vi ricordate lo abbiamo visto quando abbiamo parlato dell'approssimazione universale abbiamo detto mettiamo insieme 1, 2, b che avevamo chiamati unità di un certo tipo e le combiniamo dice diciamo in quel teorema c'è tutto quello che serve per costruire un approssimatore universale e un'unità di questo genere è l'elemento base ok? su questo torniamo poi tra poco intanto vi volevo solo accennare come quindi viene costruito una singola unità neuronale qui viene chiamata per motivi di cui adesso poi vi dirò qualcosa in più unità dello strato nascosto del singolo strato nascosto perché poi adesso andiamo a vedere l'architettura cioè come queste unità possono essere composte tra di loro come se fossero dei mattoncini Lego per costruire quelle che sono proprio le reti allora la singola unità fa effetto un'elaborazione che è quella che vi ho scritto lì che da un punto di vista matematico possiamo scrivere così abbiamo un input x che è un vettore la funzione che svolge quel neurone viene chiamata f l'indice 1 sta a indicare che stiamo parlando delle unità di livello 1 cioè il primo strato quello che sarà un primo strato nascosto perché noi andremo vedrete a organizzare tipicamente questi neuroni in un'architettura in cui abbiamo diversi strati dove questi neuroni elaborano l'informazione diciamo in parallelo potenzialmente e poi le passano a strati successivi questo è quello che c'è nella prima slide di cui adesso poi sulla quale torniamo tra poco però intanto mi sembra più significativo partire dalla singola unità allora la singola unità qual è la funzione implementata da f è questa si prende un bias che qui abbiamo chiamato b che qui viene chiamato w0 la notazione è la stessa in qualche caso l'abbiamo chiamato w0 in qualche caso b è il termine di bias e poi vedete questa è la somma su tutte le n feature di che cosa x1 per un peso x2 per un altro peso x3 per l'altro peso e così via combinazione lineare ok ci siamo fin qua bene allora questa è ci permette definire la la funzione in questo modo di dare poi una una definizione di quello che fa un'unità di primo livello perché poi vedrete che come vi anticipavo prima comporremo metteremo tante unità insieme e queste unità riceveranno un certo tipo di input ognuna di queste e poi produrrà ognuna di queste un output ok ognuna di queste riceve tanti input che sono tutti gli x e produce degli output questa unità di primo livello potrà trasferire i risultati della sua elaborazione a un'unità di secondo livello a uno strato diciamo costituito da unità di secondo livello che avranno una struttura analoga e così via creando una struttura a strati che sarà appunto quella che caratterizza la rete però un passo alla volta ci arriviamo allora la definizione quindi di unità di primo livello quindi diciamo di primo layer o primo strato è una definizione intrinsecamente ricorsiva noi abbiamo una funzione che è la nostra funzione di attivazione si chiama esattamente quella che ho definito prima che può essere la sigmoide la tangente iperbolica può essere la reilu vedremo sono tutte valide come funzioni di attivazione sono delle funzioni non lineari che hanno delle caratteristiche sulle funzioni non lineari di attivazione torneremo successivamente ma ne esistono diverse di alternative la prima cosa che fate è calcolate un valore scalare che è questo è l'argomento che sta qua dentro dell'attivazione che è la combinazione lineare poi calcolate l'attivazione lineare di quell'argomento ok questo è tutto quello che fa il singolo neurone prende combinazione lineare dell'input lo passa attraverso una funzione non lineare e determina l'outfit sì esatto qui l'ho chiamato z e b e qua sarebbe ok vi metto proprio la corrispondenza v sarebbe z e b diciamo sarebbe quello che ho chiamato doppio ok ok ci siamo fin qua sì questo qualche volta trovate questa notazione qualche volta trovate questa anche nei libri di solito le attivazioni vengono chiamate z insomma giusto per darvi una qualche diversità va bene essere uniformi coerenti però va bene anche far vedere insomma cambiando la notazione va benissimo hai fatto benissimo a chiedere assolutamente chiedete sempre perché giustamente il dubbio viene ok qui che cosa c'è allora qui è stato fatto un gioco abbastanza semplice che il seguente sono state introdotte quattro istanze casuali di un'unità in cui quindi una due tre e quattro corrispondono a quattro istanze casuali e qui la funzione di attivazione è una tangente iperbolica qui un'altra funzione non lineare quindi qui praticamente la funzione di attivazione era tangente iperbolica di w0 di 1 più w1 di 1 per x qui chiaramente elaboriamo in questo esempio con mi cancello allora vi dicevo supponiamo che x non sia un vettore ma sia uno scalare quindi lavoriamo con una sola ficcia non una dimensione l'argomento della combinazione lineare avrà questa forma w0 più w1 per x dove w0 e w1 sono due pesi va bene l'uno è semplicemente per indicare che stiamo lavorando al primo strato di quella che sarà la rete ok è un indice allora se prendete e cominciate a generare dei valori di w0 e w1 a caso ne generate quattro coppie e poi fate la tangente iperbolica di questa combinazione lineare e prendete x che varia con continuità da meno 5 5 veramente io dico con continuità in realtà poi c'è una criglia di valore ok vi costruite potete ottenere ad esempio quattro possibili combinazioni se generate a caso chiaramente magari se lo fate voi ve ne vengono diverse però una cosa che potete fare prendete un aprite un notebook python cominciate a generare dei valori in un red x tra meno 5 e 5 generate delle coppie di valori e calcolate questo e poi lo fate graficare e ottenete una cosa del genere ad esempio cose simili cioè andamenti la seconda è ottenuta con un'altra funzione non lineare che è questa max che è il massimo tra 0 e w 0 di 1 più w 1 di 1 per x cioè la funzione che viene costruita è sempre la combinazione lineare che sta qua poi prendete è questa scusate poi prendete il massimo tra quella combinazione lineare e 0 questa è quella l'abbiamo già incontrata a suo tempo è una funzione che avevamo già incontrato diciamo in un'altra parte del corso che viene chiamata red red sta per rectified linear unit va bene se rifate lo stesso gioco generate 4 coppie w0 e w1 a caso e fate variare x tra meno 5 e 5 e poi calcolate questa è una funzione non lineare il massimo tra 0 è quello che ho tenuto questa è una funzione che vale 0 se l'argomento che sta qui dentro a destra è negativo altrimenti vale chiaramente l'argomento stesso se fate questo questo tentativo ottenete delle forme tipo queste qui queste quattro che sono state riportate ok va bene a seconda chiaramente del valore che assume w0 e che assume w1 avete 1, 2, 3, 4 possibili alternativi e questo adesso tenetelo presente perché vedremo che cosa succede quando questi input diventano a loro volta quindi quando questi diventeranno ad esempio questo l'input di un stesso unità neuronale che sta a valle e vedrete come queste figure si modificano quindi tenete presente questo per il momento ok ci siamo fin qui quindi stiamo fino adesso quello che abbiamo fatto è la descrizione di un funzionamento di un singolo neurone ok adesso torno un attimo indietro perché ognuno di questi cerchi bianchi rappresenta un singolo neurone e queste due figure sono due possibili architetture di due reti neurali in cui in generale abbiamo quello che viene definito uno strato di input lo strato di input è semplicemente rappresenta che cosa il vostro input quindi avete qui x1 fondamentalmente avete x1 qui arrivano le vostre feature x2 x3 e così via va bene e stessa cosa la avete nell'esempio di destra x1 x2 x3 quindi avete uno strato di input layer significa strato che rappresenta il vostro input il vostro input sono le vostre feature possono essere i pixel della vostra immagine possono essere i campioni audio del vostro segmento audio quello che vuoi dopodiché che cosa succede? vedete se io vado a prendere questa unità neuronale a questa unità punta x1 ma anche x2 ma anche x3 se vado a prendere questa unità neuronale qui ci vanno a finire dentro x1 x2 e x3 e così via quindi ogni unità neuronale è collegata a ognuna delle feature di input va bene? e qui dentro che cosa avviene? avviene esattamente quell'elaborazione che vi ho descritto poc'anti cioè combinazione lineare di tutti gli input quindi qui ci sono idealmente e suddivido in quelle due elaborazioni che prima abbiamo chiamato una chiamata 1 e poi l'altra l'abbiamo chiamata 2 e e quelle due elaborazioni sono due elaborazioni che consistono nella combinazione lineare 1 e 2 che cosa fa il 2? l'abbiamo detto attivazione non lineare con una funzione non lineare va bene? e la stessa cosa si ripete in tutti gli altri gli altri linea scusate gli altri neuroni del nostro linea quindi qui di nuovo avrò la suddivisione 1 e 2 qui avrò di nuovo la suddivisione 1 e 2 1 va bene? io in questo caso che cosa sto definendo? sto definendo quindi un insieme di neuroni che svolgono una certa funzione che è questa che vi ho appena detto che cosa hanno come caratteristica? di avere dei pesi interni i pesi interni sono nel caso dell'esempio unidimensionale che abbiamo visto prima la coppia W0 e W1 se l'esempio non è più unidimensionale ma in questo caso è con tre feature abbiamo tre dimensioni avrò una combinazione lineare di queste tre feature con dei pesi che sono quindi a quel punto W0 W1 W2 W3 e così via e ogni unità avrà dei pesi che sono diversi da quelli degli altri potenzialmente ok? e questi pesi sono quelli che stanno esattamente qui voi prendete se volessimo rappresentare a livello grafico potreste associare a ognuna di queste frecce un peso ok? per cui x1 entra nell'unità primo neurone con un certo peso x2 c entra con un certo peso ma x1 entra con un altro peso nel neurone che sta qui sotto e così via questo se ci pensate bene è lo schema che abbiamo usato quando abbiamo parlato degli approssimatori universali perché li abbiamo detto prendiamo una combinazione lineare di più unità e ognuna di queste unità è esattamente questa però ne prendiamo insieme quante? b le avevamo chiamate poi questo tornerà nelle slide successive però intanto ve lo allora io posso cominciare a mettere insieme una serie di unità di questo tipo ognuna si porta dietro i suoi pesi questi pesi per motivi legati al parallelo diciamo con la biologia perché poi questo approccio è un approccio che è noto anche come approccio connessionista perché è evidente che stiamo parlando di connessioni che nella biologia poi sono le connessioni sinaptiche perché vedremo che il modello di riferimento è quello del cervello di un organismo ne riparliamo c'è una slide un po' più avanti dedicata a questo però intanto non ragioniamo come se fossimo ragioniamo semplicemente con modelli in cui io comincio a comporre queste unità ognuna di queste è un neurone si chiamano neuroni non a caso infatti anche lì di nuovo per via del parallelo con la biologia quindi con le scienze e con le neuroscienze io posso cominciare a comporli e li posso comporre come meglio credo quindi posso cominciare a inventarmi diverse tipologie di architettura qui a sinistra abbiamo un esempio in cui questi neuroni vengono di fatto ad avere lo stesso ruolo rispetto allo strato di input e non sono collegati tra di loro vi faccio notare quindi il flusso dell'informazione da sinistra verso destra non c'è non ci sono collegamenti connessioni tra neuroni che appartengono a questo stesso strato questo strato viene chiamato anche strato nascosto perché? perché è un qualcosa che sta tra l'input che noi conosciamo e l'output che è subito dopo che è quello che noi vedremo va bene? e che cosa fa? eh lo strato di output vedete in questo caso ho messo due unità neuronali e le ho collegate ognuna a tutte le uscite dello strato precedente qui avrò due neuroni e qui posso fare la stessa cosa che facevano i neuroni dello strato precedente o più tipicamente quello che si fa nei neuroni dello strato di uscita è eliminare la funzione non lineare cioè si fa solo combinazione lineare degli ingressi qui ne ho messi due se guardate l'architettura di destra qui ce n'è uno solo anche qui posso scegliere se mettere più neuroni oppure no sullo strato di uscita quello che si fa tipicamente è modellizzare lo strato di uscita diversamente a seconda che abbiamo un problema di regressione o un problema di classificazione se io ho un problema di regressione solitamente lo strato di uscita ha un solo neurone che semplicemente fa da collettore di tutto quello che gli arriva dallo strato precedente e fa la combinazione lineare di quello che gli arriva in me e fornisce in output un solo valore che è il valore della regressione se invece ho un problema di classificazione la rete neurale ha tanti neuroni nello strato di uscita quante sono le classi tipicamente funziona così se io ho due classi ho due neuroni in uscita se ho dieci classi avrò dieci neuroni e ognuno di questi produce un singolo output e la classe verrà assegnata al valore più elevato tra le possibili alternative quindi è come se si accendessero delle lampadine quella che dà la luce più intensa e quella corrispondente alla classe ok questa è un modo di vedete allora la differenza l'altra differenza che non vi ho detto grossa macroscopica che salta subito all'occhio tra queste due due reti qual è al di là oltre all'output qui abbiamo un solo output quindi abbiamo abbiamo due neuroni nell'output è che qui io ho replicato la stessa struttura cioè il fatto di avere a disposizione più neuroni che lavorano in parallelo e che non sono collegati tra di loro su uno strato successivo che è un secondo strato nascosto nascosto perché di nuovo non è che ci sia nulla di nascosto ma rispetto all'input che io conosco e all'output che conosco è uno strato in cui ho dell'informazione che fluisce all'interno e viene definito nascosto per cui rispone yes sure quindi So the question is regarding the activation function the nonlinearity or the activation function. Why? Why is it very meaningful to activate? Why is it very meaningful to activate? Okay, okay. I'll translate it before in Italian. So if the question is why there is the need for an activation function, the answer is nonlinearity. Activation functions provide the necessary nonlinearity to the neuron. And without such nonlinearity, you could not build a universal approximator. Okay? So, but you can have different activation function. You can have the sigmoid. You can have the ReLU. You can have the hyperbolic tangent. You can have many different alternatives. And they are, we will talk a little bit about these activation functions. They are not all the same from the point of view of some technical details, for instance, for training and so on. But they are the same in the sense that they provide the necessary, the required nonlinearity to the unit. Without such nonlinearity, you wouldn't have a universal approximator. You wouldn't be able to build a classifier with some boundary which is nonlinear. So, the answer is that the functions of activation function are nonlinearity. And that is necessary because otherwise you don't have to build those approximators universal. So, all those profiles, for example, of classifier that have a function, a decision-making decision-making nonlinearity. Okay? So, this function of activation is necessary for this. because otherwise, a single neuron would not be able to understand this. This is the same. If you go and have a look at this, at the four, this figure, this four subfigure, and you have a look, if you are not able to build these nonlinearity, and then the interesting thing is what happens when you compose these nonlinearities with other nonlinearities. And you have, you will see in the next slide, you will have shapes of varying, of an increasing variety, which is made possible only by the nonlinear activation. Okay? So, you will see that these curves, when we go and compose these units with other units that are in the valley, they have a nonlinearities that begin to take the nonlinearities of everything that we had to do to do to do to do to do to do to do to do to do to do to do from a smaller difference. And this will create a minority , that will � demost一 alla volta vi dicevo qui siamo a uno strato nascosto e ne abbiamo aggiunto un altro perché uno si potrebbe domandare ok cosa succede se anziché andare in output comincio a mettere un altro strato di questo tipo e poi potrei mettere un altro un altro ancora ma potrei anche aggiungere scusate ci sono quattro neuroni potrei rimetterne 5 10 perché no non c'è scritto da nessuna parte che debbano essere 4 o 5 10 e qui si entra in un ambito quello appunto delle architetture c'è una varietà ovviamente infinita di architetture di rete ma intanto procediamo per per gradi questi due strati che diventano due strati nascosti sono quelli che distinguono in realtà qui c'è il primo confine tra quelle che sono delle reti neurali cosiddette shallow in inglese cioè non profonde e invece quando si comincia a andare più di uno strato nascosto si parla di reti profonde cioè di di quindi con almeno due strati nascosti cominciamo ad entrare nell'ambito del cosiddetto deep learning deep learning è lo studio delle reti neurali in cui avete almeno avete un numero di strati nascosti maggiore di uno delle reti neurali che che trovate oggigiorno architetture più comuni quindi per esempio quelle in ambito computer vision hanno 8 10 strati nascosti 5 8 possono essere perché io mi posso giocare la e qui aggiungere strato nascosto significa tornando un po le cose che abbiamo visto la settimana scorsa aggiungere che cosa aumentare la capacità del modello perché aumenta il numero di parametri aumenta la sua capacità di di di di punto di avere a disposizione una varietà di comportamenti che che gli permetta di modellizzare un mondo sempre più articolato e complesso un mondo di input e intendo e allo stesso modo anziché andare in profondità potrei decidere di andare in ampiezza potrei anche fermarmi a un solo strato nascosto e cominciare ad aggiungere tanti neuroni e anche di aumento la capacità del modello aumento il numero di parametri ok al traslato la domanda è come definiamo quanti strati nascosti abbiamo la risposta è non possiamo definirlo nel senso che un qualcosa che dipende da tante cose che sono la natura del problema la capacità del progettista e spesso e volentieri si fanno delle prove cioè in questo ambito il machine learning diventa molto un'arte anche oltre che una scienza nel senso che bisogna provare fare dei tentativi e si vedono le performance le prestazioni e poi ovviamente sono dei tentativi che non devono essere auspicabilmente alla cieca ma devono essere motivati però ci sono delle regole generali ma sono prevalentemente empiriche e quindi per esempio prima si procede con un numero di parte di solito da modelli ad elevata capacità quindi si preferisce aggiungere parecchi rispetto a quella ovviamente uno deve avere a disposizione un certo budget computazionale e poi magari si regolarizzano ok quindi l'arrivo da questa so the answer is there is no a fixed answer you can it is up to the designer in as a function of the problem to decide the depth how many hidden layer you have and it is more the result of an empirical exploration of the problem so it is a science but the deep learning is also an art in this sense because you you should have as a designer the awareness of the problem and then you you you try it is not a blind trial of course but is is a matter of trial and often what is advised is start to a model with high capacity so in function of your computational budget try a network with a good depth so computational budget how many hours of cpu or gpu you can afford for your training because this this architecture needs a lot of computational effort but if you have a computational budget you can try to try a quite a quite big network and then you maybe it overfits your data but you can regularize it and use and but there is no there is no rule to define one two three or five for sir for a lot of problem the point is that you can go in width the same is true for the number of neurons you can go in width or in depth usually going in depth adds to the the the the effectiveness so this is why deep learning is so is so useful but in principle you can you could only use one hidden layer this is a universal approximator of course quello che stavo dicendo è che se andiamo in profondità solitamente le reti sono più efficaci cioè riescono ad ottenere una notevole varietà quindi diciamo solitamente si aggiungono dei strati di uscita ma in linea di principio io potrei usare una rete con un solo strato nascosto e aumentare il numero di neuroni qui questo è un perfettamente un approssimatore universale come l'abbiamo definito noi nel senso che sono ognuno di questi un'unità che fa un elaborazione poi vedete quello che facciamo è prendere queste e andare verso lo strato di uscita e fare una combinazione lineare di queste quindi ognuno di queste sono quelle b unità che messe insieme avevamo detto no erano un approssimatore universale però non ci sono regole preconfezionate quindi diciamo si entra qui in un ambito che è estremamente ampio ok allora giusto per dare un pochino di però di regole quello che possiamo vedere che c'è comunque una regolarità vedete no quindi la regolarità qui è che tutti sono collegati a tutti ad esempio anche questo non necessariamente è vero ci sono delle architetture in cui non tutti sono collegati a tutti adesso vi dirò qualcosa magari anche su di questo per cui anche qui aumenta la varietà però intanto partendo da questi esempi qui tutti gli elementi sono collegati a tutti quelli che seguono e come potete vedere il flusso delle informazioni va da sinistra verso destra queste architetture vengono chiamate architetture di tipo feed forward feed forward significa che l'informazione fluisce da sinistra verso destra non ci sono dei loop per cui si torna indietro ci sono altre architetture di rete per esempio le reti neurali ricorrenti non so se l'avete mai sentito nominare che invece introducono dei loop introducono una sorta di memoria per cui tornano indietro e per cui il calcolo non è puramente feed forward ok ma sono altre stanno da un'altra parte non avremo modo di vederle però per esempio l'altra cosa caratteristica è vabbè qui in questo caso abbiamo quindi una rete con questa si dice essere una rete per convenzione l'input layer non si conta questa viene detto una rete con due strati uno nascosto e uno di output ok quindi penso è una neural network a due layer e quindi questa chiaramente una rete neurale con tre media ok quindi di questa abbiamo uno strato nascosto uno strato di auto lo strato nascosto a quattro unità lo strato di auto due qui abbiamo due strati nascosti da quattro e un output layer con un'unità allora io adesso vi invito un attimo anche a ricordare quello che vi dicevo prima cioè il fatto che quando noi siamo andati a scrivere a parlare di approssimatori universali avevamo detto erano partiti dal nostro modello che era una cosa di questo genere che mi riscrivo qua modello che era funzione di x di teta dove teta erano insieme di parametri abbiamo detto a doppio zero più f1 di x più v2 e 2 scusate per w1 più w2 scrivo meglio qui questa w2 per f2 di x wb per fb di x dove teta vi ricordo erano insieme di parametri erano w0 w1 w2 wb e tutti i parametri interni che stanno qui dentro ok ora se voi prendete una rete in cui avete un solo strato nascosto e andate a prendere il risultato qui in cui questa non fate l'attivazione lineare fate la combinazione lineare quindi di questo di questo di questo di questo ottenete esattamente questa funzione ok si che non che non ha in realtà l'attivazione dal layer con i neuroni finali esattamente esattamente esattamente adesso su questo poi torniamo anche su questo però è per dirvi che sono collegato a quanto abbiamo visto ok e infatti quello che si può dimostrare è che una rete neurale con un solo strato nascosto è un approssimatore universale cioè quel risultato di approssimazione che vi ho detto risale agli anni 80 in cui le reti neurali sono degli approssimatori universali si riferisce proprio a una rete neurale con un singolo strato nascosto cioè voi potete approssimare qualunque funzione con un livello arbitrario di precisione a partire da una rete neurale costruita in questo modo con un solo strato nascosto è ovvio che più aumentate la precisione più potenzialmente avete bisogno di un numero di neuroni più alto potenzialmente abbiamo detto infinito ok però in linea di principio vi basterebbe andare con una sola rete neurale a aumentare il numero di neuroni il punto è che nella pratica si è visto che anziché andare a aumentare il numero di neuroni e prendere una rete con un solo stato nascosto è molto più efficiente è molto più efficace scusatemi andare a mettere meno neuroni in uno strato ma magari cominciare a mettere due strati, tre strati o quattro strati anche quello che ottenete lì è un approssimatore universale però tipicamente avete bisogno di meno neuroni e quindi è il motivo per cui si preferisce andare di solito in profondità piuttosto che in ampiazza chiaro? sono esattamente delle funzioni composte cioè matematica è la composizione di funzioni funzioni di funzioni le reti neurali DIP sono esattamente questo sono delle funzioni composte di altre funzioni in cui ognuna di quelle fa sempre la stessa cosa combinazione lineare e attivazione non lineare combinazione lineare e attivazione non lineare e si compongono tra di loro altra cosa se voi prendete un singolo neurone abbiamo detto fa la combinazione lineare dell'input e poi lo fate passare ad esempio attraverso la funzione sigmoidale se prendete un singolo neurone quindi senza lo strato nascosto ma un solo neurone quindi in cui avete l'input e prendete questo come il suo output quello che avete è una regressione logistica ad esempio combinazione lineare la regressione logistica la fa passare attraverso la funzione sigmoidale se sopra 0,5 vale 1 se sotto 0,5 vale meno 1 o 0 e di fatto quindi un neurone rappresenta una rete neurale con un solo neurone rappresenta una regressione logistica facciamo un po' di conti su questo esempio cancello un po' di cose quanti parametri abbiamo su questi due esempi da ottimizzare prendiamo questa architettura di rete in questo caso abbiamo quanti neuroni numero di neuroni nn è uguale a 4 più 2 6 neuroni giusto? ok il numero di pesi quant'è? allora il numero di pesi sono qui ci sono per ogni input ogni input va verso tutti questo input va qui va qua va qua e va qua la stessa cosa per questi altri ok? e ognuno di questi è un parametro giusto? perché io devo determinare il peso no? della connessione che è W no? la combinazione lineare di questi input ha un peso che è questo poi va verso l'altro neurone con un altro peso poi con un altro peso quindi il numero di queste frecce vi dà il numero di parametri di questa rete quindi avete il numero di pesi il numero chiamiamolo nw che è il numero dei parametri il numero dei pesi che è il numero di parametri il numero dei parametri quant'è? beh avete ognuno dei tre input deve andare verso ognuno dei quattro neuroni dello strato nascosto e in più avete anche ognuno dei quattro output dello strato nascosto che deve andare verso i due dell'uscita questo è il numero dei pesi in realtà qui non abbiamo indicato queste sono le combinazioni lineari quello che non c'è in questa figura è che ognuno di questi si porta dietro il suo termine di bias quello che abbiamo chiamato B qualche volta oppure il w0 ok questo c'è il suo w0 questo c'è il suo w0 che non è uguale per tutti quindi questo è il w0 diciamo chiamiamolo adesso lo indicheremo dopo non mi ricordo bene alcun cosa però per distinguerlo lo chiamiamo 0 del primo neurone del secondo e così via quindi ognuno di questi c'è il suo termine di bias quindi il totale ci sono ci aggiungiamo sei termini di bias giusto? ok sono quattro per lo strato nascosto e due per per lo strato di uscita totale questa rete deve apprendere 8 più 12 più 6 26 parametri ci siamo? quindi come fa a prendere questi 26 parametri? si costruisce una funzione di costo questa funzione di costo va a confrontare supponiamo che questo risolva un problema di classificazione binaria perché ha due output la funzione di costo può essere ad esempio la cross entropy binaria che abbiamo visto va a confrontare il risultato quindi cominciamo per esempio dando i valori di questi 26 pesi a caso random questi 26 pesi a caso producono due valori qui in uscita uno dei due sarà più alto e qui sarà quindi la classe corrispondente predetta sarà o 1 o meno 1 o 1 0 noi sappiamo la verità qual è se la classe che viene indicata è 1 e l'ha indovinata pagherà 0 altrimenti pagherà una penalty che è quella della binary cross entropy ok? a questo punto che cosa succede? quello che viene fatto è io ho un certo valore della funzione di costo per quella distribuzione casuale di quei 26 pesi posso fare di meglio? sì voglio minimizzare quella funzione di costo mi sto muovendo in uno spazio che ha 26 parametri quindi uno spazio in cui la mia funzione di costo ha 26 variabili devo cominciare a minimizzarla come faccio a minimizzare una funzione a 26 variabili? gradient descent discesa del gradiente le reti neurali vengono sistematicamente addestrate con metodi del primo ordine tipicamente ad oggi ci sono anche dei lavori che usano metodi del secondo ordine ma sono per il momento meno popolari discesa del gradiente per il momento funziona molto bene nella versione batch o stocastica e questo l'abbiamo visto che cosa vuol dire si comincia a quel punto a calcolare il gradiente di che cosa? della vostra funzione di costo la binary cross entropy rispetto ai parametri quindi il gradiente in questo caso un vettore con quante componenti? vediamo chi mi sa rispondere in inglese so for those who in english we have a loss function with 26 parameters we have to optimize minimize this loss function with respect to the parameters and the question is how many we need to compute the gradient how many how many how many entries does the gradient vector have? 26 26 26 yes 26 è una funzione di 26 variabili il gradiente ha 26 componenti una per ogni variabile e quindi voi dovete calcolare quel gradiente a quel punto una volta che avete il gradiente che cosa fate? nella direzione di quel vettore puntata da quel vettore fate un passo secondo il learning rate e andate giù in discesa auspicabilmente in realtà degli oggetti di questo tipo hanno come come potete immaginare un sacco di minimi locali però la cosa bella è che questi minimi locali tipicamente non differiscono enormemente tra di loro e quindi diciamo si riescono a minimizzare bene col metodo di discesa del gradiente proviamo a calcolare anche quanti parametri ci sono in questa seconda rete allora questa seconda rete ha quanti neuroni sono 4 più 4 più 1 quindi il numero di neuroni è 9 e il numero di parametri quant'è? vediamo se c'è qualcuno che me lo sa dire allora facciamo 3 per 4 ok più 4 per 4 bravi ok 3 per 4 più 4 per 4 più più 4 ok per 1 32 a questi ci sommiamo in realtà tutti i termini di bias che sono 1 per ogni neurone 32 più 9 diventa 41 in questo caso 41 parametri quindi la loss function è una funzione a 41 dimensioni ecco cosa vuol dire quando vi dicevo guardate che machine learning oggigiorno è un machine learning ad alta dimensionalità queste sono ancora delle reti che fanno quasi sorridere rispetto alle reti che ci sono oggigiorno in giro in circolazione yes nine bias one for each neuron is the bias term ok quindi tenete conto che le reti parlavamo prima di computer vision sono delle reti con un'architettura che poi semplifica rispetto a questo però si portano dietro dai 10 fino a 20 layer in profondità e 10 alla 8 parametri quindi la funzione da ottimizzare è una funzione con 100 milioni di variabili non parliamo non parliamo poi delle cose delle applicazioni tipo l'elaborazione del linguaggio naturale come chat gpt o diciamo i sistemi gpt che hanno miliardi oggigiorno di parametri ecco quel miliardo di parametri che cosa sono sono questi pesi e le funzioni di costo che lavorano su questi pesi devono andare a ottimizzare devono essere ottimizzate rispetto a quel centinaia di milioni di pesi e questo è il motivo per cui oggigiorno I'll come to your you're asking something I'll come to you in a minute ed è il motivo per cui oggigiorno addestrare queste queste reti ha un costo enorme significa allora si possono addestrare solo se si hanno altrimenti quantità di dati enormi in posizione e da questo punto di vista diciamo per esempio il web rappresenta una fonte di dati che può essere utilizzata però ci vuole una capacità computazionale organizzativa di infrastrutture per addestrare questi sistemi perché l'addestramento è un addestramento che si basa sulle GPU ma ci vogliono centri di calcolo notevoli perché poi quando cominciamo a parlare di reti con centinaia di milioni di parametri significa tempi di addestramento molto elevati quindi tante cluster di macchine con GPU a disposizione ed è il motivo per cui oggigiorno sono cose che si possono permettere da zero di fare solamente diciamo tre quattro cinque aziende quelle più potate quindi stiamo parlando dei soliti più grandi player come Google Facebook cose di questo genere fanno molta fatica oggigiorno anche diciamo non dico ovviamente le università ma anche appunto le agenzie governative a stare dietro a questo tipo di di corsa quindi questo è un po' il succo del discorso your question? yeah yeah where is the black box so the question is why do we talk about black box yes the problem is that it is not in the optimization the black box and I would say that the problem of the black box is that if you open in theory you would be able to say well this is the weight of this connection okay so I would in theory know everything I know everything of my once I once I have minimized my loss function I know each single weight okay so in principle I know everything it is it is not a black box in this sense but it is because if a new input arrives and I have millions of parameters and this light switches on and the other one is switched off and if I ask why it is not immediate to say what the neural network has done so in this sense I don't know if I answer your question la domanda era una volta che abbiamo ottimizzato perché si parla di scatole nere per queste reti allora in linea in linea teorica non c'è una scatola nera nel senso che una volta che ho ottimizzato questi 41 parametri io so esattamente il loro valore se anziché averne 41 ne ho 100 mila o 10 milioni so esattamente il valore di ognuno di quelli il problema è che quando arriva un nuovo input alla rete e questa mi accende questa lampadina piuttosto che questa è quando ho 100 milioni di parametri capire perché ha fatto quello non è immediato e quindi lì cambia il discorso se invece avete una regressione logistica vabbè avete un certo numero di pesi che difficilmente sarà 100 milioni e a quel punto forse riuscite a spiegare meglio comunque avete delle tecniche che qui non insomma è più difficile applicare in questo senso è più un black box ok allora ragioniamo un attimo stiamo ancora visto che stiamo parlando di parametri in sempre in quest'ambito supponiamo di avere una un problema in cui avete delle immagini anche qui vi ho detto data set ce ne sono tanti un data set sviluppato questa è un'agenzia canadese che il nome credo che sia arrivante da questo adesso non mi ricordo l'acronimo però c'è un data set che è pubblico su cui vengono addestrate sono state addestrate le prime reti neurali che lavorano sulle immagini non le prime scusatemi delle immagini nel scorso decennio le prime che lavorano sull'immagine era quello che vi ho sempre detto MNIST subito dopo è arrivato questo che si chiama SIFAR 10 questo è l'acronimo ripeto del credo dell'ente governativo del Canada che l'ha pubblicato 10 è perché a 10 classi quindi un problema a 10 classi come le cifre scritte a mano ma ci sono degli oggetti appartenenti a 10 classi diversi quindi ed erano delle immagini a colori c'è una versione la più piccola sono immagini 32 per 32 per 3 ok 32 per 32 pixel per 3 perché sono a colori RGB vi ricordate ne abbiamo parlato quindi sono delle immagini a colori e ognuna di quelle quindi questo è un numero che se fate i conti fa 3072 quindi vuol dire che voi avete qui un input un vettore a 3072 componenti giusto quindi un singolo neurone se voi per esempio ragionate su questa architettura adesso qui in realtà siccome dovrebbero esserci 10 di neuroni in uscita però prendete il singolo neurone no vogliamo di avere un certo numero dello stato nascosto ognuno si vede arrivare in input 3072 pesi cioè 3072 connessioni che pesano ognuno di quei 3072 input con un certo pezzo giusto ok ci siamo va bene se io allargo un po quest'immagine e anziché la versione da 32 per 32 prendo quella da 64 per 64 sono stiamo parlando di definizioni quasi ridicole 64 32 per 32 l'abbiamo visto l'altra volta 8 per 8 si vede qualcosa ma insomma rispetto ai megapixel di qualunque smartphone ovviamente sono sono piccole però già guardate che cosa succede succede che questo numero fa 12288 quindi ogni singolo neurone si vede arrivare 12288 connessioni giusto se vado a prendere per curiosità un'immagine da un megapixel quindi un'immagine tipo 1000 per 1000 che fa un megapixel ho chiaramente 3 per 10 alla sesta connessione cioè un singolo neurone si vede arrivare 3 milioni di connessioni ok quindi significa che se io vado a costruire una rete in cui ho una cosa di questo tipo vediamo un po' se riesco a disegnarla qua allora ho un input vabbè ve la faccio qua è come se io avessi qui un input x1 x2 fino a x con n e questo input è un input con 3 milioni di un vettore con 3 milioni di di entry quindi x è l'input appartiene a r 3m diciamo 3 milioni ok e e ognuno di questi vabbè punta a un certo numero di di neuroni dello stato dello strato nascosto e supponiamo che questi neuroni siano mille vediamo cosa succede? che ho 3 milioni per mille e diventano 3 miliardi di parametri e capite bene che così si va poco lontano e infatti quello che se si lavora in ambito computer vision in cui l'input è ad alta dimensionalità quindi già solo l'input è molto elevato quello che si fa è si cambia l'architettura cioè non si usa questa architettura in cui ogni elemento è collegato a tutti gli altri queste architetture vengono chiamate reti neurali dense oppure totalmente connesse ma nell'ambito computer vision sono state introdotte altre architetture vengono chiamate di rete che sono le reti convoluzionali le reti convoluzionali hanno il pregio di sfruttare quella che viene chiamata la connettività locale cioè ogni neurone non viene collegato a tutti i neuroni dello strato successivo ma solo a quelli diciamo spazialmente vicini e questo tra l'altro è vantaggioso perché se voi pensate a come è organizzata un'immagine un'immagine se voi la vedete appunto come un insieme una matrice di pixel diciamo c'è una dipendenza dei pixel dai pixel vicini una correlazione e non pixel che stanno dall'altra parte dell'immagine questo diciamo si riflette in un'architettura di questo tipo riuscite a risparmiare il numero di parametri e a costruire delle architetture che non hanno questa cosa smisurata di parametri che avreste questa esplosione di parametri se lavorate con un'architettura densa quindi la figura è organizzata in volumi però questo ci porterebbe lontano non dobbiamo adesso esplorare anche l'architettura convoluzionale noi l'unica architettura che vedremo è questa appunto delle reti totalmente connesse che però ecco scalano male su problemi in cui l'input è ad alta dimensionalità cioè con una rete noi al laboratorio vi faccio vedere come fare un riconoscimento di cibere scritte a mano con una rete appunto che prende input di questo tipo ma se dovete cominciare a prendere un input di questo tipo intendo insomma fino a 30x30 ecco non mi ricordo il menistro cosa c'ha ma un numero ridotto di pixel già se dovete andare appunto su 1000x1000 che ripeto è una fotocamera da un megapixel non ce la fate con una rete di questo o meglio insomma comincia a essere molto oneroso allora quello che si può dimostrare l'abbiamo detto è che le reti ed è stato dimostrato le reti neurali con più di uno strato nascosto queste di tipo feed forward totalmente connessi sono degli approssimatori universali ok e questo è il risultato teorico diciamo tipicamente però quello che si fa è che se hanno hanno lo stesso potere di rappresentazione cioè cosa vuol dire io potrei cavarmela vi dicevo prima con un solo strato nascosto andando molto in ampiezza ma tipicamente funziona meglio andare in profondità ok quindi da questo punto di vista ma questa è una considerazione più pratica che teoria cioè empirica questo perché da un punto di vista andare in profondità significa estrarre delle feature dell'informazione sulle feature gerarchica progressivamente si vede che il primo strato utilizza certe informazioni dell'input il secondo strato estrae delle informazioni a partire da quello e così via e quindi la rete costruisce man mano che va in profondità apprende proprio quelle feature che è l'obiettivo di tutto questo che è il feature learning cioè la rete lo apprende da solo quello che le serve va a selezionare dall'input quello che le serve e lo trasforma va bene adesso proviamo ad andare avanti e come? sì sì sì tipicamente il disceso del gradiente è dietro l'addestramento di tutti i sistemi di rete neurale allora questo l'abbiamo già visto perché ve l'ho anticipato prima quindi brevemente la rendiamo a vedere adesso andiamo a studiare un pochino più in dettaglio le reti neurali totalmente connesse ok? che sono adesso l'abbiamo capito meglio una particolare tipologia la prima probabilmente studiata ancora adesso si usa per certi problemi però abbiamo capito bene che c'ha dei problemi di scalare male se avete un input con molta altra dimensionalità va bene? quindi se l'input è altra dimensionalità qui diciamo potete usarlo per pochi neuroni potete avere pochi neuroni per ogni strato allora come potete costruire le unità del singolo strato nascosto come abbiamo detto all'inizio della lezione combinazione lineare avete n feature touching weight e un termine di bias e lo fate passare attraverso una funzione non lineare quindi una volta che avete selezionato la vostra funzione di attivazione non lineare calcolate questo scalare v come la combinazione lineare più il termine di bias e poi calcolate il valore della funzione di attivazione funzione di attivazione che vi ho già detto può essere tangente iperbolica re e lucide ok? a questo punto uno si domanda che cosa succede se ho preso un singolo neurone e comincio a comporlo in maniera da costruire un modello non lineare che ha B vi ricordate? l'abbiamo chiamato B proprio quando abbiamo parlato della costruzione degli approssimatori abbiamo introdotto il discorso sugli approssimatori universali B lo chiamiamo in questo caso anche U1 vi lo settiamo uguale a U1 dove uno sta per indicare che stiamo parlando dell'unità del primo strato e quindi c'è un certo numero lo fisso ok? nei due esempi che avevamo nella prima slide erano quattro neuroni quindi quel B uguale a U1 sarebbe stato 4 in quel caso nella figura sinistra ok? il nostro modello diventa ve l'ho scritto prima W0 più F1 di 1 per W1 più F2 di 1 più F infatti c'è no un certo numero U1 questo 1 qui indica che stiamo parlando del primo strato nascosto primo strato ok? questo è il nostro modello qui dentro ci sono nel nascoste anche tutti i parametri interni alla funzione va bene? ogni singola funzione che sono quei parametri che stanno qui cioè il generico termine FJ che sta qui dentro lo potete FJ di 1 dove l'1 questo 1 indica sempre il primo strato nascosto lo potete scrivere come di nuovo il suo termine di bias W0 di J più tutta la combinazione lineare di X1 X2 X3 fino a X con n di che cosa? del peso J ci siamo? ok? qui è come se voi no? dite ok? io prendo in considerazione ho il primo strato nascosto in cui ho messo 4 neuroni neurone 1 neurone 2 neurone 3 neurone 4 il neurone 3 e supponiamo di avere 3 input al neurone 3 arrivano tutti questi 3 input ok? e io sto dicendo che F3 di questo primo strato nascosto questo è il primo strato nascosto quindi lo scriviamo così per farvi a mente fa confusione ma facciamo un riquadro questo F3 di 1 io lo posso scrivere come funzione di X esattamente come funzione di attivazione di che cosa? di W0 virgola 3 di 1 più la sommatoria per n piccolo che va da 1 fino a 3 di W W N virgola 3 di 1 per X con N prendo questo sarà W1 virgola 3 W2 virgola 3 W3 virgola 3 e in più qui c'è anche il suo termine di bias che è il W0 la stessa cosa la posso ripetere oltre che per 3 per 1 per 2 e per 4 giusto? vi ho specializzato vi ho fatto vedere cosa succede se J è uguale a 3 ottenete questa ognuno si fa i suoi conti con i suoi pesi con le sue connessioni allora questa notazione può essere notevolmente resa molto molto più compatta e adesso facciamo un piccolo sforzo per cercare di capire cosa fa questo e come può essere fatto perché da quella notazione compatta vedrete insomma si capiscono un po' di cose un po' di caratteristiche proprio delle reti minorali ma qualcuno di voi alcune di queste caratteristiche l'ha già intuito però adesso lo facciamo vedere meglio a beneficio di tutti allora io posso costruire al solito un vettore a partire dal mio vettore x che è lo stesso ma in cima ci vado a mettere un 1 e lo identifico con x con sopra il cerchietto poi guardate mi posso costruire una matrice w1 questa matrice w1 dove l'1 sta di nuovo perché stiamo ragionando su 1 il primo strato nascosto quindi 1 allora perché siamo al primo layer nascosto ok allora qui che cosa siamo andati a mettere in ogni colonna se voi andate a vedere vedete la prima colonna w01 w11 fino a wn1 se voi andate a vedere questo che cos'è è il vettore dei pesi del primo neurone questo è il vettore dei pesi del secondo neurone qui non c'è scritto ma sarebbe il vettore dei pesi del terzo neurone subito dopo sarebbero w03 w1,3 w2,3 fino a w1,3 quindi qui andiamo a mettere in colonna i pesi che rappresentano le connessioni verso il primo il secondo fino all'ultimo uesimo neurone ok se io scrivo questa matrice in questo modo in cui vado a organizzare tutti i pesi ok quindi questa matrice nell'esempio questa è una matrice che vi vado a scrivere quante righe e quante colonne ha allora è una matrice che ha andate a vedere n più 1 righe e quante colonne u1 più 1 e basta perché sono esattamente 1 colonne ok quindi nell'esempio da cui siamo partiti all'inizio in cui avevamo un input layer con tre input e avevamo quattro neuroni dello strato nascosto questa matrice sarebbe stata 4 per 4 ok perché facciamo questo perché la organizziamo in questo modo l'informazione sui pesi in questa matrice perché a questo punto se voi andate a prendere la trasposta di questa matrice e la moltiplicate per questo vettore x vi accorgete che quello che ottenete è una cosa di questo tipo cioè voi fate questo e se prendete la trasposta quindi la trasposta di questo ha dimensione 1 per n più 1 ok questa è la dimensione quindi w questa è la dimensione di w1 trasposto a questo punto se lo moltiplicate per x che ha dimensione n più 1 per 1 ottenete che cosa una matrice che ha dimensione guardate 1 per n più 1 per n più 1 per 1 ottenete qualcosa che ha dimensione quindi w1 t per x con sopra il cerchietto ha come dimensione abbiamo detto u 1 per 1 cioè è un vettore un vettore che ha quante componenti tante quanti sono il numero di neuroni dello strato e la jesima componente se voi andate a vedere è un prodotto righe per colonne quindi è un prodotto tra questa e questa ottenete w0 prendete quella generica j più che va a moltiplicare il quindi il termine di bias che va a moltiplicare 1 più tutta la sommatoria delle feature touching weight quindi questa è la generica entry jesima di questo prodotto perché facciamo tutta questa cosa perché la jesima entry è la combinazione lineare dei dati interni appunto all'unità jesima e quello che otteniamo stiamo facendo tutto questo giro perché possiamo esprimere il tutto in maniera più compatta fino a qua ci siamo siete riusciti a seguirmi eh ok sì è un po' di matematica di algebra lineare ma niente di particolare posso andare avanti e cancellare oppure vi lascio un minuto qualche secondo ancora allora tenete presente che cosa rappresentava questo w1t trasposto per x il jesimo elemento di quel vettore adesso definiamo un vettore a a nostra funzione di attivazione se io prendo un qualunque vettore v lo definisco il vettore a risultante quello che ottengo applicando la funzione di attivazione a v1 a v2 a ognuna delle componenti di questo vettore chiaro quindi se a è la sigma qui prendo sigma di v1 poi ho sigma di v2 sigma di vg chiaro che cosa rappresenta a è un vettore che avete costruito a partire da un vettore che ha un certo numero di componenti e andate a applicare quella trasformazione lineare non lineare scusatemi a ognuna di quelle componenti allora a questo punto io vado a prendere quel vettore che abbiamo calcolato prima che abbiamo detto è w1t trasposto per x e vado ad applicare alla sua jesima componente la funzione non lineare ma questo è proprio quello che stiamo scrivendo che cosa fa il singolo neurone fa esattamente l'attivazione non lineare di questa combinazione lineare e quindi a questo punto beh a questo punto posso fare un ulteriore passo avanti perché posso definire un vettore w2 che è costruito con le combinazioni lineari scusatemi i pesi della combinazione lineare delle delle unità che sono u1t 2 fino a u1 e ognuna di quelle ha il suo peso e a questo punto quello che otteniamo è una cosa di questo tipo definisco un vettore a con il cerchietto come qualcosa che è 1 e poi qua sotto invece c'ha la quindi a partire da questo a con il cerchietto è uguale a 1 a di v1 a di v2 e così via fino a di vd ok e definisco il vettore dei pesi queste sono le combinazioni lineari finali in uscita da tutti i vari neuroni ne ho un certo numero quanti ne ho 1 e ognuno di questi neuroni quindi questi sono i vari neuroni ognuno di questi mi converge in una combinazione lineare finale con la quale costruisco il mio modello che a questo punto ed è il motivo per cui abbiamo fatto tutto questo è che cosa allora w1t per x che è quello che avviene all'interno dei singoli neuroni combinazioni lineari viene fatto passare attraverso una funzione non lineare che viene costruita esattamente in questo modo e poi faccio la combinazione lineare finale cioè io ho costruito il mio modello esattamente come a partire da un certo numero 1 di combinazioni di di neuroni scusatemi 4 nell'esempio da cui eravamo partiti avevamo 4 neuroni di primo livello 1 2 3 e 4 quindi questo valeva 4 e quindi che cosa faccio faccio la ognuno di questi quattro neuroni prendeva da 3 input tutti i valori ok uno di questi andava verso gli altri quattro neuroni combinazione lineare attivazione non lineare e infine moltiplicazione per un vettore che mi rende conto del fatto che qui vado a fare una combinazione lineare finale questo è il nostro modello con un solo output perché qui è un modello in cui abbiamo un solo output non può essere generalizzato a più neuroni di uscita allora il vantaggio di fare tutte queste elaborazioni è che io ho ottenuto un modo molto ma molto compatto di esprimere il mio modello e se guardate bene stiamo parlando di un modello con un solo strato nascosto se voi guardate le uniche cose che facciamo sono prendere l'input fare una moltiplicazione matrice per vettore prendere la funzione di attivazione non lineare applicarla ad ogni componente del vettore che ottengo prendere questo risultato e fare la moltiplicazione per un vettore dei pesi fine quindi il modello non lineare è la composizione di che cosa? di delle trasformazioni lineari fatte passare poi attraverso una attivazione non lineare e poi di nuovo combinate linearmente quindi nel nel mio modello feedforward ho una composizione di funzioni che sono trasformazioni lineari fatte passare attraverso una attivazione non lineare e poi di nuovo combinazioni lineare questo modello tra l'altro è perfettamente generalizzabile quando ho più strati cambia solamente il fatto che avrò qui all'interno una serie di composizioni di questo tipo questo è molto comodo come notazione perché è una notazione molto compatta e vi fa capire che cosa avviene poi all'interno di questi modelli quando fate una nuova predizione una nuova inferenza voi avete congelato il vostro modello calcolato tutti i pesi e quello che fa quel modello è costruire queste funzioni in cui componete queste funzioni con combinazioni lineari attivazione non lineare combinazione lineare attivazione non lineare se e con questo diciamo direi che poi per oggi possiamo concludere se andiamo a riepilogare tutto quello che abbiamo detto finora e costruiamo una rete neurale totalmente connessa con un solo strato nascosto abbiamo in generale una struttura di questo tipo guardate qui abbiamo un primo neurone un secondo neurone quanti ne mettiamo insieme? 1 1 nell'esempio da cui siamo partiti era 4 poi all'interno di ogni singolo neurone che cosa succede? che abbiamo le nostre n feature che vanno a fare che cosa? questo simbolino qua indica che cosa? la combinazione lineare ognuna con i suoi pesi w11 w21 eccetera eccetera l'indice 1 sta a indicare che stiamo parlando di neuroni di primo livello ognuno di questi si porta dietro vedete i suoi termini di bias quindi questo è il vettore che noi abbiamo chiamato x con il cerchietto quindi questo è il primo neurone che si prende la sua combinazione lineare l'ingresso e poi che cosa fa? in questa in questa questa unità in questo cerchio diciamo azzurro avviene la attivazione non lineare cioè calcola la f1 questo qui il neurone 2 fa la stessa cosa ha dei pesi diversi con i quali fa la combinazione lineare dell'ingresso calcola la sua attivazione non lineare e quindi la f2 fino ad arrivare all'ultimo neurone del primo strato nascosto ognuno di questi poi viene combinato linearmente con questi pesi w0 w1 w2 w1 questo è esattamente quello che fa questa rete questi qui sono tutti i parametri interni ai neuroni questi sono i parametri esterni quelli di quella sommatoria che ha avviva b termini ok che poi abbiamo chiamato 1 in questo caso perché ci permette poi di estendere quando abbiamo più strati questo è il layer di input questo che vedete qua che sta qui è lo strato nascosto qui avete l'output questo è tutto il vostro modello qui avviene la trasformazione delle feature la trasformazione avviene secondo questo modello combinazione lineare attivazione non lineare combinazione lineare attivazione non lineare quando ho finito tutto prendo faccio la combinazione lineare di quello che ho ottenuto se utilizzate quella notazione compatta che abbiamo ricavato prima vedete che potete calcolare una matrice che abbiamo chiamato w1 voi quello che dovete fare semplicemente è prendere il vostro vettore x e andare a moltiplicare e fare quell'operazione che era w1 trasposto per x e ottenete che cosa ottenete l'input allo strato nascosto o meglio la prima parte dell'elaborazione dello strato nascosto che è questa poi c'è l'attivazione non lineare e poi fate la combinazione con questo questo vi dà una versione vedete anche a livello grafico più compatta che potete ulteriormente compattare se andate a sostituire appunto al posto di di x prendete il vettore x con il cerchietto vedete qui avviene la w1 trasposto per x lo fate passare attraverso l'attivazione non lineare e poi lo moltiplicate per quel w2 che rappresenta l'insieme di questi pesi e ottenete l'out sono tre versioni grafiche che riflettono tre diciamo progressivi compattamenti si dice direi di sì della notazione con le matrici ma rappresentano sempre la stessa cosa va bene da qui ripartiamo domani ok va bene abbiamo abbiamo detto direi parecchie cose provate a riguardare domani quello che facciamo e ripartiamo da qua e poi ci domandiamo in quella notazione in questa notazione compatta cosa succede se aggiungo un ulteriore strato queste sono le reti totalmente connesse ogni input è collegato a tutti i neuroni e se noi ne aggiungiamo un altro strato dopo se la rete è totalmente connessa vuol dire che ogni output di questi deve essere collegato a tutti i neuroni dello strato successivo reti dense oppure totalmente connessi sono la prima elementare forma di reti neurali che è stata studiata e prima che venissero sviluppate poi delle reti specializzate a partire da queste però rimane la base anche per le architetture diverse cioè quando voi andate ad aprire a vedere a studiare una rete per esempio che fa riconoscimento di immagini spesso e volentieri c'ha dentro degli strati così detti quelli convoluzionali che vi dicevo prima per cui non tutti sono collegati a tutti ma c'è un principio di località ci sono delle trasformazioni particolari poi alla fine vedrete che trovate tipicamente due per esempio uno strato denso come questo o due eccetera oppure intervallano questi strati questi layer dense layer di altro tipo quindi diciamo è come un gioco veramente di lego in cui questi layer rappresentano i neuroni rappresentano dei mattoncini elementari un layer di tipo connesso l'avete costruito a partire da questi mattoncini e poi lo potete riusare a sua volta come strato in mezzo ad altri strati di altro tipo costruiti con questi altri mattoncini ok domani andiamo avanti su questo va bene grazie a tutti grazie a tutti grazie a tutti grazie a tutti