Allora, buongiorno e bentornati. Oggi terminiamo la trattazione degli argomenti che riguardano i classificatori lineari multiclasse. Vi ricordo molto brevemente le cose principali che abbiamo visto nelle ultime due lezioni. Abbiamo approcciato il problema della classificazione multiclasse, anzitutto con il metodo che abbiamo chiamato one versus all o one versus rest. Che consiste nell'addestrare un numero di classificatori binari pari al numero delle classi e poi comporli in quella che abbiamo chiamato regola di fusione. Abbiamo anche visto che, anziché fare il tuning di questi n classificatori nell'ottica one versus all, possiamo di fatto applicare una tecnica che si basa sull'estensione, al caso multiclasse, di quello che era l'approccio del percetrone, del perceptron, che abbiamo affrontato per la classificazione binaria. E siamo giunti a una formulazione del percetrone multiclasse con l'approccio softmax, che di fatto era anche questa una regola di fusione che permette di fondere i vari classificatori che abbiamo visto per il one versus all. Quindi sostanzialmente era un approccio che permetteva di fare il tuning tutti insieme, quindi un tuning simultaneo dei C classificatori, anziché farlo uno alla volta. Quindi diciamo in linea di principio per questo, abbiamo detto, dovrebbe essere in grado di garantire un'accuratezza migliore, anche se in realtà in pratica sono tutti e due molto utilizzati e producono risultati simili. Una cosa che, diciamo, non vi ho detto, non ci sono nelle slide, ma visto che adesso stiamo facendo questo breve riepilogo, secondo me vale appena accennare, è la seguente. Un altro approccio che potete trovare nella classificazione multiclasse è quello cosiddetto one versus one. One versus one è un approccio in cui fate il training di un numero di classificatori che è pari a c per c meno uno diviso due, cioè sostanzialmente addestrate un classificatore per ogni possibile coppia di classi. Quindi supponiamo di avere tre classi, fate il classificatore uno contro due, uno contro tre e due contro tre. Cosa significa uno contro due, uno contro tre? Significa che il classificatore viene addestrato a riconoscere come un punto, un punto come appartenente alla classe uno o alla classe due, anziché appartenente alla classe uno contro tutte le altre classi. E poi alla fine che cosa viene fatto? Di questi... Devo andare appunto una matrice in cui abbiamo le quattro classi, ok? Anziché tre prendiamo le quattro così, diciamo, è un esempio che ci permette di fare anche qualche altra considerazione riguardo al confronto col one versus sole. Viene addestrato, vi dicevo, un classificatore a distinguere la classe uno dalla classe due, poi dalla classe tre e poi dalla classe quattro. Stessa cosa viene fatto per il classificatore due, viene insegnato a distinguere dalla classe tre e dalla classe quattro, poi abbiamo il... e ovviamente dalla classe uno, ma quello l'abbiamo già fatto, il classificatore tre viene... viene, diciamo, addestrato in modo da distinguere il classificatore della classe tre dalla classe quattro e il classificatore della classe quattro di fatto lo abbiamo... praticamente lo otteniamo, diciamo, avendo risolto tutti gli altri. Quindi in questo caso abbiamo un numero di classificatore, vedete che è esattamente c per c meno uno fratto due, cioè quattro per tre dodici diviso due che fa sei, che sono esattamente questi sei classificatori che abbiamo ottenuto. Come si fa a scegliere poi qual è la regola, in questo caso l'equivalente della regola di fusione? Si va a fare una sorta di votazione, per cui il punto viene classificato in base a quello che è il classificatore più votato, cioè alla classe più votata nei vari classificatori binari. Quindi, ad esempio, se un punto viene classificato come appartenente alla classe 1 in 1 contro 2 e 1 contro 3, mentre viene, diciamo, classificato, poniamo come appartenente alla classe 2 nel confronto classificatore 2-4, quello chiaramente, oppure per semplicità mettiamo anche questo, avrà preso, diciamo, tutti e la totalità di fatto dei voti, e quindi verrà classificato come appartenente alla classe 1. Ovviamente si lavora su un, cosiddetto, majority voting, quindi chi prende la maggior parte dei voti viene poi attribuito a quella classe, va bene? Qui la differenza con il one versus all, al di là, diciamo, dell'accuratezza del classificatore, che poi alla fine non è così drammaticamente diversa, è nella complessità, perché qui devo costruire più classificatori, quindi devo addestrare un numero di classificatori che cresce come c al quadrato, perché mi addestro c per c-1, quindi qualcosa che cresce come c al quadrato. Mentre nel caso del one versus all ho un numero di classificatori che è pari a c, cresce linearmente, quindi o grande di c, ok? Però è anche vero che in questo caso vado a lavorare su un numero inferiore di punti, perché ognuno lavora su, chiaramente, mentre nel one versus all lavoro sempre su tutto il dataset, nel momento in cui addestro tutti i punti, nel one versus one, ogni singolo classificatore viene addestrato su un numero di punti che non è tutto il dataset. Se io sto addestrando il classificatore 1 e il classificatore 3, quindi a distinguere classe 1 e classe 3, scusate, quindi quel classificatore 1 e 3, lavoro solo sui punti con etichette 1 e 3, e quindi lavoro su un dataset ridotto. Quindi da un punto di vista poi della performance, bisogna, della complessità computazionale paradestrale, bisogna confrontare un po' questi discorsi. Bene, questo era giusto una breve, diciamo, flashback, insomma, siamo tornati un po' indietro su queste cose, abbiamo integrato alcune informazioni riguardanti la classificazione multiclasse, e avevamo anche visto, avevamo cominciato a parlare di, per introdurre il discorso delle etichette categoriche, di distribuzioni discrete, avevamo visto che le distribuzioni discrete emergono, diciamo, da quelli che sono, ad esempio, dei dati in forma vettoriale, degli istogrammi, come in questo esempio, che possono essere opportunamente normalizzati, in modo da dar luogo a delle distribuzioni di probabilità, e questo può essere fatto anche laddove questi dati vettoriali contengono dei valori negativi, e quindi abbiamo introdotto la normalizzazione esponenziale, attraverso la funzione esponenziale normalizzata, che è questa riportata qui, abbiamo chiamata sigma, e abbiamo detto e alla s0 diviso la somma dei valori per tutte le classi degli e alla sc, poi e alla s1 diviso sempre lo stesso numeratore, eccetera, eccetera. Abbiamo visto che questa è la caratteristica di essere un vettore le cui entri sommano a 1 e sono tutte positive, quindi hanno, diciamo, a tutto quello che serve per poter essere interpretato come una distribuzione discreta di probabilità. Ok. Poi, e questa era l'ultima cosa che avevamo visto, abbiamo detto, bene, come possiamo utilizzare una trasformazione di un vettore di questo tipo? Beh, la prima cosa che possiamo fare è utilizzarla, ad esempio, appunto, in una classificazione multiclasse, in cui di fatto quello che otteniamo sono una serie di distanze consegno di ogni punto dal classificatore relativo, quindi il punto in questione ha una certa distanza consegno, che è questa rossa, dal classificatore rosso, un certo punto di distanza consegno dal classificatore blu e un certo punto di distanza consegno, in questo caso negativa, dal classificatore verde perché si trova sul lato negativo di quel classificatore, di quel confine decisionale. E questo significa che cosa? che noi possiamo applicare, essendo questo appunto un vettore numerico che ha le caratteristiche, che abbiamo prima detto, e queste sono tutte le distanze consegno dal rispettivo confine decisionale, noi possiamo fare una cosa, semplicemente andare ad applicare la trasformazione esponenziale normalizzata. Questa trasformazione esponenziale normalizzata, che qualche volta, diciamo, prende il nome anche di softmax, come vi dicevo, la stessa operazione viene introdotta nei softmax layer, nelle reti neurali, generando, come vi dicevo, potenzialmente confusione nella nomenclatura, quindi bisogna stare attenti a cosa si intende, perché non è il softmax che abbiamo visto a proposito delle funzioni di costo, perché non è un rimpiazzamento soft della funzione massimo, ma viene chiamata comunque così, però la possiamo applicare e otteniamo questa, questa e questa, ad esempio, che sono tre distribuzioni di probabilità che vi dicono immediatamente, a livello visivo, ma anche a livello numerico, che qual è la probabilità di occorrenza, diciamo, di classificazione, di appartenenza a una determinata classe del vostro campione. Quindi il vostro classificatore vi restituisce immediatamente quella che è una distribuzione di probabilità, che è un qualcosa di estremamente utile in diversi frangenti. Oh, e adesso arriviamo a vedere un attimo anche l'ultima cosa che era rimasta da vedere, come poi questo può essere anche utilizzato, la normalizzazione esponenziale, per il discorso delle funzioni di costo scusatemi, delle etichette di tipo categorico e poi le relative funzioni di costo cross-entropi di tipo categorico. L'abbiamo visto questa cosa per il caso binario, cioè abbiamo detto, caso binario, se vogliamo esprimere un'etichetta non come un'etichetta puramente numerica, cioè anziché associare un valore 0, un valore 1, che implicitamente definisce un ordine e quindi non voglio dire che il cane è 0, il gatto è 1, quindi il gatto è maggiore del cane, che è una cosa che non ha senso in molte applicazioni, allora posso fare la stessa cosa qui ed estendere quel concetto al caso multiclasse, in cui anziché avere il cane e il gatto e basta, avrò il cane e il gatto e magari il pesce. E anche qui, se voglio, posso procedere associando a cane l'etichetta 0, al gatto l'etichetta 1, e al pesce l'etichetta 2, oppure posso fare quello che abbiamo fatto per il caso binario, possiamo trasformare ognuna delle etichette, esattamente come l'ha riportato qui, che sono, vi ricordo, 0, 1 fino a C-1, e le posso trasformare, secondo questa codifica, che viene chiamata one-hot encoding, per cui, ad esempio, alla classe 0 viene associato un vettore che ha un 1 in prima posizione, alla classe 1, un 1 in seconda posizione, e così via, fino alla classe C-1, che ha un 1 in ultima posizione, e tutti gli altri sono sempre 0. Ovviamente anche questa associazione è arbitraria, e può essere, diciamo, diversa, in senso che potrei associare questo alla classe 0, e questo, viceversa, alla classe C-1, e tutti gli altri in mezzo, insomma, diciamo, in qualche modo, rovesciarli simmetricamente. Poco importa, poco importa, perché il risultato è che le etichette, adesso non sono più delle etichette numeriche con degli ordini, questi sono dei vettori, non c'è più il concetto di ordine, per cui non posso dire che questo vettore è maggiore di questo, o viceversa. E in analogia a quanto abbiamo visto già per il, il, il, la classificazione binaria, quello che possiamo fare è, abbiamo questi vettori di uscita, e, scusate, abbiamo questi vettori, esattamente, che rappresentano, sono i vettori di uscita, che rappresentano le classi, e, quello che possiamo fare è utilizzare questa notazione vettoriale, quindi, y, p diventa un vettore a questo punto, non è più un singolo numero, e quello che possiamo fare è utilizzare questa notazione per costruire dei classificatori, ma che, alla fine, possiamo anticiparlo, ovviamente, come nel caso binario, producono, arrivano alla stessa esatto risultato, quindi, diciamo, partendo da una rappresentazione di tipo categorico, si riesce a raggiungere tutto quello che abbiamo visto, e lo stesso risultato che abbiamo visto, per etichette di tipo numerico, perché io quello che posso fare, e, ovviamente, si complica un pochino la formalizzazione matematica, per cui, siccome adesso abbiamo dei vettori e non più dei numeri in uscita, quello che facciamo è, diciamo, partiamo, se vogliamo ripercorrere l'approccio in cui siamo partiti dalla vista di regressione, andare verso una regressione di tipo multi-output, cioè multi-output, perché, ovviamente, io non voglio più ottenere un singolo numero, 0, 1, oppure meno 1, più 1, ma devo ottenere un vettore che è uno di questi, ok? Quindi è una regressione multiuscita, per cui io voglio che il mio modello lineare che è dato da questo, il prodotto tra il vettore punto e questa, che non è più a quel punto un vettore, ma sarà una matrice, cioè una matrice che ha n più 1 righe e c colonne, in cui io vado a mettere tutti i pesi, ok? Che io debbo andare a ottenere, quindi avrò tante colonne quante sono le classi e se ho un problema di classificazione, per esempio, tre classi, sarà un vettore in cui vado a mettere i tre vettori peso, quindi W sarà una matrice in cui vado a mettere i tre vettori peso, W1, W2, W3, che devono essere poi oggetto del nostro tuning, dell'addestramento per tutti i vari classificatori. Se la vediamo come una classificazione One vs. All ho tre classificatori diversi, tre confini decisionali, ognuno rappresentato da un vettore dei pesi e X trasposto W deve essere uguale a YP il più possibile. Chiaramente la relazione lineare non rappresenta YP molto bene, quindi quello che si fa, anche qui si può ripercorrere, come abbiamo fatto per la regressione logistica, una trasformazione lineare che di fatto in questo caso la nostra trasformazione esponenziale normalizzata e quindi si va a fare sigma di XP trasposto W e si va a vedere quanto è simile a YP. Noi vogliamo fare il tuning, ottenere il vettore dei pesi, non è più un vettore ma una matrice dei pesi, tale che questa sia il più possibile vera. Cioè io voglio che una volta che ho fatto passare il mio modello lineare attraverso una trasformazione non lineare che è quella esponenziale normalizzata questo sia il più possibile simile a YP. E di nuovo allora qui posso costruire delle funzioni di costo. La prima che ci può venire in mente è questa. E questa che cos'è? Una funzione di costo ai minimi quadrati. L'abbiamo vista in più occasioni, oramai la conosciamo bene. Di nuovo anche qui valgono tutte le considerazioni che abbiamo visto a suo tempo. Quindi questa può essere introdotta ma se andiamo a prendere quella basata sul log error è ancora migliore perché ha delle caratteristiche che la rendono diciamo dal punto di vista dell'ottimizzazione migliore per tutta una serie di motivi. Qui quello che cambia è la matematica perché ripeto questo è un prodotto vettore matrice non è più un prodotto vettore per vettore avete gli indici su tutte le classi quindi diventa più complicata. Su questo diciamo anche qui come per un altro paio di passaggi di questo blocco di argomenti direi che possiamo anche essere un pochino più rapidi e andare a vedere qual è il risultato però vedete al di là della differenza chiaramente nella notazione matematica ci sono anche delle similarità cioè se voi andate a confrontare questa espressione con quelle che abbiamo ricavato per due classi a proposito dell'etichetta categoriche otteniamo lo stesso tipo di approccio qui potete scrivere appunto questa notazione in questo modo questo ci interessa relativamente quello che si può arrivare a dimostrare è che se andate poi a prendere la media su tutti i punti ottenete questa espressione e questa è la funzione cosiddetta di costo cross entropy bar oppure softmax perché sono equivalenti nel caso multiclasse tutto questo per dirvi cosa? tutto questo per dirvi semplicemente che di fatto noi riusciamo ad estendere esattamente perché poi alla fine quello che si dimostra è che questa è esattamente equivalente a quella che abbiamo ricavato la volta scorsa noi siamo partiti la volta scorsa dalle etichette numeriche abbiamo costruito classificatore multiclasse con il one versus solo il per centrone eccetera eccetera se voi anziché partire da delle etichette di tipo numerico partite dalle etichette di tipo categorico che codificate con one note encoding ottenete questa che è del tutto equivalente si si si si esattamente per arrivare a questo che è equivalente a quella da cui siamo partiti va bene si c'è quella matrice è un grande dei pesi n più sarebbero le fissure sarebbe la dimensione esattamente esatto ci siamo ripeto adesso qui non stiamo a entrare tutto nel dettaglio perché altrimenti aggiungiamo inutilmente cose il succo del discorso è che la funzione di costo cross-centro piccategorica è del tutto equivalente poi alla fine a quella numerica e cambia un pochino la matematica e quello che si ottiene è questa va bene questo è tutto quello direi che ci serve più che sufficiente per i contenuti di questo corso quello che invece possiamo andare adesso a questo punto a indagare ok abbiamo visto molti modi di fare di costruire un classificatore multiclass one versus all one anche one versus one vi dicevo all'inizio della lezione poi abbiamo il percettrone multiclass abbiamo visto come ottenerli se codifichiamo le etichette numericamente oppure in modo categorico abbiamo visto che le rappresentazioni sono equivalenti il punto è come facciamo a fare delle predizioni ok una volta che abbiamo addestrato il classificatore beh facciamo le predizioni in modo analogo a quello che abbiamo visto per il classificatore binario anche qui estendiamo esattamente gli stessi discorsi che abbiamo fatto per il classificatore binario a questa casistica multiclasse cioè noi abbiamo terminato il processo di addestramento ottenendo un vettore dei pesi wc ok vettore dei pesi quindi se guardiamo ad esempio questo esempio di classificatore tre classi avremo tre vettori dei pesi w1 asterisco w2 asterisco w3 asterisco o stella e questi tre vettori dei pesi hanno appunto questo asterisco a indicare che cosa che abbiamo congelato il il nostro i pesi dei nostri classificatori nel momento in cui abbiamo identificato i pesi ottimali a valle dell'ottimizzazione va bene quindi abbiamo fatto l'addestramento una volta che abbiamo fatto l'addestramento quindi prendiamo il caso del one versus soul abbiamo la nostra regola di fusione che ci dice come andare a ottenere il valore di un punto incognito e il valore di un punto incognito lo calcoliamo come regola di fusione ci dice prendi x trasposto lo moltiplichi per w1 x trasposto lo moltiplichi per w2 x trasposto lo moltiplichi per w3 vai a vedere qual è il valore più elevato di questi tre e la y è l'arg mass tra 0 1 e 2 di questi tre valori e quindi questo è chiaramente quello che si fa qui avete evidenziati i tre confini decisionali quindi questa è chiaramente la regola di fusione e andiamo ad applicarla semplicemente l'unica cosa che cambia rispetto a quello che abbiamo visto e che fino ad ora ci siamo occupati dell'addestramento adesso chiaramente l'addestramento è terminato e io posso avere un punto incognito e domandarmi a che classe appartiene beh in questo caso verrà etichettato come appartenente alla classe dei punti rossi quindi classe la classe 1 in questo caso ok di nuovo anche qui possiamo dire questo l'abbiamo l'abbiamo già l'abbiamo già visto come si fa qual è il il livello di sicurezza di fiducia di questa classificazione cioè il classificatore riesce a valutare quanto è sicuro di della predizione che sta facendo sì semplicemente come si fa beh si va un approccio geometrico si va a valutare la distanza del punto del confine decisionale come avevamo fatto per la classificazione binaria e ovviamente quello che possiamo fare è andare a normalizzare tramite la normalizzazione esponenziale otteniamo quella distribuzione di probabilità l'abbiamo visto prima e posso dire 80% classe 1 5% classe 2 e restante 15% classe 3 oppure classi 0 1 e 2 diciamo a proposito delle metriche di qualità anche qui possiamo fare un discorso in cui andiamo a introdurre delle metriche che opportunamente mi permettono di valutare la bontà del mio classificatore bontà del mio classificatore che viene quindi valutata in questo modo anche qui in analogia con quanto abbiamo visto per il problema a due classi quello che si fa allora qui abbiamo la nostra etichetta predetta lo vedete perché solitamente c'è questo simbolo sopra ok che viene ottenuta tramite l'applicazione della nostra regola di fusione ok vado a introdurre una funzione di identità che mi confronta la predizione del mio classificatore con quello che il la verità il viene chiamato gold standard anche cioè l'etichetta dei miei punti del del dataset di addestramento quelle io le so vado a vedere il mio classificatore ad esempio sul dataset di addestramento quanto è riuscito a classificare correttamente quei punti poi in realtà vi anticipo che non lo faremo soltanto nel ma anche questo l'abbiamo detto quindi più che vi anticipo ve lo ricordo e lo rivedremo poi anche più avanti noi facciamo questo non solo sul dataset di addestramento cioè questa accuratezza la andiamo a valutare sì sul dataset di addestramento per vedere come il processo di addestramento poi alla fine è andato a convergenza e quindi quali valori di performance riusciamo a ottenere ma lo andremo a fare anche su un dataset di test che il nostro sistema non ha mai visto ok e che quindi rappresenta una sorta di simulazione di quella che è la realtà in cui poi il nostro classificatore si troverà ad operare questa funzione identità semplicemente vi restituisce 0 se la vostra la previsione fatta dal vostro classificatore è uguale all'etichetta del punto e 1 se sono diverse o questo quindi andiamo a contare in questo modo il numero di errori quindi il numero di misclassification questo lo possiamo fare chiaramente indipendentemente dal fatto di avere due oppure più classi e il numero di misclassification sarà chiaramente la somma su tutti i punti del dataset di questo valore e l'accuratezza esattamente come era nel caso binario è definita come 1 meno il tasso d'errore e quindi io posso andare ad esempio a controllare che cosa succede durante il processo di addestramento quindi iterazione per iterazione posso andare a graficare sull'asse dell'y sia il valore della funzione di costo che è quella che effettivamente noi andiamo a minimizzare qui per esempio facciamo tre run tre corse di discesa del gradiente con un learning rate di 10 alla meno 2 su un problema che è un problema a cinque classi con un certo numero di punti cinque classi di cui non c'è l'esempio ma come quelli che abbiamo visto finora con un certo numero di punti a bassa dimensionalità qui la funzione utilizzata è la funzione softmax di tipo multiclasse e avete queste tre curve vedete che vi collocate alla fine a convergenza convergono tutte e tre su un valore che è di poco inferiore a 40 dopo 500 interazioni se rifate la stessa cosa per potete rifare la stessa cosa che abbiamo visto a suo tempo anche per la classificazione binaria in cui per ogni interazione andate a vedere con quel vettore dei pesi qual è il valore del numero di errori che vengono fatti sul dataset cioè quante risposte vengono sbagliate e vedete all'inizio ovviamente essendo i pesi totalmente casuali sono alte poi pian piano scendono scendono un po' con un andamento a scalino è un andamento che non è proprio direttamente sovrapponibile con quello della funzione di costo perché di nuovo noi andiamo a minimizzare la funzione di costo perché è una funzione che riusciamo a ottimizzare e a minimizzare in maniera diciamo più comoda per costruire l'algoritmo di ottimizzazione la discesa del gradiente ha bisogno di determinati requisiti della funzione di costo vi ricordo deve essere derivabile deve essere liscia non contenere discontinuità eccetera eccetera però questa è un obiettivo che noi utilizziamo per di fatto andare a minimizzare il numero di misclassification cioè il numero di errori è ovvio che sono tutte e due alla fine decrescenti ma non lo sono allo stesso modo ci possono essere qui vedete dei salti diversi questo è un andamento tra l'altro più discreto però alla fine questo è il nostro obiettivo questa è una sorta di quello che viene chiamato un tramite un proxy per ottenere la minimizzazione del numero di errori ok anche il concetto di matrice di confusione si estende pari pari nel senso che anche qui posso andare a rappresentare per un classificatore multiclasse che cosa succede nel processo di classificazione tramite una matrice in cui avrò un numero di righe un numero di colonne pari al numero delle classi in cui la entry all'incrocio tra la riga i e la colonna j vi riporta il numero di punti del training set per i quali l'etichetta vera apparteneva alla classe i e l'etichetta predetta era la classe j se io vado a prendere questa entry qui questo vi dice il numero di volte che quindi qui abbiamo associato il quelli che sono i punti effettivi cioè le classi effettive e sulle colonne le predizioni questo significa che c'è ad esempio un punto blu che è stato scambiato dal nostro classificatore per giallo ok oppure un punto rosso che è stato scambiato dal nostro classificatore per un punto blu e così via chiaramente sulla diagonale principale abbiamo quelli che sono stati indovinati correttamente quindi rispetto a questo dataset di esempio questa è la sua matrice di confusione vedete che abbiamo otto punti rossi che sono stati classificati correttamente uno due tre quattro cinque sei sette e otto e poi però abbiamo anche vedete questi due punti che erano rossi ma che sono stati sbagliati infatti se andate a vedere c'è un punto blu che non è in realtà era rosso e è stato classificato come blu e c'è un altro punto che era rosso ma è stato classificato come verde e così via quindi avete una una fotografia di quali sono stati gli errori quanti sono stati e anche quali cioè come come sono stati scambiati gli errori quindi quale classe magari è più vicina a un'altra classe da un punto di vista dell'errore e questo in questo caso sono tutti uguali perché non c'è un numero più elevato delle altre ma se qui anziché avere un uno io avessi avuto un tre beh avrei potuto concludere che per esempio la maggior parte degli errori della classe rossa erano errori dovuti al fatto che veniva scambiato con una classe verde e questo è utile nella diagnostica del classificatore quando voi costruite un classificatore machine learning la prima cosa che fate è andare a vedere queste matrici di confusione per vedere che cosa è successo e farvi un'idea di quello che è stato appunto il motivo che può aver condotto a quell'errore se riuscite a capirlo potete andare a modificare qualcosa e quindi magari costruire riaddestrare un nuovo classificatore che ha prestazioni migliori quindi un tasso di errore inferiore quindi ciao qui torno un attimo su questo vediamo se c'è ancora qualcosa che ok diciamo qui per quanto riguarda l'estensione di questi concetti direi che è quasi tutto l'altra cosa potremmo aggiungere e non ve l'ho messa perché anche qui appesantirebbe utilmente la trattazione che anche il concetto di accuratezza pesata può essere esteso più classi ok quindi tutto il problema dello sbilanciamento permane e può essere trattato da un punto di vista della valutazione dell'introduzione di una metrica opportuna valutando l'accuratezza bilanciata anche nel caso multiclasse e anche nel caso multiclasse potremmo introdurre una classificazione di tipo pesato quindi attribuire i pesi pesi diversi a punti diversi del dataset quindi volendo concludere con una frase tutto quello che abbiamo visto per la classificazione binaria di fatto può essere trasferito nella classificazione multiclasse quindi le funzioni di costo il fatto di avere etichette numeriche e o categoriche che è equivalente e le metriche matrici di confusione e accuratezza nonché la possibilità di introdurre delle classificazioni di tipo pesato va bene? quindi con questo concludiamo la classificazione multiclasse lineare e quello che invece facciamo adesso è fare una breve digressione prima di iniziare il nuovo argomento sul diciamo una cosa che viene utilizzata parecchio nella pratica e quindi diciamo che può essere utile e può essere questo il punto utile in cui parlarne brevemente che è il cosiddetto apprendimento mini batch le tecniche diciamo di addestramento cioè di nuovo torniamo un attimo sugli algoritmi di ottimizzazione perché a conclusione di aver visto regressione lineare classificatore lineare multiclasse è un qualcosa che può essere sicuramente utile e abbiamo visto e abbiamo detto più volte abbiamo visto che cos'è l'algoritmo di discesa del gradiente abbiamo detto più volte che un algoritmo che la fa sicuramente da padrone è uno tra i più utilizzati per l'addestramento dei modelli di machine learning e le considerazioni che andremo a fare adesso sono valide per questo tipo di algoritmo di discesa del gradiente e e riguardano il fatto di cercare di accelerare il processo di minimizzazione delle funzioni di costo allora questo perché perché man mano che la quantità di dati a disposizione è cresciuta nel corso degli ultimi direi dieci anni in particolar modo in maniera vertiginosa si è fatta sempre più pressante l'esigenza di cercare di snellire un po' le le prestazioni degli algoritmi di addestramento quando voi cominciate ad avere finché avete mille diecimila punti del dataset di addestramento va bene diciamo la cosa è gestibile ma quando i punti anziché essere mille diecimila cominciano ad essere un milione la cosa comincia a diventare più problematica sia come tempi di esecuzione ma soprattutto anche un problema estremamente pressante è quello dell'occupazione di memoria perché voi dovete caricare in memoria un milione di punti anziché mille e questo può sono un milione di vettori se ognuno di quei vettori è ad alta dimensionalità potete cominciare a dover disporre di gigabyte e gigabyte di memoria e anche questo è un problema forse più ancora del tempo di esecuzione allora ci sono diverse strategie chiaramente per cercare di ovviare questa problematica una delle quali è quella cosiddetta dell'apprendimento mini batch batch significa lotto in inglese quindi l'algoritmo di scesa del gradiente mini batch può essere tradotto come algoritmo di scesa del gradiente mini lotto dove proprio quello che si fa è scomporre il dataset in tanti lotti quindi andare fondamentalmente a una tecnica di sottocampionamento del dataset di addestramento si vanno a prelevare dei sottoinsimi del dataset e si va a lavorare su quelli allora questo in generale è valido questo questo approccio perché la struttura della funzione di costo è una struttura di tipo additivo quindi noi abbiamo una serie di somme di funzioni di costo puntuali e quindi dobbiamo minimizzare questo termine queste funzioni sono dello stesso tipo e quindi diciamo possiamo applicare questo tipo di logica che adesso vi andrò a descrivere un pochino più in dettaglio l'unica cosa che vi dico è che la struttura di questo tipo quindi una somma su tutti i punti di una funzione di costo puntuale è di fatto la struttura di tutte le funzioni che abbiamo visto nell'apprendimento con supervisione e quindi questo metodo si applica tutte le tecniche e i modelli che abbiamo visto quindi per esempio la funzione di costo minimi quadrati la funzione di costo soft max binaria in generale è un tipo di scomposizione scusatemi questa che è vera per funzione di costo di fatto di qualunque tipo nell'ambito del machine learning non solo funzione di costo per problemi di tipo supervisionato ma anche di tipo non supervisionato o altri tipi spesso e volentieri si ricade in una struttura di questo genere quindi questo tipo di approccio diciamo è sicuramente utilizzato in maniera molto trasversale allora in cosa consiste gli algoritmi di ottimizzazione locale che abbiamo visto sono detti anche full batch perché lavorano su tutto il lotto e che cosa fanno minimizzano tutto l'insieme di termini quella sommatoria dei punti che va supponiamo di avere un dataset di mille punti quindi questi sono i nostri mille punti ok quello che fa il full batch è andare a minimizzare il termine di quella sommatoria quindi quella sommatoria per P che va da 1 fino a P grande dei vari termini viene minimizzata andando a tenere in considerazione tutti i termini del batch dell'otto e quindi cosa facciamo partiamo da un punto iniziale e una volta che abbiamo valutato la funzione di costo in tutti tenendo conto di tutti i punti con il nostro learning rate andiamo a calcolare il gradiente prendiamo un passo di discesa e andiamo al punto successivo e poi qui ripetiamo di nuovo andiamo a valutare tutto il batch eccetera eccetera allora immaginate che appunto di avere un milione di punti anziché 10 mille o 10 cento o mille ma di averne un milione se voi andate a dividere in due questo dataset auspicabilmente da un punto di vista statistico 500 mila punti da una parte 500 mila dall'altro vi danno auspicabilmente lo stesso contenuto informativo cioè ci sarà parecchia ridondanza in quei punti ok non saranno diciamo è un discorso ovviamente di tipo probabilistico però presumibilmente se voi lo dividete in due diciamo una metà avete una buona probabilità che vi permetta di avere lo stesso comportamento dell'altra metà o comunque non siano così difformi la logica è un po' questa cioè se io a questo punto anziché andare la domanda che ci si fa è ma se io anziché andare a prendere un passo di discesa sfruttando tutti i punti ne vado a prendere ad esempio la metà oppure un 10 in realtà poi i mini batch spesso volitieri sono anche abbastanza piccoli e poi adesso ragioniamo su cosa vuol dire andare a a livelli di granularità diversi quindi andare a prendere dei batch sempre più piccoli ok però intanto ragioniamo dividiamolo in due se io vado a fare un passo in cui vado a calcolare un nuovo vettore quindi vado a calcolare il gradiente solo sulla prima metà e poi da qui riparto e vado a calcolare il gradiente sull'altra metà che cosa succede? succede che ognuno di quei passi è chiaramente molto più veloce quindi io aggiorno il vettore dei pesi del gradiente in maniera molto più rapida molto più snella ok ognuno allora in questo caso abbiamo due iterazioni per andare a vedere tutto il mio dataset cioè io ho spezzato in due questo dataset sono andato a vedere prima la parte 1 poi la parte 2 e ho prodotto però due aggiornamenti anziché una sola nel full batch quindi in questo caso in nme no n scusate ma p perché l'abbiamo chiamato p finora continuiamo a chiamarlo p quindi in ho spezzato in in p mezzi e p mezzi la numerosità del mio dataset i miei lotti valgono p mezzi e con una sola passata su tutto il mio dataset ho ottenuto due aggiornamenti del gradiente ok questa è la logica che c'è di fatto dietro questo tipo di di approccio cioè io quello che si va a fare è andare a suddividere il dataset in una serie di mini lotti dipende quanti dipende ovviamente è un iperparametro anche questo fondamentalmente che dipende da tutta una serie di cose dal tipo di di modello di funzione di costo dalla quantità di memoria che abbiamo a disposizione dal diverse cose che è difficile per le quali non è semplicissimo neanche dare delle regole a priori questa serie di di lotti vedete mi coprono tutto lo spazio dell'input e io prendo un singolo lotto e su questo vado a fare un passo di scesa del gradiente e a partire da un valore iniziale questa volta tengo conto di due indici perché uno mi dice a che passo sono diciamo nella mia discesa globale l'altro mi dice il singolo passo sul singolo lotto e vedete da W00 ottengo W01 poi W02 che sono i tre lotti che vado a vedere ok? e poi W03 produco un nuovo vettore di discesa del gradiente vado a prendere il secondo lotto ne produco un altro vado a prendere un terzo lotto ne produco un altro ancora e alla fine quando ho visto tutti i punti del mio dataset ho fatto un avanzamento di tre discese del gradiente ho visto tre vettori ho prodotto tre vettori di gradienti diversi quindi sono andato avanti di tre passi ora il punto è che ovviamente da cosa siamo partiti dal presupposto che se ne avevo un milione lo dividevo in due i 500.000 di uno i 500.000 dell'altro non mi producevano delle differenze così sostanziali tra di loro questo equivale a dire che il gradiente della funzione di costo sulla metà dei punti era lo stesso nel momento in cui anziché avere solo la metà dei punti io introduco più minilotti e quindi vado a diminuire la numerosità di ogni lotto e questo può cominciare a non essere più vero può non essere vero neanche per l'esempio da cui siamo partiti 500.000 e 500.000 questo significa che cosa? che in realtà questi aggiornamenti di questi di questi vettori gradienti non sono coincidenti con il gradiente che otterrei se lavorassi sull'otto intero quindi sono delle versioni se vogliamo le possiamo vedere come rumorose del gradiente però la cosa interessante è che da un punto di vista statistico il discorso funziona cioè questo processo arriva comunque a convergenza ci arriva magari in maniera con un percorso un po' più frastagliato cioè il metodo di scesa del gradiente punta esattamente verso il minimo qui non punterò verso il minimo della funzione di costo mi muoverò secondo una sorta di spezzata vediamola così per cui ogni non ho più una bella curva che va a convergenza se pensiamo ad esempio alle funzioni convesse avrò una spezzata che si muove un po' in maniera un pochino più tortuosa perché un po' più rumorosa tanto più rumorosa quanto sono piccoli la numerosità di quei lotti ma quello che si può dimostrare è che comunque arriva a convergenza questo approccio è un approccio che in generale viene chiamato discesa del gradiente stocastica in inglese stocastic gradient ai lotti esattamente e si può addirittura rendere diciamo estremizzare perché io posso andare a considerare lotti di dimensione unitaria cioè con un solo punto cioè prendo un punto calcolo il gradiente della funzione di costo lo aggiorno e vado avanti questo è chiaramente più vado a diminuire la dimensione dell'otto arrivando fino al singolo punto più il metodo sarà rumoroso da un punto di vista dell'identificazione del gradiente ma sarà veloce nell'aggiornamento del singolo passo è chiaro che lì vanno bilanciate un po' tanti fattori poi però quello è il cosiddetto metodo puro di discesa del gradiente stocastica è una discesa molto rumorosa come vi dicevo se voi andate a vedere i grafici di certe funzioni riprendete per esempio quelle funzioni in cui avete le curve di livello e partite ad esempio da qua il metodo no ad esempio che abbiamo visto facciamo una curva così questo può fare magari una cosa di questo genere ok e quindi avvicinando si avvicina però ecco quello che conta è che da un punto di vista statistico si può dimostrare che converge in valore medio la cosa interessante che converge e vi permette di risolvere tutti quei problemi ad esempio di occupazione di memoria perché voi caricate su un batch di 128 campioni e non solo tra l'altro nelle GPU tra l'altro potete sfruttare un livello di parallelismo quindi diciamo ci sono ci sono diverse altre questioni che vi permettono anche di di ottimizzare in funzione dell'hardware questo tipo di processo però è questo fondamentalmente il trucco che c'è dietro un'altra cosa che non vi ho detto è che è bene tenere presente perché diciamo si parla spesso specialmente nell'addestramento delle reti numerali del concetto di epoca un'epoca di addestramento che cos'è esattamente il corrisponde a che cosa al numero di passaggi necessari per andare a prendere in considerazione tutti gli elementi del dataset quindi in questo caso avevamo tre lotti lo avevamo diviso in tre e quindi un'epoca è quella che corrisponde alle discese del gradiente su tutti i tre lotti quindi ognuno di questi viene chiamata epoca va bene? ci siamo? ora questo è qualcosa di estremamente ripeto utilizzato specialmente per esempio per l'addestramento delle reti neurali perché poi in combinazione con il fatto di utilizzare delle GPU le GPU sono delle e dell'hardware che sono unità di elaborazione grafica che viene utilizzato appunto originariamente solamente nelle schede grafiche per fare tutte quelle operazioni hardware che permettono di effettuare la visualizzazione il rendering a livello grafico di appunto di dati e oggigiorno trovano un impiego sempre più massivo specialmente nell'ambito dell'intelligenza artificiale perché vi permettono di fare contengono dentro tante unità che fanno pochissime cose operazioni vettori moltiplicazioni vettori per matrici fondamentalmente sono farmi molto bene ma lo fanno con un elevatissimo livello di parallelismo ora le operazioni vettori per matrici abbiamo visto che sono il pane quotidiano di questi modelli di machine learning e e quindi fare addestramento per esempio su GPU non solo addestramento ma anche inferenza risulta essere molto conveniente perché perché riescono a fare tante di queste cose tante di queste operazioni in parallelo l'algoritmo mini batch vi permette di caricare questi lotti e in maniera molto efficiente sfruttare questo livello di parallelismo per poter accelerare il processo di addestramento coniugare questi due aspetti permette ha permesso di fatto di è stato una delle chiavi di di di crescita di questo di questo tipo di di di di metodi e quindi alla base di quelli che sono i successi del machine learning moderno quindi accelerare la performance specialmente quando avete dei dataset estremamente ampi è uno degli obiettivi di questa tecnica e qui avete un esempio di approccio mini batch viene anche chiamato online learning poi in qualche caso è uno vi dicevo degli obiettivi l'altro obiettivo è abbassare i requisiti di memoria per cui voi dovete caricare in memoria caricati i batch non più tutto il dataset qui avete un confronto tra discesa del gradiente full batch che sono le curve in nero a sinistra avete rispettivamente il valore della funzione di costo e a destra l'equivalente rappresenta diciamo la corrispondente rappresentazione in termini di numero di errori di classificazione e su un dataset che è quello delle cifre scritte a mano ve ne avevo parlato una delle primissime lezioni che è il dataset quindi è una classificazione di problema multiclasse riconoscere tra dieci possibili classi un numero scritto a mano e qui avete un addestramento in cui appunto viene confrontato l'approccio standard curva in nero con l'approccio mini batch curva in viola e vedete che l'approccio mini batch è estremamente più allora ogni certo sì esattamente esattamente qui di fatto stiamo parlando di epoche esatto confronto questo ok beh no in realtà no non è un'epoca perché altrimenti qui proprio la singola iterazione che di aggiornamento direi del del del fammi pensare dovrei dunque vediamo un attimo ragioniamo un secondo fammi pensare che cosa può essere no perché non sarebbero direttamente confrontabili a quel punto quindi dovrebbe essere un'epoca e lui per ogni epoca riesce ad andare subito più basso dunque fammi pensare sì sì è un'epoca direi che è un'epoca sì è corretto potete pensare che qui riportano le iterazioni quindi c'è il dubbio è se sono iterazioni riferite alla singola iterazione del quindi del singolo batch oppure dell'epoca se fosse il singolo batch non avrei il confronto direttamente nella curva nera perché lì il singolo batch è tutto il dataset quindi è probabilmente direi più corretto di vedere questa come come epoca ognuna di queste magari lo verifico dove ho preso questa figura nel libro che cos'è si intendeva esattamente però quello che rimane vero è che tipicamente si va più velocemente a ad ottenere la minimizzazione ecco però questo magari lo verifico poi ve lo ve lo so dire la prossima volta ok bene allora con questo abbiamo veramente concluso questo blocco di slide e possiamo iniziare il nuovo argomento questo magari lo controllo ve lo so ridire questo discorso delle iterazioni quindi se un'iterazione riferita appunto a un'epoca oppure a una sola passata su un singolo batch ok allora quello che andiamo a vedere adesso invece è un andiamo a caricare le slide il blocco di slide successivo con cui con il quale iniziamo un nuovo un nuovo argomento questa volta proprio è un nuovo argomento diciamo con la n maiuscola nel senso che cambiamo proprio prospettiva rispetto alla tipologia di apprendimento se ricordate la suddivisione la tassonomia che abbiamo introdotto nelle primissime lezioni abbiamo parlato di apprendimento con supervisione apprendimento senza supervisione apprendimento per rinforzo ecco mentre l'apprendimento per rinforzo non riusciremo a perché non fa parte a vederlo perché non fa parte dei contenuti del corso l'apprendimento invece non supervisionato è un qualcosa a cui riusciamo a dedicare a qualche lezione ed è ed è quindi questo è il momento in cui iniziare a vedere appunto i primi argomenti ed è un una parte estremamente anche questa interessante perché diciamo mentre chiaramente il tutto quello che abbiamo visto a proposito ad esempio della classificazione della regressione quindi l'apprendimento con supervisione rende conto di una serie di sicuramente di applicazioni pratiche estremamente anche immediate l'apprendimento con non supervisionato è un qualcosa che è molto interessante perché probabilmente oltre ad avere anche lui una serie di applicazioni pratiche di cui poi parleremo è probabilmente anche una chiave di lettura di molte cose che ancora sono rimaste in sospeso nelle teorie dell'apprendimento cosa voglio dire che mentre le macchine che in questi anni sono state costruite sono in grado di estrarre della conoscenza da quantità moli di dati sempre crescenti che però sono appunto una tipologia di apprendimento per esempio di tipo con supervisione e se vi ricordate l'esempio da cui siamo partiti all'inizio era il bambino che impara a riconoscere un cane o un gatto con la supervisione di qualcuno che gli fa vedere del genitore che gli dice quello è un cane o quello è un gatto punto chiave che rispetto alle macchine il bambino lo riesce a fare con pochissimi esempi allora dove è la chiave rispetto a una macchina chiaramente dove è la chiave di volta di tutto questo probabilmente è nel nel supervision nel fatto di essere supervisionato oppure non supervisionato cioè il fatto di poter apprendere da degli esempi anche non etichettati ma di essere in grado in maniera autonoma di creare una di riconoscere eventuali pattern all'interno di un dataset e molti ritengono una chiave di volta di quello che è ad esempio l'intelligenza umana ma questo è un discorso un po' più filosofico però diciamo è da tenere presente allora noi che cosa studieremo da un punto di vista dell'apprendimento perché la differenza qual è torniamo un attimo indietro che è che nel momento in cui io ho un apprendimento con con supervisione io ho insieme xp yp cioè ho un punto del mio dataset di addestramento e la corrispondente etichetta nel quindi questo è supervisionato nel momento in cui io solamente xp io ho accesso al solo punto non ho nessuno che mi dica guarda che quello è un cane oppure un gatto allora lì chiaramente risolve un problema di tipo diverso però questo spesso e volentieri quello che volevo dirvi prima con quella digressione che ho derubricato come così un po' filosofica è che in realtà poi probabilmente è un po' più che appunto c'è dietro un mondo anche diciamo di collegato alle neuroscienze eccetera è che tramite il non supervisionato io di fatto riesco a ottenere un qualcosa che per il supervisionato è di estremo aiuto probabilmente il riuscire da pochissimi esempi a ricavare poi una classificazione corretta deriva dal fatto che sotto il cervello umano riesce a creare un'idea del mondo non supervisionata che permette di coadiuvare il processo poi supervisionato per cui gli bastano molti meno esempi venendo a noi che cosa studieremo studieremo l'apprendimento unsupervised non supervisionato da due punti di vista per due applicazioni differenti che sono rispettivamente la riduzione della dimensionalità e il problema del raggruppamento del clustering ok la riduzione della dimensionalità è un problema in cui noi cerchiamo abbiamo una serie di punti in ingresso quindi i nostri xp e quello che facciamo è cercare di produrre una versione di questi punti quindi il nostro dataset di addestramento ne produciamo una versione a bassa dimensionalità cosa significa? significa che se il nostro input ha dimensione ogni elemento ha una dimensione per esempio mille ok quindi sono dei vettori con mille feature quello che vogliamo vedere è se riusciamo a costruire un dataset che ovviamente abbia le stesse caratteristiche quindi diciamo mantenga il più possibile non abbia le stesse caratteristiche ma mantenga il più possibile delle caratteristiche del dataset originale ma non in mille dimensioni ma ad esempio in dieci cinque magari tre dimensioni magari due questo per due due tipologie di motivi la prima è che all'aumentare il numero delle dimensioni chiaramente cresce la complessità di tutto quello che viene fatto con queste dimensioni cioè un vettore con mille elementi è un vettore che entra nel training ma poi anche nell'inferenza quindi nella classificazione e regressione con mille componenti e questo implica un numero di operazioni sicuramente superiore al fatto di avere un vettore che invece di mille ce n'è dieci di componenti quindi la prima motivazione per provare a effettuare una riduzione della dimensionalità è per permettere di scalare a dataset più grandi scalare a dataset più grandi significa appunto poter gestire dataset via via crescenti e per fare questo spesso e volentieri bisogna scendere di dimensione il secondo motivo è permettere quella che viene chiamata analisi umana in the loop nel ciclo cioè diciamo nel momento in cui si analizza un dataset provare a capire qualcosa di più di quel dataset con l'intervento appunto umano di chi cerca di analizzarlo e di capirne le caratteristiche per fare questo una cosa che si fa un prerequisito e lo abbiamo tra l'altro utilizzato più e più volte nella costruzione di tante cose che abbiamo visto finora è provare a dare una rappresentazione grafica ora se noi abbiamo un punto un singolo punto a mille componenti chiaramente quella rappresentazione grafica ce la possiamo scordare dobbiamo scendere di dimensionalità e vedere cosa succede se arriviamo a tre o due dimensioni a quel punto riusciamo a rappresentare o anche uno eventualmente a rappresentare il singolo punto del dataset quindi tutto il dataset e magari capire qualcosa di più su quelle che sono le caratteristiche della distribuzione statistica eccetera e quindi fare un'analisi di quel tipo allora per tutti questi motivi spesso e volentieri la riduzione della dimensionalità è un primo passo che viene applicato quindi una sorta di pre-processing in diversi problemi di tipo supervisionato e il punto è che la riduzione della dimensionalità significa che io prendo un oggetto che ha dimensione 1000 se torniamo al nostro esempio di partenza per semplificare e lo voglio portare a dimensione 10 e chiaramente potenzialmente perdo qualcosa cioè è un'operazione che potenzialmente dà luogo a una perdita di informazione e ovviamente noi vogliamo fare in modo che questa perdita di informazione sia la minore possibile da questo punto di vista vedete che è collegato concettualmente questo tipo di operazione anche al concetto di compressione noi di fatto vogliamo comprimere il dato ottenere una rappresentazione che sia più snella ma che conservi il più possibile l'informazione del dato originale ok allora da qui in avanti quello che faremo è diciamo i ragionamenti che vengono fatti è che per appunto comodità di manipolazione matematica di questi oggetti che questi dati siano centrati rispetto alla media cosa vuol dire che noi prendiamo la media di tutti i punti del dataset lungo ogni dimensione quindi vado a vedere la prima feature vado a calcolare la media su tutti i punti del dataset la seconda feature vado a calcolare la media di tutti i punti sul dataset e così via lo faccio per tutte le dimensioni e poi sottraggo tutte queste medie da ogni dimensione quindi ogni ogni vettore viene ricalibrato sottraendoci la media lungo ogni direzione lungo ogni lungo ogni dimensione va bene ok qui da qui in avanti vedrete anche delle rappresentazioni nelle figure che sono come queste che sono riportate qui in questa slide in cui equivalentemente ci saranno delle volte dei punti come ad esempio a sinistra per rappresentare un punto del dataset un vettore scusatemi del dataset quindi ad esempio questo punto qui viene rappresentato questo vettore tramite un punto che ha queste due coordinate questa è la prima coordinata e questa è la sua seconda coordinata allo stesso modo l'avrei potuto rappresentare immediatamente appunto un vettore una freccia e in qualche caso ci sono anche delle rappresentazioni miste quindi trovate sia i punti che le frecce troverete di solito le frecce vengono utilizzate per quelle che vengono chiamate delle basi dello spazio vettoriale quindi se io ho un insieme di vettori c1 c2 fino a ck lo rappresento così e a livello grafico lo rappresenteremo con delle frecce allora qui ho introdotto appunto questa nomenclatura il nome il concetto di base o insieme ricoprente di vettori ed è la prima cosa che andremo a richiamare questo è un concetto che avete incontrato quando avete fatto il corso di matematica al triennio quando avete fatto gli spazi vettoriali l'algebra lineare il concetto di base cioè di come rappresentare un punto in uno spazio nelle dimensioni tramite la combinazione di vettori che vengono chiamati base ok lo richiameremo brevemente è un concetto abbastanza semplice abbastanza elementare ed è un concetto che ci permette di introdurre quella che è la rappresentazione di un dato in un insieme di riferimento quindi in un insieme ricoprente fisso allora una base o insieme ricoprente si dice che rappresenta perfettamente qui sottolineo questo perfettamente perché questa rappresentazione da cui partiamo è la rappresentazione esatta di quel punto in un determinato spazio se noi riusciamo a trovare una serie di coefficienti di peso w 1 w 2 w k che moltiplicati ognuno per un vettore diverso di quella base ci permettono di ottenere il punto in questione quindi molto semplicemente questo per tutti i punti del nostro dataset quindi l'insieme di vettori c1 c2 fino a ck rappresenta quella che viene chiamata una base allora questo l'avete visto quando avete fatto no anche in fisica in matematica no avete dato o lo facciamo comunemente quando diamo delle rappresentazioni di un punto sul piano cartesiano che cosa facciamo noi abbiamo due vettori ok che sono questi due possiamo chiamare e1 ed e2 ok che sono i vettori rispetto ai quali noi definiamo qualunque altro punto dello spazio cartesiano perché se io dico che un punto ha coordinate ecco 2 1 significa che sto dicendo che lungo l'asse x quindi rispetto a questo vettore che è un vettore unitario eccetera eccetera che ha certe proprietà in questo caso di essere anche ortonormale a norma 1 ed è ortogonale al vettore 2 quindi abbiamo due abbiamo detto coordinate lungo il vettore 1 e 1 rispetto al vettore 2 quindi significa che io posso esprimere il punto P ok come un qualcosa che ha coordinate 2 e 1 e che quindi è esprimibile come combinazione lineare di queste di questi due vettori e1 e e2 ok questo è il concetto di base questo può essere esteso oh qui abbiamo imposto che questi due vettori siano ortogonali non necessariamente questo deve essere così però questa è la nozione di base in geometria in fisica insomma che si introduce nei corsi di base ok quindi in generale una base o insieme ricoprente è un insieme di vettori c1 c2 fino a ck dove k è la dimensione del nostro spazio in questo caso k valeva 2 ok tale che ogni punto quindi il singolo punto P questo punto chiamiamo x con p via così siamo coerenti con la notazione sinistra lo potete esprimere come c con 1 per v doppio p1 più c2 v doppio p2 più ck v doppio p k ok quindi il punto generico lo potete esprimere come combinazione lineare di k vettori questa combinazione lineare ha k coefficienti moltiplicativi v doppio 1 v doppio 2 fino a v doppio k c'è anche la p per indicare che siamo sul punto p cioè vale per tutti i punti del mio dataset deve valere questa cosa ok la rappresentazione perché sia perfetta vuol dire che questa se è perfetta è un'uguaglianza stretta ok e allora per essere perfetta devono valere delle condizioni 1 che k deve essere uguale a m cioè il numero di vettori della vostra base deve essere uguale alla dimensione dello spazio in cui il punto vive pensiamo torniamo al nostro sistema cartesiano se io voglio rappresentare un punto in due dimensioni ho bisogno di questi due vettori e 1 e 2 se io ne ho uno solo posso rappresentare tutti i punti che stanno sulla retta x o y ma non i punti del piano allo stesso modo se ho un punto in tre dimensioni e io voglio rappresentarlo tramite due dimensioni non ce la faccio mi perdo la terza dimensione in due dimensioni riesco a spazzare tutto il piano ma non vado nella terza dimensione quindi k deve essere uguale a n perché ci sia la rappresentazione perfetta l'altra condizione è che tutti i vettori che definiscono la base devono essere tra di loro linearmente indipendenti cioè non ci deve essere un vettore della base che può essere espresso come combinazione lineare degli altri vettori perché di nuovo perdo la possibilità di rappresentare tutte le dimensioni se questo avviene allora la rappresentazione perfetta è possibile e io posso andare a scrivere questa relazione in maniera più compatta come questa cioè io vado a mettere costruirmi una matrice c e costruisco in questo modo vado a incolonnare i vettori c1 lo vado a mettere qui nella prima colonna c2 nella seconda e ck nell'ultima e cappesima colonna e chiamo questa matrice c quindi questa è una matrice che contiene k vettori colonna ognuno dei quali ha una dimensione che è pari a n ok e anche k è uguale a n quindi di fatto è una matrice n per n ok e lo vado a moltiplicare per wp che è il vettore dei pesi ok il prodotto mi deve restituire xp affinché si parli di rappresentazione perfetta e questo deve valere per tutti i punti che sto prendendo in considerazione il mio data quindi c contiene quelli che vengono chiamati vettori della base o vettori ricoprenti spanning vector in inglese quindi questi sono i vettori della base le combinazioni dei pesi invece vengono organizzate in un vettore wp che è wp1 p2 p3 ogni punto si porta dietro il suo vettore dei pesi ok vi faccio notare una cosa che nel momento fino adesso stiamo parlando di rappresentazione di un vettore in un certo spazio vettoriale ok tramite i suoi punti uno potrebbe dire va bene ma questo perché dobbiamo fare questo come è legato al discorso da cui siamo partiti che era il problema della riduzione della dimensionalità è legato in questo modo che se io rinuncio alla rappresentazione perfetta e quindi ad esempio rinuncio perché voglio andare a vedere cosa succede se k è minore di n cioè voglio andare a vedere che cosa succede se ho un vettore in 30 dimensioni quando lo porto a 12 e quindi non rappresento più il punto il mio vettore singolo in maniera perfetta ma accetto un'approssimazione una riduzione del concetto dell'informazione del concetto di informazione legata a quella rappresentazione perché sto scendendo di dimensionalità voglio vedere se riesco a farlo con quale livello di accuratezza e voglio vedere che cosa succede perché questo è quello che faremo cioè andremo ad abbassare il valore della dimensione quindi rinunciando alla pretesa di rappresentare perfettamente quel punto per andare a vedere che cosa succede nel momento in cui scendiamo in termini di dimensione quindi andiamo a rappresentare quel punto con una base in uno spazio a più bassa dimensionalità ed è il motivo per cui introduciamo questa notazione va bene allora con questa notazione vedete possiamo fare diverse cose vi ricordo che noi abbiamo detto che quello che vogliamo dalla rappresentazione no vi ricordo dove eravamo arrivati eravamo arrivati nella slide precedente a dire che noi quello che vogliamo è questa notazione compatta valga per tutti i punti del mio data dove la riporto qui cwp vogliamo che sia uguale xp ok ma questo è un qualcosa in cui noi fissiamo ad esempio quindi ancora non siamo in riduzione della dimensionalità però vi ho detto l'obiettivo qual è continuiamo a ragionare intorno alla rappresentazione perfetta quindi k uguale a n i vettori c1 c2 fino a ck sono linearmente indipendenti rimane il problema di dire ok x lo conosco c lo impongo io perché so che cos'è perché so in quale base quale spazio voglio trasformare i miei punti quindi conosco x e conosco c quello che non conosco è w che è il vettore di pesi ok voi pensate ad esempio tornando al problema bidimensionale io in questo punto in questo spazio e supponiamo di volerlo rappresentare in un sistema di coordinate che non è più questo ma è ad esempio questo questa è la prima coordinata e questa è la seconda coordinata quali sono le coordinate in questo nuovo sistema di riferimento stiamo cercando di rispondere ad esempio a questa domanda ok quindi io voglio sapere questo è c1 e c2 voglio sapere fissati c1 e c2 è fissata la rappresentazione di xp qual è ad esempio il vettore dei pesi ok quindi domande di questo tipo allora torniamo a noi qui rischiaviamo c da wp scusate c da wp uguale xp ok allora come possiamo fissato c e fissato xp dire ricavare il vettore di wp che continuare ad essere un vettore dei pesi perché anche qui per noi poi ecco vedete anche qui manteniamo un po' di filo conduttore tra le cose che abbiamo detto a proposito del supervisionato anche qui nell'avviso del non supervisionato cioè quel vettore wp viene chiamato vettore dei pesi e ricavarlo significa fare il tuning fare il tuning significa minimizzare un'opportuna funzione di costo quale può essere in questo caso molto semplicemente una funzione di costo che guardate prende la vostra rappresentazione nello spazio delle basi caratterizzate da c il vostro punto e per tutti i punti cerca di fare in modo che questa ognuno di quei termini sia zero sia nullo cioè per ognuno di quei punti io do la rappresentazione esatta questa è una funzione di costo ai minimi quadrati che però applichiamo un problema totalmente diverso rispetto a quelli che abbiamo visto finora però è una funzione di costo che posso minimizzare tramite metodi di ottimizzazione locale non solo posso a una struttura che magari ci riflette lo vediamo domani nella prossima lezione è esattamente quella che abbiamo visto a proposito della regressione quindi equazioni che abbiamo chiamato equazioni normali quindi quelle che ammettono una soluzione in forma chiusa e questo significa che io impostando il problema in questo modo posso ricavare una volta noto c e noto xp un vettore wp e quel vettore wp è un vettore che mi rappresenta è una rappresentazione di xp in un mondo che non è quello nativo diciamo dove vive xp in partenza ma in un altro mondo che è quello delle basi che sono descritte da c e quindi è quello che viene chiamata una codifica del punto rispetto alla matrice che descrive la base la matrice c quindi il vettore wp che minimizza questa funzione di costo viene chiamato codifica del punto rispetto alla matrice c perché perché voi potete vedere a quel punto il vostro punto xp come rappresentato attraverso il processo di codifica dal punto wp e viceversa se io voglio tornare xp quindi attraverso la codifica io rappresento xp in termini delle sue coordinate nel nuovo sistema di riferimento se vogliamo mantenere quel parallelo col sistema di riferimento e poi invece ci sarà un processo di decodifica che vi permetto di dire ok prendo il vettore dei pesi wp faccio questa operazione c wp ma c wp se la rappresentazione è perfetta mi permette di riottenere il punto xp qui è perfetta la rappresentazione quindi sono in 50 dimensioni rimango in 50 dimensioni cambio solo e come vi dicevo prima la rotazione quando non so se l'avete fatto da qualche parte nei corsi di fisica o di matematica altri sistemi di riferimento quindi trovate le coordinate di un punto in un nuovo sistema di riferimento ma lì non perdete nessuna informazione perché è una rappresentazione perfetta qui partiamo dalla rappresentazione perfetta per arrivare alla perdita di informazione tramite riduzione della dimensionalità e vogliamo ovviamente che quella perdita sia la minore possibile ok però l'obiettivo è un po' quello come se noi avessimo un punto in tre dimensioni questo punto e io lo volessi rappresentare solo in due posso fare una proiezione sul piano e chiaramente due punti che andano sono sulla stessa verticale altezza diversa mi vanno a finire a collassare sullo stesso punto quindi perdo qualcosa però insomma vediamo di perdere il meno possibile l'obiettivo è questo però direi che allora questo lo possiamo affrontare anche nella lezione prossima direi che possiamo fermarci qui se non ci sono domande qui ripartiamo da qui la prossima lezione perché vediamo un attimo come questa minimizzazione di fatto è equivalente vi richiamo qualcosa che abbiamo già visto sulla regressione lineare quindi le equazioni normali e poi ripartendo da qui vediamo un po' di altre cose va bene non domande? no da casa avete domande? no no ok allora fermiamo la registrazione a rivederci e a rivederci a rivederci