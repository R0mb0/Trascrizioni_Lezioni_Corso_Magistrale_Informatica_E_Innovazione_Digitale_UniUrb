benissimo allora tanto bentornati e oggi abbiamo abbiamo in programma un'esercitazione nella quale riprenderemo alcuni concetti di appunto che abbiamo visto a proposito dell'apprendimento non supervisionato e quindi in questo notebook che vi ho preparato che vi ho predisposto ci sono alcune vedrete alcune alcune celle di questo di questo codice che sono dedicate all'analisi dei componenti principali che abbiamo visto lezione abbiamo visto cosa consiste abbiamo detto che è una tecnica di riduzione della dimensionalità che ci permette di proiettare i dati su un sottospazio con una proiezione lineare quindi una trasformazione lineare su un sottospazio con perdita di informazione però ci permette di identificare anche delle direzioni lungo le quali far avvenire questa proiezione e quindi delle direzioni che rappresentano altrettanti vettori che poi costituiscono una base nella quale andare a proiettare quei dati e la caratteristica che sono le direzioni che appunto le componenti principali le direzioni che contengono la maggiore variabilità del dato ok quindi la massima varianza vi ricordo si costruisce a partire dalla matrice di covarianza che è costruita a partire dalla matrice dei dati e di data design matrix e poi entrano in gioco gli auto vettori e gli autovalori di questa di questa tecnica in questa in questa tecnica scusate e in particolare gli auto vettori determinano proprio quelle direzioni di cui parlavo poc'anzi gli autovalori rappresentano qual è la magnitudo della varianza in questione allora per cercare di capire bene di cosa stiamo parlando importiamo le nostre librerie solito numpy matplotlib e anche seaborn e vediamo di costruire un esempio ad hoc che ci faccia meglio comprendere quali sono quali sono le caratteristiche di queste di questo tipo di analisi allora qui diciamo quello che vi ho riportato è una cella nella quale la prima cosa che facciamo ovviamente è andare a settare il generatore di numeri pseudo casuali e poi nella seconda alla riga numero due definiamo una matrice x che è proprio la matrice dei nostri dati in questo caso per semplicità per cercare di tenere sotto controllo quello che stiamo facendo da un punto di vista anche visivo lavoreremo in due dimensioni quindi generiamo un insieme di punti in questo modo facciamo il prodotto tra due matrici la prima è una matrice 2 per 2 che è stata generata a caso con dei numeri che sono uniformi nell'intervallo 0 1 la seconda è una matrice 2 per 200 quindi abbiamo 2 per 2 scusate scriviamo ok matrice 2 per 2 che viene moltiplicata per una matrice che ha due righe e 200 colonne il risultato sarà una matrice e poi viene trasposta il risultato di questo prodotto è chiaramente una matrice matrice 2 per 200 che è trasposta sarà una matrice 200 per 2 e diciamo l'effetto di questa moltiplicazione di prendere questi numeri casuali scalarli in una secondo secondo dei numeri che sono generati secondo una distribuzione gaussiana e se li andate a plottare se li andate a plottare quello che ottenete è una cosa di questo tipo vedete sono dei numeri che abbiamo ottenuto che sono distribuiti tra grosso modo meno 2 e 2 con qualche eccezione sulla prima coordinata e tra meno diciamo 1 e più 1 sulla seconda coordinata quindi si ottiene in questo modo facendo questa moltiplicazione tra matrici generando quelle due matrici ok quindi semplicemente questo è il dato da cui partiamo questa nuvola di punti in questa nuvola di punti se analizzate bene come è costruita questa nuvola di punti riuscite a vedere qualcosa che abbiamo visto anche a lezione era l'esempio da cui siamo partiti a lezione per introdurre la distribuzione scusatemi per introdurre il concetto di analisi dei componenti principali perché abbiamo detto che questi dati sono distribuiti lungo due assi lungo due direzioni una è questa e una è questa altra la direzione di massima varianza è sicuramente questa l'altra la seconda direzione principale è assolutamente questa buongiorno quindi a questo punto quello che possiamo possiamo dire è che abbiamo identificato a livello visivo quelle che sono le direzioni principali proviamo ad applicare l'analisi dei componenti principali a questo tipo di problema benissimo quindi allora procediamo andiamo a vedere che cosa possiamo fare con questa nuvola di punti beh con questa nuvola di punti possiamo anzitutto andare a importare importare sempre da scikit-learn un modo modulo che si chiama decomposition che contiene al suo interno la classe pca per fare analisi dei componenti principali dopodiché invocando appunto l'accesso a questa classe possiamo definire un oggetto che chiamiamo pca minuscolo vedete che pca se andate a vedere la documentazione come sempre ha vari parametri di input però diciamo prendendo tutti default e andando a specificare solo questo che si chiama n components andiamo come potete immaginare a specificare il numero delle componenti lungo le quali noi vogliamo effettuare la riduzione della dimensionalità in questo caso non facciamo riduzione della dimensionalità perché partiamo da uno spazio che vive in due dimensioni e andiamo verso uno spazio bidimensionale vogliamo semplicemente proiettare di fatto i nostri la nostra nuvola di punti nelle due direzioni di massima varianza quindi effettuiamo una trasformazione di quello spazio da due dimensioni a due dimensioni va bene? dopodiché quello che succede vabbè adesso questa è già andata in esecuzione la rimandiamo in esecuzione per sicurezza ecco quello che succede è che viene effettuato il calcolo e una volta effettuato il calcolo è definito ovviamente l'oggetto pca viene effettuato l'addestramento vero e proprio l'addestramento che in questo caso è non supervisionato noi stiamo dando dei punti vedete l'input è solamente x non c'è la y come abbiamo visto nelle svm nella regressione logistica nei classificatori problemi di regressione lineare abbiamo solamente i dati senza le etichette e quello che fa questo metodo fit è prendere appunto l'oggetto pca che ha appena creato prendere come input la matrice che abbiamo generato con le coordinate dei punti e da questa matrice andare a ricavare le componenti principali allora una volta che abbiamo fatto il fit di questo oggetto possiamo accedere a degli attributi dell'oggetto questi attributi se andate a vedere sempre nella documentazione c'è un attributo che si chiama components con poi l'underscore che come potete immaginare vi dice quali sono i vettori delle componenti principali una volta che lui ha fatto il fitting ha calcolato con quel metodo che vi ho detto che vi ho spiegato le azioni in realtà ce n'è più di uno ha calcolato di fatto gli autovalori e gli autovettori della matrice di covarianza e gli autovettori stanno qua quindi se io gli dico print pca.components underscore lui mi stampa vedete questo qui questo vedete è una matrice di vettori ce ne ho due perché due sono le componenti questa è la prima componente questa è la seconda componente queste sono le due componenti principali di quella nuvola di punti che abbiamo generato ok ognuno di questi vettori vedete ha due coordinate chiaramente quindi vi dice quali sono le coordinate in questo caso 0.94 e 0.32 se voi andate a vedere la nuvola di punti come l'abbiamo plottata 0.94 qualcosa che sta qui e 0.32 qualcosa che sta qua quindi vi da le coordinate rispetto allo 0 vedete della più o meno qualcosa che sta qua se voi tracciate avete un scusatemi questo è il punto avete un vettore quel vettore vedete vi definisce la prima coordinata la seconda la prima direzione scusatemi la seconda direzione è meno 0.3 0.94 meno 0.3 0.94 significa che andate a prendere un punto che sta più o meno qua sulla x e abbiamo detto 0.94 sulla y quindi molto in alto e quindi è un qualcosa che è ortogonale a prima perché è qualcosa che sarà qui e quindi adesso non mi viene bene ma ecco 0.3 adesso sono fuori scala però insomma se andate a vedere dovrebbe venire l'ortogonale a prima ma per essere sicuri di non farlo a mano e di farlo per me adesso poi lo andiamo a disegnare così almeno direttamente vi faccio vedere come può essere fatto l'attributo dell'oggetto pci che si chiama explained variance con l'underscore vi dà due valori per ognuna delle due componenti che vi dicono che sono quindi questi qui sono gli autovalori della matrice di covariante sono due autovalori a ognuno di quei due autovalori è associato un autovalore scusate a ognuno di quegli due auto vettori è associato un autovalore questo al primo e questo al secondo vedete che il primo ha è un numero più più grande perché perché l'ampiezza del primo autovalore è più grande dell'ampiezza del secondo autovalore del terzo se ce ne fossero altri del quarto e così via sono ordinati e ognuno di questi vi dà una misura della varianza cioè di quanto varia il dato lungo quella direzione poi va bene gli ho fatto calcolare anche la media perché in realtà quella matrice andrebbe insomma andrebbe controllato sempre noi l'abbiamo generata ed è una matrice scalata cioè a media zero eccetera però insomma qui abbiamo stampato anche la media della trasformazione vi ricordo che la cosa va sempre scalata la matrice di input cioè va sempre riportato la media a zero quindi l'ho fatta stampare sull'oggetto PCA è stata già generata così quindi diciamo non abbiamo problema di quel tempo allora qui c'è una cella un pochino ripeto di quelle non strettamente necessarie per l'utilizzo dello strumento ma più necessarie per più utili per quanto riguarda la comprensione nel senso che è una cella con un po' di codice per generare le due frecce delle due direzioni ortogonali adesso non starei troppo nel dettaglio anche qui però brevemente vi dico come può essere diciamo quali sono le cose principali poi se ci sono dei dettagli magari qualcuno è curioso ci guardiamo più approfonditamente però ecco qui diciamo la sintesi è che questo questa funzione ok questa vi permette di disegnare dei vettori quindi in particolare due vettori perché due sono le nostre componenti principali ok quindi gli passiamo v0 v1 e vedrete che quello che gli andiamo a passare sono proprio noi facciamo un ciclo for in cui andiamo a invocare questa funzione draw vector e gli passiamo ad ogni iterazione un vettore diverso quindi in realtà v0 v1 non sono i due vettori delle due componenti ma sono vedrete due parametri che rappresentano la media e la media più un altro parametro v che in realtà qualcosa che è collegato ai parametri della scomposizione adesso ci arriviamo per grandi intanto guardiamo questo draw vector che cosa fa vediamo un attimo prima di fare il draw vector direi forse si capisce meglio così proviamo a guardare dopo guardiamo qui dentro che cosa fa proviamo a guardare questo codice a partire dalla riga 10 allora la prima cosa che facciamo è fare lo scatter la nostra nuova di punti ok scatter plot quindi li andiamo a plottare va bene quindi prima coordinata seconda coordinata questo parametro alfa 0 2 da solo la trasparenza dei punti perché li disegniamo un pochino trasparenti dopodiché facciamo un ciclo for alla riga 11 questo ciclo for viaggia su due parametri un primo parametro lunghezza e un secondo parametro vettore dove li va a prendere zip vuol dire che vado a prendere i due parametri il primo fa riferimento a questo valore e il secondo a quest'altro ok quindi alla prima iterazione length prende la prima varianza dell'oggetto pca va bene e vector prende la prima componente quindi alla prima iterazione vado a prendere primo autovalore e primo auto vettore e li vado a mettere nella variabile length e nella variabile vector ci siamo ok alla seconda iterazione vado a prendere il secondo autovalore e il secondo auto vettore fine ci siamo fin qui cosa fa quel ciclo for dopodiché guardate costruisco un vettore v che prende la mia componente quindi alla prima iterazione la prima componente principale si chiama vector va bene prende quella e la moltiplica per un valore 3 in questo caso è semplicemente un fattore di scala perché per farci stare bene nel grafico per aggiustarlo e per la radice quadrata della lunghezza cioè creiamo un vettore a partire dalle coordinate del vettore originale e lo allunghiamo proporzionalmente alla varianza più la varianza è grande più grande sarà il vettore quindi la prima componente sarà un vettore più lungo della seconda ok questo perché a livello grafico viene più bellino vedrete che allineiamo la prima componente lungo la direzione di massima variabilità e sarà più lunga della seconda che è ortogonale alla seconda iterazione facciamo la stessa cosa solo che in vector c'è l'auto vettore che rappresenta la seconda componente principale e la sua lunghezza sarà più piccola perché l'autovalore corrispondente è più basso mi seguite fin qui ok poi invochiamo questa funzione draw vector a cui passiamo il valore medio ok del dato quindi quanto vale la media ci centriamo questo è l'origine a partire dalla quale io vado a disegnare quella freccia l'origine è il valore medio di quella nuvola di punti qui mi vado a mettere praticamente nel centro di quella nuvola di punti e il secondo parametro è il valore medio più questo vettore cioè aggiungo ad ogni componente di quel vettore il valore medio se il valore medio è 0 io gli dico 0 e gli passo il vettore come seconda componente e allora guardate che cosa succede questo draw vector che cosa fa? allora definisce un elemento grafico che si chiama x che rappresenta gli assi della nostra figura va bene? se gliel'abbiamo già passato va a prendere quelli che gli abbiamo già passato altrimenti ne definisce uno nuovo scusate li prende se gliel'abbiamo già passato e altrimenti definisce un nuovo oggetto grafico cioè plt get current axis cioè vai a prendere gli assi ci riferiamo agli assi con x della nostra figura del nostro nuovo oggetto grafico poi qui quello che fa definisce un dizionario tramite questo costruttore dizionario in cui gli passiamo una serie di coppie chiave valore quindi che definiscono lo stile dell'oggetto grafico tutto questo ripeto non vi servirebbe per il machine learning in quanto tale però ve lo faccio vedere perché sono alcune cose insomma giusto per capire qualcosa in più è l'occasione per chi vuole vedere poi in ambito, ve lo dicevo anche l'altra volta, in ambito data science la visualizzazione dei dati riveste una sua importanza quindi può essere interessante quindi passate una serie di parametri tipo lo stile della freccia la lunghezza scusatemi, lo spessore della lunghezza il fatto qui non andiamo a comprimere nessuno dei due diciamo dei due fattori quindi shrink A e shrink B sono dei fattori in cui potete andare a definire se deformare questo oggetto oppure no cioè la freccia il colore della freccia che sarà rosso e poi quello che dite semplicemente andate ad annotare questo oggetto grafico che ripeto è la vostra figura ok? e noi gli passeremo semplicemente lui andrà a creare qualcosa su cui noi andiamo che è questa lui lavora su quella figura che abbiamo appena generato dove abbiamo messo la nuvola di punti e lì va ad annotare praticamente quella figura andando a metterci a creare una freccia con le proprietà che abbiamo definito qua sopra quindi una freccia con quello stile quello spessore quel colore partendo dal punto V0 e andando verso il punto V1 che li abbiamo specificato prima quindi partendo dal punto medio e andando verso le coordinate della prima iterazione della prima direzione principale e poi della seconda direzione principale scalate opportunamente che ci crediate o meno il risultato è una cosa di questo tipo spero adesso che sia ragionevolmente che sia ragionevolmente ragionevole che viene fuori una cosa di questo genere potete provare a fare un po' di prove per andare a capire però di fatto il valore medio di questa nuvola di punti è qua gli diciamo la prima iterazione è di andare a descrivere una freccia che è in rossa che parte da qui e va verso queste coordinate vedete queste coordinate sono se voi andate a vedere in realtà se andate a tracciare andiamo a rivedere quali sono le coordinate della prima direzione principale 0,94 0,33 allora 0,94 0,33 mettiamolo qui è un qualcosa che sta più o meno qui ecco perché questo è 0,94 0,33 più o meno occhio e croce allora lui che cosa ha fatto? ha tirato una freccia dal punto medio fino a qua in realtà è scalata di che cosa? è moltiplicata per la varianza e per 3 per esagerare un pochino queste frecce e ha disegnato quella freccia e poi l'ha fatto con la seconda coordinata seconda coordinata se andiamo a vedere la seconda direzione scusatemi aveva meno 0,32 0,94 meno 0,32 e 0,94 andate a vedere è un qualcosa che dovrebbe stare meno 0,32 praticamente qui e 0,94 in realtà starebbe qua quindi è esattamente lungo questa direzione però viene compresso del fattore della variante e quindi genera quella freccettina più piccola però le direzioni sono quelle insomma tutto questo giusto per farvi vedere a livello grafico in questo esempio bidimensionale quali sono le due direzioni che lui ha identificato ok quindi il succo del discorso è quella funzione draw vector disegna queste due frecce ecco va bene però la cosa importante è che cosa rappresentano e chiaramente noi non riusciamo a fare questo discorso solamente in due dimensioni non riusciamo a farlo però giusto per farvi vedere vedete qui abbiamo disegnato questi punti con quel livello di trasparenza potete giocare con la libreria grafica insomma c'ha tantissime opzioni qui volendo cambiare un po' di cose e renderla esteticamente più conforme ai vostri gusti ma la sostanza rimane quella allora direi che però andiamo avanti su questa cosa e proviamo a capire che cosa succede andiamo a vedere che cosa succede nel momento in cui facciamo una vera e propria riduzione della dimensionalità cioè la domanda che ci si può fare che viene immediato e dire ok disponiamo di questo bello strumento che fa questa cosa molto sofisticata due righe di codice riesco a fare l'analisi dei componenti principali perché ovviamente dietro quelle due righe di codice se ne portano dietro ovviamente n quindi chiaramente un utilizzo della libreria che cosa succede se io vado su un numero di componenti che ad esempio è 1 cioè cerco di proiettare quelle serie di punti che hanno due coordinate quindi due feature vivono in uno spazio bidimensionale in uno spazio a una sola dimensione perdo chiaramente dell'informazione vado a fare una proiezione che è una trasformazione lineare lungo quale spazio qual è lo spazio che vado a identificare è uno spazio che è quello di una retta quale retta è? la direzione della prima componente principale cioè vi sto dicendo prendi quei punti e proiettali su un'unica su un'unica componente che è la prima componente principale quella nella direzione della massima varianza cioè lungo la direzione per intenderci di questa freccia più lunga e se io vado a invocare il metodo fit sulla mia matrice di punti posso poi accedere quindi vado a fare l'apprendimento non supervisionato quindi di nuovo definisco l'oggetto PCA in cui vado a specificare guarda che le componenti questa volta sono una vado a fare l'addestramento cioè vado a prendere tutti i punti calcolare autovalore autovettore eccetera eccetera della matrice di covarianza poi posso accedere a un metodo transform questo metodo transform fa esattamente quello che ci si può aspettare cioè io ho fatto il mio addestramento e per ognuno di quei punti lui mi dà le coordinate nel nuovo sistema di riferimento cioè mi dice qual è la coordinata di ognuno dei punti nel nuovo sistema il metodo transform ora la coordinata di ognuno di quei punti bidimensionali nel nuovo sistema di riferimento è un numero perché è uno spazio unidimensionale giusto? quindi XPCA quale tipo di shape è un vettore che quale tipo di forma ci aspettiamo che possa essere poi gliela facciamo stampare la mandiamo in esecuzione adesso quello che ci aspettiamo che possa avere noi abbiamo generato 200 punti e viveva in uno spazio bidimensionale vedete coordinate 202 di quel vettore la nostra matrice originale quella che abbiamo chiamato design matrix è una matrice 200 righe e due colonne 200 punti ognuno caratterizzato da due coordinate la matrice XPCA che è il risultato della trasformazione secondo la PCA ha una forma che è 200,1 cioè sono sempre 200 punti chiaramente ma ognuno ha una sola coordinata ho fatto riduzione della dimensionalità va bene? ok il dato trasformato come vi ho scritto qua vi è stato ridotto una singola dimensione non solo abbiamo e implementato nella libreria il metodo di trasformazione c'è anche il metodo di trasformazione inverso cioè noi possiamo comunque tornare dal mondo a dimensionalità ridotta al mondo a dimensionalità originale lo abbiamo visto anche alla teoria quando abbiamo fatto l'encoding e il decoding passiamo da un mondo a n dimensioni a un mondo a dimensioni ridotta e poi possiamo tornare nel mondo a n dimensioni però è chiaro che quando ci torniamo ci accorgiamo che qualcosa abbiamo perso giusto? vi ricordate quando vi ho fatto gli esempi le figure che abbiamo visto nella teoria e qui vedremo proprio questo effetto perché se io applico vedrete c'è un metodo che permette di effettuare la trasformazione inversa se applico quello torno a delle coordinate bidimensionali poi le faremo plottare tutte e vediamo che cosa succede allora quello che succede è questo io posso applicare eccoci posso applicare a xpca che vi ricordo è sono le coordinate dei punti nella dimensione ridotta posso applicare passare questo come input a un metodo che si chiama inverse transform dell'oggetto pca che permette di fare la trasformazione la retrotrasformazione la trasformazione inversa e quindi mi dà una nuova matrice che avrà di nuovo coordinate cosa ci aspettiamo 200,2 che però non saranno coincidenti con le coordinate da cui siamo partiti perché noi nel andare verso dimensionalità più bassa abbiamo perso delle informazioni se io avessi due punti che vivono in uno spazio tridimensionale stanno sulla stessa verticale se li proietto su questo piano qui saranno un unico punto e quando ritorno indietro come faccio a sapere qual era che questa era questa altezza e questa era questa non lo so il risultato per vedere meglio di cosa stiamo parlando prima possiamo far stampare x new x new se andate a vedere è 200,2 come vi dicevo e se mandiamo in esecuzione appunto di nuovo questa cella che cosa fa questa cella alla riga 3 fa lo scatter plot quindi stampa i dati originali x con un certo livello di trasparenza quindi per ognuno di quei 200 punti andiamo a prendere la coordinata 0 la coordinata 1 e le andiamo a plotare poi facciamo la stessa cosa con questo che è x new va bene ok e andiamo a vedere che con un livello di trasparenza leggermente diverso quindi un po' più alto e andiamo a vedere che cosa succede guardate allora vedete che sotto più trasparenti e in azzurro abbiamo la nuvola di punti originale quella sopra in arancione che cos'è è il plot in due dimensioni del dato trasformato in una dimensione e poi su cui abbiamo applicato la trasformazione inversa quindi di nuovo gli abbiamo ridato due dimensioni che cosa è successo vedete che lui è andato lungo la direzione chiaramente qui vedete che si tratta di una direzione che è coincidente con quella della massima varianza quindi lui ha proiettato tutto lungo la prima direzione principale ci siamo? e poi se noi facciamo la trasformazione inversa chiaramente lui non riesce più a come esattamente nell'esempio che vi dicevo prima non riesce più a riandare no? siamo qui a dire qual era l'altezza di questi due punti diversi lui qui ha proiettato e se gli dite riandiamo in due dimensioni l'unica cosa che sa fare è dire sono tutti punti distribuiti così lungo quella retta ma ovviamente li abbiamo schiacciati lì ma è ovvio che questo sia così per cui l'effetto è questo insomma va bene adesso vediamo un esempio direi abbastanza secondo me anche questo carino per capire un po' di cosa parliamo quando parliamo di riduzione della dimensionalità fino adesso abbiamo visto ragionare su due dimensioni adesso andiamo un po' su un problema un pochino più in cui la dimensionalità è sicuramente più elevata allora prendiamo un dataset che è un piccolo dataset prototipo di cifre scritte a mano questo è diciamo in qualche modo una versione semplificata di quello che è un dataset più famoso che è il dataset si chiama MNIST che è quello che vi dicevo è stato il punto di partenza per tutti i benchmark dei sistemi di computer vision basati su reti neurali cioè la pubblicazione di quel dataset ha dato il via a tutti una certi studi per cui è passata poi la storia come una sorta del moscerino della frutta diciamo delle reti neurali perché era quello su cui venivano fatti esperimenti per riconoscere in automatico delle cifre scritte a mano questa è una versione più piccola semplificata poi vi farò vedere anche quello lì che comunque oggigiorno anche quello è un dataset ampiamente gestibile ma non lo era quando è stato reso pubblico pubblicato nel 99 stiamo parlando di 25 anni fa allora questo è un dataset che è costituito da 1797 immagini ognuna del quali è una griglia di 64 pixel 8 righe 8 colonne ok quindi una griglia di 64 pixel e ognuno di quei praticamente di quei pixel è rappresentato da un numero che rappresenta l'intensità del pixel in una scala da 0 fino a 255 quindi da un valore massimo che è tutto bianco a un valore minimo tutto nero e in mezzo ci sono i valori intermedi di grigio va bene quindi che cosa vede il computer immaginate che acquisisca un'immagine da una camera molto bassa risoluzione una risoluzione di 64 è molto piccola in realtà potrebbe essere un valore che poi viene scalato insomma ridotta l'immagine a partire da un'immagine più alta risoluzione e però insomma questo è su questo lavoriamo per capirlo un po' meglio nonostante sia un'immagine a molto bassa risoluzione perché ripeto 64 pixel è nulla rispetto se prendete appunto una fotocamera di qualche megapixel che c'è su qualunque telefonino di oggi ci permette di fare alcune considerazioni interessanti anzitutto qual è la dimensionalità cioè di questi oggetti cioè che cosa noi rappresentiamo che cosa vede il computer come immagine vede di fatto una matrice 8x8 che poi noi convertiamo vedrete in un vettore un vettore che ha 64 entri quindi per lui un'immagine è un vettore di 64 numeri ognuno compreso tra 0 e 255 e infatti se voi andate a fare questa cosa questo dataset cosa che non vi ho detto è contenuto all'interno di Scikit-learn Scikit-learn come vi dicevo vi mette a disposizione alcuni dataset di prova per cercare di capire un po' meglio come funzionano gli strumenti quindi dal modulo dataset se importate load digits digits appunto sta per cifre è proprio questo dataset di cui vi parlavo e quindi con load digits riuscite a creare questo oggetto e questo oggetto ha diversi diciamo diversi campi diversi record se voi andate a prendere il campo data e poi fate stampare la shape vi dice che cosa che il campo data di digits è una matrice 1797 per 64 cioè sono 1797 immagini ognuna delle quali rappresenta una cifra scritta a mano che è stata acquisita ed è stata codificata in una matrice di 8 per 8 pixel va bene e qui gli ho fatto stampare la prima occorrenza vedete la prima occorrenza è un vettore di 64 entri ok che ha 0, 0, 5, 13 così via in realtà qui è tra 0 e 16 non è 255 il valore massimo è stato riscalato con il modo che l'intensità di ray siano tra 0 e 16 se non sbaglio comunque poco importa un valore massimo rappresenta il pixel tutto bianco un valore 0 un pixel nero e in mezzo ci sono tutti i valori intermedi ok ci siamo fin qui per capire ancora meglio che cosa fa che cosa rappresenta questo dataset guardate qui potete invocare matplotlib plt.figure generate una figura di cui potete specificare la dimensione ok e poi potete invocare questa funzione che si chiama imshow che sta per image show che vi prende come input un'immagine io in questo caso sono andato a prendere del dataset digits l'immagine con l'indice 0 quindi diciamo il primo di questi vettori e gli ho detto mostrami quell'immagine lui prende e gli ho specificato anche di mostrarla qui potete specificare diverse opzioni nella libreria sono funzionalità grafiche una mappa di colore cmap sta per color map in questo caso la mappa di colore è una mappa di colore che lui ha già al suo interno che è una mappa di grigi in questo caso rovesciata cioè lui metterà siccome io quando scrivo il nero è dove abbiamo scritto e quindi avrebbe tutto bianco tranne quei pixel dove viene scritta la cifra qui glieli facciamo invertire in modo da far vedere perché sennò sarebbe che cosa sarebbe sarebbe un qualcosa che diciamo lui vedrebbe come invertito rispetto a quello che noi vogliamo cioè se gli specifico la scala di grigi inversa viene fuori una cosa di questo tipo se io avessi messo giusto per farvi capire grey che non è quella inversa sarebbe venuta una cosa di questo tipo vedete ok quindi semplicemente qui è per dirgli la mappa di colori che vogliamo è quella corretta per la visualizzazione anche qui sono delle funzionalità della libreria grafica adesso se avete voglia potete andarli a vedere divertirvi un pochino cercare di capire meglio che cosa fa questo è semplicemente un altro parametro ulteriore che gli va specificato per dire se ci sono dei punti adiacenti qual è la tecnica di interpolazione ce n'ha diverse ma per fare il rendering di questa figura il rendering di questa figura però è questo lui vi aprirò quest'immagine 64x64 scusatemi 8x8 pardon e vedete vi ha costruito questa griglia di 8x8 e ognuno di questi punti all'incrocio di questi punti è andato a mettere dove al bianco corrisponde a un valore massimo di intensità dove aveva il nero chiaramente minimo e se voi andate a cambiare vedete questo chiaramente si vede che nonostante sia bassissima risoluzione è 1, 0 se voi cambiate e andate a cambiare il esatto se mettete 1 viene fuori un 1 se mettete sono 1000 quindi 150 andiamo a vedere cosa c'è il punto 150 cosa sarà un altro potrebbe essere uno 0 un 4 se andate a vedere il punto 198 questo è un 4 direi 256 questo non si riconosce neanche ecco vedete che si fa fatica anche a riconoscerlo a vedete questo è un 3 quindi lì dietro in quei vettori in quei 1797 vettori da 64 numeri ci sono delle immagini che noi abbiamo codificato in questo modo va bene chiaramente sono delle immagini ripeto bassa risoluzione però è sufficiente per andare andarle a vedere con la funzionalità image show potete caricare una un'immagine e andarla a plottare a graficare e questa è una funzione della della libreria matplotlib ok finora di nuovo funzionalità grafiche niente che sia della sostanza che volevamo andare a a studiare la sostanza diciamo l'oggetto del nostro studio oggi è l'analisi dei componenti principali allora andiamo a vedere perché qui abbiamo utilizzato questa funzione image show che appunto siccome noi sappiamo che sono delle immagini lì c'è una libreria che vi permette di accedere appunto a tutte le funzionalità di rendering grafico e vi ve la tratta esattamente come se fosse un'immagine ok ma se noi abbiamo un dato ad alta dimensionalità che non è un'immagine vogliamo cercare di capire qualcosa di più anche solo a livello per esempio di visualizzazione o comunque cercare di capire se c'è dell'informazione anche in una in una dimensione ridotta di quel tipo di dato come possiamo fare possiamo fare riduzione della dimensionalità e ad esempio una tecnica è la pca allora qui l'obiettivo è prendere questi dati che vivono in uno spazio a 64 dimensioni e proiettarli in due dimensioni così poi andiamo a vedere anche cosa succede nel momento in cui li andiamo a graficare cioè prendiamo ognuno di quei punti che vive in uno spazio a 64 dimensioni perché sono vettori a 64 componenti e lo proiettiamo tramite la tecnica della pca in uno spazio bidimensionale e vediamo cosa succede e succedono delle cose abbastanza interessanti vedrete allora come facciamo definiamo il nostro oggetto pca appartenente alla classe pca maiuscolo e gli andiamo a specificare il numero di componenti come abbiamo fatto prima in questo caso dopodiché definiamo gli oggetti che chiamiamo progette che si ottengono come come metodo dell'oggetto pca il metodo andiamo a invocare questo metodo che si chiama fit transform che cosa fa fit transform fa fit e poi il risultato del fit ci applica il metodo transform noi l'abbiamo visto qui sopra se vi ricordate l'abbiamo fatto fit e poi transform vi ricordate anziché fare prima fit e poi transform potete direttamente fare una cosa del genere fit transform che equivale a fare quei due in cascata a invocare quei due metodi in cascata cosa gli passiamo gli passiamo come input la matrice quella matrice 1797 per 64 che rappresenta ognuno di quei punti del dataset rappresenta un'immagine di una cifra scritta a mano ok gliele passiamo tutte e gli diciamo file fitting apprendimento non supervisionato lui applica la pca calcola le componenti principali le componenti a cui sono associate le varianze più ampie che sono quindi gli autovalori di quella scomposizione in autovalori e autovettori lui le prende prende le due più grandi le principali e le va a utilizzare per proiettare questi punti ognuno di quei punti in uno spazio a due dimensioni quindi ognuno di quei vettori da 64 componenti si ritrova a essere rappresentato con due sole componenti ok dopodiché andiamo a fare una print della shape del dato originale e la shape del dato originale è 1797x64 vedete la shape di projected cioè dei dati proiettati è 1797 i punti ce li ho sempre tutti per due e poi gli ho fatto giusto per controllo stampare anche target target è un campo dell'oggetto digit quindi quando importiamo quell'oggetto quel dataset lui ha digit.data che è la matrice la array in cui mette tutti i dati e poi c'è anche un campo target di digit che vi dice per ognuna che cosa rappresenta quella cifra e perché questo in realtà è un dataset che noi usiamo tipicamente per fare classificazione e per dire il primo era uno zero l'avevamo visto vi ricordate il secondo era un uno il terzo era un due avevamo provato vediamo un po' se il terzo è un due proviamo ad andare a vedere se io vado a mettere qui vedete che è un due se voi volete andare a vedere qual è l'ultimo abbiamo detto scusate che l'ultimo era vedete qui è un otto andiamo a vedere di 1796 vediamo se viene un otto eh più o meno direi che ci possiamo credere che è un otto ok quello precedente dovrebbe essere un nove vediamo un po' se ci piace un po' di più sì ok va bene capito come funziona questo dataset come è organizzato quindi se voi andate su digits.target avete per ognuno di quei 1797 punti la classe corrispondente se fosse un problema di classificazione sarebbe un problema di classificazione con quante classi 10 perché le cifre sono 10 chiaro ok per noi in questo momento non è un problema di classificazione vogliamo capire cosa succede se facciamo una riduzione della dimensionalità quindi noi questo campo qui non lo usiamo abbiamo fatto apprendimento non supervisionato gli abbiamo passato solo i dati non le etichette ci siamo differenza tra supervisione e non supervisione benissimo adesso andiamo a vedere che cosa succede se andiamo a graficare questi dati cosa vuol dire graficare questi dati chiaramente ha senso solo graficare questi non possiamo graficare un vettore in 64 dimensioni grafichiamo un vettore in due dimensioni e lo facciamo così con la funzione scatter di matplotlib a cui passiamo projected che vi ricordo sono tutti i dati proiettati nello spazio di due dimensioni gli passiamo tutte le righe che sono 1797 questa è la prima coordinata tutte le righe e questa è la seconda coordinata e gli diciamo vai a mettere un punto all'incrocio tra la prima e la seconda coordinata poi gli diciamo un'ulteriore cosa se andate a vedere le funzioni grafiche li potete specificare questo l'abbiamo visto anche per un'altra esercitazione di andare a colorare quei punti secondo che cosa secondo quello che è il campo target di digit cioè lui andrà a mettere un colore diverso a seconda che questo punto sia uno zero un uno un due oppure un nove quindi ogni punto lo colora secondo dieci possibili alternative ci siamo in questo modo noi vediamo quei punti in due dimensioni e li sappiamo riconoscere tutti i punti che hanno etichetta zero avranno un colore tutti i punti che hanno etichetta uno avranno un altro colore e così via e l'obiettivo è vedere se vado a proiettare in due dimensioni cosa ci aspettiamo chiaramente io avrò perso dell'informazione però se non ne ho persa troppa con la pca mi aspetto che questa sia una nuvola di punti in cui qualcosa forse riesco ancora a distinguere cioè tutti i punti che hanno un colore saranno vicini tra di loro forse sì e altri saranno vicini tra di loro ma magari lontani da quest'altra nuvola di un colore alcune nuvole di colore associato a che ne so al 3 saranno forse vicine alla nuvola di punti associata al 9 perché le cifre 3 e 9 forse si assomigliano è questo che vogliamo andare a vedere ed effettivamente se voi mandate in esecuzione una cosa di questo genere adesso vi finisco di dire giusto brevemente allora questo è quindi il colore edge color vuol dire che ognuno di quei punti possiamo andare a specificare il colore dei bordi nessuno il livello di trasparenza e dove va a prendere questi colori li va a prendere li va a colorare secondo il target qui diciamo la mappa di colore ci sono diverse mappe di colori adesso qui non entriamo nel dettaglio facciamo scegliere a lui diciamo tra queste dieci possibili classi quali sono lui va a prendere delle palette di colori che voi potete specificare tutte per esempio su toni che vanno dall'arancione al marrone che ne chiamiamo più autunnali oppure insomma di altro tipo ce ne sono diverse più brillanti più anche qui volendo se andate a vedere la documentazione trovate un sacco di cose se mandate in esecuzione adesso quindi noi prendiamo questa mappa di colore vedete che è tutta su una certa sfumatura di colore vedete prima componente seconda componente allora cosa rappresenta questa figura qui sulla destra gli abbiamo fatto anche plottare la la la color la color bar cioè è la leggenda diciamo dei vari colori vedete che ci sono dieci possibili alternative che sono le cifre da 0 a 9 e lui significa che per esempio tutti i punti che hanno come target come etichetta 9 li ha colorati con un marrone che sono più o meno questi andate a vedere ok se prendete il il 3 sono tutti i punti che andate a vedere hanno hanno allora questo è lo 0 questo è l'1 questo è il 2 questo è il 3 sono questi verdi qua verde più intenso e se andate a vedere stanno qui e non sono lontani vedete dal 9 ma è interessante vedere che ad esempio lo 0 è ben distinto da tutti gli altri è molto riconoscibile lo è di meno ad esempio l'1 che si sovrappone molto con questi altri viola che è il 7 è ragionevole che l'1 e il 7 effettivamente anche se proiettati in uno spazio di bassa dimensionalità si si sovrappongono questo regge come discorso allo stesso modo se andate a prendere non so vediamo un po' questi gialli con questi blu allora il giallo è l'8 questo blu è l'1 è anche l'8 se ci pensate delle volte può essere confuso con un 1 i pixel si accendono tutto nella zona centrale dell'immagine e anche il 2 rispetto per esempio il 2 sarebbero questi verde più chiaro e direi che sì diciamo tendono un po' allora hanno delle sovrapposizioni qui al centro con il giallo che è l'8 anche qui diciamo ci può stare perché nell'8 se aggiungete questa parte ha un 2 e quindi vedete che già è una cosa del genere questo è un esperimento interessante perché vi fa capire molte cose vi fa toccare con mano cosa vuol dire fare riduzione della dimensionalità in questo caso vi permette di visualizzare dei dati ad alta dimensionalità e questo già è un primo valore aggiunto dell'analisi dei componenti principali vi fa vedere che tutto questo avviene chiaramente con perdita però vi fa capire qualcosa per esempio del dataset che avete è chiaro che voi avete perso delle informazioni facendo la riduzione della dimensionalità però già da qui vedete che forse c'è una qualche buona possibilità di andare a separare questi dati e poterli classificare correttamente perché se io passo da 64 dimensioni a 2 dimensioni comunque vedete riesco a distinguere non sono tutti sovrapposti tra di loro questo mi fa capire che ho buone possibilità di costruire un classificatore che li vada a distinguere che vada a discriminare questi 1797 vettori durante il training tra di loro e impari a distinguerli e per fare ci siamo fin qui ok per fare per vedere appunto toccare con mano questa cosa proprio proviamo a vedere a questo punto di costruire un classificatore che fa esattamente questo ah prima di fare questo no c'è un'altra cosa no il classificatore non lo vediamo adesso lo vediamo dopo quando abbiamo fatto anche ok lo vediamo in fondo al notebook quindi diciamo qui per il momento la conclusione che possiamo trarre è che è possibile probabilmente è ragionibile riuscire a discriminare queste cifre scritte a mano e questo è un aspetto non trascurabile poi vedete un'altra cosa che potete vedere vedete i 4 sono abbastanza ben distinguibili ed è giusto il 4 è molto diverso dalle altre cifre se ci pensate e allo stesso modo anche il 3 tutto sono nato da qui diciamo chiaramente dove ci sono più sovrapposizioni lì potenzialmente ci sono più possibilità che vengano scambiati gli uni per gli altri chiaramente qui da tenere conto del fatto che abbiamo proiettato in due dimensioni quindi parte di questa confusione di questa sovrapposizione può essere dovuta e senz'altro dovuta anche alla riduzione della dimensionalità domande? se avessimo ad esempio fatto la trasformazione in uno spazio 3D avremmo avuto delle nuvole più separate giusto? in teoria in linea di principio sì chiaramente adesso questo vi faccio vedere poi un altro grafico che vi fa vedere proprio cosa succede al variare della dimensionalità non chiaramente andando a graficare i punti però di fatto avremmo tipicamente avuto una minore sovrapposizione sì ok allora qui vi ho fatto un esempio in cui andiamo a vedere proprio quello qualcosa di collegato quello che stavamo dicendo allora di nuovo rifacciamo riprendiamo la nostra classe pca quindi ricreiamo un oggetto pca minuscolo e andiamo a prendere il metodo fit e gli passiamo di nuovo tutti i dati ok qui in questo caso vedete che non gli passiamo il parametro in components cosa vuol dire? se non glielo passiamo lui va a prendere tutte le componenti quindi significa che va a fare l'analisi dei componenti principali su 64 dimensioni va bene? ok poi qui vabbè ho commentato un po' di print se volete diciamo di supporto se volete ma non diciamo lo commentate se volete andare a vedere quali sono di nuovo le shape se volete andare a vedere la somma di questo attributo di cui adesso andiamo a vedere a dire qualcosa in più potete farlo insomma e decommentarli ma al momento direi che non non ci è utile andiamo direttamente invece alla riga 6 che cosa fa questa questa riga 6 allora se voi andate a prendere l'oggetto pca che abbiamo appena creato ha vi ricordate vi ho detto prima diversi attributi tra cui appunto le componenti non l'abbiamo visto prima e la varianza c'è questo campo che si chiama explained variance ratio che è un attributo che vi dice che cosa lui va a calcolare la varianza dei dati lungo ogni direzione ok dopodiché mette questo in una scala da 0 a 100 e vi dice ok la prima direzione di tutta la varianza se io faccio 100 la varianza del dato la prima direzione mi alla prima direzione è associata una varianza più grande delle altre che per esempio il 27% di questo totale alla seconda direzione sarà associata una varianza minore che ad esempio è il 10% poi abbiamo il 5% il 4% e così via ok quindi lui mi mette in una scala percentuale tutte le varianze che rappresentano vi ricordo le magnitudo cioè l'ampiezza degli autovettori quindi sono gli autovalori scusatemi vi rappresentano gli autovalori che sono associati a ognuno di quegli autovettori ok ci siamo quindi questo explained variance ratio è un numero tra 0 e 100 ok in percentuale quindi in realtà sono numeri minori di 1 se li mettiamo sarebbe 0,27 0,10 se facciamo 27% 10% e quello che potete fare è andare a plottare qui utilizzo una funzione di numpy che si chiama cum sum che fa la somma cumulativa cosa vuol dire vuol dire che vi va a plottare vi va a mettere sul andiamo a plottare a calcolare per ogni per ognuno di quei 64 lì abbiamo un vettore di 64 componenti che sono le 64 componenti principali supponiamo che la prima valga 27% lui prende la prima e mette un valore che è 0,27 e a quel punto lo va a plottare e quello che ottiene io adesso ho detto 27% ma sicuramente non sarà 27% però giusto per farvi capire non sono andato tanto lontano il primo punto è qua quindi in realtà non è 0,27 sarà 15% poi va a prendere il secondo il secondo poniamo che sia a questo 15% ci somma il 12% quindi 15 più 12 fa 27% il secondo punto lo va a mettere qua ed è un 27% poi ci aggiunge perché fa la somma cumulativa quel cum sum cum sum sta per somma cumulativa quindi fa siamo arrivati a 27 prende la terza componente qual è la varianza associata alla terza componente in una scala da 0 a 100 è abbiamo detto le prime due erano 15 e 12 supponiamo che sia 10 27 più 10 fa 37 e lo va a mettere qua e va avanti così e lo fa per tutte le 64 componenti questo stiamo facendo stiamo facendo la somma cumulativa di ognuno di questi punti e poi la andiamo a plottare in un grafico allora questo grafico perché è interessante poi ci potete mettere vedete in questo caso le etichette che vogliamo agli assi ci abbiamo messo in funzione del numero delle componenti quella che è la varianza spiegata cumulativa cioè questo grafico è interessante perché vi dice che cosa se voi prendete la prima componente principale vi rende conto del 15% della varianza se ci aggiungete la seconda arrivate abbiamo detto a 27 se ci aggiungete la terza a 37 e così via è interessante perché vi rendete conto di che cosa questo grafico che abbiamo creato ripeto solamente a partire noi abbiamo fatto la nostra pca e poi abbiamo fatto la funzione numpy di somma cumulativa e questo lo potete fare sempre e vi dà un'informazione molto importante perché vi dice se voi proiettate i vostri dati in uno spazio più bassa dimensione quando mi posso fermare per esempio per avere il 90% di tutta la variabilità dei dati allora io vado a vedere il 90% grossomodo stare qua e vedete che io solo con 22 componenti su 64 riesco a trattenere il 90% della variabilità del dato e questo mi dice un'informazione importante mi dice che se io proietto in uno spazio anziché a 22 componenti comunque l'informazione che perdo è relativamente bassa e quindi è un'informazione questo grafico importante nel momento in cui si vanno a fare queste operazioni di riduzione della dimensionalità con la PCA chiaramente allora questa c'è la successiva è una variazione su questo tema ed è una variazione però interessante perché vediamo un po' allora facciamo due grafici anzitutto perché ci permette comunque di evidenziare un aspetto complementare allora qui vedete quello che facciamo è di nuovo utilizzare anzitutto vediamo procediamo per ordine qui viene utilizzata la funzione subplot ok subplot vi ricordo vi crea più sotto grafici di un'unica unità grafica quindi creiamo una figura in questo caso con due sottofigure due righe e una colonna e qui andiamo a prendere la prima di queste due figure quindi noi facciamo una figura organizzata in due righe e una colonna e saranno queste due figure qua che vengono generate va bene vedete due righe e una colonna andiamo a prendere la prima la prima la definiamo qua e poi andremo a prendere la seconda sottofigura e la definiamo qua va bene e poi facciamo PLT show di tutte le due figure di tutte le due sottofigure allora la prima che cosa fa plottiamo l'attributo explain variance che è quello che abbiamo appena utilizzato sopra nella somma cumulativa e specifichiamo un colore vabbè in questo caso un arancione l'etichetta è numero di componenti l'etichetta è lo x l'etichetta è lo y è la varianza spiegata eig sta per eigenvalues eigenvector è una scomposizione in autovalori e autovettori quindi vabbè l'abbiamo riportato anche nella stringa che descrive l'asse y e quello che conta è cosa succede quando andiamo a plottarlo queste ripeto explain variance è un attributo di pca che è un vettore a 64 componenti e vi dice la prima componente qual è il valore dell'auto scusatemi della varianza quindi dell'autovalore corrispondente ed è il più alto di tutti infatti spiegava il 22% cos'era il 12% del dato il 15% più o meno poi se prendete il secondo a questo corrisponde un valore più piccolo poi un altro più piccolo vedete che man mano che vado avanti questa è l'ultima componente che è quella che spiega la minor parte del dato però vedete cosa torna qua che dopo il 22% ricordate abbiamo detto 22 componenti il 90% e qui torna che dopo la 22esima non diminuisce più cioè la varianza è al suo valore minimo la varianza è spiegata da quella direzione e quindi questo è un po' complementare al grafico che abbiamo visto precedentemente quello che potete andare questo è il valore diciamo assoluto se accedete a il explained variance ratio questo è explained variance underscore explained variance ratio underscore è quello che abbiamo utilizzato per la somma cumulativa è la è la stessa informazione però nella scala 0-100 e se andate a vedere vedete guardate ritroviamo quello che abbiamo visto prima questa è quella scala 0-100 per cui vi dice la prima componente è guardate 15% della varianza rispiega è associata a quella prima componente se andate alla seconda componente avrete qualcosa dintorno al non so può essere 12% quindi 15 più 12 fa 27% e così via ne aggiungete le aggiungete quando arrivate qui vedete che aggiungendo avete già spiegato la maggior parte della varianza in questione quindi sono due questi sono essenzialmente lo stesso grafico solo che questo riporta questi valori assoluti e questo li ha messi su una scala percentuale e sono di fatto quello che c'è dietro a questo grafico qua ok però sono dei grafici importanti perché vi permettono di tenere sotto controllo la la variabilità del dato quanta ne perdete nel momento in cui andate a ridurre la dimensionalità utilizzando la PCA domande? è chiaro come abbiamo costruiti? quindi poi se qualcosa non vi è perfettamente chiaro per esempio io qui ho introdotto questa funzione cum sum di Numpy e di nuovo andate a vedere la documentazione e cercate di capire meglio come funziona ma fa la somma cumulativa degli elementi di un vettore bene allora direi che è rimasto un po' di tempo per provare a vedere vediamo se riusciamo se no finiamo la prossima volta a vedere qualcosa sul clustering ok quindi sull'apprendimento non supervisionato abbiamo visto riduzione della dimensionalità ma abbiamo visto anche qualcosa del clustering in particolare abbiamo visto l'algoritmo K-Means e Scikit-Learn vi permette di mettere a disposizione diversi algoritmi di clustering tra l'altro anche che non abbiamo visto e tra questi ovviamente c'è il nostro K-Means e quindi proviamo a vedere come come lavorare con questo con questa implementazione e andiamo a lavorare sulla stesso dataset che abbiamo usato sopra che è quello sulle cifre scritte delle cifre scritte a mano ok quindi proviamo a fare il clustering di quell'insieme di punti è un dataset che andiamo di nuovo a importare qui in questa in questa cella in questa cella vedete importiamo anche dal modulo cluster che è il modulo di Scikit-Learn che vi permette di fare di invocare diversi algoritmi di clustering andiamo a importare la classe K-Means la classe K-Means vi permette di accedere appunto all'algoritmo K-Means poi vedete carichiamo lo stesso dataset di prima gli facciamo printare la shape e ovviamente abbiamo 1797 punti ognuno 64 coordinate questo oramai abbiamo capito perché e quindi c'è bisogno di tornarci su per esempio lo stesso dataset andiamo sotto e la cela sotto già vedete in quattro righe di codice questo allora va sottolineato perché perché io come vedete con quattro istruzioni riesco a fare una cosa che non è banale che è il clustering l'algoritmo di clustering ha dietro diverse cose l'abbiamo viste la teoria ha dietro la definizione di un di centroide la definizione di della funzione che va a minimizzare la distanza no? qual è la funzione che minimizza la distanza dei punti dal centroide stesso ci sono dietro un po' di cose è un metodo iterativo e qui diciamo la facilità di utilizzo di queste librerie che sono che hanno raggiunto un livello di diciamo è una robustezza appunto che le rende estremamente utili è evidente è chiaro che qui chiunque anche chi non ha idea di che cosa ci sia effettivamente dietro ha molta facilità di poter utilizzare questi strumenti questo vale per questo semplice esempio che è il clustering ma vale per altre librerie che fanno anche altre cose anche magari più complicate per quanto appunto bisognerebbe capire cosa vuol dire intenderci cosa vuol dire complicato però il punto è oggigiorno c'è una grande facilità di accesso a questi strumenti e questi strumenti permettono molto facilmente di ampliare la diffusione e il potenziale numero di utenti che li utilizzano non sempre diciamo tutti lo fanno probabilmente con la stessa consapevolezza quindi questo penso che sia il valore aggiunto di chi studia queste cose cioè rispetto a uno che non ha un'idea di cosa vuol dire l'algoritmo che i means dietro e dice ah so che fa il clustering un po' si informa su cosa vuol dire il raggruppamento prende quattro righe di codici ah ho fatto il raggruppamento di questo dataset bello il clustering sì ho fatto il clustering ma magari non sa che cosa c'è dietro ecco sapere che cosa c'è dietro probabilmente dà una consapevolezza diversa e se voi cominciate a cambiare qualcosa anche nelle se voi andate a vedere dopo magari ci guardiamo insieme la documentazione dell'algoritmo della classe che i means dietro per esempio potete utilizzare due tipologie di algoritmi uno è quello che abbiamo visto però ci sono altre versioni c'è una versione che può essere proposta che permette di velocizzarlo in certi casi perché usa delle tecniche disuguaglianze triangolari eccetera e lì sapere che se vado a cambiare quello succede qualcosa oppure no o saperlo valutare è probabilmente di pertinenza di chi riesce a padroneggiare questa cosa chi comincia semplicemente a usare lo strumento un po' come se fosse una scatola effettivamente una scatola nera probabilmente non riesce a fare queste cose o non le fa così con questa immediatezza che dovreste essere in grado di fare voi quindi tutto questo discorso per dire il valore aggiunto secondo me di studiarle queste cose e capirle bene da dietro per sapere appunto cosa c'è dietro ecco questo è il non di tutto si riesce ad approfondire in maniera nello stesso modo però secondo me ecco la vera differenza poi tra chi su queste cose riesce a lavorare grazie al fatto che ci sono queste librerie che sono oggettivamente molto semplici da usare cioè chiunque ha un minimo di dimestichezza si mette fare un po' basta avere un po' di dimestichezza con python qui quattro righe uno fa delle cose veramente che prima ci voleva molto di più però ecco ripeto non è non tutti forse lo riescono a fare con questa consapevolezza chiuso questo inciso qui cosa facciamo k-means diciamo primo parametro numero di cluster random state uguale a zero numero di inizializzazione è un altro parametro poi adesso magari andiamo a vedere insieme la documentazione random state molto semplicemente di che cosa di che cosa si tratta e vi ricordate che inizializza sfrutta dei meccanismi casuali anche per l'inizializzazione qui dobbiamo settare il seed numero di init dovrebbe essere un parametro che vi permette di dire quanti tentativi andate a fare adesso andiamo a vedere facciamo prima andare a vedere insieme perché poi tutti non è difficile ricordarsi vi vado a condividere la documentazione che avevo messo da parte prima quindi se mi date un attimo provo a condividerla a parte che ovviamente ci potete arrivare anche voi nel frattempo autonomamente ecco qua allora se andate a vedere la documentazione vedete c'è un primo parametro che è il numero di cluster noi gli abbiamo specificato 10 perché vogliamo suddividere secondo le 10 cifre di default lui prenderebbe 8 allora se andate a vedere init vi dice noi gli abbiamo specificato non gli abbiamo specificato niente e il metodo di inizializzazione anche qui noi abbiamo visto c'è sia quello casuale puramente casuale che è quello che noi abbiamo spiegato a lezione però vedete c'è anche un altro che è il k-means plus plus che ve l'avevo forse accennato a lezione è un metodo diciamo tra virgolette più un pochino più smart perché seleziona i centroidi cercando di analizzare il dato non sparandoli puramente a caso questo migliora tipicamente la convergenza tra l'altro questo a proposito di spunti di nuovo per i progetti provare a vedere che cosa cambia nel cluster in quanto cambiate da k-means più più a random potrebbe essere un'altra cosa interessante il default lui utilizza questo perché di fatto è una soluzione tipicamente più intelligente rispetto a sparare a caso dopodiché ci sono altri parametri c'era questo n-init che di default è settato ad auto e vi dice il numero di volte che l'algoritmo viene lanciato con differenti centroidi iniziali che sono quelli generati casualmente e il risultato poi alla fine è il miglior output che viene dato tra tutti questi allora se viene inizializzato ad auto vedete che il numero di run dipende dal valore di init cioè ne fa 10 per l'algoritmo casuale e uno solo se utilizzate k-means più più quindi vedete come anche tutte queste cose si capiscono se uno comincia a guardare un pochino meglio nel dettaglio ad esempio la documentazione e si sa che cosa che cosa fa massimo numero di iterazioni e noi l'abbiamo visto abbiamo visto che è un algoritmo iterativo quindi voi sapete che cosa succede se modificate questo valore dal default di 300 e l'aumentate vuol dire che cercate una più lunga convergenza oppure se avete un dataset molto numeroso e volete estrarre un risultato prima avete dei problemi da un punto di vista del tempo di esecuzione di calcolo potete ridurre questo numero ovviamente la qualità del risultato ne potrà risentire la tolleranza perché qui diciamo potete dichiarare la convergenza se i centroidi si modificano finché i centroidi si modificano da un'interazione all'altra non potete andare avanti nel momento in cui non si modificano cioè il valore è al di sotto ad esempio di 10 alla meno 4 vuol dire che lì l'algoritmo si è stabilizzato è arrivato a convergenza e magari siete alla centesima iterazione inutile che è arrivata a 500 potete specificare anche questo questo è il random state che vi ho detto per l'inizializzazione del centroidi e così via poi se andate a vedere ci sono due possibili alternative di algoritmi di default utilizza questo che è l'algoritmo cosiddetto di Lloyd che è quello che l'ha sviluppato che è quello che vi ho spiegato a lezione né più né meno di fatto cioè come sostanza è quello c'è un altro che si chiama Elkan che diciamo vi dice che può essere più efficiente su alcuni dataset che hanno dei cluster ben definiti perché è efficiente perché sfrutta quella che viene chiamata disuguaglianza triangolare semplicemente va a utilizzare questa proprietà matematica per snellire alcuni calcoli però di nuovo anche qui potete fare delle prove per vedere che cosa che cosa di questi rappresenta insomma come cambiano ad esempio dei risultati quindi di nuovo l'invito è anche andare a vedere un po' la documentazione per capire un po' meglio come funziona bene tornando a noi noi abbiamo specificato tutti i valori di default tranne il numero di cluster di fatto che 10 e l'inizializzazione del seed per avere sotto controllo la generazione di numeri pseudo casuali dopodiché se mandate in esecuzione ve lo mando in esecuzione adesso e fate la stampa dell'oggetto k-means vedete k-means è un oggetto in cui il parametro numero di cluster vale 10 random state vale 0 e poi con questo oggetto come al solito cosa facciamo fit e poi predict fit fa il vero e proprio addestramento e predict fa la predizione cioè per ogni punto noi avevamo 1797 punti vi dice a quale cluster appartiene in un'alternativa tra 0 e 9 dice questo è il cluster 0 questo è il cluster 1 questo è il cluster 5 cioè li raggruppa per criteri di omogeneità di similarità quindi quello che ci aspettiamo da questa cosa qual è? è che lui raggruppi il più possibile quelli che rappresentano le stesse cifre scritte a mano e vediamo cosa succede se effettivamente è in grado di fare questo senza vedere le etichette quindi stiamo facendo una sorta di diciamo classificatore senza avere la supervisione questo vogliamo vogliamo l'obiettivo diciamo è un po' questo se voi fate predict e poi fit è equivalente a fare fit underscore predict quindi che i means punto fit underscore predict e gli passate la vostra matrice di dati ottenete un vettore che ha 1797 componenti ognuna delle quali un numero 0, 1, 2 fino a 9 che vi dice a quale cluster appartiene quel punto e quindi di fatto stiamo facendo come se facessimo una classificazione vi faccio vedere questo per vedere come funziona questi algoritmi di clustering per vedere che cosa faccio cosa riuscirà a costruire lui dei raggruppamenti ragionevoli oppure no noi ci aspettiamo che le cifre scritte a mano che rappresentano gli uno finiscano in buona parte in un unico raggruppamento vorremmo idealmente questo che tutti i tre finiscano in un altro raggruppamento e vediamo se effettivamente è così questo è l'obiettivo per fare questo intanto nella nella cella successiva quello che viene fatto è definire un oggetto figura con i suoi assi tramite la funzione del subplot in cui vedete definiamo un subplot con due righe e cinque colonne questa è la dimensione di ognuna di quelle sottofigure e che cosa facciamo andiamo a creare dei vettori che chiamiamo centri centers a partire da che cosa che i means se voi andate a vedere c'ha un attributo cluster centers quell'attributo cluster center è per ogni oggetto quindi stiamo parlando di 1797 oggetti giusto ognuno di quelli ha il suo centroide di riferimento questo attributo cluster punto cluster underscore center underscore vi dice le coordinate del centroide per ogni punto va bene ci siamo fin qui se voi andate a vedere la documentazione avete proprio questo adesso vediamo un attimo ve la rimetto un attimo su se ci riesco guardate qui sono la documentazione di prima vedete gli attributi dell'oggetto vi dice quali sono allora se io prendo l'oggetto noi l'abbiamo chiamato k-mings per il minuscolo e ho vedete punto cluster center underscore quello è un array che ha una shape numero di cluster numero di feature ok quindi un array con nel nostro caso 10x64 e vi dà tutte le coordinate dei centri del cluster poi avete anche un label che vi dà le label di ogni punto eccetera tornando a noi quindi io prendo quell'attributo ok e abbiamo detto che è 10x64 quindi ho sono i 10 le coordinate dei 10 centroidi e ogni coordinata è espressa come un vettore di 64 e qui faccio questo 10x64 lo rendo un 10x8x8 cioè ognuno di quei 10 centroidi lo codifico non più come un vettore di 64 ma come una matrice 8x8 voi mi potete dire perché stiamo facendo questo adesso ve lo dico stiamo facendo questo perché ognuno di quei centroidi rappresenta che cosa un punto dello spazio a 64 dimensioni ma siccome un punto dello spazio a 64 dimensioni abbiamo visto è potenzialmente un'immagine di 64 pixel io in questo modo sto prendendo un vettore a 64 dimensioni che rappresenta il centroide che è un qualcosa che non esiste nel mio dataset ma è un qualcosa che rappresenta un baricentro no? nulla vieta di andare a fare come facciamo adesso di riconvertirlo come se fosse un'immagine che è un'immagine sintetica cioè una cifra scritta a mano che di fatto non esiste e andiamo a vedere che cosa sono quelle 10 cifre scritte a mano che non esistono che cosa codificano e se voi andate a costruire questi 10 centri che sono quindi ripeto 10 matrici 64 per 64 in questo modo poi potete andare a utilizzare la funzione image show andiamo a vedere se ha senso dovrebbe avercelo vediamo un po' se ce l'ha qui viene invocato questo ciclo for questo ciclo for prende un asse nell'oggetto degli assi dei subplot questo flat dunque non mi ricordo adesso di preciso cosa fa dovremmo andare a vedere credo che prenda questi assi sì è necessario perché se no non funziona ma adesso non mi ricordo di preciso poi magari ci guardiamo però diciamo il succo del discorso è adesso non ci interessa rispetto ripeto ogni singolo dettaglio o meglio lo guardate con calma ecco adesso se no non arriviamo dove voglio farvi arrivare allora prende ognuno degli assi della figura ripeto abbiamo due rive cinque colonne dieci figure stiamo divinerando dieci sotto figure una figura con dieci sotto figure andiamo a prendere alla prima iterazione la prima di queste dieci alla seconda la seconda di queste dieci quindi andiamo a prendere gli assi della prima sotto figura della seconda della terza e così via e andiamo a prendere i centri qui abbiamo dieci centri quindi alla prima iterazione andiamo a prendere center in center sarà il primo di questi centroidi, il secondo, il terzo eccetera. Quindi a ogni asse, a ogni sottofigura sarà associato un centroide. Quello che facciamo è settare di ognuna di queste figure, gli diciamo non vogliamo i tick, cioè sugli assi non vogliamo che ci siano il vuoto, i tick sono diciamo quelle scansioni grafiche dell'asse delle x e dell'asse delle y, e andiamo a invocare molto semplicemente image show. Image show cosa gli passiamo? La figura che vogliamo che venga mostrata, che è il centro, alla prima iterazione il primo centroidi, alla seconda il secondo centroidi e così via. E poi gli diciamo utilizza come mappa di colori una mappa di colori binaria, o nero o bianco vedrete, interpolation è un altro parametro adesso non ci interessa più di tanto andare a vedere. E quello che succede, se lo mando in esecuzione, guardate cosa succede, lui ha generato dieci figure, come le ha generate? Questa è la prima sottofigura, la seconda, la terza, eccetera. Lui ha preso il primo centroide, l'ha organizzato, è un vettore da 64 elementi, l'ha organizzato in una matrice 8x8 e ha prodotto il rendering. E questo se lo guardate, questa non è una figura che esiste nel passetto che noi abbiamo scaricato, però vedete sono tutte distinte. Quindi lui ha identificato dieci punti in questo spazio a 64 dimensioni attorno ai quali aggregare dei, diciamo associare a ognuno di questi punti uno dei punti del dataset, uno o più dei punti del dataset. E se andate a vedere sono tutti diversi questi, cioè queste di fatto sono delle rappresentazioni sintetiche che non esistono in quel dataset, però dei vari centri di quelle nuvole di punti. e vedete c'è lo 0, c'è il 5 direi il 4, questo è probabilmente un altro 5, non lo so, non è proprio una cosa ovviamente, c'è un 1, un 2, cioè se vedete ha fatto un ottimo lavoro comunque anche nella ricostruzione, cioè lui ha identificato dei centroidi che è ragionevole come ha operato. e vedete che avete qui la dimostrazione che lui crea delle cose baricentriche che sono diverse tra di loro, che sono rappresentative probabilmente di cose diverse che ci sono all'interno di quel dataset. Questo si vede bene in questa figura. Non so se abbiamo tempo per vedere tutto di questa esercitazione, semmai finiamo un'altra volta. Quindi lui ha creato questi centroidi. Adesso l'obiettivo è vedere, proviamo a fare un gioco, proviamo a vedere come funziona come classificatore. Cioè lui ha creato 10 raggruppamenti, quindi implicitamente a ognuno di quei 10 raggruppamenti noi possiamo dare un'etichetta, giusto? E poi possiamo andare a confrontare l'etichetta risultante con l'etichetta del dataset. E tutto questo lo abbiamo fatto senza andare a vedere l'etichetta del dataset, quindi in maniera non supervisionata. Quindi chiaramente non ci possiamo aspettare un'accuratezza che sia quella di un classificatore supervisionato. Però siccome sembra fare delle cose ragionevoli, proviamo a capire se effettivamente questi raggruppamenti hanno un senso, andando a usarlo in maniera impropria, non si usa come classificatore il cluster, si usa come, vi ho detto, fase predominare, per cui a quel punto io potrei, la cosa bella, è che io potrei andare a prendere ognuno di questi 10, che sono 10 baricentri, e il mio dataset che era 1797 punti, ridurlo a 10 punti. Perché io potrei andare a prendere questo come punto rappresentativo del cluster, che rappresenta il 2, questo come punto rappresentativo del cluster che rappresenta lo 0, e se dovessi fare per esempio una ricerca di similarità, cioè se mi arriva una nuova cifra scritta a mano e la voglio dire, per esempio, voglio costruire un altro classificatore, non ve ne ho parlato, ma che c'è, sono i classificatori per esempio cosiddetti nearest neighbor, cioè prendono un dataset e dicono io ho un altro oggetto, qual è l'oggetto più simile a quello che tu hai nel dataset? Allora io per costruire una cosa del genere dovrei definire una metrica di similarità, un modo per confrontarli, vabbè, comunque ammesso che questo si possa fare e si fa, dovrei prendere questo oggetto e confrontarlo con tutti i 1797 del mio dataset. Se io ho fatto prima il clustering, posso semplicemente prendere questo oggetto e confrontarlo con 10 centroidi e dire è più vicino a questo ad esempio, e dire ok, per me è un oggetto simile a quelli della classe del raggruppamento zero e ho risparmiato 1697 confronti. Ecco, per esempio il clustering, per cosa può essere utile. Ok, in questo caso però andiamo a vedere, gli facciamo fare anche la classificazione, ok? Allora il problema della classificazione è un po', è facilissimo andare a vedere come funziona, però c'è un aspetto un pochino antipatico che è questo. Il punto è che K-Means chiaramente non sa l'identità del cluster, cioè le etichette che lui trova possono essere permutate, l'abbiamo visto qua, cioè il cluster che lui chiama zero è associato a questo baricentro e il cluster che lui chiama nove, otto scusate, mi è associato a questo. Quindi se io prendo quelle etichette e vado a dire ah sei stato bravo a beccare tutti gli zero e lui per zero mi interpreta questo e quindi ovviamente sbagliamo tutto. Allora quello che dobbiamo fare, dobbiamo fare una piccola, un piccolo preprocessing per cui dobbiamo riportare questi tutti qua, per esempio, perché sono gli zero, questo va nel due e così via. Per fare questo c'è questa cella di codice che c'è qui dietro che fa una cosa di questo tipo. Allora prima vi faccio vedere, vi ho messo sotto anche l'esempio perché è un po' antipatica da capire. Allora leggiamola insieme tutte e due, vediamo intanto il codice. Allora qui ho utilizzato, ho dovuto prendere SciPy perché ho importato la moda, la moda è proprio una funzione statistica che sapete, l'avrete fatta, che vi dice il valore più frequente di un insieme di elementi di un dataset. Qui che cosa ho fatto? Ho definito un vettore che ho chiamato YPRED che è un vettore all'inizio di zeri che ha la stessa dimensione del vettore YK-means. Allora YK-means l'abbiamo talcolato qua sopra, vi ricordo. Sì, ecco, è questo qua. Il risultato, a ogni punto viene associato il suo cluster. Quindi questo vettore YPRED ha esattamente la stessa struttura di YK-means, cioè sono 1797 valori, però all'inizio tutti zero, ok? Va bene? Se io vado a fare, questo sarebbe YPRED, se vado a fare la stampa di questo vettore, vedete è 1797. Ok? È un vettore a 1797 ent. Bene. Ok. Dopodiché faccio un ciclofor che fa dieci iterazioni e costruisce quella che viene chiamata una maschera, cioè è un vettore booleano in cui vado a vedere di quel vettore di centroidi quali sono tutti quelli che il cluster mi ha detto etichetta zero, etichetta uno, etichetta due e così via. Quindi alla prima iterazione dico vai a prendere tutte le posizioni in cui l'etichetta vale zero, poi uno, eccetera, due, eccetera. E quello che fa, prende, questo è un vettore booleano, lo usa per indicizzare YPRED, cioè va a prelevare le mie predizioni che corrispondono all'inizio allo zero, poi all'uno, poi al due, adesso cerchiamo di capire meglio con l'esempietto che vi ho riportato sotto. E di quello va a vedere il valore più frequente di digits di target di Musk. Perché fa così? Perché fa un ragionamento di questo tipo. Adesso questo, ripeto, serve solo per aggiustare, per far matchare l'output del K-means con le etichette. Allora lui fa un ragionamento di questo tipo. Supponiamo, guardate, ve l'ho riportato qui per, sperando di fare una cosa un pochino più comprensibile del solo codice. Allora, supponiamo che di avere un vettore delle predizioni, questo è un esempio molto semplificato, chiaramente, in cui vedete, avete in questo caso dieci predizioni, quindi non è più ovviamente 1797 punti, ma sono 10 punti. All'inizio io ho creato un vettore, scusatemi, questo è y, se lo prendo, ok, all'inizio l'ho messo a tutti zero, ok? Questo è il vettore che io vado a creare all'interno di quel ciclo for. Questo è il vettore delle etichette vere che noi conosciamo. Supponiamo che sia i primi tre punti classe zero, poi classe uno e classe due. Cosa succede? Questo è il vettore che produce invece l'algoritmo K-Mings, che magari ha identificato i cluster, vedete correttamente, ma li ha chiamati con delle etichette diverse. Allora io devo dire, devo trovare il modo di andare a rimappare questi, spostarli, diciamo, nel loro posto, ok? Diciamo, dire che questi non sono più degli uno, ma degli zero, questi saranno degli uno e questi devono diventare gli due, ok? Allora ragioniamo sul cluster di questi due, ok? Allora io creo quella che è chiamata una maschera. All'inizio vado a vedere qual è il, dov'è che Y di K-Mings mi ha prodotto degli zero e Y di K-Mings mi ha prodotto, produco un booleano che è questo che mi dice dove è true. Ok, vai a prendere i valori true secondo questa maschera. Allora, Y true secondo questa maschera è questo vettore 2, 2, 2, 2, 2. A questo punto, vedete, vado a dire vai a prendere questo e vai a vedere qual è il valore più frequente perché è ovvio che io mi aspetto che il clustering, magari qui non ci sono errori, magari uno di questi è un 1, però il valore più frequente è il 2 e dico, ok, l'etichetta secondo il cluster, il K-Mings quando lo uso come classificatore in maniera impropria è quella, il valore che lui mi ha dato più frequentemente e questo diventa 2 e quindi a questo punto io vado a dire ok, Y pred di maschera è 2 e vado a sostituire 2 la stessa cosa la ripeto per l'1 e per lo 0 e alla fine ottengo qualcosa che è Y pred che sarà 000 111 222 e questo è quello che posso andare a confrontare effettivamente poi con con l'etichetta vera questo è un giro un pochino complicato però spero di avervelo spiegato che fa questo cicloforo ok adesso non c'è tempo temo per andare avanti fatemi vedere quanto manca però mi dispiacerebbe andiamo a vedere dai cosa succede dai facciamo un piccolo sforzo ce la facciamo in 10 minuti a vedere che cosa succede se no lo lasciamo in sospeso adesso abbiamo tutto quello che ci serve l'unica cosa che dobbiamo fare è importare il nostro QR Fc score e prendiamo la verità digit.target le etichette vere e le etichette predette che sono quelle che ho calcolato adesso con questo metodo a partire dal k-mix rimacciando guardate cosa viene fuori 74% 74% lui li indovina senza avere visto nessuna etichetta cioè uno strumento non supervisionato che riesce comunque a fare un buon lavoro senza esser stato addestrato da una supervisione e potete anche andare a fare ovviamente tutta l'analisi delle matrici di confusione quindi cercare di capire che cosa effettivamente succede per cui se io vado a importare la confusion matrix vi ricordate confusion matrix display e vado a fare questa cosa vado a creare la matrice di confusione a cui passo di nuovo la verità cioè quelli che sono le effettive etichette e quello che ha il risultato del mio clustering che sto impropriamente usando come classificatore e vado a fare il display a partire da queste predizioni di questo l'abbiamo vista già a proposito dell'esercitazione sulla regressione logistica quindi non ci stiamo di nuovo sopra l'abbiamo già incontrato quindi e di nuovo faccio il calcolo dell'accuratezza ovviamente ritrovo il 74% di accuratezza ma soprattutto ritrovo questa matrice di confusione e questa matrice di confusione vedete è molto informativa perché lungo la diagonale vi dice tutte le le predizioni corrette vi ricordo che sulla riga ci sono le etichette vere sulle colonne c'è associato le predizioni vedete dove è che sbaglia parecchio sbaglia per esempio l'uno che viene spesso e volentieri confuso con l'8 non è del tutto irragionevole se uno fa l'8 molto stretto ci sono 64 pixel quindi la risoluzione è bassa un altro caso frequentemente appunto di confusione è il 9 con il 3 effettivamente un 9 e un 3 insomma possono essere confusi tra di loro vengono sbagliati anche 8 con 3 anche qui è irragionevole secondo me qualche altro errore ma pochi di un 8 con un 2 insomma avete un quadro della situazione però riesce a fare delle cose egregie non guardando solamente cioè guardando solamente ai dati senza avere l'idea appunto di cosa voglia dire un'etichetta là il confronto ovviamente che potete fare è a questo punto dire ok abbiamo visto questa è veramente l'ultima cosa che vi faccio vedere rifacciamo la stessa cosa utilizzando un classificatore che abbiamo visto prendiamo lo stato dell'arte dei classificatori lineari probabilmente che ora o la regressione logistica o l'SVM diciamo uno di questi due prendete un un SVM la importate ok questo l'abbiamo fatto un'esercitazione quindi non c'è bisogno di tornarci troppo sopra fate un train test split quindi lì siccome è supervisionato devo dire prendo una parte e la dedico al training poi una parte verifichiamo qual è l'accuratezza e x è digits data y è digits target quindi di nuovo la x e in questo caso abbiamo l'etichetta e questa volta l'etichetta le usiamo per addestrare l'SVM quindi x train x test y train y test sono il risultato dell'invocazione della funzione train test split a cui passo i miei 1797 punti da 64 qui in questo caso ho diviso 50 e 50 metà data set di addestramento metà andiamo a vedere cosa succede di test il mio modello è una SVM questa è la classe support vector classifier kernel lineare quindi modello lineare questo è il fattore di regolarizzazione abbastanza alto quindi diciamo sto cercando di andare verso un margine abbastanza robusto quindi abbastanza non troppo soft model fit gli passo x train y train fa l'addestramento predizione con il metodo predict sul data set che lui non ha mai visto stampa della matrice di confusione a cui passo y test e y pred y test è la verità y pred è di quei 50% dei punti la mia previsione facciamo il display della matrice di confusione e calcoliamo l'accuratezza con accuracy score risultato 97% di accuratezza e questa è la matrice di confusione chiaramente l'SVM fa un ottimo lavoro riesce a riconoscere quasi perfettamente queste semplici immagini scritte a mano 97% e vedete fa un po' di errori però ecco potete andare a vedere dove li fa questi pochi errori li fa scambiando per esempio un 9 per un 5 qualche altro errore qua e là lo fa scambiando un 4 per uno 0 riesce a fare un ottimo lavoro e ovviamente la il clustering più basso eravamo a 70 78% non mi ricordo 74% è chiaro che c'è c'è margine però non avevamo usato le etichette quindi diciamo qui abbiamo un vantaggio competitivo di partenza ho riuscito a farvi vedere tutto quello che avevo in mente di farvi vedere direi che sono a disposizione per domande se avete domande ovviamente se no ci fermiamo qui c'è parecchia roba oggi in questo notebook quindi magari prendetevi tempo per darci un'occhiata con calma e provare a vedere un po' di cose però ho preferito farvi vedere più cose così avete l'occasione poi di approfondirle di riprenderle per quello che pensate sia sia opportuno per voi va bene ma intanto se non c'è altro intanto blocchiamo la registrazione allora grazie a tutti