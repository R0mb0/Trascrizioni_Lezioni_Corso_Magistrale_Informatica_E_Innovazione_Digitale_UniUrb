Allora, buongiorno. Dunque, oggi vediamo l'esercitazione riguardante il modello di apprendimento con supervisione che abbiamo studiato un po' di tempo fa nelle lezioni di teoria. E abbiamo imparato che cos'è un modello di regressione logistica. È un classificatore, vi ricordo, di tipo lineare che supporta sia la classificazione binaria che quella multiclasse. E oggi ne vediamo un utilizzo mediante la libreria PsychicLearn che, come vi anticipavo, offre un supporto estremamente ricco e flessibile per diverse tipologie di modelli machine learning. Quindi la prima cosa che facciamo è costruire un notebook Jupyter, come quello che appunto trovate tra l'altro sulla piattaforma del blended learning, quindi potete anche andarlo a scrivere voi direttamente e incominciare a inserire nella prima cella tutti gli import di quelle che sono le librerie necessarie. Quindi importeremo PsychicLearn, ovviamente, poi importeremo NumPy, da qui avanti, vi faremo riferimento con il nome mnemonico NP, per comodità nostra. Poi importiamo Matplotlib, in particolare il pacchetto PyPlot, come PLT lo chiameremo, per poter effettuare i grafici. Seaborn è un'ulteriore libreria grafica che ci può essere utile per creare dei grafici un po' più dall'aspetto estetico, un pochino ancora più gradiboli, ma già Matplotlib fa dei grafici, a mio parere, particolarmente professionali, quindi dovrebbe essere già sufficiente. Poi all'interno di PsychicLearn andiamo a importare il modulo dataset che contiene dei dataset che possono essere utilizzati come prova per testare i vari algoritmi e importiamo il modulo LinearModel. Il modulo LinearModel è un modulo di PsychicLearn che contiene le classi che permettono di costruire modelli lineari. In particolare, tra questi modelli lineari, quindi, ci sono ad esempio la regressione logistica, che noi andiamo a importare qui, e vedremo anche, questo lo vedremo la prossima volta, l'SVM. Cioè, la prossima esercitazione è terminata questa. Non so se abbiamo tempo oggi, se la iniziamo, ma è quella immediatamente successiva. Poi, c'è un modulo Matrix. Il modulo Matrix è un modulo dedicato alle metriche. È un modulo di PsychicLearn in cui trovate implementate diverse metriche per, ad esempio, valutare i modelli. Per esempio, il calcolo dell'accuratezza. E qui importiamo questa classe che si chiama 0-1 loss, che serve per fare quello che intuitivamente, forse, alcuni di voi già possono immaginare, cioè calcolare qualcosa di tipo 0-1, quindi nell'ambito di una classificazione, andare a vedere se la classe assegnata a un determinato campione è corretta oppure no. Poi c'è un altro modulo, si chiama Model Selection. Qui dentro trovate un'utilità, una funzione estremamente utile, che si chiama Train Test Split. Train Test Split è un modulo che vi permette di gestire quello che viene chiamata la suddivisione, lo split tra il dataset in training e testing. Come vi ho detto più volte, noi facciamo l'addestramento di un modello, lo facciamo a partire da un dataset di addestramento, che è appunto il dataset di training. Poi però vogliamo testarlo in condizioni che siano il più possibile vicine a quelle reali operative. Questo significa che prima di mettere il nostro modello effettivamente in un contesto operativo reale, vogliamo avere la possibilità di testarlo in condizioni più possibili realistiche. E per fare questo quello che si fa tipicamente è prendere il dataset di addestramento e suddividerlo in due parti. Una frazione che è a nostra scelta arbitrariamente, possiamo decidere di assegnare per esempio il 30% del nostro dataset, dei campioni del nostro dataset al test, e quindi addestriamo il modello sul 70% e sul restante 30% che il modello non ha mai visto, questo è un requisito fondamentale, andiamo poi a fare il test per vedere che cosa succede in una condizione che dovrebbe il più possibile essere, riprodurre una situazione di tipo reale. Perché? Perché a quel punto noi testiamo il modello su qualcosa che lui non ha mai visto. Quindi questa è la logica. Poi vedremo come viene utilizzato. Qui c'è un pacchetto colors che serve, appunto da cui importiamo queste funzioni, queste classi di state color map, che servono per fare un po' di cose sui grafici, sulle mappe di colori. Queste sono meno strettamente indispensabili. Ok, andiamo a vedere che cosa facciamo qui. Qui facciamo una cosa che avevamo già visto la volta scorsa quando abbiamo fatto l'esercitazione sulla regressione. Importiamo un dataset. Questo dataset è il dataset dei fiori di Iris, quindi abbiamo 150 punti, ognuno dei quali è etichettato secondo tre classi, fiori di Iris virginica, fiori di Iris setosa o versicolor. Questa è una struttura dati, immagazzinata questa variabile che è una struttura dati sua interna, però questa struttura dati sua interna è di fatto un record in cui abbiamo un campo data e un campo target. Quindi iris.data è un array numpy a cui possiamo accedere. Infatti qui vi ho fatto stampare le prime dieci righe e tutte le colonne di questo array. E qui vi ho fatto stampare anche il campo target di questo dataset. Poi oltre al campo target abbiamo anche il campo target names. Quindi l'esito di questi tre print sarà quello di stampare esattamente e quindi vediamo un attimo cosa succede. Sarà di stampare, vi dicevo, le prime dieci righe, i target di tutti i 150 punti e i loro nomi. Quindi vedete, io qui ho fatto stampare le prime dieci righe, quindi questo è un array numpy, quindi iris.data è un array numpy che contiene quattro colonne che vi ricordo sono le quattro feature di questo dataset che sono la larghezza del petalo, la lunghezza del petalo, la larghezza del sepalo e la lunghezza del sepalo del fiore. Quindi queste sono le misure e per ognuno di questi vedete che è associata a un'etichetta 0, 1 o 2 in cui 0 rispettivamente dovrebbero essere i fiori di iris setosa, 1 in versicolo e 2 di irginica. E queste sono le tre classi. Ok? Benissimo. Allora, quello che andiamo a fare adesso è andare a costruire un classificatore. Un classificatore che in prima istanza va a costruire un classificatore binario, prima cosa che andiamo a fare, e quindi andiamo a costruire un classificatore binario a partire da questo dataset che in realtà ha tre classi e come facciamo? Molto semplicemente andiamo a vedere sorta di one versus all nel senso che andiamo però non per costruire il classificatore multiclasse ma semplicemente per andare a diciamo artificialmente a utilizzare quel dataset come se fosse un dataset per una classificazione binaria. Ad esempio mettiamo classe 1 tutti i fiori di iris verginica e classe 0 tutti gli altri. Quindi partiamo da un problema di classificatore binario e iniziamo con una sola feature. Quindi andiamo a prendere come sola feature, ad esempio, e costruiamo quindi il nostro dataset di addestramento X andando ad accedere a iris campo data è la stessa cosa che iris punto data come abbiamo scritto prima in cui andiamo a prendere tutte le righe quindi tutte le 150 righe e andiamo a prendere la colonna 3 che è quella corrispondente alla larghezza del petalo. Quello è il nostro, è la nostra unica feature. Quindi all'inizio partiamo da un'unica feature e andiamo a vedere che cosa succede. Cioè un classificatore e vogliamo costruire, quindi l'obiettivo è costruire un classificatore che distingue i fiori di iris virginica da tutti gli altri basandosi solo sulla larghezza del petalo. Ovviamente il valore delle etichette e delle label sarà iris di target. Andiamo a vedere, iris di target uguale a 2. Ok. E dove è uguale a 2 andiamo a selezionare i fiori di iris virginica. Quindi il risultato di iris di target uguale a 2 è un boolean, poi viene convertito con questa funzione s type in un intero, quindi la dove era true diventa 1, la dove era false è 0. Quindi il risultato è un array numpy y che ha degli 1 in corrispondenza dei punti del dataset che sono fiori di iris virginica. Ci siamo fin qui? Questo è quello che facciamo alla riga 4 e alla riga 5. Quindi costruiamo un vettore x e un vettore y che sono dei vettori perché x è solamente la colonna numero 3 del nostro dataset e y sono le corrispondenti etichette con l'accorgimento di selezionare i fiori di un solo tipo e assegnare all'altra classe quelli degli altri due tipi. Anche qui potete ovviamente poi per prova mescolare le carte e vedere cosa succede cambiando se andate a prendere una sola feature, cambiando l'etichetta e quindi anzi, il mio invito è provare a fare queste prove per vedere cosa succede e qual è il comportamento alla fine del classificatore. Dopodiché, vedete, alla riga 6, quello che dovete fare è creare un oggetto che qui io ho chiamato logreg, che richiamano chiaramente come volete che sia qualcosa auspicabilmente di significativo e invocate la classe Logistic Regression. La classe Logistic Regression ha diverse tipologie di input, di parametri. Il mio consiglio è, anche qui, la documentazione di Scikit-Learn se andate ovviamente sul web la trovate ed è una documentazione molto ricca, quindi se voi andate a vedere avete accesso a tutte le informazioni per cui vi dice la classe Logistic Regression, poi magari l'apriamo insieme e ci vediamo un'occhiata, però vi dice la classe Logistic Regression che cosa fa, quali tipi di metodi supporta, quali tipi di parametri dovete passare come input, alcuni, diciamo, potete utilizzare chiaramente il default finché non sapete bene come muovervi, però anche qui andare a provare e vedere che cosa succede è secondo me interessante. Qui vi ho riportato quando cambiate alcune di queste, diciamo, di queste, impostazioni. Qui vi ho riportato un virgolettato in cui nella documentazione dell'ultima versione dovrebbe essere la 1.52 vi dice che cosa, che vedete io qui ho specificato due campi, uno penalty che ho selezionato non, penalty non significa che non vogliamo regolarizzazione, vi ricordate, vi ho detto i termini di regolarizzazione possono essere aggiunti alle funzioni di costo per diversi scopi, qui non vogliamo regolarizzazione quindi vogliamo minimizzare la funzione di costo nativa che è una cross entropy se non ricordo male, però ho dovuto specificare un solver, i solver sono appunto gli algoritmi che vengono utilizzati per fare la minimizzazione della funzione di costo. ce ne sono una serie a disposizione, in questo caso quello che riporta la documentazione è che questo che io ho specificato è l'EBFGS, è un algoritmo di ottimizzazione che approssima questo algoritmo che prende il nome di Broiden, Fletcher, Goldfar e Shannon che sono quelli che l'hanno sviluppato, che appartiene alla categoria dei metodi quasi newtoniani, ora noi non abbiamo visto i metodi quasi newtoniani, sono, ve lo dico io, una evoluzione dei metodi del secondo ordine, del metodo di Newton che approssimano il comportamento dei metodi del secondo ordine cercando di risolvere il problema, vi ricordate, vi ho detto il problema del secondo ordine, il problema che devono tenere in memoria una matrice molto grande che è la matrice siana, potenzialmente molto grande, diciamo, se scaliamo di dimensione di numero di punti questo può essere un problema di memoria e quindi diciamo cercano di risolvere alcune di queste, di questi colli di bottiglia e vengono chiamati appunto metodi quasi newtoniani. Non ci interessa adesso entrare nel dettaglio di come funziona, anche se sarebbe sicuramente interessante. Diciamo che quello che vi dice la documentazione è che questo tipo di solver, di ottimizzatore, può gestire un ampio range di training di tipo diverso e quindi è il solver che loro vi propongono come default. Però ce ne sono altri che potete testare, poi ci diamo magari un'occhiata insieme. Allora, la prestazione dice che però ovviamente ha i suoi limiti, nel senso che per esempio se utilizzate dei dataset in cui non avete scalato minimamente, quindi sono le features sono in ordine di grandezza diverse, oppure dataset in cui avete delle feature categoriche di tipo one hot in code, vedete, l'abbiamo visto, in cui alcune categorie sono più rare di altre, ecco, questo è un algoritmo di ottimizzazione che può avere qualche problema, quindi magari ci sono delle alternative nella libreria che potete provare e utilizzare. Ok? Questo è come un esempio di un parametro, ma ce ne sono diversi, poi magari diamo un'occhiata insieme, ripeto, alla documentazione, però il mio invito è farlo ovviamente in autonomia di routine, cioè imparare a guardare la documentazione per capire cosa c'è, quali sono i parametri che prende in input una funzione oppure una classe per creare un oggetto, quali sono i metodi che supporti, eccetera. Va bene? Allora, adesso cerchiamo di andare a capire che cosa facciamo. Una volta che abbiamo creato l'oggetto, quindi questo è un oggetto di tipo regressione logistica, questo oggetto supporta il metodo fit che è l'equivalente dell'invocazione dell'algoritmo di addestramento, cioè noi con logreg.fit stiamo dicendo vai a fare l'addestramento su questo oggetto, quindi il metodo che supporta l'addestramento è ovviamente un metodo che ha, e anche qui potete andare a vedere la documentazione, ma intanto ve lo dico io, ha due input, in questo caso, vedete, tra l'altro c'è anche la guida context density che vi dice che prende come input x e y, se non poi eventualmente prende anche il sample width, cioè potete andare addirittura a pesare i campioni, come vi ho detto con i metodi pesati, però è sufficiente che gli passiate x e y, dove x è un array che deve avere la shape, numero di campioni, numero di feature, ed è il training vector, è l'array come l'abbiamo costruito noi, e y è un altro array di dimensioni, di shape, numero di campioni, virgola, che dice quali sono le classi. Quindi noi stiamo dicendo vai con quell'ottimizzatore a costruire un algoritmo di regressione logistica che è quello che abbiamo definito accedendo alla classe precedente, tra l'altro vedete che anche qui, volendo, ci sono tutta una serie di opzioni, dopo le vediamo meglio, forse nella nella nella guida, direi, però anche qui, volendo, vedete, riuscite a vedere parecchie cose, perché vi dà tutta una serie di informazioni, vedete, vi dice che la regressione logistica è questo classificatore, questo è la stessa classe, vi permette di gestire il caso multiclasse, vedete che utilizza, vi dice, un one versus rest, uno schema one versus rest se l'opzione multiclasse è settata a OVR, altrimenti usa la cross entropy loss se l'opzione multiclasse è multinomiale, e questo noi l'abbiamo visto, le due differenze, abbiamo visto l'approccio one versus all, one versus rest, e abbiamo visto l'approccio invece, quello diciamo, in cui vengono simultaneamente creata la regola di fusione, vedete che vi dice quali sono, ci sono altri solver, alcuni dei quali, questa è una sintesi, tra l'altro, esattamente, non è neanche una sintesi, credo che sia proprio la stessa, a cui accedete andando con il mouse sopra una determinata classe, vi dà direttamente la documentazione della funzione, quindi se no, sulla pagina web di scikit-leap trovate la cosa equivalente, vedete che vi dice che questa classe implementa la regressione logistica regolarizzata, utilizzando o questa libreria LibLiner, che è un'altra libreria di solver, vedete, Newton-CG sta per metodo Newton del gradiente coniugato, un altro metodo del secondo ordine per come ottimizzatore, SAG e SAGA, SAG sta per Stochastic Average Gradient, è un metodo del primordine stocastico, vi ricordate quando vi ho parlato dei metodi batch, vi ho detto no, possiamo fare un batch di punti e poi aggiornare i pesi, eccetera, eccetera, ecco, sono metodi che utilizzano questo tipo di logica, SAGA è una versione aggiornata adattativa di SAG e poi c'è l'LBFGS che è quello che abbiamo applicato, che è quello che vi ho detto prima, che è un cosiddetto quasi newtoniano. Poi ci sono una serie di altre informazioni che potete applicare eventualmente la regolarizzazione e qui vi dice i vari parametri, vedete, la penalty, potete specificare una regolarizzazione tipo L1, L2, quindi andare in cerca per esempio di un vettore dei pesi che sia più o meno sparso, anche qui ne abbiamo parlato a proposito della selezione delle feature, poi ci sono anche altri metodi che non abbiamo visto come l'EasticNet, eccetera, se mettete non come abbiamo fatto noi non aggiungete nessuna penalty, ElasticNet le aggiunge tutte e due, in realtà è una combinazione delle due, va bene adesso. Ve lo dico perché anche queste cose le potete esplorare volendo in un'ottica di un ipotetico progetto, cioè sono tutte cose, noi abbiamo visto chiaramente lezione di teoria un'impostazione generale, poi vedete come queste librerie vi permettano di specializzare alcune cose e se uno sceglie per esempio un dataset su cui andare a fare classificazione e prova a vedere che cosa succede cambiando il solver, cambiando, mettendo, introducendo magari dei termini di regolarizzazione oppure no, cercando di fare selezione delle feature, ha tantissime manopole su cui agire che gli permettono di andare a vedere cosa cambiano i risultati, se cambia qualcosa e se cambia qualcosa cercare di capire perché cambia. Quindi secondo me andare a prendere un caso in cui avete un classificatore di base come può essere questo, la regressione logistica e provate a variare alcune di queste leve, di queste manopole, può essere già una buona idea per provare a costruire un idea, un progetto. Va bene, poi qui ci sono un po' di altre cose, tantissime informazioni, per cui anche qui il mio consiglio provate a andarvelo a vedere e a leggere. Vedete, i solver, l'abbiamo già detto, sono questi, vedete, vi propone sei solver alternativi che sono gli algoritmi di ottimizzazione, poi vi suggerisce anche, dice per esempio per dataset piccoli libline era una buona scelta, mentre saga e saga sono più veloci, perché sono quegli algoritmi stocastici del primordine che permettono, vi ricordate, ve l'ho detto, a lezione di andare più veloce. Provei multiclasse invece potete usare solamente questi. Benissimo, allora, poi altre cose che potete specificare, il massimo numero di interazioni, il solver porta, no, quante interazioni gli diamo per arrivare a convergenza, sono metodi, vi ricordate, di ottimizzazione locale quando abbiamo parlato dei metodi di ottimizzazione e abbiamo visto che questi metodi sono tutti metodi di tipo iterativo. In questo caso il default è 100, quindi noi non gli abbiamo specificato nulla, vuol dire che lui va a farci 100 iterazioni, però potete allungarle, accorciarle, vedere cosa succede, cambia qualcosa, qualche volta no, qualche volta sì. Potete specificare il multiclasse. Auto di default lui riconosce se il problema è di tipo binario o multiclasse e applica eventualmente una scelta, cioè se è binario lui lo riconosce e se invece è multiclasse lui seleziona in automatico, vedete, allora seleziona OVR se il dato è binario e altrimenti seleziona il multinomiale, quindi applica direttamente la funzione di costo cross-entropi di tipo multinomiale. Potete specificare un certo livello di verbosità, eventualmente il numero di CPU su cui andare a parallelizzare, anche qui ci sono ulteriori cose abbastanza interessanti che si possono andare a vedere, ci sono degli attributi della classe, anche qui non c'è tempo ma è sicuramente molto interessante eventualmente andarli a vedere. Qui ci sono dei riferimenti bibliografici e c'è anche un esempio su come funziona. L'esempio però direi che facciamo prima andare a vedere noi nel nostro, però ecco, questo è giusto per dirvi quanto può essere ricca questa documentazione e darvi spunti per sicuramente apprendere qualcosa di più e impratichirvi. benissimo, allora nel momento in cui andiamo a fare il fit facciamo l'addestramento con i parametri che abbiamo specificato su. A questo punto una volta che abbiamo fatto l'addestramento mandato in esecuzione che cosa succede? Cosa rimane da fare? Beh, possiamo andare a fare qualche predizione e abbiamo addestrato il nostro modello questo logreg supporta oltre a fit anche se andate a vedere i metodi qui vediamo un po' se riusciamo non riesco a tornarci su va bene comunque dopo ve lo rifaccio vedere supporta chiaramente come tutti gli stimatori così vengono chiamati di scikit-learn quindi i modelli di scikit-learn supportano il metodo fit e poi il metodo predict il metodo predict vi permette di accedere alle funzionalità una volta che avete congelato il modello dire che se ho un nuovo punto qual è la predizione che mi fa questo modello e allora per vedere testare questo tipo di funzionalità qui quello che ho fatto è preparare un array che ho chiamato xnew scelto come? beh semplicemente mille punti tra 0 e 3 ok poi viene fatto un reshape questo è reshape meno 1,1 cosa vuol dire? che questo genera un array che ha dimensione lo andiamo a vedere mille mille virgola il risultato vedete io ho generato mille punti tra 0 e 3 ok dove 3 è la dimensione diciamo del petalo perché stiamo lavorando solo su questo quindi l'ho scelto il range compatibile ovviamente con e poi ho fatto questo reshape e come l'altra volta l'avevamo visto nella precedente esercitazione avevamo fatto un new access vi ricordate? aggiungiamo una dimensione perché gli array se no quello che genera lui è mille virgola ok e invece così vedete gliel'ho fatto stampare è mille virgola 2 questo punto gli ho fatto stampare la shape gli ho fatto stampare i punti da 1 a 10 quindi che sono questi in realtà ok i punti di x new sì potevamo fare anche del 0 quindi almeno prendiamo anche il primo che è più sensato eccoci qua i primi dieci punti che sono questi ok un attimo però non mi torna una cosa scusate ah ecco erano questi non li visualizzavo ecco erano qua perfetto questi sono i primi questi come? la dimensione è mille esattamente no è mille vediamo un po' quello che abbiamo sì no certo però io gli ho fatto torno un attimo su nel codice allora noi siamo qua io gli ho fatto stampare 1000,1 e la shape una volta che ho fatto questo reshape che ho aggiunto l'asse se no sarebbe stato 1000,1 adesso è 1000,1 sì sì perché noi lavoriamo con una sola feature quindi gli dobbiamo dare solamente una caratteristica una feature ok gli ho fatto stampare i primi dieci punti e sono questi qui vedete sono questi che ho appena questi qua per intenderci ok va bene questi sono i primi dieci punti lui ne ha creati 1000 tra 0 e 3 equispaziati ci siamo fin qui dopodiché qui facciamo un'altra cosa che è questa dopo aver fatto la stampa andiamo ad accedere ad un altro metodo c'è il metodo predict e lo vediamo tra poco però prima di fare questo guardate c'è anche un altro metodo che si chiama predict proba che sta per predict probability che è nulla di più che vi va a fare la predizione quindi qui prende questo vettore di 1000 punti per ognuno di quei 1000 punti dice ok se tu avessi un fiore di lunghezza 0,003 di che classe è? se tu avessi un vettore di lunghezza 0,006 di che classe è? se tu avessi un fiore di lunghezza di non lunghezza cos'è la larghezza del petro adesso non mi ricordo qual è la caratteristica che abbiamo selezionato pari a 0,009 a quale classe apparterebbe? questa è la logica che stiamo costruendo e con il metodo predict proba andiamo ad accedere alle probabilità vi ricordate che la regressione logistica con il fatto che abbiamo la possibilità con la funzione logistica di spalmare quei valori tra 0 e 1 che sono l'esito del prodotto scalare tra x trasposto e w poi lo facciamo passare attraverso la funzione sigmoidale e otteniamo un valore tra 0 e 1 sopra 0,5 siamo in una classe sotto 0,5 siamo in un'altra classe e il valore che otteniamo è di fatto interpretabile come una probabilità siccome in questo caso poi è binaria la classificazione se una classe gli diamo 0,7 l'altra c'è ora 0,3 ovviamente su quel singolo punto e qui otteniamo esattamente con y proba questa questo vedrete è un array che ha numero di righe pari al numero di punti numero di colonne pari a 2 che è la probabilità di appartenenza alla prima classe e alla seconda classe poi invece e l'ho chiamato yh per dire y non sopra il cappellino la nostra predizione è il solito predict a cui passo sempre x cioè le predizioni a quel punto cosa sono? è un array di tipo 0,1,0,0,1,1 e così via prima classe e seconda classe a questo punto vi ho fatto stampare la shape di questa del vettore delle probabilità delle predizioni e vi ho fatto anche stampare i primi dieci punti con tutte le righe che sono tutte le colonne scusatemi che sono due e poi sono andato anche dalla riga 5,40 alla riga 5,60 adesso vediamo perché e ho stampato lì anche le corrispondenti predizioni e andiamo a vedere che cosa è successo in questa stampa ci siamo fin qui allora la stampa è quello che abbiamo ottenuto è questo guardate questo è il vettore delle probabilità i primi dieci punti vedete che sono due colonne quindi questa è la probabilità di appartenere alla classe 0 e questa è la probabilità di appartenere alla classe 1 ok? quella classe che vogliamo riconoscere della classe 1 sarebbe posteriormente esattamente dovrebbe essere così se non bisogna andare a vedere sopra come l'abbiamo assegnato ma credo di sì però lo verificate dovrebbe essere abbastanza semplice e vedete che qui le probabilità questi sono due valori di probabilità vedete che qui sono molto sbilanciate questa è qualcosa per 10 alla meno 10 quindi chiaramente è probabilità nulla e questa è 9,9 quindi 99,99% classe 0 tutti i primi 10 punti sono molto sbilanciati però se vado a prendere i punti dal 540 era mi sembra al 550 vedete che le probabilità diventano 0,53 0,46 cioè qui lui dice ancora è classe 0 perché? perché si sta il petalo sta diventando più la dimensione era del petalo la larghezza del petalo credo non mi ricordo però adesso l'ho riandata a vedere era sta aumentando e arrivato a un punto in cui esattamente vedete qui avete la transizione per cui la predizione sarà questa perché verrà leggermente favorita lui non è molto sicuro di quello che sta dicendo dice al 50,6% però probabilità del 49,3% che sia appartenente alla classe 0 vedete quante informazioni molto ricco come come output come possibilità di accedere alle informazioni quindi ve l'ho fatta stampare un po' e vedete vi ho fatto stampare esattamente qui anche quello che succede vedete i primi quattro punti erano se non sbaglio guardate questi sono ancora 0,0,0,0 e 0 anzi scusatemi l'ultimo no è un 1 è dove proprio avviene la transizione ok quindi avete quattro punti 0 e poi cominciano gli 1 e se andate sotto vedete che io gli ho fatto stampare questa è y hat quella che abbiamo quindi è il risultato dell'invocazione del metodo predict questo è il vettore y hat vedete che lui vi dà i primi quattro sono degli 0 e gli altri sono tutti 1 ok poi c'è un 1.6 andiamo a vedere adesso che cos'è così gli abbiamo fatto stampare gli abbiamo fatto stampare quello che abbiamo chiamato il decision boundary cioè il confine decisionale il confine decisionale come viene calcolato calcolato qui alla riga 19 è il valore di x new cioè dell'x della feature in cui il vettore delle probabilità sulla colonna 1 diventa maggiore di 0,5 perché quel vettore di probabilità è ordinato quindi noi gli stiamo dicendo vai a selezionare nella colonna 1 che è la colonna di destra tra quelle due il primo elemento che ha probabilità maggiore o uguale di 0 e alla fine lo lo andiamo a mettere in questa variabile e l'andiamo a stampare vabbè questo non è che lo dovete fare ogni volta chiaramente che utilizzate questo modello però è giusto per farvi vedere qualcosa in più perché lui ci dice farvi capire anche che avete accesso a tutte queste informazioni tipicamente poi non è che uno va a vedere ogni volta il confine di decisione anche perché in questo caso siamo in una dimensione in due può essere utile però insomma oltre non lo è ed è 1,63 cioè nel momento adesso poi vi faccio vedere come è stata generata questa figura nel momento in cui siamo sotto a 1,63 questi sono tutti classificati come classe 0 sopra verranno classificati come classe 1 e avete questo profilo a scalino di questa curva di colore arancione ok questa come l'ho generata e come sono state generate anche le altre due blu e verde che cosa sono ve lo dico adesso con le altre righe qui siamo arrivati qua sono questi plot qui il primo plot è x new asse delle x quindi vedete l'asse delle x è sempre x new quindi qui andiamo a plottare in funzione di x new che cosa il vettore delle probabilità tutte le righe colonna 0 vettore delle probabilità tutte le righe colonna 1 e gli specifico g che sta per green verde quindi la curva verde è quella delle probabilità di avere la classe 1 l'altra curva che è quella blue di default è quella delle probabilità di classe 0 in funzione di x new e poi c'è un terzo plot che è x new in funzione delle previsioni 0 0 0 0 1 e adesso con plt.show andiamo a vedere quello che già in realtà lui ha già fatto prima perché gliel'abbiamo già dato che è questo allora andiamo a vedere la prima curva è quella blue in cui gli abbiamo detto che cosa in funzione prendi per ogni valore di x new quelle che sono le probabilità il vettore delle probabilità della classe 0 che era la prima colonna del vettore y probabilities e vedete che lui ha 100% poi piano piano comincia a piegare vi ricordate qui eravamo andati a vedere arriva a un certo punto a 0,48 49 poi qui c'è la transizione e va la curva verde è la seconda colonna del vettore delle probabilità che ovviamente è complementare duale simmetrica e il vettore la curva arancione è quella del nostro vettore delle predizioni che è sempre 0 fino a che non arrivate al confine decisionale e poi diventa 1 allora questo esercizio ci ha fatto vedere diverse cose perché ci ha permesso di riepilogare anzitutto di vedere come implementare questi questi questo modello utilizzando la libreria ma e quindi ci ha dato l'opportunità di vedere diverse cose gli algoritmi di ottimizzazione che sono supportati che utilizza eccetera ma ci ha anche permesso di riepilogare e diciamo esemplificare anche attraverso questo grafico alcuni concetti che erano quelli di confine di decisione appunto il concetto di probabilità che è intrinsecamente associato all'output di una regressione logistica e così via ci siamo? ok qui ok questo va bene probabilmente si è semplicemente decision by un dare lui lo memorizza come un floating non ci è probabilmente non ci serve invece andiamo alla cella sotto in cui facciamo un po' di altre cose un altro po' di grafici intanto andiamo a vedere questa è una cella puramente viene generato un grafico vedete a partire da dalla stessa figura di prima perché avete il confine di decisione che ha 1,6 e qualcosa e queste due curve quello che cambia sono che una è diventata tratteggiata quella blu quell'altra è diventata verde e ci ho aggiunto ci sono l'ho ripreso in realtà poi tra l'altro sono stati aggiunti sarebbe più corretto dire un po' di cose extra che adesso andiamo a vedere allora la prima cosa è diciamo invochiamo la funzionalità sempre prendiamo plt che è il nostro modulo di matplotlib figure è un metodo che vi permette di generare una figura in cui qui specifichiamo la dimensione ok larghezza e altezza dopodiché andiamo a a fare questa cosa andiamo a plottare vedete qui abbiamo abbiamo sempre il primo parametro è la x e il secondo è la y il primo andiamo a prendere tutti i valori della x qui torniamo al dataset di addestramento ok vai a vedere laddove la x cioè quali sono gli indici del nostro dataset in cui la corrispondente y è uguale a 0 e vai a vedere anche ovviamente quali sono gli indici della y in cui la y è uguale a 0 e plotta questi punti come colore blu b s square come dei dei punti che sono dei quadratini di colore blu e lui fa una cosa di questo genere è andato a vedere nel dataset tutti i punti che appartengono alla classe 0 nel dataset di addestramento e li ha plottati qui ok dopodiché è andato a fare la stessa cosa con i punti invece di classe 1 cioè quelli che hanno etichetta 1 vai a prendere le righe corrispondenti nel vettore nampai x dove y è uguale a 1 e vai a fare il plot di x virgola y e quello devono essere dei triangoli lo vedete da questo simbolo che è un certo circomplesso di colore verde sì allora forse sono di più in realtà forse allora sì hai ragione perché qui ne mette solo 3 6 9 12 sì direi di sì che hai ragione tu perché altrimenti non tornano i ricotti certo probabilmente sono sovrapposti a questo punto ce ne saranno più con la stessa lunghezza e questo è vero giustamente sono pochi e bisognerebbe andare a vedere quanti sono dopo magari ci guardiamo potremmo farlo anche così se andiamo adesso dopo magari lo facciamo intanto intanto fatemi film e dopo andiamo a verificare questa cosa ok su quanti sono può essere che in effetti lì sotto ce ne siano più sovrapposti però dovrebbe essere proprio mi sembra strano che siano così coincidenti forse non so adesso dopo ci guardiamo su questo poi andiamo un attimo avanti poi su questa cosa ci torniamo eventualmente quello che ho fatto che viene fatto alla riga 4 è il plot del decision boundary vedete che lì viene plottato praticamente che cosa una retta che va da coordinata x decision boundary condenata y meno 1 coordinata x decision boundary condenata y 2 quindi una retta che va da meno 1 a 2 piazzata su 1.6 che è il confine di decennione e infatti è questa cioè questa è una retta che in realtà qua si prolunga da meno 1 e qua va fino a 2 poi non viene visualizzata perché verrà tagliato il grafico specificando i limiti ok e quindi vi fa vedere questa retta nera come farla tratteggiata di nuovo qui specificate vedete k e il colore black nero due punti linea tratteggiata dopodiché viene fatto il plot delle due curve cioè si va a prendere in funzione di x new tutti quei punti dell'asse delle x il vettore delle probabilità sulla prima sulla prima colonna scusatemi che è questa e sulla seconda la prima viene lottata in blu e sono tratteggiati la seconda in verde ed è una linea continua ed è specificata dal fatto che avete due lineette o una lineetta con questo campo potete specificare lo spessore della linea in questo caso è 2 e poi un'etichetta quelli di classe 1 sono iris virginica quelli di classe 0 non iris virginica dopodiché potete anche aggiungere del testo vi faccio vedere qualche funzionalità anche di matplotlib perché sono carine per cui possono essere utili non sono strettamente funzionali al al machine learning però arricchiscono chiaramente una qualunque qualunque tipo di di progetto di presentazione e vi fanno capire bene diverse cose la funzionalità text che cosa vi permette di scrivere un testo in questo caso scriviamo decision boundary e gli dobbiamo dire dove lo vogliamo mettere anzitutto e qui lo mettiamo nella coordinata x specificata del decision boundary più 0,02 e questa è la coordinata y 0,15 quindi se io vado a cambiare qualcosa in quelle coordinate ad esempio vedete qui lui me l'ha messa qua questa scritta decision boundary se io la volessi alzare gli devo specificare una diversa coordinata y quindi se la vado a mettere qui 0,25 lui vedete me l'ha alzata se gli vado a dire 0,5 me la mette a metà vedete ok quindi in questo modo posso andare a specificare dove voglio posizionare la scritta poi ovviamente c'è la stringa che dice cosa voglio scrivere se lo voglio scrivere in italiano scriverò confine decisionale e lui chiaramente mi cambia la stringa poi la dimensione del font 14 in questo caso ma potete farlo più grande se gli dite 24 diventa una cosa del genere ok il colore i colori sono codificati k è nero ma lo potete fare anche red russo se vi piace di più e lo potete anche posizionare centrato rispetto alle coordinate che voi avete detto oppure no non mi ricordo adesso potete andare a vedere nella documentazione come si fa ad allinearlo a sinistra o a destra rispetto alla posizione che gli avete detto ok poi potete fare altre cose con plt.arrow arrow sta per freccia potete andare a disegnare quelle due frecce verde e blu che sono che sono evidente lì sono due frecce che vengono disegnate esattamente di nuovo a partire da che cosa da una coordinata x e nella corrispondente allora qui dunque vediamo un po' gli date le coordinate x e y sono i primi quattro parametri sì se non sbaglio fatemi ok quindi gli dovete dire quanto è lunga la freccia sì ok quindi siete su vediamo un attimo un attimo qual era adesso non voi fatemi pensare 0,08 meno 0,3 e 0 ok sì praticamente questo dovrebbe essere il valore da cui parte parte questa freccia e va indietro di meno 0,3 questo è il valore della y quindi lunga l'asse delle x questo è il valore della y e va viene quindi questo è l'offset quindi il delta x e delta y voi gli date il punto iniziale le coordinate x e y e poi gli dite il delta x scusatemi x è il primo parametro e y è il secondo e poi gli dite delta x e delta y e lui vi disegna una freccia secondo queste coordinate e quindi per esempio e poi gli dite qual è l'ampiezza della testa della freccia la lunghezza della testa della freccia il colore dei contorni e della faccia del riempimento che in questo caso è blu e il risultato è questa freccia blu qui io se avessi specificato ad esempio un punto diverso un delta y diverso se avessi messo 0,3 la freccia sarebbe venuta inclinata presumibilmente verso l'alto vediamo un po' severo vedete cosa è successo vedete invece chiaramente la voglio ripristinare così e la mandiamo in esecuzione stessa cosa la potete fare con la freccia sopra che è la freccia verde la freccia verde vedete gli specifico punto x punto y delta x e delta y di nuovo ampiezza della testa della freccia tutti gli altri parametro la lunghezza e il colore poi qui ci sono le titichette dell'asse delle x e delle y che sono queste stringhe potete specificare quello che volete con il font che volete potete associare una leggenda qui l'abbiamo centrata a sinistra potete specificare un altro comando ad esempio center right ve la mette a destra e la leggenda e lui automaticamente questa leggenda che noi gli abbiamo come fa a ricavare la leggenda gliel'abbiamo specificato prima abbiamo detto la label e quindi lui in base a questa label crea la leggenda e il risultato è questo che è essenzialmente la stessa figura che avevamo generato sopra però arricchita di un po' di informazioni mi sembrava interessante farvi vedere come anche i grafici con questi strumenti possono essere arricchiti oggigiorno la parte soprattutto chi fa analisi dei dati data science utilizza questi strumenti in maniera intensiva e sono strumenti molto utilizzati molto ricchi di tante cose quindi anche qui chi è interessato può approfondire va bene siamo fin qui ok allora adesso andiamo a vedere un po' di altre cose allora il modulo dataset vi permette di di scikit-learn come vi dicevo ha al suo interno dei dataset che potete utilizzare come dataset di prova ma anche che sono dei dataset reali come quello appunto di fiori di iris che vi ripeto è un dataset che risale credo all'inizio del novecento se non ricordo male era uno statistico britannico che ha raccolto l'ha studiato l'ha fatto un po' di studi su questi su questi fiori per con scopi appunto di evidenziare alcune caratteristiche statistiche di queste di queste di queste piante allora però trovate anche all'interno del modulo dataset anche qui se andate a vedere la guida la documentazione avete tutte le varie informazioni di tutti i tipi le tipologie di dataset che si porta dietro scikit-learn non sono tantissime però insomma ce ne sono un po' e avete anche la possibilità di generare i dati sinteticamente cosa vuol dire sono dei dati artificiali che generate invocando una funzione in questo caso questa funzione si chiama make blobs adesso si capirà meglio perché il nome perché genera proprio della sorta di raggruppamenti globulari vedete il funzionamento è molto semplice di questa funzione la invocate e specificate i parametri che sono numero di campioni in questo caso io ho specificato 300 numero di centri due in questo caso gli stiamo dicendo crea due blob due raggruppamenti due cluster di dati random state specificate siccome sono dei dati generati casualmente qual è il seed del generatore di numeri pseudo casuali in modo che potete rigenerare la stessa distribuzione quante volte volete è importante per il debugging se cambiate ovviamente il seed lui vi genera altri punti e qui specificate un parametro che si chiama cluster std la deviazione standard che vi dice quanto questi punti sono dispersi adesso poi facciamo il grafico così si vede anche meglio che cosa intendiamo il risultato di questa funzione vedete sono due array un array x e un array y l'array y è un array che vi dice per ogni punto a quale delle due classi perché di fatto qui abbiamo specificato due centri due cluster appartiene a quel punto e x vi dice invece le coordinate che in questo caso saranno delle coordinate vedrete bidimensionali perché genera dei punti quello che non vi ho detto bidimensionali ok andiamo a stampare la shape di x e la shape di y che quindi sarà 300,2 e vedrete 300 punti virgola intanto vi faccio vedere vedete ecco la shape è questa cioè ha generato 300 punti coordinate bidimensionali e a ognuno di questi è a 300 punti associata vedrete un'etichetta che è o 0 oppure 1 lo possiamo andare anche a stampare anzi dopo c'è il plot dopo prima del plot di questa informazione sì e la dà automaticamente ok cioè lui decide di generare dei punti che sono associati a un centro e dice ok in genero 150 associati a questa classe dispersi vedrete nello spazio e poi prende l'altro centro e in generale altri 150 in questo caso perché sono 300 ok quindi le etichette sono relative ai due centri sì qui poi viene fatto invocato uno scatter plot potete fare anche con la funzione plt plot ma la funzione plt scatter prende direttamente i punti vedete tutti i punti quindi tutte le righe quindi tutti i 300 punti colonna 0 tutti i 300 punti colonna 1 e fate il plot quindi per ogni punto vedete la coordinata qui ho anche stampato ah c'è un esempio tra l'altro subito sotto quindi il grafico è sotto gli ho fatto stampare i primi 10 punti dell'uno e dell'altro e quindi lui che cosa fa queste sono le coordinate per ogni nello scatter plot prende il primo punto e dice ok coordinata 3,3 5,2 quindi si mette 3,3 qui e a 5,2 qua e qui mette un bel punto poi il secondo 2,9 0,7 e quindi si mette qui e genera un altro punto e così via queste sono le etichette vedete i primi 10 punti e dice ok questa è classe 0 lui sa e questa è classe 1 risultato è una cosa di questo tipo qui una cosa che non vi ho detto è questa noi gli stiamo dicendo in questo caso di utilizzare come colori y cioè di prendere questo vettore per colorare diversamente i punti ok quindi lui li colora in base all'etichetta gli stiamo dicendo la dimensione di ogni punto size uguale 50 potete fare quei punti più o meno grandi più o meno piccoli e gli stiamo dicendo una color map in questo caso winter ce ne sono di diversi tipi per cui in questo caso ha preso una color map invernale ma potete specificarne altri cioè vedete un colore verde l'altro blu e gli ho specificato in questo caso anche un valore di trasparenza alfa 05 se è andato a cambiare quel parametro tra 0 e 1 lui li fa più o meno trasparenti o più o meno o più o meno intensi dunque dunque dunque dove eravamo ok quindi questi sono i punti che lui ha generato adesso la cosa interessante è vedere che cosa succede se io vado a variare qualcuno di questi parametri se per esempio vado a aumentare la deviazione anzi prima andiamola a diminuire da 1 la mettiamo 0.5 guardate cosa succede vedete sono molto più coesi e meno dispersi se invece la vado ad aumentare e ci metto 2 guardate cosa succede sono estremamente più sovrapposti e questo vi fa capire bene che se io voglio andare perché poi l'obiettivo è con questi dati andare a costruire un classificatore capite bene che un classificatore qui lineare se la cava molto male situazione estremamente confusa chiaramente questa è una situazione particolarmente problematica per qualunque classificatore direi però se torniamo a 0.5 quelli invece sono separabili linearmente se arriviamo alla situazione intermedia da cui siamo partiti abbiamo possibilità di costruire un classificatore lineare che se la cava ragionevolmente ma che fa un po' di sbagli e questo è l'obiettivo vedete quindi questo è un strumento utile per fare un po' di ragionamenti allora altro su questa cella direi che non c'era e qui andiamo a vedere cosa facciamo in quest'altra cella allora in quest'altra cella quello che facciamo utilizziamo la classe logistic regression invochiamo quindi creiamo l'oggetto logreg di nuovo il solver è questo provate magari altri solver vedete se cambiano le prestazioni se succede qualcosa di diverso oppure no questo potrebbe essere una cosa interessante da testare qui l'unica cosa ecco vedete gli ho specificato un valore di diverso sono andato a 500 numero di iterazione anziché il 100 di default anche qui altro parametro potete andare a esplorare stampe di controllo delle shape giusto per vedere che cos'è ah una cosa torno un attimo indietro prima di questo scusate volevo farvi vedere cosa succede se il numero di cluster diventa 3 vedete lui ha creato tre raccolpamenti ne potete creare anche più di più comunque magari abbassiamo la deviazione standard guardate e il quinto l'ha messo qua ok torniamo qui e qui andiamo invece vi dicevo abbiamo creato creato il nostro oggetto logreg come oggetto di regressione logistica facciamo una stampa di prova delle shape di x e y di controllo e poi addestramento con il metodo fit qui passiamo x e y dopo che abbiamo fatto l'addestramento beh andiamo a vedere cosa succede invochiamo il metodo predict gli passiamo il dataset di addestramento vogliamo vedere che cosa che cosa ha combinato quali assegna la classe 0 quali alla classe 1 la cosa interessante è che una volta che abbiamo le predizioni che cosa possiamo fare andare a calcolare il numero di errori questo lo possiamo fare in n modi diversi qui ad esempio possiamo andare a dire ypred dove è diverso da y e mettere questo in un vettore che chiamiamo vettore degli errori dopodiché andiamo a sommare il numero di errori e otteniamo il numero di errori totale lo andiamo a scrivere abbiamo diciamo abbiamo fatto un certo numero di errori su il totale su queste istanze quindi gli andiamo a passare numero di errori la prima variabile la lunghezza del vettore delle predizioni che ci dice quante predizioni ha fatto quindi tot errori su tot predizioni e la funzione where di numpy vi dice anche dove il vettore degli errori è 1 e dove è 0 quindi anche dove abbiamo sbagliato e glielo facciamo stampare e vediamo un attimo cosa succede poi andiamo a vedere le ultime due cose allora vedete la shape x era 300,2 la shape y era 300 e qui vi dice vedete in realtà io potrei bypassare tutto il discorso che abbiamo fatto prima sulla regressione logistica su una una sola variabile abbiamo fatto tutti i grafici molto carini ma in realtà poi alla fine quello che si fa tipicamente è addestrare il modello andare a a vedere quanti errori fa e andare a calcolare quindi che di fatto è la curatezza allora qui abbiamo fatto 8 errori su 300 quindi non male e abbiamo anche detto su quale istanza sono cioè nel vettore degli errori il vettore degli errori è un vettore che ha la lunghezza pari alle predizioni e quindi vi dice esattamente dove ha commesso un errore e quindi sapete anche su quali punti ha sbagliato ok per andare a vedere andiamo a vedere le coordinate ad esempio del punto 5 se volessimo andare a fare una cosa a una stampa al volo vediamo cosa succede print x d5 allora x d5 è un punto di coordinata 1,37 e 1,53 1,37 e 1,53 è qualcosa che presumibilmente sarà qui abbiamo detto è questo qua è questo secondo me uno che sta da queste parti presumibilmente adesso in questa zona qua adesso non ho esattamente chiaramente la possibilità potrei anche fare una cosa ancora migliore che andarlo a evidenziare diciamo così potremmo anche andare a vedere qual è il volo potremmo fare così abbiamo detto che è il punto con coordinate 5 x di 5 potremmo fare una cosa del genere diametri Grazie. Eccolo là, guardate chi è. Gliel'ho fatto plottare ed è questo qui. Vedete questo che ho appena fatto plottare il punto. Avete capito come ho semplicemente preso il punto di coordinata 5 ed era questo. Questo lui lo sbaglia. Cioè un punto che è stato assegnato alla classe blu e lui dice che è verde. Quindi quegli errori sono tutti errori che stanno da queste parti, dalle parti del confine decisionale, chiaramente. Ma in questo caso le predizioni sono sempre presi dal dataset. Sì, lui sta facendo l'addestramento, esattamente, chiaramente. Qui stiamo facendo le predizioni sul dataset di addestramento. Stiamo andando a vedere qua che cosa sbaglia nel dataset di addestramento, perché sbaglia anche nel dataset di addestramento. Poi uno, quello che vi dicevo prima, rimane vero. Io potrei fare a destra me, generare un numero, ad esempio, 300 punti, tenermene un po' da parte per il dataset di destra. E questo lo facciamo, se non sbaglio, più in basso. Va bene, ci siamo fin qui? Ok. Vi dico due parole sull'ultima cosa che c'era in questa cella. In questa cella c'erano le ultime due righe che erano queste. che era l'utilizzo di quella funzione zero one loss che avevamo importato all'inizio. Zero one loss è una funzione del gruppo delle metriche, a cui semplicemente passate y reale e y predetta, e lui vi conta il numero di errori. E non solo vi conta il numero di errori, in realtà poi quello che vi conta è l'error rate, cioè vi dice in percentuale quanti ne avete sbagliato. Quindi di fatto vi dà l'accuratezza. Infatti qui quello che possiamo fare è la print, e dire l'accuratezza sul training set è, qui specifichiamo il formato, cioè 0.3, vuol dire con tre cifre dopo la virgola, è uno meno l'error rate che abbiamo calcolato con la funzione zero one. Ci siamo? Che è 0,973. Chiaramente se voi fate 8 diviso 300, ottenete l'error rate, quindi 1 meno 8 diviso 300 vi dà lo stesso risultato, cioè... Ok, è chiaramente lo stesso numero. 0,973. Va bene? Ci siamo? Come l'ha calcolato? Non è l'unico modo per calcolare l'accuratezza, adesso dopo c'è un esempio in cui vi faccio vedere come di solito viene fatto. E con questo concludiamo. Allora, lavoriamo sempre su questo dataset. Ok? Ah, ecco, poi c'era anche qui sotto. Questi sono tutti i punti degli errori. Vedete? Error points. E andiamo a vedere where il numero degli errori... Where, scusatemi, errors è uguale a 1. E questo viene assegnato error points. Dopodiché error points ci dice quali sono le coordinate dei punti, gli indici dei punti che il nostro classificatore ha sbagliato. Gli andiamo a plottare, vedete in questo caso S gli ho dato una size, gli abbiamo dato una size più grande, 300, gli abbiamo specificato una certa ampiezza della linea, gli abbiamo detto che il colore dell'edge è rosso, il colore della faccia non abbiamo specificato nulla, quindi è trasparente, gliel'abbiamo fatta stampare. Vedete? Questi sono quelli che sbagliamo. Ed è molto ragionevole, perché lui avrà trovato un confine lineare e vi sono dalle parti del confine decisionale. Quindi questo è interessante perché vi fa vedere... E anche qui se andate a aumentare o diminuire, insomma a cambiare la distribuzione dei punti, vi accorgete ovviamente, che ne sbaglia di più, di meno. Potete fare un po' di prove e anche qui, diciamo, è un invito a esplorare ulteriormente alcuni aspetti di questa esercitazione. Ok, adesso qui vi faccio vedere come l'accuratezza può essere calcolata anche con altre funzionalità della sottolibreria Matrix. Qui se importate accuracy score, è un modulo che è una funzione che vi permette di calcolare lo score di accuratezza a partire da Y e Y predette. Quindi le label effettive e le label che ha predetto il vostro classificatore. E di nuovo ritrovate 0,97 per esempio. una modalità equivalente. Una cosa invece che non, diciamo, arricchisce ulteriormente, non è così scontata e che vi permette di, invece di utilizzare la libreria scikit-learn, è la creazione delle matrici di confusione. Anche queste l'abbiamo vista a lezione. Sappiamo cosa sono e qui vedete che avete, sempre dal modulo Matrix, avrete la possibilità di importare la classe Confusion Matrix e Confusion Matrix Display. La prima serve per il calcolo della matrice di confusione. La seconda vi permette di fare, vedrete, di rappresentarla graficamente in modo abbastanza anche questo carino. Utilizzo molto semplice. create un array, create un array, un numpy, invocando la funzione Confusion Matrix, quindi passate a questa funzione y e y-prep. Vi ricordo il vettore delle eticheppe che voi sapete, di cui sapete tutto, quindi quel dataset chiaramente, e queste sono le predizioni. Questo lo potete fare anche su un dataset non solo di addestramento, ma anche di testing. Qui ancora stiamo ragionando sull'addestramento. Poi gliel'ho fatta stampare e... Vabbè, intanto vediamo cosa succede, poi vediamo invece l'ultima cosa. Ok. Vedete, lui vi ha stampato una matrice in cui vi dice 146. Allora, vi ricordo che ci sono associate alle righe le predizioni reali, associate alle colonne, i valori reali, associate alle colonne, le predizioni, vedete che qui funziona abbastanza bene. Lui in totale commette otto errori che avete sul, chiaramente sull'antidiagonale, sulla diagonale avete le funzioni, le, scusatemi, le predizioni corrette che sono 146. Invocando questa funzione da... Questo modulo, vedete, Confusion Matrix Display, questa classe, punto from prediction, gli dite, cioè prendi prea la visualizzazione della matrice di confusione a partire dalle predizioni, y, y predetto e scegli come, qui potete specificare tutte le mappe di colori, quindi specificare le tonalità che volete, quindi in questo caso è una mappa di colori su un blu. E viene forna così, questo tipo, che è molto carina da vedere, vedete, avete true label da una parte che è associata alle righe e invece predicted label è associata alle colonne, i numeri sono sempre quelli che avevate prima, cioè 146, 4, 4 e 146, però vedete, avete anche una scala che vi dice l'intensità, di... chiaramente queste sono sul bianco perché sono pochissime, se cambiamo i numeri avete anche un'intensità di colori diversa che vi dà anche un'immediata visualizzazione di quelli che è il livello di errore che avete commesso. Ok, last but not least andiamo a vedere cosa succede quando valutiamo, addestriamo un modello, facciamo il fitting quindi del modello e lo valutiamo su un test set separato che è quello che nella pratica comune si fa, fino adesso abbiamo lavorato un po' sui dataset di addestramento in basso, in realtà sul primo esempio avevamo creato un modello e l'avevamo testato su dei punti unidimensionali che avevamo generato noi nella griglia ma adesso facciamo un altro esempio un pochino più più significativo. Ok, qui riprendiamo il dataset di Iris adesso possiamo andare su alcune cose più veloci perché ovviamente qui facciamo l'import del dataset l'abbiamo fatto anche prima l'ood poi prendiamo solamente le prime due feature lavoriamo su due feature anziché su quattro per rendere il programma un pochino più complicato perché se provate su questo dataset a fare la regressione logistica con quattro feature lui riesce a fare un ottimo lavoro anzi poi dopo magari come esercizio lo potete fare. Quindi la nostra X sono i nostri dati tutte le righe colonne fino alla 2 quindi 0 e 1 le prime due colonne. quale la forma gli facciamo stampare utilizziamo questo è commentato quindi dopo invece vedete ve l'ho già messo se vogliamo usare tutti i dati magari poi lo vediamo diciamo decommentiamo questa riga e le Y saranno chiaramente il target. Dopodiché cosa facciamo? Utilizziamo quella funzione molto comoda che dicevo prima train test split train test split come funziona? Gli passate la X e la Y quindi la matrice con tutti i vostri campioni e le etichette corrispondenti e gli dite la dimensione del test size che è 0,33 vuol dire che in questo caso stiamo dicendo vai a prendere il dataset che sono 150 punti il 33% di quei 150 punti lasci da parte per il testing il restante che è 67% lo vai a dedicare all'addestramento qui specificate uno stato iniziale con un valore del seed in modo da avere la riproducibilità perché lui chiaramente li va a scegliere a caso quindi se volete ricostruire che cosa sta facendo dovete ripartire dallo stesso seed iniziale e di nuovo che cosa fate? classe logistic regression create l'oggetto logreg qui specificate il solvere e eventuali altri parametri addestramento con il metodo feed a cui passiamo X train Y train cosa che non vi ho detto prima è che cosa restituisce questa funzione restituisce X train X test Y train Y test lui seleziona dei punti e li mette da parte per il training con le relative etichette e poi seleziona altri punti che sono mette da parte per il testing con le relative etichette noi in questo caso al metodo feed cosa passiamo? dataset di addestramento con le relative etichette e lui lavora su quelle e impara su quelle dopodiché andiamo a fare le previsioni le predizioni questa volta le facciamo sempre a partire dall'oggetto logreg con il metodo predict ma gli passiamo X test questa volta non più X train lui quelle non le ha mai viste è così che si lavora nel machine learning dovete testare su cose che la macchina non ha mai visto il sistema il modello non deve averlo mai visto altrimenti state barando chiaramente dopodiché una volta che abbiamo fatto il predict andiamo a calcolare il numero di errori per esempio in questo modo vado a vedere dove è diversa la predizione dal test e faccio la somma degli errori oppure invocando la funzione che abbiamo visto prima che è la funzione accuracy score una volta che abbiamo questo oppure la funzione zero one loss sempre equivalenti sono e vedete quello che succede è che abbiamo solito 150 matrice di 150 per 2 abbiamo fatto in questo caso 9 errori su 50 perché 50 erano i punti del dataset di testing che sono quel 33% vi dice quali stanze l'error rate è 0.2 sul sul training vedete sul test set è leggermente più basso è diverso possono differire tipicamente sul test set diventa un po' più alto l'errore ma in questo caso ovviamente sono pochi punti perché io gli ho fatto calcolare a queste due righe l'errore sia sul sul test set ma anche sul training set se vado a cambiare qualcosa anche qui vi invito a farlo vedete facciamolo al volo insieme vediamo cosa succede se usiamo tutti i dati quindi se x non è solamente una feature ma ne usiamo 4 vado a fare l'addestramento vedete che ho fatto 3 errori su 50 quindi è diventato più basso gli errori qui sono errori a livello di test e quindi sul training set un 2% sul test set il 6% mentre prima era noi andiamo a fare così almeno 1,8 e 2% insomma sono abbastanza bassi come come come errori quindi il modello riesce a cavarsela molto bene in questo caso un tipo di data set che si riesce a risolvere bene vedete potete fare anche ho qui una cosa che non vi ho detto prima ma che avrei dovuto dire è che quello che stiamo facendo a differenza dell'esempio da cui siamo partiti all'inizio dell'esercitazione in cui abbiamo trasformato quel data set in un data set di classificazione binaria ok abbiamo detto un fiore e tutti i restanti qui automaticamente non gli abbiamo specificato nulla quindi automaticamente nel momento in cui specifichiamo la classe logistic regression lui è andato a riconoscere che ci sono tre classi e ha applicato un una regressione logistica di tipo multiclasse potete anche qui andare a specificare se volete il one versus rest oppure l'altra tipologia con il multinomiale vedrete che non ci sono dovrebbero essere molte differenze dal punto di vista delle performance come dicevamo anche a lezione ma interessante che a questo punto potete andare a creare una matrice andare a vedere i risultati e andare a creare una matrice di confusione in cui specificate nel vettore dei nomi questi tre nomi queste tre stringhe create la matrice di confusione tramite la funzione confusion matrix e la stampate e poi di nuovo potete fare il display a partire dalle predizioni vedete che qui abbiamo specificato y test e y print quindi sul test sul dataset di test che cosa ha fatto e lui ha fatto questi errori l'altra cosa che gli diciamo vedete vi faccio vedere gli ho aggiunto display le etichette a partire da questi true names cioè lui andrà a mettere sulla associate alle righe dove ci sono le dove prima aveva messo true label metterà le tre label cioè quello che fa è andare a mettere vedete qui c'è true label e lui c'è andato a mettere queste tre così potete anche fare una cosa di questo tipo che arricchisce il vostro il vostro grafico e vedete che qui già si vede meglio perché 18 11 12 hanno diversi livelli di colore e poi ci sono degli errori gli errori vedete qui la matrice di confusione è interessante perché vi permette di fare anche l'analisi degli errori perché ci sono cinque casi in cui del fior di iris virginica vengono scambiati dal vostro classificatore per iris versicolor e quattro casi invece di fior di iris versicolor che il vostro classificatore predice come essere di scusatemi di di iris virginica viceversa non vengono mai sbagliati fior di iris setosa per esempio e anche questo insomma e anche qui andando a cambiare andando a mettere delle feature diverse potreste avere dei risultati diversi e così via ok questo direi che per oggi è tutto e su su quello completa il quadro che riguarda la questa classificazione scusate questa esercitazione sulla classificazione quindi sulla regressione logistica e niente direi che qui su questo se non ci sono domande possiamo fermarci sono in debito intanto ragionate se avete qualche domanda o qualche curiosità sono in debito di un di un di un paio di informazioni riguardanti le i progetti e magari può essere può essere utile adesso spendere 5 minuti su questo perché magari parliamo parliamo di questi va bene ok direi che c'è altro qualche domanda su questo no allora direi che fermiamo intanto la registrazione ok ok giusto sì dicevo che vi davo l'elenco dei progetti sì lo lasciamo nella registrazione hai ragione sì questa può essere una buona idea ok quindi vi dico semplicemente allora la cosa è presto risolta nel senso che a proposito delle esercitazioni il mio consiglio è andare su blended learning e leggersi il file che vi ho preparato in cui vi ho detto come va preparato il progetto la relazione in generale e anche in particolare su quali c'è una guida abbastanza puntuale le sezioni che dovete mettere i suggerimenti di come procedere quindi diciamo è un credo il primo passo ed è abbastanza spero utile per un po' guidarvi nella costruzione del progetto e della relazione detto questo la domanda che mi facevate l'ultima volta è quali tipologie magari di progetto possono andare bene qualcosa un po' vi ho già risposto vi ho detto anche che vi avrei detto quelli ragazzi dell'anno scorso magari che titoli hanno preso allora vi dico un po' di cose giusto riguardanti questo così che possiate magari avere qualche spunto non perché dovete rifare quelle chiaramente sarebbe bene modificare ma potete anche riprendere quelle come idea e poi farlo a vostro modo migliorandone alcuni aspetti esplorandone altri quindi potete anche riprendere alcuni di quelli non c'è niente di male però ecco non proprio lo stesso chiaramente oppure semplicemente vi dà uno spunto di che cosa può essere fatto e quindi poi alcune erano fatte meglio alcune peggio però insomma allora eccoci qui dunque allora ci sono stati c'è stato un progetto che avevano fatto due studenti che hanno lavorato insieme avevano lavorato sul riconoscimento dell'attività umana e avevano costruito praticamente a partire da dati in cui c'erano l'input erano delle tracce di segnali da accelerometri quindi erano delle tracce numeriche e ci avevano costruito sopra una rete neurale questo era un progetto non banale ma questi diciamo erano un po' avvantaggiati perché partivano da una conoscenza pregressa di questo di questo modello però mi avevano chiesto se potevano farlo e lì classificavano tra le varie attività cioè c'erano praticamente dei segnali di tracce da accelerometro e la rete neurale classificava quelle tracce se l'utente che l'aveva utilizzato stava ad esempio camminando oppure stava seduto e quindi era un problema multiclasse risolto con delle reti che tra l'altro noi non vedremo neanche a lezione perché avevano applicato le convoluzionali però ripeto questo è un caso un po' particolare perché avevano avuto modo di approfondirle già per altri motivi chiaramente dal mio punto di vista se anche volete esplorare qualche cosa al di fuori lo potete fare l'importante è che siate un minimo coscienti di quello che fate e che sappiate riprodurre poi quest'altro invece era un lavoro a partire da un dataset su raccogliendo dei dati diciamo che sono pubblici sui siti della federazione italiana pallavolo e a partire da lì aveva creato questo dataset in cui c'erano le partite giocate in casa e fuori con i risultati e l'obiettivo era costruire aveva costruito una macchina un modello con svm quindi con support vector machine e anche la regressione logistica aveva confrontato quindi aveva fatto due confronti per vedere se riuscivano questi modelli a fare una predizione del fatto sapendo le partite se erano state vinte o perse se erano state giocate in casa o fuori e anche c'erano altre statistiche tipo numero di punti schiacciate ricevizioni e cose di questo genere mi sembra di ricordare per la predizione se una data partita era stata vinta oppure no cose di questo tipo ed era carino ed era interessante sulla base ecco c'erano delle statistiche nel dataset c'erano statistiche di ricezione di attacco eccetera era anche questa una cosa abbastanza interessante poi c'era stato un lavoro ah questo è un altro lavoro in cui aveva esplorato questo studente dei dataset pubblici era stato era andato a cercare dei dataset da un lavoro di ricerca c'è un dataset pubblico sulla rilevazione addirittura delle fake news quindi era un dataset di testo quindi c'erano dei testi apparsi appunto su delle che erano stati raccolti per motivi di ricerca che erano stati pubblicati in cui c'erano degli articoli di pagine credo web o giornali eccetera su notizie che erano poi state verificate più o meno vere lui aveva fatto tutto un lavoro abbastanza complesso sul utilizzando il bag of words di cui vi ho detto qualcosa la volta scorsa e aveva applicato una volta che aveva estratto il bag of words dei classificatori che erano regressione logistica svm e poi ne aveva aggiunto anche un altro che non abbiamo visto noi extra che aveva preso sempre da Syk Hitler per classificare 0-1 è un tipo quindi aveva fatto il pre-processing col bag of words e anche con un altro metodo per classificare 0.1 oppure scusatemi 0 oppure 1 cioè è un testo relativo a una notizia falsa oppure questo non era banale però insomma si era chiamata abbastanza bene quest'altro era un altro lavoro secondo me carino ripeto ed era anche più accessibile ma ha fatto bene classificazione della qualità del vino in base alla proprietà fisico-chimica questo è un dataset che c'è sul sul sito dell'università della California Irline dove ne trovate diversi di questi siti il mio consiglio è cominciare a guardare lì guardare da quei dataset che ci sono lì perché sono un sacco di informazioni e vi magari vi viene qualche idea se volete partire dai dataset che può essere forse la cosa più semplice da cui partire e lì questo è un dataset in cui ci sono una serie di dati riguardanti un insieme di vini quindi sono circa 4800 vini censiti di cui ne avevano usate una frazione per l'addestramento e un'altra per il test e le feature erano tipo l'acidità percentuale di acido citrico zuccheri cloruri anidride solsoporose densità pH solfati alcol eccetera quindi tutti dei parametri che vi dicevano quali sono le caratteristiche chimico-fisiche del vino e poi c'era ovviamente qualcuno che si era preso la briga di etichettarlo secondo diverse categorie di dire un vino più buono più o meno buono e loro hanno costruito dei classificatori su questo e i classificatori erano di nuovo un SVM e poi anche una piccola e poi avevano fatto un po' di elaborazioni su questo e quindi era abbastanza carina anche questo un altro invece aveva fatto un'analisi sulla praticamente da un dataset sempre da UCI dell'Università della California in Vine ed è un dataset sul praticamente sul di tipo censo quindi sul se non ricordo male allora il reddito praticamente su una popolazione credo degli Stati Uniti certo una certa fetta di popolazione degli Stati Uniti in cui in base alle feature quindi che sono ecco vedi era questo sì le classi di interesse erano due un'etichetta le persone con reddito annuale inferiore o uguale a 50.000 dollari e l'altro superiore e l'obiettivo era sulle basi delle caratteristiche demografiche e professionali stabilire se un individuo guadagna più o meno di 50.000 dollari a partire da queste feature e lì nel dataset c'è tutto qui il problema era che è un dataset sbilanciato e quindi ha studiato un po' di cose di come andare a risolvere il problema dello sbilanciamento delle classi e questo è un altro aspetto interessante da indagare per cui da questo punto di vista anche questo era un bel progettino anche qui ci sono tante cose da esplorare volendo un altro è stato un classificatore ha costruito un classificatore partendo da dati che aveva trovato su dataset che aveva trovato non so se su UCI forse su un sito dell'università di Caltech credo vedo che mi ha riportato il link quindi sì sugli esopianeti quindi praticamente era attraverso costruire un modello di machine learning per classificare dei dati erano dei dati del telescopio Kepler in cui aveva anche qui per ogni potenziale oggetto celeste da classificare una serie di dati di feature e doveva il classificatore classificare se era un esopianeta oppure no ok e anche qui c'erano parecchie cose quindi aveva per esempio i dati erano raggio di questo oggetto rispetto al raggio della terra durata del transito rispetto al al credo rispetto all'osservazione insomma tutta una serie di dati adesso non me li ricordo li vedo qui al golo sì temperatura della superficie della stella eccetera associata e così via e lì aveva costruito un svm anche questo ed era andato a vedere che cos'era successo nella capacità di classificare andando a variare un po' di parametri facendo un'analisi valutando l'accuratezza ma anche altre metriche come la precision la recall perché le metriche non sono solo quelle che vi ho fatto vedere ce ne sono altre quindi anche qui avete libertà di esplorare alcune cose creando un po' di matrici aveva creato un po' di matrici di confusione cose di questo tipo poi ce n'era uno riguardante dati forse ve l'avevo detto l'altra volta presi da un ambito medio quindi in base delle feature classificare la possibilità del recupero funzionale a seguito di un intervento chirurgico anche qui avete il dataset con le feature e sapete in questo caso era un problema direi di regressione se non ricordo male e andare a dire per esempio dopo tot mesi c'è un recupero funzionale oppure no cose di questo tipo o qual è la valutazione del recupero dopo tot mesi questo però non era un dataset pubblico poi c'era un altro progetto che riguardava per esempio il confronto tra un classificatore one versus all e one versus one vi ho accennato a lezione che cosa sono la differenza cioè abbiamo parlato del one versus all vi ho detto anche il one versus one che cosa fa e vi ho detto anche che fanno un numero di confronti diversi quindi il one versus one ne fa di più ma lavora su dataset più piccoli allora lui è andato a vedere quali erano le prestazioni computazionali se c'erano differenze che cosa succedeva prendendo dei dataset di esempio in questo caso quello che ho utilizzato era esattamente l'ha fatto con il make blob come dataset sintetico e poi ha utilizzato un altro dataset preso credo sempre da UCI no? in questo caso no? questo era stato preso da ah da Kaggle che come diceva è un'altra piattaforma che diciamo non c'è dietro un'università però insomma dove ci sono comunque un po' di cose vengono insomma diversi dataset per diversi gare contest vengono spesso volentieri caricate anche lì e l'aveva preso da lì aveva testato queste due cose su questi su questi due esempi last but not least un altro dataset da i University of California Irvine è un dataset di un'applicazione OCR Optical Character Recognition cioè lettere scritte a mano insieme di lettere scritte a mano prese da un dato alfabeto quindi di caratteri ok lettere intese come caratteri come se non la A la B la C la D e un classificatore che imparasse a riconoscerle e quindi c'erano praticamente delle delle feature ok che rappresentano la distribuzione dei pixel praticamente a partire dalle delle immagini a partire dalle quali il dataset vi vi permette di costruire 16 attributi numerici che rappresentano la distribuzione dei pixel e con questi 16 attributi numerici questo è una cosa che c'è già nel dataset quindi non doveva non doveva essere fatta ma c'erano questi già questi 16 attributi numerici aveva fatto un confronto tra diversi modelli che erano il support vector machine poi aveva anche messo dei modelli che noi non avremmo modo di vedere tipo random forest classifier che c'è sempre su scikit-learn quindi provando a mettere un po' mettere un po' all'opera qualche modello ulteriore andandosi un pochino a studiare la documentazione era carina molto carina anche questa cosa e aveva addirittura utilizzato un uno strumento per ottimizzare gli iperparametri quindi era completa anche questa c'erano parecchie esplorazioni quindi il mio consiglio è prendere un piccolo problema senza pretendere di risolvere cose molto grosse e provare in questi problemi quindi partendo magari in un dataset che può essere di vostro interesse provare a dire ok su questo dataset provo a costruire questi modelli magari li vado a confrontare voglio esplorare questa dimensione quindi magari degli iperparametri oppure andare ad analizzare un confronto tra modelli per vedere cosa succede e quali si comporta meglio provo magari a risolvere un problema specifico vi ho detto prima appunto il dataset è sbilanciato allora cosa faccio ci sono delle soluzioni posso andare a vedere l'accuratezza bilanciata posso andare anche a io a provare a risolvere il problema del dello sbilanciamento andando a magari pesare di più alcuni alcuni punti e altri no questa è una tecnica di cui abbiamo parlato ce ne sono altre che non abbiamo molto di vedere ma magari qualcuno può essere curioso andare a vedere che cosa succede oppure vi dicevo prima andare a fare feature selection avete un dataset in cui avete delle feature andate a vedere che cosa succede se ne selezionate alcune piuttosto che altre trasformare fare preprocessing se facciamo della PCA della PCA quindi PCA che cosa succede analisi componenti principali cambia qualcosa nella classificazione nella regressione problemi possono essere problemi di classificazione questi erano molti di classificazione ma possono essere anche di regressione su tutti il problema da cui siamo partiti per la regressione che ogni tanto abbiamo citato a lezione è quello del costo degli appartamenti e problemi di questo tipo ce ne sono tonnellate e trovate ripeto partendo io partirei dal sito dell'Università della California che mette a disposizione diversi dataset che vengono utilizzati per scopi di ricerca e che alcuni dei quali sono tranquillamente riutilizzabili anche per applicare modelli semplici sul per un progetto per un corso ecco poi quando avete un po' le idee un po' più chiare ne riparliamo me lo proponete io ovviamente vi dico sì guarda vi dico sì può andar bene oppure no guarda cambia questa cosa magari vi do qualche consiglio su non mettere certe cose magari possono essere fuori portata oppure aggiungere qualche altra particolare perché magari il rischio è che venga una cosa troppo semplicistica diciamo troppo poco con troppa poca sostanza questo è un po' insomma l'insieme delle dei suggerimenti che mi sento di darvi adesso su questo fate un po' una riflessione poi magari ne riparliamo adesso intanto blocchiamo la registrazione perché adesso veramente penso che sia tutto poi se ovviamente avete domande o curiosità queste potete sempre fare anche perché diciamo abbiamo sforato l'ora per cui dobbiamo fermarci e Grazie a tutti.