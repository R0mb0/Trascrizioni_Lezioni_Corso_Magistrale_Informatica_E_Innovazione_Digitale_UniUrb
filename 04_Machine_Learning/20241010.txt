ok allora tanto bentornati alla machine learning tra l'altro non possiamo non menzionare il fatto che proprio in questi giorni stanno assicurando appunto dei premi nobel che hanno un più che un collegamento forte diciamo con il machine learning perché sia quello della fisica che nella chimica sono stati assegnati a scienziati che hanno dato di contributi nel machine learning quindi dell'intelligenza artificiale ma in particolare del machine learning proprio quindi sia nella fisica per contributi sulle reti neurali pionieristici che nella chimica proprio che è stato assegnato ieri per lo studio tramite metodi computazionali basati su machine learning della conformazione delle proteine quindi diciamo da questo punto di vista è una materia che sicuramente diciamo qualora ve ne fossero dei dubbi ovviamente molto molto attuali venendo venendo a noi invece più modestamente torniamo alle nostre alle nostre nozioni di base di che ci servono per provare a capire qualcosa di più ecco di questo di questo mondo abbastanza già articolato e complesso del punto del machine learning e tra questo abbiamo visto la volta scorsa il metodo di scelta del gradiente utilizzato eravamo arrivati qui ad analizzare queste queste tre funzioni scusate non eravamo più avanti eravamo su queste tre funzioni perdono in cui avevamo visto e era verificata quella che una proprietà matematica cioè il fatto che il vettore gradiente punta in direzione ortogonale alle linee di livello e questo era vero per queste tre funzioni diverse tra di loro ma rimane chiaramente sempre vero quella proprietà matematica qui in questo in questa slide invece ho un'esemplificazione di quello che è un punto uno dei due se vogliamo punti un po deboli del metodo di scelta del gradiente che il comportamento a zig zag che si verifica in condizioni particolari ed è un comportamento che è frutto del fatto proprio che il gradiente ha una direzione ortogonale di fatto alle linee di livello e quando la configurazione della funzione è tale per cui le linee di livello diventano vedrete adesso vi faccio vedere qual è la differenza vi dirò meglio qual è la differenza tra questi tre casi ma sono fondamentalmente tre variazioni della stessa funzione in cui cambiano dei parametri della funzione che però dal dal primo quindi dall'alto fino al all'ultimo che in basso quindi dal primo caso che questo al secondo e poi al terzo sono caratterizzati dal fatto che vedete le linee di livello sono via via più schiacciate fino a formare nel terzo caso una sorta di valle questa la dovete vedere come se fosse in tre dimenze queste sono ovviamente delle proiezioni su due dimensioni di una di una superficie che sta in tre dimensioni che potete vedere come una sorta di scodella che via via diventa via via più schiacciata una funzione convessa e qua in fondo è come se fosse un canyon con una valle molto lunga e molto stretta due pareti quasi parallele ai lati di questo canyon mentre qua è una sorta di superficie più più più più simile a una sfera se vogliamo, una sfera qui nel primo non ci starebbe, però è una sfera allungata che via via diventa più schiacciata, vedetela così, come se noi potessimo comprimere questa figura. E da un punto di vista matematico questo si traduce in questo modo, cioè queste funzioni hanno questo andamento, sono delle funzioni che possono essere scritte così. Ok. Me ne scrivo qui a sinistra. Allora, è una funzione che in generale può essere scritta come A più B, trasposto, prodotto scalare W, W più W, trasposto, C, W. Allora, questa è una scrittura vettoriale di una funzione che ha la caratteristica di essere una funzione quadratica, poi, in più dimensioni, ok? Quindi è un po' l'equivalente di un'espressione del tipo A più BX più CX quadro se fossimo in una dimensione. dimensione. Quindi è l'estensione a più dimensioni. Qui W è il vostro vettore in cui andate a mettere tutte le coordinate del vostro... non le coordinate, avete altrettante coordinate con i valori che prende la variabile indipendente. B e C sono dei parametri costanti che definiscono la funzione, sono dei vettori che definiscono la funzione. Ad esempio, io posso assegnare, e anche A chiaramente, A è uno scalare, B è un vettore, C è una matrice, ok? Quindi B è un vettore con n componenti, C è una matrice n per m e W chiaramente è un vettore a n componenti. Nell'esempio di questa figura, A è uguale a 0 e anche B trasposto è un vettore con tutti i 0, quindi il prodotto B trasposto per un qualunque vettore W è un prodotto scalare che dà 0 come risultato. Quindi l'unica cosa con cui rimaniamo è G di W uguale W C per C per W. Allora, la cosa interessante è vedere cosa succede se ad esempio C ha una forma di questo tipo. Allora, vi ho detto che è una matrice n per n, quindi nel nostro caso sarà una matrice 2 per 2, giusto? Perché stiamo trattando un problema in due dimensioni. E vedete che assegnando dei valori alla matrice otteniamo, se voi andate a fare poi il prodotto riga per colonna di W, T per questa matrice, per W, ottenete una funzione in due variabili, W1 e W2. Che quello che potete fare è semplicemente andare a sostituire al posto di W trasposto un vettore che ha le componenti W1 e W2, al posto di W ovviamente le componenti W1 e W2, tenete presente che W è un vettore colonna, W trasposto sarà un vettore riga, quindi sarà 1 per n, per n per n che determina un vettore 1 per n, moltiplicato per il vettore n per 1, che è il vettore colonna, determina uno scalare che è il vostro risultato, ok? Quindi quello che potete fare è andare a sostituire, ad esempio, ripeto, se dite che W è uguale a qualcosa che ha componenti W1 e W2, e andate a sostituire W trasposto il vettore che abbiamo appena scritto trasposto, C ha questi valori e W ovviamente è il vettore che abbiamo appena scritto, ottenete un'espressione in funzione di W1 e W2 che conterrà dei termini quadratici, perché a un certo punto W1 verrà moltiplicato con un altro W1 e W2 lo stesso, poi avrete anche dei termini misti eventualmente. Potete farlo come esercizio, un semplice esercizio, in cui vi rendete conto che quella che ottenete è un'espressione quadratica in W1 e W2. Sì, esattamente come... sia questo che questo sono le vostre variabili, ok? Questo è un modo, è un'espressione matriciale che vi permette di scrivere una funzione di due variabili, se C e W sono 2 per 2 e 1 per 2, diventa in generale vale anche per n variabili. Nel caso particolare abbiamo imposto A uguale a 0 e B uguale a 0, avrebbero potuto anche questi essere dei termini diversi, sarebbero stati dei termini costanti, dei termini di primo grado, diciamo, ok? Va bene? Ci siamo? La cosa interessante, vi dicevo, è che se voi andate a fare lo studio di questa funzione e andate a vedere quali sono le linee di livello, e qui, diciamo, ve lo già riportate, non è una cosa, diciamo, difficilissima, ma adesso non è immediata, ottenete un grafico che è questo. Adesso lasciate stare sopra la figura della discesa del gradiente che ha, di cui parliamo tra poco. Ma le curve di livello sono queste, vedete? Quello che vi dicevo prima, una sorta di scodella un po' schiacciata, quando si è schiacciata dipende da che cosa? Dipende dai valori che abbiamo assegnato alla matrice C, ok? E questo lo vedete anche perché nel momento in cui assegnate una matrice C, invece dei valori che sono questi, basta cambiare una sola cosa, cioè andare a tenere fissi i valori che avete sul... Scusate, qui cancello. Vi dicevo che quelli che sono, che vengono fissi, sono questi tre valori fondamentalmente, che sono mantenuti fissi, e quello che viene variato è invece il quarto valore, che è la entry down, riga 1, colonna 1, che viene mantenuta invariata, quindi quella che è quella che è. Se la abbassiamo, per esempio a 0,1, otteniamo una funzione che è, è una funzione dello stesso, che ha la stessa forma, ma ha dei parametri differenti, e induce questo, vedete, il fatto che la curva di livello, le curva di livello del tipo lo stesso di funzione, perché lo vedete sono delle ellissi concentriche, ok? Però sono più schiacciate. Se, nel terzo caso, quel parametro, anziché 0,5 e poi 0,1, lo fate diventare 0,01, quindi diventa una matrice fatta così, quello che vedete è la terza figura, quindi questa sorta, figura ellittica, diciamo, in una superficie, se vogliamo a forma, guardata da sopra, di ellisse, ed è una specie di scodella molto lunga, schiacciata, ok, ai lati. E questo, semplicemente cambiando, un valore di questa funzione, quindi questo è interessante, un valore di questa matrice. Allora, mi interessa dire, cosa, prendendo queste tre funzioni, cosa fare con il metodo di discesa del gradiente? Allora, proviamo a minimizzarle con il metodo di discesa del gradiente, ipotizzando, ad esempio, di partire da un punto iniziale, W con 0, pari, scusatemi, lo 0 lo mettiamo, all'alte, pari, ad esempio, a questo vettore. ok? E vedete, infatti, corrisponde esattamente al vettore W con 0, questo punto, che è quello da cui partiamo, e sarà sempre lo stesso per tutti i tre esperimenti di minimizzazione. L'altra cosa che facciamo, e facciamo, vi dicevo, un run di discesa del gradiente, con 25 iterazioni, quindi un numero di iterazioni pari a 25, e un valore di alfa pari a 10 alla minuta. Questa configurazione è la configurazione della discesa del gradiente, quindi, che vale per tutte e tre le funzioni che andiamo a minimizzare. Se noi mandiamo in esecuzione un algoritmo di discesa del gradiente sulla prima funzione, quello che accade è qui, questo che vediamo. Cioè, vi ricordo, partendo dal punto iniziale, lui trova un punto che viene calcolato a partire da quella che è il punto del gradiente, e quindi lui trova una direzione di discesa che è determinata dal gradiente. Il gradiente è ortogonale alle linee di livello, lui si muove qui, ortogonale alle linee di livello, attraversa quest'altra serie di linee di livello, e va a finire qui, nel secondo punto, che è un punto determinato da, il fattore alfa, è anche determinato ovviamente dall'ampiezza del vettore gradiente. Dopodiché da qui, troverà un altro punto, un altro punto, e segue questo percorso fino ad arrivare nei pressi, diciamo, del minimo, che è ovviamente qui, al centro di quella, di quella elisse più interna. Se ripetete la stessa cosa sulla seconda funzione, vedete che queste 25 iterazioni si schiacciano molto di più. Perché si schiacciano molto di più? Perché schiacciandosi le linee di livello, di contorno, lui tende, il vettore gradiente è una direzione che deve adattarsi a questo, e deve essere sempre ricordato e ortogonale alle linee di livello. E per essere ortogonale da qui alla linea di livello, lui deve per forza partire lungo questa direzione. Quando si ritrova da quest'altra parte, per effetto che ha fatto un passo lungo, determinato da alfa e dal vettore, dalla dimensione, dal magnitudo, dall'intensità del vettore gradiente stesso, a quel punto riparte di nuovo in direzione ortogonale, e così via. E vedete che questo effetto è chiaramente molto più marcato qua giù, perché qua giù ovviamente queste linee sono pressoché qui parallele, cosa che non accadeva qui nel primo caso, e un po' di meno nel secondo. Quindi qui comincia a rimbalzare avanzando di pochissimo, e qui si vede bene questo effetto zigzag, che è frutto proprio del fatto che il gradiente è sempre ortogonale alle linee di livello, e in certe situazioni, che dipendono da come è fatta la funzione, questo può essere un problema, perché chiaramente qui dopo 25 iterazioni ci fermiamo ben lontani dal minimo. Qui siamo a metà strada, mentre qui eravamo arrivati quasi nei pressi del minimo. Quindi questo è un esempio che vi fa capire più cose. Una, che è una funzione, diciamo, in più variabili, può essere scritta in formato vettoriale, tramite opportune appunto moltiplicazioni, vettori con matrici, vettori con vettori, e questa è una cosa che utilizzeremo in diversi, abbiamo già in qualche modo cominciato a utilizzare, ma è una notazione che utilizzeremo sempre di più in diversi esempi del corso. La seconda cosa è che appunto, un primo problema che viene, che può emergere nel metodo di scesa del gradiente, è proprio questo del comportamento a zigzag. Però, come vi dicevo, il metodo di scesa del gradiente è stato nel corso degli anni, degli ultimi anni, oggetto di studi abbastanza intensi, e questi hanno prodotto diversi miglioramenti della versione base, e, diciamo, diverse versioni, diverse modifiche, alcune delle quali permettono di ovviare in maniera abbastanza, direi, agevole, questo tipo di problematiche, utilizzando quello che viene, delle versioni di scesa del gradiente, che sono note in letteratura come, cosiddette, momentum accelerated, cioè con l'accelerazione di quello che viene chiamato il momento. Non ci entriamo nel dettaglio, però sappiate che se trovate qualcuno di queste, di questi algoritmi di scesa del gradiente, in cui c'è questa, questa, questa parola appunto, momentum accelerated, che fanno un po' di cose, tra cui anche cercare di compensare questo problema di, di, dello zigzag. Va bene? Ok? Dimmi pure. Il passo che facciamo, non mi stai parlando di questi tre casi, rimane sempre uguale? Allora, il learning rate è sempre uguale. In realtà il passo non è detto che sia uguale, perché dipende da quanto vale il gradiente. Ok? E anche qui adesso dirò qualcosa sul fatto che in realtà possiamo rendere questo indipendente, se andiamo a fare una normalizzazione. No, ci arriviamo tra poco. Benissimo. Allora. Posso cancellare qui? Adesso vediamo, andiamo a vedere l'altro, diciamo, punto un po' critico del metodo di scesa del gradiente, che è quel comportamento, vi dicevo, di rallentamento, dell'avvicinamento, man mano che andiamo verso punti stazionari, che sono punti che, matematicamente, sono caratterizzati da un vettore gradiente nullo, e quindi man mano che ci avviciniamo a questi punti di stazionarietà, il nostro vettore gradiente comincia a avere delle entri che sono quasi tutte nulle, comunque approssimativamente può essere vicino appunto ognuno al valore nullo, che è una condizione che possiamo esprimere in diversi modi, fondamentalmente il gradiente diventa circa nullo, quindi vuol dire che cosa? Che nei punti di stazionarietà il gradiente dove è perfettamente nullo, la norma di quel vettore è nulla, non c'è ombra di dubbio, ma questo significa che anche laddove comincia comunque a essere quasi nulla, anche se non abbiamo proprio raggiunto il minimo, siamo in una zona abbastanza piatta, quindi siamo arrivati a fondo valle, se siamo ancora nel nostro esempio in tre dimensioni, quindi abbiamo due coordinate, nella terza rappresentiamo la nostra funzione, quindi siamo su una collina, stiamo scendendo verso fondo valle, man mano che ci avviciniamo e il profilo della pendenza è sempre minore, e quando arriviamo a fondo valle il gradiente è nullo. E allora questo significa che cosa? Significa questo, che se andiamo a vedere la distanza che compie un singolo passo del metodo di discesa del gradiente, è sempre data dalla norma del vettore tra il punto WK e il punto WK-1, giusto? Questo l'abbiamo visto anche nelle scorse lezioni. Se lo andiamo a scrivere in questo modo, andando a mettere al posto di WK il punto WK-1-α gradiente, che è appunto come noi ricaviamo il punto WK quando utilizziamo il metodo di discesa del gradiente, questi due termini si evidono e ovviamente rimaniamo con α per, vi ricordate, l'avevamo visto, l'ampiezza del vettore direzione, che in questo caso è determinata dall'ampiezza del gradiente, è quello che vi dicevo prima che il passo è determinato dall'ampiezza del gradiente. Più siamo in una zona ad alta pendenza della funzione, più questo vettore è ampio, e quindi faremo dei passi molto lunghi in avanti. Più ci avviciniamo a un punto stazionario, più questi passi diventano piccoli, fino a potenzialmente diventare, svanire, e questo è il problema del gradiente che svanisce, che è un problema che è abbastanza ricorrente, nel senso che si può presentare in diverse situazioni, quindi la conseguenza immediata è proprio quella, che abbiamo dei progressi che possono essere ampi nei primi passi dell'algoritmo, ma diventano più piccoli man mano che ci avviciniamo ai punti stazionari. Anche questo è qualcosa che può essere in qualche modo compensato, anche qui ci sono delle variazioni sul tema dell'algoritmo di discesa del gradiente, quindi una possibile soluzione è quella che viene chiamata metodo di discesa del gradiente normalizzato. Essenzialmente cosa si fa? Si normalizza il vettore gradiente rispetto a quella che è la sua ampiezza, e in questo modo la distanza viene a dipendere da alfa, per cui insomma sono tutte soluzioni tecniche, e esclusivamente sono piccole soluzioni tecniche che possono essere appunto incluse negli algoritmi di discesa del gradiente, e sono quelle che di fatto vengono incluse nei tool che sono stati sviluppati nelle librerie di machine learning più note. Noi non andremo nel dettaglio, perché poi ci sarebbero altri dettagli tecnici per introdurre appunto queste variazioni nel metodo di discesa del gradiente, però essenzialmente rimane quello nella sua forma base. Qui abbiamo un esempio di un modo di una funzione, o meglio di due funzioni, che sono due funzioni di una variabile, in questo caso che vi fa vedere proprio questo comportamento di rallentamento nei pressi dei punti stazionari, e giusto per essere anche qui un po' precisi e rigorosi vi dico di che funzioni si trattano, abbiamo una funzione, è questa a sinistra, rappresentata a sinistra, che è una funzione di una variabile, quindi non scrivo in forma vettoriale, chiaramente non metto il segno di vettore, ma metto semplicemente la variabile v doppia, ed è una funzione di doppia alla quarta, quindi un polinomio di ordine 4 fondamentalmente, in cui mancano i termini terzo, secondo e primo ordine. Questa è una funzione che viene minimizzata in questo esempio con dieci passi di discesa del gradiente con alfa pari a 10 alla meno 1. E vedete che in questi dieci passi, se io parto dal punto che ha coordinate meno 1, e questo è un punto che sulla funzione ha questo valore, quello che vedo è che il primo passo, vedete, è un passo consistente. Perché? Perché sono in un punto ad alta pendenza, il gradiente ha un valore abbastanza elevato, per cui mi ritrovo in questo punto, per cui la funzione avrà questo valore. Vedete che al primo passo già mi sono mosso di parecchio. Dopodiché, piano piano, cosa succede? Che il vettore gradiente diventa via via più piccolo, perché vedete qui ho un fondo valle, diciamo, di questa funzione, estremamente pronunciato, per cui lì il gradiente chiaramente risulta nullo. La derivata della funzione più da questa zona qua è pressoché nulla. E quindi l'avanzamento, vedete, è molto piccolo. Quindi devo ricorrere a certe tecniche, appunto, come quella del gradiente normalizzato, per cercare di ovviare a questo problema. Stessa cosa, in qualche misura, accade nella figura di destra, dove cambia la funzione. Questa è una funzione un po' particolare, un po' ad hoc, però giusto per dirvi. Non è che capita di minimizzare funzioni come questa, che vi sto descrivendo, però giusto per capire quale può essere il problema. anche se, diciamo, non capita analiticamente, però in realtà i profili di certe funzioni di costo sono estremamente complicati comunque. Qui abbiamo il massimo al quadrato, tra 0 e 1 più 3, un doppio, meno 2.3, tutto elevato alla terza, più il massimo al quadrato tra 0 e 1 più meno 3, un doppio, più 0.7 elevato alla terza. Ok. Allora, qui c'è stato un'esecuzione a partire dal punto... Intanto guardiamo un attimo che... Allora, se voi andate a fare lo studio di una funzione, il plot, quindi il grafico di una funzione che ha questa struttura, ottenete questa funzione particolarmente strana, è una funzione non convessa, perché vedete che ha dei punti in cui la funzione poi scende, poi ha, diciamo, delle caratteristiche abbastanza strane. In particolare, qui c'è un punto di sella, anche qua. Ok. E ha dei minimi che sono... Scusatemi, i punti di sella sono al particolare 23 trentesimi e 7 trentesimi. E chiaramente qui ha un minimo locale, invece, nel punto 0,5, che, come vedete, è abbastanza evidente. Allora, il problema è che se iniziamo da questo punto, con coordinate 0, e facciamo con alfa 10 alla meno 2, 50 step di iterazioni, vedete che il primo, come esattamente nel caso sinistra, vi porta abbastanza avanti nella minimizzazione, poi man mano che arrivate vicino al punto di flesso, chiaramente rallentate, quello è un punto di stazionarietà della funzione, e voi rimanete dopo 50 iterazioni, lì ancora siete, e siete abbastanza lontani dal minimo. Ma in questo caso, visto che con il punto di flesso, abbiamo la derivata 0, cioè anche senza questo slow-croning, si bloccherebbe comunque lì? Sì, sì, ma lo slow-croning è proprio per effetto della derivata che è 0, però il problema è che in questo caso la derivata è 0, non in un punto di minimo, non ci va bene. Si bloccherebbe comunque, e infatti in quel caso la, diciamo la, la soluzione spesso e volentieri è utilizzare sia metodi appunto che normalizzano rispetto a, il valore del gradiente, ma spesso e volentieri la soluzione è anche ripartire da dei punti diversi. Quando non siamo sicuri di che cosa succede, andiamo a misurare la funzione di costo e proviamo a fare qualche run in cui magari ripartiamo da qua. A quel punto ci accorgiamo che da qui è tutta discesa e otteniamo valori più bassi. Va bene? Scusate, posso spiegare una domanda? Sì, per favore. In questo caso, quando abbiamo un problema, come possiamo soluzionarlo e non arrivare al punto minimizzato? Il punto minimizzato, il punto minimizzato? Sì, ho detto solo a te, a te, a te, a te, a te, e a te, a te, a te, a te, a te, a te, a te, a te, a te, a te, a te, a te, a te, la domanda è appunto come possiamo fare per ovviare un problema di questo genere? The solution is, one, to adopt solutions which normalize the gradient with respect to its module, but if we are stuck in a stationary point, this could be not sufficient, but what we can do is we can start, and this is typically done from different points. So we randomly choose another point, like, for instance, this one, and if we choose this one, we have a smooth transition towards the minimum, and this can solve the problem in this case. So the solution is always try different point of initialization of the gradient descent and take the best run. Ok? Ok, understand. Thank you. You're welcome. Allora, quindi, abbiamo risposto appunto che partendo da punti diversi di inizializzazione, questo può essere un qualcosa che ci aiuta. Benissimo. Ok. Prima di passare alle funzioni di secondo ordine, volevo fare un attimo un piccolo passo indietro, anche perché più volte abbiamo detto, ma abbiamo tirato in ballo l'aggettivo convesse, quindi abbiamo detto le funzioni convesse sono delle funzioni che, diciamo, ci piacciono, nel senso che da un punto di vista dell'ottimizzazione sono più semplici perché se noi utilizziamo un algoritmo di ottimizzazione locale comunque finiamo, se troviamo il minimo locale, a trovare il minimo globale. Adesso vi do giusto, ero in debito con voi di una definizione un pochino più formale di funzione conversa, quindi è una piccola digressione con un po' di formalismo matematico, però mi sembra corretto darvela così, insomma, avete modo di avere anche quella perché, diciamo, intuitivamente abbiamo detto se siamo in una dimensione 2 la funzione ha una certa forma che è quella della tazza, della scodella, eccetera. Però, diciamo, vediamo, vi ridò un po' se l'avete fatta una rinfrescata appunto su quella che è, facciamo un breve, un breve riepilogo di quelle che sono cose che forse avete visto, forse qualcuno non l'ha visto, quindi a maggior ragione può essere utile avere da parte una definizione un pochino più rigorosa e formale. per fare questo abbiamo un pochino allora, benissimo, proviamo a dare qualche definizione, la prima definizione che è trovata in matematica, se hai riprendito un libro di matematica, queste sono definizioni che dovessi ritrovare generalmente, quella di insieme convesso, un insieme C un attimo un attimo allora, vi dicevo, un insieme C viene detto convesso se per ogni x1 e x2 che appartengono a questo insieme, che appartengono quindi a C, e per ogni scalare θ compreso tra 0 e 1 vale che θ x1 più 1 meno θ moltiplicato per x2 appartiene all'insieme. Questa è la definizione di insieme convesso. Dopo la definizione di insieme convesso, quello che possiamo trovare è la definizione di funzione convessa, cioè data una funzione che prenda, ha come dominio rm e va in r, una funzione di più variabili, quindi definita su un dominio che è convesso, quindi definita in un insieme convesso, e quindi ha quelle caratteristiche che cliccavamo sopra. Questa funzione si dice convessa se per ogni x1 e x2 che appartengono al dominio, chiamiamolo così, di f, e per ogni valore θ compreso tra 0 e 1 vale la seguente disuguaglianza. La funzione valutata nel punto θx1 più 1 meno θx2 è minore uguale di θ per la funzione valutata nel punto x1 più 1 meno θ per la funzione valutata nel punto x2. Ok. Questa è la definizione matematica che vale per funzioni di n variabili. Allora, se noi le visualizziamo in due dimensioni o in tre dimensioni, sono oggetti che hanno la forma appunto della tazza, della scodella e hanno la caratteristica che se riempiamo questi oggetti, queste tazze, queste scodelle, diciamo, l'insieme risultante è un insieme convesso a sua volta. L'insieme risultante viene chiamato anche epigrafo della funzione. Molto, molto semplicemente se noi abbiamo una funzione di una sola variabile e facciamo appunto l'esempio di una funzione di una sola variabile che ha questo andamento, ad esempio, questa è una funzione intuitivamente convessa. Se voi andate ad applicare la definizione, la definizione matematica, quello che trovate è che potete trovare un punto che potete chiamare x1, ad esempio, e un altro punto che potete chiamare x2, in corrispondenza di quali chiaramente la funzione ha un valore f di x1 f di x2 e quello che potete fare è andare a disegnare questa retta che passa per questi due punti della funzione, ok? E questa retta è una retta che ha equazione allora, se voi andate, la prima cosa che potete fare è andare a vedere che l'espressione θ x1 più 1-θ di x per x2 quando θ è compreso tra 0 e 1 vi permette di coprire tutto lo spazio tra x1 e x2 perché guardate cosa succede, se θ è uguale a 0 siete su x2, se θ è uguale a 1 siete su x1, θ è un valore compreso tra 0 e 1 tutti i valori intermedi vi permettono di ricoprire tutti i punti che stanno qui al variare di θ questa retta è una retta che è identificata da dalla forma funzionale da θ fx1 più 1-θ fx2 perché? perché analogamente se θ è uguale a 0 io sono nel punto f di x2 cioè sono qua se θ è uguale a 1 sono nel punto f di x1 e quando θ è compreso tra 0 e 1 sono su tutti questi punti intermedi ok? ora quello che ci dice la definizione di funzione convessa è molto semplicemente che la funzione si trova al di sotto di questo segmento identificato che ho appena evidenziato adesso cioè quello che viene chiamato appunto l'epigrafo della funzione che sarebbe questa cosa qua è un insieme a sua volta convesso in particolare la funzione sta sotto quel quel quel quel quel quel segmento identificato da quei due punti di quella retta che attraversa la funzione e questo diciamo è una definizione un po' più è la definizione formale ecco di funzioni converse questa è in una variabile ma vale appunto anche in più variabili e mi sembrava corretto darvela ecco poi noi non la utilizzeremo più di tanto cioè la definizione utilizzeremo spesso il concetto di funzione convessa che intuitivamente abbiamo tutti presente però ecco sapere anche che esiste una definizione matematica è un qualcosa che sicuramente non sicuramente può essere utile ecco va bene detto questo adesso torniamo alla nostra alla nostra presentazione alle slide che stavamo vedendo e quindi interrompo la condivisione di di questa lavagna allora qui eravamo arrivati adesso quello che dovremmo fare è andare avanti con quelle che sono le tecniche di ottimizzazione del secondo ordine perché quelle del primo ordine le abbiamo finite però prima di procedere con le tecniche le tecniche scusate di ottimizzazione del secondo ordine che sono appunto la slide successiva volevo anche fare un'ulteriore digressione in cui vi volevo far vedere come funziona ma molto brevemente non il funzionamento dietro ma proprio l'utilizzo di un di uno strumento di differenziazione automatica c'è uno strumento che calcola in automatico derivati in particolari gradienti di funzione eccetera perché è importante questo perché vi dicevo che sono la base di dei software moderni che fanno poi ottimizzazione e che sviluppano vi permettono appunto di creare dei modelli di machine learning quindi giusto per farvi vedere l'utilizzazione mi sembra opportuno farvi così un'anticipazione di un po' di cose che poi avremo modo di vedere in laboratorio in realtà non utilizzeremo direttamente questo strumento che vi faccio vedere adesso ma implicitamente utilizzeremo degli strumenti che poi hanno all'interno quel tipo di logica va bene quindi adesso vi condivido un attimo un piccolo frammento di codice che fa esattamente questa cosa che vi stavo dicendo allora quello che vedete qui è un esempio di un notebook jupiter semplicemente è un interfaccia un sistema che permette di scrivere del codice python e mandarlo in esecuzione in maniera molto agevole ci torneremo sopra quando quando iniziamo l'esercitazione per cui adesso non mi soffermo su questo quindi questo è codice python che importa delle librerie in un ambiente che si chiama google collab ma ne riparliamo quando quando quando affronteremo la prima esercitazione però intanto già vederlo non è