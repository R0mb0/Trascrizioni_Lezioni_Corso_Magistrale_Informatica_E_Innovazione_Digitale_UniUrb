allora quindi è funzionale quest'ultimo argomento che in realtà va a condannettare un pochino il percorso che avete fatto a partire dal primo semestre dell'incontro di una C-learning sulle varie forme di apprendimento voi nel primo semestre avete visto che le macchine possono imparare questo lo dico tra virgolette questo concept di macchina può imparare è un po' peculiare però l'idea è che avete visto l'apprendimento di supervisionato direi probabilmente di qualcosa di apprendimento non supervisionato facendo vedere la macchina fanno esempi di una situazione la macchina, l'autorismo, il sistema la genere che è autonoma impara a riconoscere quella situazione a tutte le due situazioni avete visto la classificazione avete visto la regressione quindi dati tanti dati e ciascuno per esempio nel supervisionato con lo suo ribbon a pianto che mi dice sì questo è un cane sì questo è un gatto una volta che appunto l'algoritmo vede tante volte i denti di questo tipo e poi la loro etichetta comincia a capire quali sono le caratteristiche distintive del gatto quelle del cane e impara a classificare il prossimo anno con il corso del DPL vedremo proprio quali sono dove ho fatto i primi algoritmi quindi immagino che gestiscono tipicamente i dati numerici o categorici che ho visto le geometrie fanno il forest quali cose di più non so se avete visto i classificatori baiersiani insomma e dal prossimo anno vedremo come le reti neurali in particolare quelle profonde sono molto brave per esempio in questi tasti di classificazione in cui vi do una foto e ti dico lì mi sembra un gatto mi sono una macchina oppure un aereo e ci trovo tutto questo lo vedremo bene in prossimo anno allora qui ci spostiamo un pochino rientro nel quattro tipo di apprendimento e abbiamo appunto supervisionato il non supervisionato forse vi è stato azionato il concetto di semi supervisionato non so comunque faremo il fatto il prossimo anno quando vedremo per esempio gli occupatori e comunque sempre diciamo i semi supervisionato assomigliano un pochino al supervisionato con le differenze che la label non necessariamente per esempio arriva da una label che ha messo l'utente in maniera esplicita in maniera esplicita ma può essere essa stessa e automatizzata vi vi vi vi vi vi vi vi porverò che con cani e ecco questa è la quarta forma in cui invece io all'alberitmo dico poco e niente ok? quindi non gli fornisco dati non gli fornisco label non gli fornisco niente del mio mondo gli dico questo esplora il mondo cominciamo a esplorarlo e attraverso il famoso reward ok? il rinforzo reward quindi il feedback che viene esplorazione il mondo da è un'altra gente la gente impara apprende learning pian piano a capire cosa deve fare ok? quindi con questo concetto di reward ci agganciamo a tutto il mondo degli MDP che abbiamo appena visto ok? ok allora vi faccio con tutti un esempio sono l'argentino sono all'azvergast e non decidere quale blog machine voglio utilizzare il problema è che scrivo nel seguente questo è il problema è molto famoso appunto double bandit in italiano non mi viene la traduzione per cina e devo scegliere se voglio giocare con la slot machine blu o con quella russa e so questo se gioco con la slot machine blu sicuramente vinco al 100% vinco vinco vinco un dollaro se gioco con la slot machine eh ho detto tu vabbè con questa vinco sicuramente un dollaro se gioco con la slot machine con la sandecia posso vincere 2 dollari oppure posso perdere se ne non vinco niente devo scegliere quale delle due su quale delle due voglio giocare ok? allora fino ad oggi eravamo abituati a questo ovvero eravamo abituati a un problema in cui come vedete qui io lo so ho un problema descritto nella sua compliettezza quindi oltre a sapere quanto guadagno 1 piuttosto che 2 o 0 so anche con che probabilità questo capita in questo caso vabbè la probabilità 1 è facile in quest'altro caso al momento in questo slot machine da l'interno parte c'è scritto come invece ho nella descrizione del problema che vedete qua in questa posizione vi arrivo il fatto che se giocavo con la slot machine rossa con probabilità 0,75 guadagniamo 2 dollari con probabilità 0,25 guadagniamo 0 quindi qui è la descrizione del problema paro paro a come abbiamo fatto qui qui vedete un MTP come abbiamo fatto fino adesso abbiamo le T quindi le funzioni di transizione che sono queste probabilità scritte sugli archi i reward che sono questi qua sempre che trovate sugli archi di quell'azione lì e niente quindi questo è il problema nella sua compliettezza quindi io argentino dovrò riuscire a fare i calcoli famosi che abbiamo appena fatto con una come abbiamo fatto oggi con l'innovazione di Ben e capire se mi conviene giocare a V o giocare a V o giocare a V ok? questo se volete potreste risolverlo chiaramente non lo risolvo ora ma sarebbe un problema che sapreste gestire perfettamente questo punto di ricorso fate i vostri calcoli se non lo capite se si conviene giocare a V o giocare a V o vi porta sicuramente un reward 1 ok? quindi quello che dovete calcolare è se giocare rosso vi porta un reward 1 oppure no ok? date questa probabilità che va a Parola ovvero non dobbiamo più completo controllo conoscendo il nostro problema ok quindi non vedete sopra come come vedete sopra questo termine legge offline planning online online avendo io una situazione completa del problema sono in grado effettivamente di stabilire qual è l'azione migliore ad ogni passo da fare l'abbiamo visto l'abbiamo fatto siamo stati in grado di riempire questa riga definendo qual è la poi sui migliore data l'attuale conoscenza del mondo e questo l'abbiamo fatto in queste ultime due tre lezioni diverse volte ok offline planning guarda un problema che è ben scritto l'abbiamo formalizzato rivolgo a causa di spenna e di iteration quali si per risolvo il problema stabilisco l'azione migliore la faccio online planning purtroppo il mondo lo conosco solo parzialmente per esempio in questo caso non informazioni su qual è la possibilità di perdere di vincere giocando rosso quindi un po quello che avevate in questa immagine qua io capisco l'unica informazione che ho e peraltro anche questa non ho detto che io l'abbia per esempio ce l'ho e quanto guadagno se gioco di qua o gioco di là però per esempio non ho nessun tipo di informazione sulle probabilità quindi in questo caso graffa degli spazi dovrei riformulare mettendo vedete i corpondi di domanda ok questo cambia parecchio il punto di vista perché a quel punto non posso più risolvere un mdp e quindi non posso più sapere a priori qual è la scelta migliore ok da qui il concetto direi parma interno vero apprendo muovendone esplorando l'ambiente apprende per rinforzo via esplorando comincia a se vogliamo passare a chi ci sbattola per avere quello che succede con la stia anche che perche in qualche gioco però che lo chiamano sapissimo che effettivamente è meglio fare una cosa piuttosto che un'altra che poi è l'apprendimento per rinforzo quello che è stato un pochino se vogliamo ideato e pensato a partire da quello che l'apprendimento dell'essere umano curato ok allora qui c'è qualcosa che mi convince giusto anzi non mi piace questo dove devo sistemare ovvero adesso ve lo correggo ve lo ricarico corretto questo slide e qual è questo quindi l'idea non ho informazioni l'unica cosa che posso fare è esplorare il consiglio del giocare ok quindi let's play non possiamo per altro che giuntare pian piano scoprire qual è effettivamente la cosa che mi conviene fare allora qui allora dove è tutto quello che vi ho raccontato guardate un attimo che cos'è che mi ricomincia se mi rimarrate quello e guardate questo allora l'errore qui nella prima riga nel senso che se io gioco blu questo lo so lo so da questo grafo qua ma lo sapevo anche da questa immagine qua che vinto sicuramente sempre quindi qui adesso vedrete che nella forma a giocatore io dopo 56 volte con l'azione blu quindi quella è sicuramente un riume al giù cosa imparo alla fine della sequenza ho giocato sei volte blu che probabilità molto alta vada niente ho imparato questo diciamo invece rossa e una volta mi esce due una volta mi esce zero una volta mi esce zero quello che imparerò è che a seconda di quante volte mi esce due e di quante volte mi esce zero è più probabile che capiti una cosa oppure più probabile che ne capiti un'altra e allora quel punto forse piano piano vedendo l'ambiente secondo secondo me non ti rende qualcosa sarò in grado di fare una scelta un pochino più ragionata più informata mettiamola così dove l'informazione è questo che cosa scopro giocando l'informazione è esattamente che cosa scopro giocando quindi non c'è un mdp perché non posso risolvere niente mi mancano informazioni fondamentali quindi inutile non posso scriverla con la roba di perché non ce l'ho e quindi non posso risolverlo con computando e risolvendo un sistema di equazioni devo per forza agire ok che mi pensano di agire poi non è in baglio un quarto giro per capire che cosa fare qual è la scelta migliore da fare ok quindi un po questo per il passo in questo che vedete in questo questo è qua in cui avete il vostro gentino alcuni elementi non rispettiamo a farlo perché ci riportiamo dietro dalla primissima lezione ovvero la gente dell'ambiente semplice e comunque un'interazione continua tra gente ambiente questo davvero action per se per se con un'interazione con un'interazione con unicamente in questo caso quindi la gente agisce fa un'azione gioco blu l'ambiente di risponde dando di uno hai vinto un dollaro la gente agisce dando di un rosso e dal mondo che risponde dando di zero oppure due ok quello zero oppure due è the word anche perché l'azione si porta dietro quindi l'idea base questa la gente vive nel mondo riceve feedback nella forma di reward la utilità della gente si definisce proprio a partire da quei reward che vengono raccolti ok e chiaramente quello che dobbiamo imparare è qual è l'azione che odio di massimizzare l'utilità attesa quindi qui non c'è nulla di nuovo rispetto a quello che abbiamo detto quegli mt quindi massimizza l'utilità attesa in questo momento quindi e il nuovo attivamento io tengo dall'ambiente tutto il processo di apprendimento si basa su questa esplorazione che se vogliamo dire in altro modo su quello che osserviamo da azioni che eseguiamo dall'app am dall'esito delle azioni che vediamo e vedrete che ricorrerà questo termine sempre sempre le campione no letteralmente la produzione ovvero è un episodio di esplorazione io faccio un episodio di esplorazione eseguo un'azione e raccolgo vedo l'outcome di questa azione e raccolgo l'informazione che da questo outcome arriva cioè reward ok tanto per rafforzare quanto l'ha detto a voce tutto quello che vi sto raccontando in realtà non si disposta in termini di componenti fondamentali da un mtp ovvero comunque un sistema in cui ho un insieme di stati in cui ho un insieme di azioni che posso svolgere in cui so che queste azioni non sono deterministiche vero non è certo possono essere ma non è certo che l'azione deterministica cioè risulta sicuramente meno spostarmi da esse ad esse prima e non poter e ancora so che queste azioni possono costarmi o farmi guadagnare qualcosa che esiste ancora un concerto di reward il framework rimane lo stesso quello che cambia questa è ancora quello che vado a cercare è proprio la di carcelazione migliore da fare un'avvisata quello che cambia è che io non conosco né di nera oppure non conosco occhio erdo di fatto non ho completa conoscenza delle funzioni che mi servono ma non è che si mi era in questa politica ok l'unica cosa che posso fare è provare azioni e imparare da quello che osservo capiti ok questo in realtà non aggiunge nulla a quello che vedete lo sta solo formalizzando ok dubbi dubbi finora tutto fa allora vedete che vi mostrerò insieme gli algoritmi che consentono di modellare questo processo di apprendimento che rinforzano quindi noi adesso dovremo capire in che modo quale può essere la soluzione migliore per effettivamente fare questa cosa cioè io devo imparare a gente quindi no poi la mia polisi ottima migliore ottima e gli algoritmi si distinguono per due proprietà principali allora ci sono degli algoritmi che vedrete hanno come obiettivo il fatto di apprendere il modello del mondo cosa vuol dire apprendere un modello del mondo vuol dire inizialmente non conosco TOR oppure Nettine R esploro il mondo per cercare di dargli un valore ok quindi per esempio questa cosa qui è una serie di esplorazioni del mio mondo e io per esempio da questa cosa qui se l'avessi scritta bene per il vi ricordo che nella prima riga dovreste vedere tutti i dollari 1 e avrei scoperto che l'azione è play blue ha un reward 1 e una transition function probabilmente pari a 1 perché ho tenuto una serie di uni esplorando il mondo quindi imparo il mio modello imparo qual è il reward associato a un'azione imparo qual è la probabilità che quell'azione abbia successo facendo tante volte quelle azioni e scoprendo effettivamente facendo fare due volte quelle azioni in quel caso l'algoritmo viene definito model based quindi va alla ricerca di un apprendimento che riporta alla definizione del modello quindi alla definizione delle due punte di transizione e reward a partire dalla quale poi risolve il problema come un LDP classico che corrisponde a quello che abbiamo fatto finora ok in altri casi invece non mi interessa sono quelli un pochino meno intuitivi rispetto a quello che avete già visto non mi interessa imparare il modello di traduzione non mi interessa capire quindi la probabilità con cui effettivamente un'azione ha o meno di certo raccolgo i reward e scelgo quindi qual è l'azione migliore da fare sulla base dei reward raccolti quindi in qualche modo mi limito a raccogliere i valori q di ciascuno stato di ciascuno stato S a partire dalla posizione reale mi limito a raccogliere tutti questi valori q e a scelgo il punto di migliore tra tutte le azioni che posso fare ok quindi di base computo dinamicamente il valore q e sulla base di q il cil'e qual è la cosa da fare poi in maniera ortogonale possiamo distinguere questi algoritmi anche sulla base del fatto che siano passive o active apprendimento passivo oppure attivo la differenza è molto semplice nel caso che non è anche per appena apprendimento passivo l'idea è questa alla prima interazione gli informi scontri del momento di politica un po come abbiamo fatto per la policy evaluation quindi riferimento delle politiche alla prima interazione di bigu ma tu non sei niente sei un argentino che vive in un mondo che non conosce e quindi non può sapere qual è la serie di deve fare ok però ti fornisco suggerimento di un insieme di azioni da svolgere parti da quelle azioni per esplorare il tuo ambiente quindi in maniera un po guidata se vogliamo e parti da quelle azioni per esplorare l'ambiente dopodiché rivaluta la politica cioè cerca di capire se quella politica che io ti sto proponendo assolutamente ottimo o meno sulla parte delle produzioni di raccogli cioè dei reward e rispondendo l'ambiente che hanno raccogliere quindi ad ogni interazione ad un'esplorazione a partire da quella politica lì raccoglie informazioni ricordo tutta la politica se necessario perché scopri che per esempio un'azione che questa politica che proponeva è assolutamente non conveniente che apporta per esempio un reward mi vero l'apprendimento attivo invece è diciamo così più complicato possibile quello che se vogliamo può verare anche in più comune volendo però ci arriviamo un po alla fine con di questo forzo proprio l'ultimo argomento che vediamo la gente non sa niente entra nel mondo non sa proprio niente di niente non conosce il modello non conosce non diviene stabilità la storia politica deve cominciare a sfruttare in maniera fisicamente stocattica e raccogliere tutte le informazioni eventualmente per compiere un modello del mondo oppure per definire più bello e quindi di conseguente lezione questo è un po diciamo la punta dell'iceberg perché se vogliamo anche il punto dell'iceberg è il punto in cui vogliamo arrivare e questo è il full earning e l'alberismo sarsa tra le idee di cui si fa un'unità di verità ok in questo slide qua vi ho messo una sorta di recap così per farvi vedere che quindi possiamo avere diversi algoritmi che combinano un po' queste due proprietà può essere apprendimento passivo model base passivo model free app del model base eccetera ok quindi pian piano li vedremo vedremo esempi e in un percorso che dalla diciamo così dall'algoritmo più semplice va via via con il parent a trovare soluzioni un po più complesse ci porterà a esplorare un po' tutte queste combinazioni allora partiamo dalla passivo come avete capito e quindi lo bravo e tra passivo e attivo un po' più semplice perché quanto meno al mio argentino mi viene data questa informazione così iniziale ma quindi ci viene fornita una policy input ripeto può essere sbagliata e non è detto che sia giusta anzi probabilmente non è perfettamente giusto però quanto meno il suggerimento che si arriva che cosa vuol dire vuol dire questo per aiutarvi a capire vuol dire questo ok siamo alla prima iterazione ci viene suggerito però messa cosa fare ok quindi siamo in un contesto come questo guardate che visto che abbiamo discusso molto bene prima siamo in un contesto come il dice bonanza questo è un esempio veramente molto famoso ecco nell'ambito degli mdp siamo in un contesto come questo del lancio del dado ok in cui ci viene suggerita una prima policy la differenza rispetto a questo è che non c'è tutta sta roba sopra cioè io so che posso fare solo l'azione di roll e stop ma non so qual è la probabilità dell'esito dell'azione non so qual è il guadagno di ciascuna azione il costo di ciascuna azione non so niente lo scopro vivendo quindi non posso risolverlo come abbiamo fatto prima danno un valore a questi stati perché non avrei le informazioni su cui andare a fare quei calcoli e con cui riempire le famose equazioni di Bellman ok quindi vedete non conosco le funzioni di traduzione non conosco i reward l'obiettivo rimane imparare imparare imparare a prendere definire la policy quindi l'apprendimento è all'order vivo evato ok non devo fare una scelta su quale azione prendere perché mi viene suggerita la policy devo solo eseguire la policy suggerita quindi l'azione per ciascuno stato e imparare dall'esperienza che cosa succede a seguito di quell'azione ok non è un slide planning perché perché mentre per apprendere per capire per trovare la policy migliore io intanto inizio a muovermi nel mondo ovvero è come voi che andate a giocare al il daddy walker la prima volta in vita vostra non conoscete quasi nulla se non il fatto che sedendovi al tavolo potete lanciare un dado e però se vi conviene una volta che avete ottenuto un save smettere di giocare o se vi conviene continuare a giocare lo scoprirete solo pian piano che giocate perché per esempio non avete neanche l'informazione sul fatto che non avete se avete un save e decidete di smettere di giocare e guadagnereste 6 lo scopriete quando siete lì quindi vedete questo è you actually take the action significa che e il fatto sono esposto all'errore infatti viene utilizzato tantissimo questo tipo di tecnica negli ambienti simulati no in un contesto di simulazione in cui effettivamente un errore ne lo posso anche concedere allora e un primissimo esempio quindi di il primissimo esempio che vi faccio vedere è un esempio di passive learning quindi ho un esempio associato alla policy però non apprendo non voglio apprendere il modello non mi interessa quindi quello che vedrete adesso è un algoritmo di apprendimento per rinforzo passivo model free senza modello e viene chiamata viene chiamato da ex evaluation è davvero uno dei più semplici quindi valutazione diretta ok quindi quello che vedrete adesso nell'esempio è che la gente agisce seguendo la policy che gli diamo in input ogni volta che arriva in uno stato visita uno stato scrive qual è la somma dei discounted rewards che sta che ha occupato quando faccio un'azione guadagno qualcosa mi scrivo quanto ho guadagnato ok in modo tale da poter poi dopo esplorazioni multiple definire il valore di quello stato come media delle multiple esplorazioni che faccio questo viene definita dare che valutazione come vedete qui non c'è un concetto di prova definire una t prova definire una r no calcolo solo ed estribivamente immediatamente il valore degli stati allora da questo esempio quello che ho capito sicuramente quello che vi sto raccontando in maniera teorica e questo è il mondo che stiamo esplorando abbiamo già una politica definita perché come vedete ci sono certezze che mi dicono qual è l'azione che parli negli stati in particolare sempre guardando questo l'immagine di questo mondo io capisco che gli stati sono 5 ok a b c p e che ci sono due stati terminali sono a e p e lo capisco perché da lì dentro e lo rischio più un po' quindi nel terminale però indicato anche con questo doppio riguardo vedrete magari anche in altri eventi è abbastanza frequente l'utilizzo di un simbolismo che utilizza per esempio il cerchio per indicare lo stato normale e il cerchio per indicare la cosa terminale e gli stati iniziali anche questi riesco a capirvi da questa da questo grafico da questa immagine qui perché uno è il piano e l'altro è ok sono le i due stati in cui posso svolgere un'azione a più scende ma in questo caso è un grafico ok quindi quello che la gente si trova a dover fare devo aver capito che il mondo ha 5 stati che deve partire da b e da e può partire da b o da e arriverà in a o in b e iniziare a esplorare questo mondo quindi esegue come vedete una serie di esplorazioni che vengono chiamate in questa immagine qui episodi oppure campioni quindi nessuno di questo è un campione raccolgo un campione di osservazione ok allora l'episodio 1 mi dice questo questo è proprio quello che fa l'agentino ora e 10 ok sono in b la mia polisi quindi mi dice che devo andare a destra ok la polisi me lo dice quindi non è che vado o vado a destra o a sinistra vado a destra perché vado a destra perché è un agente passivo non invene con la polisi si segue quella che mi viene fatta in input la prima volta poi chiaramente il bene finirà una lì e vado a sinistra e arriva in sì benissimo quindi che cosa faccio io sono sempre un agente in grado di interagire con il mondo in questo a nell'eseguire questa azione ho la possibilità di percepire due cose lo stato in cui arrivo c e quanto guadagnare fare con l'azione in questo caso ho un word meno uno e sono stato da rio c ok fatto la stessa cosa quindi a questo punto se finisco in c chiaramente l'azione successiva per scudate partirà da c vado a vedere in c la mia polisi c cosa mi suggerisce di fare mi dice di andare ancora a destra benissimo eseguo l'azione va a destra e percependo sul mondo scopro che arrivo in d e che guadagno meno uno dopodiché quando sono in d siccome quello è uno stato terminale l'unica azione che posso fare è un po' come l'azione stop del nostro dice che abbiamo visto prima posso seguire solo l'azione exit che mi porta in uno stato diciamo così non saprei mai come disegnere in cui non posso fare nulla appunto ai guadagni a dirci ok ok e questa è la prima esplorazione ne faccio un'altra non ve la sto a leggere esplicitamente perché di fatto è identica a questa quindi rifaccio un'altra volta la stessa esplorazione mi sposto a sinistra da b seguendo la polisi finisco in c guadagno meno uno eccetera eccetera benissimo dopo aver esplorato due volte che cosa capita partendo da b provo a esplorare che cosa capita invece partendo da e quindi dico vabbè ok qui più o meno non vi so due volte non c'è la stessa cosa sono contento vediamo come si chiude partendo da e in e viene suggerita l'azione nord che mi porta in c e mi fa guadagnare meno uno da c viene chiaramente come prima suggerito di andare a destra quindi lo rifaccio e finisco in d da d al solito eseguo l'azione exit che mi porta un guadagno più 10 ecco qui c'è una cosa particolare invece che nell'ultimo episodio nell'ultimo sample finché vado devo andare a nord effettivamente chi vado ma quando sono in c e provo ad andare a destra non finisco in d come tutti gli episodi precedenti ma finisco in a quindi un guadagno un guadagno un reward meno uno di questa azione e il problema di creazione exit su a mi porta a un reward legativo meno 10 ok? benissimo quindi esplorazione del mondo a quattro episodi mi ha portato a definire i reward di tutte queste azioni che ho fatto ma noi sapremo anche già guardando questo ehm questo serico episodi ehm volendo definire anche un mdp che si sta dietro nel senso che credo tutti voi mi sapreste dare una sorta di ehm valore quantomeno approssimato della probabilità delle transition function di attuazione però non è questo che ci interessa in questo momento quello che vogliamo fare è solo una delegazione che vedete nella nella nell'ultima immagine qua una valutazione direct ok? valutazione diretta del valore di ciascuna statua quindi quello che sto facendo è di ecrisci quello che sto facendo qua per tornare al formagismo che conoscete è esattamente di ecrisci sto cercando di compilare il vtp sto cercando di compilare il vtp c volete dare un valore ok? valore di efficacia su ogni questo stato come faccio? in questo caso sono la valutazione reward come? guardate come faccio? allora b parto da b in b e con lo scoperto che partendo da b e seguendo la qualità suggerita probabilmente finirò in b quanto mi è costato? vediamo allora il guadagno finale di questo episodio di esplorazione del mondo è più 10 per arrivare in b però ho speso 2 no? quindi la prima azione mi è costata 1 la seconda mi è costata 1 e quindi sarà un 10 meno 2 quindi questa cosa nel primo episodio me lo scrivi tutti guardate il valore di b che me è attiva nel primo episodio riguarda 8 che va da 10 il guadagno e non 1 il guadagno di 8 può essere il guadagno di 8 cioè il valore come vedete io qui non sto cercando un modello questo per aiutarmi rispetto a quello che prima dicevo alle valutazioni non so se fanno un modello sto provando a farlo valutare con le separazioni direttamente per il valore nel secondo episodio di nuovo quindi il dvd si porta gli risori quindi c'è una stessa stessa cosa mi dice che guadagnerò 10 ma spendo 2 e quindi ancora una volta valori di b come somma di reward vi ricordate? come somma di reward è di nuovo 8 vi compare solo in questi due episodi quindi il valore di b finale è il valore medio il valore di b finale è il valore medio di questo episodio lì quindi è questo guardate primo episodio scrivo così questo secondo episodio scrivo così perché c'è solo in due episodi il valore medio di trento ok? quindi va bene in questo caso la media 8 più 8 diviso 2 ok? e infatti vedete che il valore di b8 guardiamo se si che quello è un po' di antipatico già antipatico questo è quello meno intuitivo ma si può anche esattamente nello stesso modo quindi dici nel primo episodio quant'è? allora quanto parvado di insieme al primo episodio? primo episodio sono qua parto da C quindi guadagno i 10 e spendo 1 quindi è tale no? quindi insomma delle reward simulativi chiaramente partendo da quello stato fino a quando il gioco non finisce cioè questo no da margine anzi no da margine portanto le mie esplorazioni i miei episodi di esplorazione terminano quando termina il gioco quindi io sono in grado di dare un valore ai miei stati solo quando è terminato la mia esplorazione cioè sono giunte in uno stato terminale poi appunto il valore di ciascuno stato stato è la somma ambitiva ovvero quanto mi piace come stato finale diciamo così mi piace più 10 mi piace meno 10 e quanto mi possiamo arrivare quindi in questo caso partendo da C arrivare al stato terminale di mi costa 1 quindi meno 1 quando mi dico di tipo mi guadagno 10 e questa è la prima esplorazione la seconda esplorazione mi dice meno 1 più 10 ancora la terza esplorazione la stessa vendita cosa la quarta esplorazione la quarta esplorazione la quarta invece mi dice un qualcosa di diverso mi fa finire il nuovo stato quindi mi costa 1 dal C finire nello stato terminale e quando serviva un quello stato in realtà aveva un bel sorpreso e quello lo stato che se faccio a terminare il gioco mi dà la peruola e quindi ho un 9, 9, 9, meno 12 ok? quindi qui ho un 7 meno 12 ci va 11 meno 11 diviso diviso diviso quanti episodi sono? 4 quindi adesso con quel valore medio 4 episodi no? quindi il valore di quel stato che avete visto è lì ok? bene? ok quindi il calcio del valore degli stati è calcolato come il valore medio del valore di quel stato che è un più basso con somma delle reward additivi ok? e o discount chiaramente ok? questo non può essere anche un discount dipende da... anzi tipicamente un discount di problemi che spesso no? perché diciamo che per semplificarvi la questione spesso da me è uno e quindi diventa una sua quantità di quello però assumete che possa non essere così benissimo quindi quindi e sull'altro non si va neanche a fare questo più 10 questo meno 10 mentre questo qui lo facciamo velocemente perché alla prima accelerazione e vale 8 da qui alla seconda accelerazione e vale meno 12 ok? 8 8 vale 8 meno 12 e 8 si è quindi meno 4 diviso 12 orazione e meno 1 ok? e questo è il motivo per cui vale meno 1 allora allora vediamo sono arrivato in fondo eh? ok allora con questo termine la lezione con questo esempio di eh direct evaluation quindi stima diretta stima diretta del valore che si è sponoso ok? e si è sponostato e qui avete un altro esempio analogo a quello che abbiamo eh ci accompagnato nelle tutte le scuole insomma quindi lo stesso esempio in cui eh non conosco però il problema non lo conosco ho solo una policy che mi viene suggerita che come per la eh che abbiamo visto ma come può essere per la descrizione che abbiamo fatto la settimana ascolta la quale suggerite sul sul sul destro destro destro destro destro destro destro inizio a esplorare prova a andare subito cosa succede? questi sono tre episodi di esplorazione del mondo e la prossima volta partiamo da qui per capire un po' quindi a partire da queste esplorazioni a partire da queste esplorazioni cosa ci posso fare però vi anticipo già che a partire da quelle esplorazioni io computo posso computare il valore di questi stati esattamente come abbiamo fatto qua quindi questa è una valutazione una stima diretta del valore dei medi ok? fatta con il risultato delle esplorazioni successive che eh abbiamo fatto quindi formalmente è questo nell'esplorazione che è quello lì che abbiamo già fatto sono quattro esplorazioni quindi quattro sample ok? in ciascun sample li ottengo il valore del reward e computerò il valore di quello stato come somma tra reward e il valore del stato in cui finisco esattamente come abbiamo fatto il valore finale per ciascuno stato per quella specifica politica che è quella che vi viene suggerita è 1 su n per la sommatoria su tutti i sample raccordi ok? ok? allora e con questo vi saluto ok? e sì non vi dico i problemi perché da quelli partirei per affrontare l'argomento e la soluzione successiva ok? ci vediamo allora giovedì quindi la prossima settimana facciamo le nostre lezioni giovedì venerdì com'è da calendario iniziale grazie