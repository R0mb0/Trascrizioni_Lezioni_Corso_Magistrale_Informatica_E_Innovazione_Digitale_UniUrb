Allora, quindi la settimana scorsa quello che abbiamo fatto è stato indirgiare il concetto di processo di marco decisionale, abbiamo visto che si basa su un concetto di utilità attesa, un aspecto di utility che mette insieme due elementi fondamentali, cioè la possibilità che un'azione abbia successo e quanto quello stato in cui andiamo a finire, questo asso prima, in cui è debbendo l'azione A, partendo da S e finiremo, ci piace. Quindi quanto quell'azione ci porta nello specifico dei processi di marco decisionale un reward più o meno positivo. Abbiamo quindi un pochino esplorato questo primo mondo per introdurre effettivamente il problema attraverso un esempio, quindi abbiamo visto gli elementi chiave di questo esempio, che ovviamente eviterò di ripetere, e siamo giunti quindi a introdurre quelli che sono gli elementi fondamentali di un processo di marco, gli elementi, ok? Quindi abbiamo visto che il problema sarà sempre distritto e questo lo vedremo anche oggi. Il mio consiglio, questo slide qua, cercate di memorizzarla bene, vedrete che per esempio in un conto la domanda di teoria è la propria, in che modo viene definita un MDP. Quindi proprio la domanda di teoria vi chiede di dire quali sono gli elementi chiave, quindi l'insieme degli stati del nostro mondo, e questo c'è sempre in tutti i problemi che facciamo, tant'è che, buongiorno tipicamente, voi avete sempre, in moltissimi casi, da riempire una tabella di questo tipo qua, in cui avete un certo numero di stati che descrivono il vostro problema, ok? Quindi, l'insieme di stati, l'insieme di azioni che possono essere fatte in questo mondo, non tutte le azioni sono possibili in tutti gli stati, questo è il problema, il problema è ovviamente completo bene. Quindi, nel caso che abbiamo visto fino alla settimana scorsa, questo caso qui, le azioni erano quattro, up, right, eccole, up, down, test, right, in questo caso erano tutte possibili in tutti gli stati, vedremo che, in esempi diversi invece, il problema vi dirà, nello stato C puoi fare solo questo, nello stato A puoi fare solo questo, bisogna che lo leggete bene, perché chiaramente questa cosa va descritta e ha un impatto poi su quello che è tutta la computazione che avalde andremo a fare. Abbiamo poi sempre una funzione di transizione che ci dice qual è la probabilità di successo dell'azione. Quindi ci dice se io passo l'azione A in S, ho probabilità X di finire in S prima, probabilità Y di finire in S seconda e in S. Abbiamo sempre la definizione di una funzione di reward, ovvero quella famosa funzione che ci dice quanto ci piace quell'azione, quanto ci piace lo stato in cui finiamo, quindi ben da un pochino dal punto di vista, però ci dice quanto effettivamente per noi è desiderabile fare quell'azione A, partendo da S. In entrambi i casi, sia per le transition function che per le reward function, potrebbe essere che il problema vi dice per esempio che l'azione A è un punto certo, cosa vuol dire che questa qui sarà una probabilità 1, se è un punto certo, so al 100%, al 100% della volta che faccio l'azione A a partire da S e finirò in S prima. Se già avverete il problema, questo semplifica spesso il calcolo, così come la funzione di reward potrebbe essere a 0, questa azione non mi dà né vantaggio né svantaggio, oppure questa azione mi dà vantaggio né svantaggio 1, vedremo al solito tutte le volte che questo tema dovete essere in grado di formalizzare e quantificare queste due funzioni. Abbiamo sempre un'informazione su quello che è un stato iniziale, da dove partiamo, e abbiamo quasi sempre l'informazione su quali sono gli stati terminali, cioè dove sostanzialmente il nostro problema termina, qual è il nostro stato obiettivo. Vedrete che anche per risolvere i problemi che tra un po' andremo a vedere, tra un po' andremo a vedere, io vi spiego un pochino questa organizzazione qua, questa formalizzazione qua, perché questi sono gli elementi, i fondamentali di un MDP, che dobbiamo prima di tutto chiarire, ovvero, cosa intendo prima di tutto, prima di metterci a fare vari calcoli e insomma definire i valori di un vario stato, eccetera, chiariamo un po', formalizziamo bene il problema in termini di transition function, reward function, eccetera. Ok. Risolvere l'MDP significa trovare la politica ottima, tipicamente, ovvero trovare quella mappa che vi avevo mostrato l'altra volta, che per ogni stato del problema mi dice qual è l'azione migliore da fare. Questo è il nostro obiettivo. Tipicamente, quando vi si chiede risolviamo un MDP, l'obiettivo è trova la politica ottima. Sappiamo come trovare questa politica ottima e per fare questo vi ho introdotto, quindi la settimana scorsa, gli elementi fondamentali che vanno a costituire, arriviamo qua, ecco, quelle che sono le equazioni di Bellman, ok? Quindi per trovare la politica ottima noi dobbiamo essere in grado di dare un valore ad ogni stato, ok? Il valore di ogni stato con le equazioni di Bellman viene computato in questo modo qua. Quindi il valore di ogni stato è dato dal valore massimo degli stati che, vado dal valore massimo tra gli stati, sì, che posso raggiungere muovendomi da S e deseguendo l'azione A, stati che io posso raggiungere con una probabilità pari a P e ottenendo un reward pari ad R, a seconda dell'azione, a seconda dell'estate che posso raggiungere. Ok? Quindi attraverso l'applicazione di un'efficienza di Bellman effettivamente riusciamo a dare un valore a tutti gli stati e poi a capire qual è l'azione migliore che in ogni stato vogliamo fare, possiamo fare. Cioè quella che l'azione migliore, cioè quella che ci sposta in uno stato tra quelli che possiamo raggiungere con valore più alto, con maximum expected utility più alta, quindi quello più preferibile. Ok? Allora questa cosa, se adesso vi è poco chiara, spero che tra poco vi sarà chiarissima con tutti gli esempi che facciamo. E allora, in particolare, in particolare quindi, ci siamo detti, ok, ho un problema come questo, come questo, con, dicevamo che qui sono 3 per 4, 12 stati, in realtà non sono proprio 12, perché questo è uno stato che non raggiungeremo ormai, così io sono due stati terminali, quindi, infatti non dobbiamo computarne il valore, dobbiamo calcolare il valore per tutti gli altri 9, ok? Quindi, diventa un sistema di equazioni simile a questo, quindi questa qui, che trovate in questo slide, è esattamente l'equazione di Bellman, specificatamente istanziata per il problema che stavamo guardando, che abbiamo scoperto la settimana scorsa, e per il calcolo del valore nella cella, nello stato di 1, questo, ok? Quindi che cos'è? è il valore massimo di che cosa? Della probabilità di finire in uno stato per il suo valore, più, dicevo, il reward di quell'azione che però era stata portata fuori, perché in questo caso era uguale per tutte le azioni che andavamo a fare. Ora, come faccio a risolvere questa equazione? Questa è un'equazione che, per calcolare V di 1,1, ha bisogno di questo stesso valore. Abbiamo detto questo, chiaramente non è possibile. Un'alternativa sarebbe procedere a livello algebrico. Cosa vuol dire a livello algebrico? Scrivo la stessa identica equazione per tutti gli stati del sistema. A quel punto che cosa ho? In questo caso, 9 equazioni con 9 variabili. Perché 9 variabili? perché, ovviamente, al massimo, il calcolo di un valore di uno stato dipende proprio nel caso peggiore dagli altri 8. Quindi, sono tecnicamente sistemi rispondibili, uno non più variabili rispetto a uno di un po' più. non è da male risolvere algebraico, algebricamente, scusate, perché non sono equazioni lineari. Perché abbiamo degli elementi come questo, max, che non ci consente di risolvere il sistema in maniera alberga. Dopo vi faccio vedere in mezzo a caso in cui posso risolverlo. Togliendo questo max, saperemo perché lo visto a fare. quindi abbiamo cominciato a ragionare su come posso effettivamente arrivare a una soluzione e vi avevo introdotto il primo algoritmo che è questo, dichiarazione del valore in cui supponiamo che diamo un valore iniziale di 0, di 0 è la prima interazione, corrisponde al concetto di prima interazione, a tutti gli stati del sistema. Valore iniziale che, per esempio, potrebbe essere il 0. Sulla base di quel valore calcolo il valore dei vari stati all'istante successivo, cioè all'istante, all'interazione successiva, cioè all'interazione 1. In questo caso, l'unico elemento che interviene è il famoso reward nei due stati finali che sono terminali che sono questi due, 1 e meno 1. Dopodiché abbiamo visto come, questo l'abbiamo visto sul finire dell'ultima lezione, come iterazione dopo iterazione questo valore comincia a propagarsi all'indietro dando un valore agli stati piacenti, ok? Sino un effetto di propagazione che iterazione dopo iterazione mi porta ad assegnare un valore a tutti gli stati, ok? Ora, vi dicevo che questo tipo di algoritmo, o comunque qui abbiamo finito la lezione, quindi abbiamo un po' affrontato la questione, ma non particolarmente con attenzione, questo tipo di algoritmo garantisce convergenza, ovvero, tutte le domande sono due, quando è che termini di fare questo update? 1, domanda 1, e ha veramente garanzia di convergenza, allora, termino quando, tramite l'azione, quella successiva, il valore del stato non cambia, ok? e questa cosa è tecnicamente già la scuola, questo lo trovate in questo immagine qui, in particolare, nel plot A, ok? In cui, proprio per questo evidente che avete appena visto, ti viene mostrato come varia il valore di vari stati, in questo caso di 5 stati, quindi questo è stato 4, 1, non stato 3, 1, stato 1, 1, le varie celle, ok? Come cambia questo valore in questo caso di una rispettazione? Ok? Inizialmente potrebbe essere anche un valore negativo, dipende dal, dai valori di gamma, da varie altre questioni, in questo esempio qua, siccome avevamo un gamma, un living reward pare a 0, ecco, qui avevamo un living reward pare a 0, esatto, che il gamma qui non c'è specificato, quindi immagino sia 1, va bene, ah no, 0,9, scusate, eccolo questo, ok? Inizialmente 0,9, living reward pare a 0, non ho valori negativi. Nell'esempio invece delle slide precedenti, questo, vi ricordate quanto era il reward? Era meno 0,04, quindi questa cosa qui fa sì che alle prime operazioni, come vediamo in questo grafico, potremmo avere alcuni stati dei valori negativi, al valore negativo, quindi di fatto, poi un esperienzo sembra che spenda solo per fare un guadagno che ne arriva dagli stati più importanti. Poi, ad allontare di una recuperazione, quindi se pensiamo il valore degli stati inizialmente lontani dagli stati terminali iniziano ad aumentare prima di un'altra convergenza in un momento in cui raggiungo appunto la convergenza sull'alore di settimità che posso, o garanzia che questa convergenza che mi raggiunge posso terminare l'iterazione per il calcolo dei valori, ok? Chiaramente nei nostri problemi che affronteremo che affrontiamo a mano durante il compito e in queste lezioni non ci arriverà mai a avere un numero di operazione particolarmente alto, quindi la convergenza è garantita nell'arco di 3, 4, 5 operazioni in modo tale che i calcoli che dovrete fare siano compatibili col tempo che avete, ok? E l'ultimissima cosa prima di iniziare con gli esercizi è questa. Allora, nelle questioni di Bellman oltre a questa formula per il massimo di effetti di effetti di effetti di effetti di effetti di effetti di effetti che poi ho visto di effetti quando andiamo a convergenza con un valore ottimo per quello stato e c'è anche questo elemento che al momento non ho ricordato che si chiama QV, il faccio affinanzato, ok? Il QV è il valore Q, ok? associato a ciascuna azione a partire dallo stato S, interrompetemi, a partire dallo stato S, io, nell'esempio che guardavamo, so che posso fare quattro operazioni, quattro azioni, up, down, left, right, ok? Per ciascuna di queste quattro azioni io ho un QV, il QV cosa mi dice? Doveva calcolare? Va calcolare sostanzialmente quanto quell'azione desiderabile in termini di probabilità di raggiungere uno degli stati successivi e re-word che quell'azione mi porta, ok? Quindi ho un'azione che quando diciamo mi dice quanto questa azione qui specifica è desiderabile in termini di esito dell'azione cioè probabilità effettivamente che questa azione mi porti nello stato che tu voglio andare ok? E quanto mi costa o mi piace quell'azione ok? Quindi potrebbe essere che in realtà quell'azione mi piace tantissimo mi porterebbe in uno stato S' primo che anche mi piace tantissimo che è estremamente desiderabile guardate su questi due elementi mi piace tantissimo l'azione e mi porta in uno stato S' primo successivo desiderabilissimi bellissimi entrambi di reward alto e stato successivo un modo desiderabile ma la probabilità è talmente bassa che alla fine quell'azione lì di ben formazione si sono tornato in un'unione in un'unione di trovare a fare perché è altrimenti poco probabile che di fatto io abbia l'edito di questo lato ok? Quindi combina queste informazioni quindi io questo coveglio lo calcolo però in azione andiamo a dire che con 4 a down right left calcolo questo coveglio poi è quello che ha un coveglio maggiore cioè quella che sostanzialmente ha un compromesso probabilità di successo reward stato che raggiungo superiore e quindi coveglio in realtà che mi ha dovuto scegliere la politica perché vi ricordo che la politica è associare a ciascuno stato S l'azione migliore l'azione migliore è scelto sulla base del coveglio ok? Mentre il valore che do allo stato S è il massimo dei coveglio ve lo faccio vedere qua quindi questa cosa che si capisce molto bene in quest'ultima slide allora qui ho dato un valore VDS che è il migliore possibile che ho raggiunto K uguale a 100 con la centesima interazione e ho raggiunto la convergenza qui ho i coveglio associati a ciascuna di quelle quattro azioni guardatele bene vi aiuta a interpretare questa immagine qui mi sta dicendo che nello stato 1 questa questa cella qua giù io ho 4 azioni up down left right l'azione up è quella che ho coveglio maggiore 0.49 quindi 0.49 sarà il valore che do a questo stato perché è il valore coveglio maggiore 3.4 3.4 azioni quando vado a definire la policy dirò che la policy in 1.1 prevede che io vada provi non vada provi a eseguire l'azione up sperando di andare qui perché perché è quella con coveglio maggiore quindi è quella che ha il maggiore compromesso probabilità di successo spostamento in uno stato che mi piace molto questo qua sopra ok è così tutte le altre quindi vedete che la policy non so se l'aveva ricordata a memoria ma l'avevamo già anticipata perché la policy in questo problema sarebbe stata up up write write write che effettivamente sono le 5 azioni che corrispondono al livello maggiori su questi 5 stagli ok allora vediamo di passare agli esempi così magari tutto vi sembrerà più facile di quello che attualmente sembra ok allora io tornerò un problema più o meno così anzi partiamo subito da questo ok perfetto ottimo Luis abbiamo tutto e partiamo da questo così alziamo subito la sticella e andiamo a vedere questo compito quindi abbiamo detto che prendevamo la soluzione deve essere scaricata questo esatto ok così ho il testo d'esame dovreste vederlo anche a casa sì perché condividevo il pdf allora siamo qua quindi vediamo di eccola benissimo ma volevo aumentare non riesco qua dalla tastiera dove è Allora, leggiamo insieme questo esercizio e iniziamo a risolverlo. Allora, io mi infilo ad essere il più finata possibile. Il mio consiglio però per voi è di farlo più spazio e più controllo della situazione. E eventualmente se vedete che a prati perco qualche pezzo mi aiutate perché cancellando poi rischio di... Allora, consideriamo questo MTP che modella questa corsa ad ostacoli. Allora, in questo caso abbiamo un pro-ostacolo, benissimo, grazie, Melda, che si trova nella produzione in G. Lo stato terminale G è come se G fosse sostanzialmente l'arrivo di questa corsa. La gente si può muovere sia a sinistra che a destra. Quindi se io vi ricorderete che sono avuto a dire più delle flystores, però come dicevamo, la prima cosa che dobbiamo fare è indicare l'insieme degli stati. Poi dovremmo indicare l'insieme delle azioni, ok, nella formalizzazione. Poi dovremmo indicare le funzioni di transizione e i reward. Nonché lo stato iniziale F0 e lo stato terminale che diciamo essere lo stato di archivo, volum. Ok? Queste sono le 5 cose che dovremo prima di tutto definire per chiarire il problema. Allora, sull'insieme degli stati penso che sia abbastanza chiaro che gli stati qui siano esattamente le... quante sono? Le 7 celle che vedete, cioè ABC, B e FG. Vengono chiamate quindi gli stati. Quindi A, B, C, B e FG sono i 7 stati del nostro problema. Allora, prima avevo fatto questa prova di accendere la telecamera e andava molto bene perché riuscivate a vedere bene anche lo schermo. Quindi mi confermate che vedete... Sì, ok. Quindi questi sono i 7 stati del problema. Le azioni stiamo leggendo che sono sicuramente almeno 2, left e right. Io scrivo così perché poi dovremo usarle talmente tante volte qui. Senti di rispondere. Sto cercando di essere... No, ho un motivo. Ok. In ogni stato posso fare questa cosa, left e right. Ok? Però, attenzione, se la gente si dovesse trovare nella posizione C, quindi nello stato C, non può muoversi a destra, ma può solo saltare. Ok? Quindi, nel descrivere il problema... Io volevo continuare, volevo dire un attimo questo. Sì, perfetto. Sì, vedete bene. Sì, vedete. Nel descrivere il problema dovremo occupare di che le azioni left e right sono disponibili per ogni S, eccetto C. Perché in C ho solo due azioni che sono left e C, ma non posso fare la right, posso fare jump. Ok? Ok? Ok. Ora, il problema è che questa azione di saltare non ha esito certo. Quindi, questo significa che adesso ci possiamo muovere sotto nella descrizione delle funzioni di transizione. Allora, io vi formalizzeremo questo... Stiamo leggendo in questo modo. Allora, per ogni S, sempre ad eccezione di C, va bene, eccetto C, per ogni S, la probabilità di andare a sinistra e finire nel suo primo, A è riuscito a certo. Poi, la probabilità dell'azione di andare a sinistra e finire nel suo primo, anche questa è 1. Quindi, l'azione di andare a sinistra e andare a sinistra, ma sempre è tutto certo per ogni S ad eccezione di C. Mentre in C abbiamo detto che le azioni da svolgere che posso fare sono andare a sinistra, quindi qui guardate, vi scrivo direttamente in C perché questo è specifico per lo stato C. Quindi, nello stato C, se vado a sinistra, finisco in B, ok? Così vedete anche un esempio specifico, invece che vogliessi che prima. Vado in B e questa cosa ha il punto certo. Mentre, se da C decido di fare un salto, finirò in E con probabilità 0.5, finirò in D con probabilità 0.5. Quindi potrei vedere quanto quello sa e fermarmi in D. Ok? Ok? Ora, qui ovviamente... Scusa? Scusa. Perché me lo ricordo a memoria, ma hai ragione. Hai ragione, c'è scritto sotto. C'è scritto qua sotto. Adesso ci arriviamo. Ve lo finisco di leggere. E allora, la gente si muove sinistra-destra, se la gente è nella posizione C, non può andare a destra, ma può solo saltare. E questo potrebbe risultare finire da cadere qui, oppure finire saltando con successo l'ostacolo. Ok? I reward sono mostrati nella figura, dopo li scriviamo. E assumiamo un discount factor gamma pari a 1. Gamma pari a 1, se lo scriviamo qua sopra, non potrebbe essere precisi, ci aiuterà sicuramente nel fare i calcoli che stiamo per andare a fare. Però, insomma... Ok. Quindi volevo dire un'altra cosa. Sì, sul gamma non ho aggiunto nulla oggi nel farvi riassunto, perché ne abbiamo parlato all'ingolo la settimana scorsa. Lo avete visto come un elemento nel calcolo delle equazioni di Bellman. Vedremo un esempio, un gamma diverso da 1, lo tratteremo, ma di base, in questo caso, ce l'abbiamo a 1, partiamo da questo. Allora, per quanto riguarda le funzioni di transizione, quindi questo, il problema lo consigliamo come stavo scrivendo io. Quindi se le azioni right-left sono deterministiche, come ho scritto, mentre l'azione di saltare non è deterministica, e con probabilità 0,5 finisco in E, con probabilità 0,5 fallisco il salto e finisco in D. Ok? Ricordati che la gente non può esistere la suonorize in C, ma questo l'abbiamo scritto sopra. Ok. Ultima cosa, reward. Ok? I reward delle azioni. Quindi, questo va formalizzato. Qui allora, io sulla formalizzazione, ci guardo o non ci guardo, questo serve a voi per avere, chiaramente, quando poi andate a risolvere il problema, se avete formalizzato le cose bene, le avete scritte bene, quello che qui è scritto in testo, ce l'avete scritto come formule, chiaramente è più semplice per me. Ok. Ok. Per quanto riguarda un reward, quindi direi che possiamo scrivere questo. Quindi, se eseguo l'azione right, qualunque sia S, il reward è più 1, poi non sappiamo, vediamo il sito top, dove posso eseguire l'azione right, dove non la posso volere, se eseguo l'azione left, il reward è 1,2. Ok. Poi, invece, per l'azione jump dobbiamo scrivere il reward un pochino più precise. Quindi, se eseguo l'azione jump dobbiamo scrivere il reward, a partire sempre dal ciclo superforzo, come un concorso in cui la posso eseguire. L'azione jump dobbiamo scrivere il reward, se finisco in D il reward è più 1, mentre se finisco in D il reward è più 4. Quindi, voi scrivere quello bene, voi che avete spazio, io ne ho posto in foglio, ordinandole ultimamente queste cose, non ho finito, perché c'è un'ottimissima cosa che devo scrivere, ovvero il fatto che in G, quindi il reward, per se eseguendo l'azione right, t-mg è uguale a 10 più 10. Tutto va di un pochino. Ok? Pi 10. E, anzi, là giù vedete che in G non potrebbe eseguire l'azione left né l'azione right, ma questo almeno non so per essere, perché di fatto in Gelo quando terminale, è chiaramente lì il gioco termine. Ok. Il gioco termine, quindi abbiamo detto che lo stato terminale o gol è G, e lo stato indicale è BCE. Quindi continuiamo con scritto di elementi chiaro che questo problema. Chiaramente questa roba me la cancellerò, dovrò per parte di perdermi una roba e voi mi aiuterete, ma andiamo anche in tutto. Ok? Allora, andiamo avanti. Problemi con la descrizione, è qua chiaro cosa dobbiamo fare? Cioè, diciamo, ancora tu voglio fare, non l'abbiamo, ma chiaro qual è il problema. Quindi questo è il primo sforzo che dovete fare, formalizzare così il problema. Ok. Ora, in questo, come vi facevo vedere l'altra volta, perché questo è il compito che vi ho fatto vedere già l'altra volta, poi non l'avremmo risolto, ci sono due esercizi. Il primo un pochino più facile, il secondo che vi richiede un po' più di computazione, di sforzo, di calcolo. Ok? E infatti ve lo ve lo dato come esercizio per la roba. Allora, leggiamo la prima cosa che dobbiamo fare. Per una policy, quindi in qualche modo tu fai un problema e ci dici, guarda, io ti dico già che assumiamo che la policy sia già specificata, ovvero l'azione che devi fare, questo vedrete tra un attimo, ci avvicina molto al secondo modo di calcolare i valori del problema MTP per ciascuno di quegli stati, che si chiama operazioni delle politiche. Possiamo già fare questo esercizio perché qui non c'è alcuna operazione, però si avvicina perché in qualche modo tu vedete che adesso già ci viene detto, guarda, non ti chiedo di calcolare tutto, scrivendo che tu puoi fare quello che vuoi, che in tutto già ti definisci una policy che per ogni stato specifica l'azione di modersi in avanti, ovvero in ogni stato puoi fare solo l'azione di andare a destra oppure di saltare. Cosa vuol dire? La policy è questa, ti ricordo. Allora, la policy ci dice l'azione da fare? Un giro, lo metto perché abbiamo detto che l'ho stato terminato. Allora, vuol dire la policy mi sta dicendo che se sono in ma posso andare solo a destra, se sono in B posso andare solo a destra, se sono in C ricordo che io non posso andare a destra perché l'ho definita il problema, l'abbiamo scritto, posso solo saltare, se sono in D posso andare a destra, se sono in A destra, se sono in A destra. Questa è la policy che ci viene indicata. Queste sono le azioni che posso fare, che sono consentite. Quindi per una policy che ci dice di andare solo in avanti e quindi di dare solo questa situazione in ciascuno stato, calcola il valore per questa policy dello stato C. Calcola il valore per questa. Calcola il valore per questa. Calcola il valore per questa. Io vado a prendere un altro gesto, voi provate ad iniziare un attimo a puntare, come questa cosa vi fa vedere questo. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Grazie. Allora, abbiamo scritto prima, io spero che voi l'abbiate sotto, che... Ecco, l'ho formalizzato prima così, tu lo puoi, dite se mi aiutate, se mi seguite. Allora, io posso eseguire la semengiante, so che questa opzione può avere due enti, portarmi in D oppure in E, l'abbiamo scritto prima. Con questa probabilità mi porterai in D e seguendo quella formula lì, quindi questa probabilità andrà moltiplicata al reward di quell'azione, ok, più gamma per il valore di D, ok? E così sto valutando l'esito, diciamo, peggiore di quest'azione, cioè quello in cui cado, faccio vedere l'ostacolo e rimango in D, invece che poter andare in E. Ora, siccome appunto quest'azione ha due possibili esiti e gli stati in cui finisco sono due, con gli assi primo, sono i due stati in cui posso finire eseguendo l'azione A, salta. 1 e B e l'altro E. Quindi vado a sommare questo, ovvero il fatto che finisco effettivamente in E. Ok. Siamo? Ok. Perché questo lo conosco, questo lo conosco, questo e questo non lo conosco. No? VB e VB è, ma non sembra un valore come lo abbiamo. Quindi dobbiamo per forza calcolarlo. Cosa vuol dire calcolarlo? Vuol dire che devo fare qualcosa per B per E e poi scoprirete che E dipende da S, quindi calcoleremo anche di S. Perfetto. Iniziamo con VB. Ok. Di quanto che continuo a ribadire il fatto che in questo caso le cose sono un po' più facili perché so già che voglio andare avanti. Quindi questo B per me... Scusa. Scusa. E questo si può andare a cuvegli. Esatto. Bravissima. In questo caso mi consiglio del cuvegli perché ho già deciso che per me in qualche modo, alla fine V era un massimo, no? Dei valditi e i cuvegli, in questo caso mi interessa solo un'azione, non mi interessa qualunque accrevo, non mi interessa solo una e a questo mi attengo per calcolare quali dei valori degli stati. Quindi se voi voleste proprio scrivere, va avanti insomma, move forward. In V non c'è questo concetto di jump, move forward e write. Ok? Ok? Ok? Per me è un po' più facile perché... Allora, l'azione è alta, quindi se ne... Adesso io parlo di un livello, quindi non... Ricciamo fare un livello di sintomi, sì, sì, più gamba per un dietro. Ok? Testa qua da sotto. Due. Ancora una volta, posso andare sulla destra. Quindi questo va tutto, ve lo dovete scrivere qui, questo è un foglio perché è inestabile esplicitarlo anche per le puntate di dietro. Quindi il livello di un po' più in F, con un curve forward, e poi lo andremo a vedere, è più gamba per un dietro. Beneissimo. Niente non è. Oggi è che, anche qui, è un... o VDS, anche qui, è... Da F, quindi va in V... Con un reward, F. Questo è per questo è... Va in V... Più gamba per un DG. Ok. Finalmente possiamo iniziare a fare qualche calcolo. Perché? G è un salto terminale e quindi non ha il passato valore. In quanto sono terminale, sono dentro di un formato G. Ok? Posso calcolare il valore di QDS. Questa qui, quanto vale? Questa è la probabilità di finire in G. Vale... Questa è la probabilità. Ok? 1. Ok? Moltiplicato per 10, come diceva, il nostro collega, perché questa è un'azione che ci piace molto. Finalmente raggiungiamo il traguardo e quindi siamo felici. Perciò questo vale dirsi. Ok? Benissimo. Ok. Andiamo a retorno. Allora, 2, E, a questo punto abbiamo il valore VDS. Ok? Gami l'abbiamo detto su WIDO, perché io l'ho lasciato, ma per farvi il castiglio di formule complete l'avremo dato tutto il toglievo di presidenza. Ok? Ok? In reward 5, in questo caso, è 1. Non c'è il problema se ci vanno le formule grazie che ci ricordiamo. Rimettiamo il problema così se... Ok. Quindi il reward per andare da E ad F è 1. Sono proprio qui sulla parla di. Moltiplicato per la probabilità che è 1. Perché ancora qui siamo nel contesto di aziende certe. Quindi questa roba ha preso 1 di. Benissimo. Passo la piccola cosa sto, tragamma è sempre 1. Questo è sempre 11. Questa ancora si barattina. È un'azione certa. Va beh, è reward 1 ed è un'azione certa. Ok? Quindi questo è stato proprio qui. Mi fermate se vedevi più qui? Il fio è son... Questo è la sua 7. Allora, quindi sto per fare cose. Si complicano un pochino, ma questo è veramente semplice in realtà. Quindi ancora siamo nel mondo del sattibile. Allora, qui cominciamo a sostituire. V di E e un U di S. V di P. Va beh, è dove di P. Ok? Poi, l'azione jump. L'azione jump. Vediamo. Perché qui bisogna stare attenti. Allora, l'azione jump che ha successo, cioè che da C ne porta né a reward 4. È questa. L'azione jump che da C ne porta né a reward 4. Quindi è sostituito col 4. E probabilità di successo è 0,5. Prima un mezzo si aiuta per fare i calcoli. Ok? Sopra invece l'azione E. Questa qui che da C ne porta in B a reward 1. Ci piace un po' di meno. E sempre il progettato a un mezzo. Ok? Quindi qui abbiamo un mezzo per 13 più un mezzo per 15 questo. Ok? Poi con i punti. Quindi è un mezzo per 28 quindi è 14. Sono troppo veloce anche di 3. Ok, perfetto. Allora, fino a qui sono bene? Sì. Ok, perfetto. Allora, fino a qui sono bene? Sì. Sì. Sì. 1,5 per 12 più un mezzo per un più 12. 1,5 per un più 15. Ok? Ok. Quindi guardate. Quindi scriviamo così. 1,5 per 13 più 1,5 per 15. Ok? Quindi questo qua diventa un mezzo per 15. Quindi diventa un mezzo per 15. Quindi diventa 14. Allora, principi della lega C insomma. del 14 di per 15. Allora che vogliamo ad missing a 10. Quindi facciamo Dana 15. Quello è materiale presentato una cellulare che utilizziamoci un terminato. In particolare ci si chiude. Secondo il presente. Che bolletto del 15RR a 14 di per 22. Quindi puoi fare il primo modo. Allora, principi del esercizio, considerando solo una fase specificata, per poter andare sulla quantità, per tutti i record di scienza. Abbiamo fatto un quattro undici. Ok? Io adesso in realtà ve l'ho fatto facendo tutti i vari passaggi, perché spero che mi rimanga un po' in testa. Ok? Volendo, quando diventate un po' usi, insomma così, un po' più confidenti con questo genere di problema, vi avvolgerete che in questo caso, se le probabilità erano 1, si potevano un po' semplificare i calci di sotto, però ci sta fare passo passo. Ok? Ok. Allora, siccome sicuramente voi non vi accontenterete di questa cosa, ma vorrete anche fare l'esercizio sotto, lo risolviamo, così vi faccio vedere in che modo cambia tutto quello che abbiamo fatto nel caso in cui io non abbia una policy definita. E dobbiamo effettivamente risolvere l'algoritmo di interazione dei valori. Ok? Allora, magari questo adesso ve lo riposto, e poi se lo provate a finire a caso, a parte che avete le soluzioni, ma giusto per farvi altri esempi, non per altri motivi, insomma, perché dopo ad un certo punto diventa sguardo all'orario, diventa solo computazione, diventa solo matematica. La cosa importante, spero che abbiate capito che la cosa importante è impostare bene l'esercizio, perché poi i calcoli saranno comunque passibili. Ok? Allora, in questo caso, invece sotto, io non ho idea della politica. che li cancelliamo qua tutto, perché tecnicamente posso fare... non lo so quale l'azione migliore da fare in un nuovo lavoro, ancora non l'ho ripreso, ma vuol dire che io devo veramente fare questo, e lo scrivo bene per lo stato A. Allora, non ve lo scrivo bene per lo stato A, perché è un po' facile, perché là posso andare solo in avanti. Ovviamente no, non posso andare indietro solo all'inizio. Ve lo scrivo per lo stato B. Allora, devo fare un po' questo. Allora, questo valore dipenderà da che cosa? dal valore massimo, ok, con tutte le azioni che posso fare in B, le due value che da B, eseguendo la C, mi porteranno da qualche parte. In particolare qua, questa cosa qua, vuol dire, e uso lo stesso formalismo che le prime le cliente, che io ho due azioni possibili. Quindi, una è via sinistra e l'altra è via destra, ok? Quindi, il valore di B dipenderà dal massimo di quei due. Così come l'azione che qui andrò a mettere, dicendo, è la migliore, l'azione che qui andrò a mettere, dicendo, è quella associata al cuverso alto. Quindi è quella che, sostanzialmente, la lettera alto, mi porta un cuverso alto. Come calco i cuveri per let e come calco i cuveri per alt? Allora, il cuveri per let è questo. Ve lo scrivo in maniera formale ora, poi diventerà modificato, quando è formale intensiva, quindi tutto. Adver let, che va a voto per la sinistra, da B, finisci di matica. Multiplicato qualche cosa, per il famoso lavoro che abbiamo scritto prima, che da B, facendo le reazioni let, mi porta a prima, più gamma, non lo scrivo, che è un'attività quadro che è uno, per, guarda, più, G, o un po' di ergola, perché qui non c'è nessuna sommatoria. Perché non c'è nessuna sommatoria? Perché queste, quelli qui, sono certe, ovvero, lo so, che si va da sinistra, e sicuramente che lo si va da sinistra. Quindi non devo considerare, come è il caso prima di jump, il fatto che salto, o finisco in via, finisco in via. Ok. Stessa cosa sotto per write, quello che faremo, perché se vado a vedere, chiaramente nel principio l'anno, finisco in via, e scrivo la stessa identica cosa. Più V, V, C. Ok? Dobbiamo calcolare più. Allora, come al solito, ci diciamo, vabbè, io non lo conosco V di A, V, C, perché di base, allora, questa, vale 1, l'abbiamo detto all'inizio, quando abbiamo vestito il nostro prodotto da A. Ma non c'è ricerto, è mai ricerto. Questo reward vale meno 2, perché quello che mi porta indietro è questo reward, vale 1. non so quanto vale 2 di A, e quanto vale 2 di C. Ok. Siamo nel contesto dell'algoritmo di interazione dei valori, questo, in cui, quindi, adesso scrivo una tabella un po' diversa, quindi non scrivo di politica, ma scrivo i valori. in cui per K uguale a 0, prima interazione, tutta storia una valoria di 1, l'iterazione dei valori, assumo che hanno valori di 1. Quello che sto calcolando adesso, quindi, è un K uguale a 1. Quindi, per K uguale a 1, l'iterazione di questa, K uguale a 1. Quanto vale di B, dipende da tutti gli altri, c'è una coincidenza. Questo è un vero, V di C, lo stesso è un vero, e quindi questa cosa risulta anche cosa? L'azione LEP, questa, ha valore 1 per meno 2 più 0. Quindi, l'algoritmo di interazione dei valori, stiamo applicando, ci dice, siccome ho una dipendenza multipla, B dipende da A da C, C dipende da B e da D, non riesco a risolverlo, allora assumo, l'istante 0, K uguale a 0, valgono tutti 0. Poi, K1 lo calcoleremo, e facciamo questa cosa tante volte, fino a quando non raggiungiamo la convergenza, cioè i valori che raggiungiamo, sono quei, non cambiano mai, per le iterazioni. Questa è la prima iterazione, un valore. ok, quindi, questo qui, 1 per meno 2 fa, meno 2, 1 per 1 fa 1, quindi, vado a se, che V di B è uguale a, quindi, questo fa 1, 1, ma l'1. quindi io qui adesso scrivo 1 ok vi suggerisco sempre perché a seconda degli esercizi vi può far comodo in questo caso non vi serve ma in altri esercizi vi può servire oltre a memorizzare il valore 1 memorizzate anche qual è l'azione che è spruzzata a quel valore perché? perché quell'azione lì che è un famoso diciamo così l'azione è spruzzata al governo ok è l'azione migliore quella che se dovessimo definire una politica andremo a scegliere ovvero in B io sceglierei sicuramente di indeguire l'azione right perché è l'azione che mi garantisce right il risultato montone ok memorizzate quello da qualche parte o lo scrivete qui oppure se ne vedete bene significa nel foglio perché potrebbe succedere ripeto non è il caso che puoi scegliere 5 ma è un altro po' più care che la domanda successiva vi dico ok bravi avete trovato tutti i valori adesso dammi la politica dal valore 1 voi non sapete qual è la politica voi sapete che 1 è il valore di quello stato ma la politica è dato uno stato dammi l'azione quindi dovete risalire all'azione che vi garantisce quel valore di rinchi se lo risalate subito quel valore di rinchiare allora siccome risolverlo tutto potrebbe richiedere più o meno tutta la lezione più o meno adesso vi ricordo la pagna facciamo fare più fatica e farlo un'azione di risultato provato risolverlo a casa di noi mi fa più di dire vuole risolverlo tutto dicemi voi io ce l'ho fatto non ho problemi quantomeno potrei risolvermi eh ma che quanto vuole tutta la realtà forse mi dispiace ha trovato la palla a casa se avete problemi allora a parte che le soluzioni ci sono in questo compito ci sono le soluzioni e quindi i valori finali li avete però più che quello mi interessa che voi risate risolverlo quindi fate tutti questi calcoli per avere i valori all'istante k uguale a 1 come vedete le due occhi vi chiede anche valori all'istante k uguale a 2 quindi vorrebbe dire che all'istante successivo voi sapete che vc dipende anche la vc no stava andando i valori i valori dipenderanno dai valori a e dc all'istante 1 no all'istante ok allora diciamo diciamo che questa interazione dei valori non di ma di così vi costringo a pensare a ragnare una stessa motivazione di fanno esempio sennò è troppo insomma non riesco a fare sufficienti esercizi comunque allora va bene allora facciamo così adesso faccio questo esempio iterazione su questo sì iterazione dei valori su questo altro esempio vediamo se con questo altro esempio riuscite poi a risolvere questo anche questo con 15 rai let 15 quindi sono tante operazioni cerchiamo di insomma convergere un po' prima volevo farvene vedere un altro che andava convergente velocemente vediamo se allora andava convergente velocemente questo qui poi volevo fare le interazioni delle politiche vabbè andiamo un po' per ordine allora vi leggo quest'altro problema considerate questo è un nuovo problema domande su guardami su le domande che volete posso andare avanti proviamo ragionare un altro ok considerato questo esempio gli stati sono questi nodi che ho fatto questo grafo qua e le transizioni sono chiare posso passare da un stato all'altro seguendo le frecce attenzione che le frecce non sono unidirezionali ma non sono anche tutte bidirezionali cosa vuol dire io da per esempio guardando questo grafo capisco che posso solo rimanere di base non posso andare in B perché la freccia qua è unidirezionali mentre da B ci posso fare un b program fin dietro ok i reward per ciascuna transizione sono indicati dal numero che abbiamo qua sull'arco ok per esempio andare dallo stato B allo stato A mi dà un reward di 10 ma andare da A con l'auto anello rimanendo in A mi dà un reward pari a 0 questo ok alcune transizioni sono consentite come da B scusa non sono consentite come da B quindi da B non ci posso andare come vi dicevo e le transizioni sono deterministiche il che vuol dire che la nostra funzione la funzione di transizione sarà molto semplice in questo caso perché sarà sempre a 1 ok quindi se la gente può scegliere tra uno stato tra andare da una parte dall'altra raggiungerà quello stato sempre comunque con probabilità 1 ok ora dobbiamo scrivere qual è l'azione ottima scusate in ciascuno step ciascuno stato punto e se il discount è gamma uguale a 1 quindi ancora una volta discount tra virgolette facile ok allora possiamo descrivere per il caso di questo modo qui quindi gli stati sono questi a b c d f ok le azioni dipende ok qui potete decidere di formalizzarlo oppure no quindi voi volete di basta avere il grafo sotto oppure lo volete scrivere per esempio qui l'unica azione consentita è quella di bar forza a quindi ho due azioni che posso fare che l'unica azione mi porta un a e che l'unica di un c per esempio ok quindi questo se volete lo potete scrivere oppure potete tenervi sotto il grafo che di base scrive in modo diverso diciamo quello che possiamo scrivere anche in maniera abbastanza facile questo cioè che qualunque sia la azione che in esse faccio ho la certezza che per questo tipo di problema la probabilità è sempre 1 mentre per quanto riguarda un word dovresti scrivere una cosa di genera per esempio questo io so che in a posso fare una sola azione e che quell'azione mi lascia in a che è una volta inzione oppure posso scrivere questo e qui è un'azione qui si apri perché da p posso fare due azioni abbiamo che mi porta oppure potete scrivere l'ex che le volete scrivere l'ex che adesso io l'immagine a sinistra mi porta in a con word 10 oppure da d a vado ad s r ok questa cosa qui la potete scrivere tutta ok che qui ok torina che azione devo fare qui è facile perché la quindi per forza farò quella per esempio questa parlando in via rivoluzioni possibili qual è la migliore non sappiamo dobbiamo scoprirlo e vedremo come in c quali azioni posso fare quindi in cina posso fare due qual è la migliore e devo ripetere in città questo è quello che questo percibio vi chiedo di fare ok allora abbiamo detto che per rispondere a questa domanda io devo seguendo l'algoritmo di interazione dei valori calcolare valori per tutti gli stati fino a convergenza ovvero fino a quando all'interazione successiva non cambia in più niente a quel punto sono in grado di dire per ciascuno stato qual è l'azione migliore da fare adesso la polisi con l'azione lasciando tutto a un punto di domanda allora allora qui la lunghezza dell'orizzonte mi dice quanti passi successivi devo considerare per calcolare quel valore ok assumendo appunto che gamma uguale a 1 che vuol dire che che gamma uguale a 1 non è aspetta prova a spiegarmi meglio facendoti questo momento guarda allora sui niente per esempio decido di andare in c poi decido di andare in d poi decido di tornare in c poi decido di tornare in d questa cosa qui se ci pensi può durare al infinito ok se io metto un limite a all'orizzonte di azioni svolgibili cioè il numero di passi massimi che io voglio fare e di cui voglio tenere in conto per calcolare il valore di ciascuna di queste ad un certo punto mi fermo proprio perché dopo 15 passi non mi interessa più cosa succeda ok in questo caso qua potrebbe essere la potresti rileggere dicendo per calcolare i valori al massimo per 15 di ok però non è detto che saranno le 15 al massimo parli quindi tu consideri sto valore solo al massimo non davanti di 15 ok allora la nostra famosa tabella che dovremmo andare a riempire sarà questa quindi a di ok e poi potrete fare questo v0 v1 v2 ok eccetera eccetera eccetera fino a convergenza allora per il momento inizio di valori non sappiamo che possiamo dare un valore a questi ok e adesso vediamo come calcolare il valore a 1 ok allora facciamolo qui dentro però però è facile perché questo è solo un'azione consentita quindi quel famoso massimo sulle azioni ok in questo caso lo posso identificare perché tanto l'azione è una sola non vado a cercare nessunissimo ok in questo caso quindi è la transizione che però abbiamo detto deterministica quindi è 1 ok moltiplicato per il reward di quell'azione benissimo più il valore di a all'istante precedente che è vero benissimo mi ha già perso là quindi 1 di a è 0 andiamo avanti 1 di b perché l'anzimo è un pochino più interessante forse non è un pochino allora 1 di b in questo caso invece attenzione le azioni che posso fare sono due left e right left vado in a right vado in c ok allora la transizione è ancora deterministica questo non cambia mi semplifica moltissimo per il reward per andare a sinistra 10 tutto più il valore di a che è 0 mi porta a 10 ok mentre andando a destra è 1 3 quanti non hai inizio giusto più il valore di c più 0 quindi questo è 3 allora quanto sarà meglio tutti è il massimo di questi due quindi di 4 10 e come vi dicevo prima scriveteli qua sotto questo quel 10 lo tengo se vado a sinistra quindi supponendo questo perché supponendo che noi convergiamo non è questo il caso ma supponiamo di convergere se io poi devo andare a compiere questa tabella posso subito dire quindi l'azione più conveniente è quella che andare a sinistra un po' di storco sei convinto no io ok ok capito perché l'interazione dei valori è abbastanza semplice il primo ho detto alla fine secondo me si pronta solo di fare calcolo ma riuscite a farlo anche da soli l'altro è da soli probabilmente allora 1 sì allora attenzio due azioni left e right il suo valore sarà il massimo di questa quindi entrambi 0 vero? bello 0 per più 1 per 0 più 0 attenzione che dovete prendere sempre il valore distante precedente quindi non 10 che avete appena calcolato ma quello distante precedente e la stessa cosa è stretto quindi in questo caso il valore rimane 0 potrebbe capitare che c'è una azione più probabile qui vedete entrambe le azioni left e right sono i più probabili scusate non i più probabili con il suo valore quindi le voce che segnali tutte 2 secondo me molti di questi a questo punto abbiamo bisogno di calcolare il vd cioè di calcolarlo di formalizzarlo no perché vdd che sarà 1 moltiplicato per 0 più 0 e 1 moltiplicato per 0 più 0 con un corone valore valido 0 e lo stesso valido 0 perché i reward sono a 0 entrambi, mentre in S fa se può essere qualcosa di strano, perché c'è una azione sola possibile, quindi non devo, sarà un massimo, c'è una azione sola possibile, transizione 1, terra del cuore 1. Bene? E questo è l'alpanello. Poi, il primo è tutto. Allora, A non può che valere zero, perché c'è solo un'opzione possibile, che è l'alpanello, con reward zero. A valeva zero prima, quindi non valeva zero, valva per sempre zero. Questo in realtà lo possiamo già prevedere. Vogliamo vedere B. Di cambia, direi. Vediamo il partito di corre. Allora, Allora, con un calcolo B. Let write max, transizione 1, per, 10, che è reward, più 1. No, non cambia. Questo zero qua è il quale. se vado a sinistra, e se vado a destra, uno più zero, e quindi rimane a destra. Rimane a destra. Ok? Se qualcuno cambia, è possibile, è possibile. E se avete un left, si potrebbero cambiare, sì. Allora, left, right, transizione 1, per, se vado a sinistra, reward zero, visto sì, più il valore di B. Oh, finalmente. il valore di B. x. Quindi questo, vale di x. E andrò a sinistra, quindi, vi porto un campaggio di x. Allora, la destra, invece, le porterai di 0, qui 0, 0. Quindi, cosa farò? Adesso, il punto di x, vale di x, e ne ho detto questo. left, x non cambia, perché dipende sempre da questo, da questo, quindi, x non cambia, questo, questo, e potrebbe cambiare, perché e, mi consente di arrivare in s, ed x adesso ha un valore diverso. Quindi, v de, da cosa ho dato? Se vado a sinistra, reward zero, v de 0, quindi non guarda di niente. Se vado a destra, è, se vado a destra, right, spogliamo per andare a destra, ma anche right così, che cosa faccio? Transizione 1, reward, 0, più vdf, che è 1, 3. Giusto? Allora, io vado a stato che vale 1, tutto vdf. Sto facendo questa, questa, operazione 4. Ok, questa qua, la versione, quindi, mi importa, a dare 1, con azione destra. Mentre, vds è 1, 1, per, reward 1, più lo stato procedente, più 1, va 2. ok? Ok? Allora, questo qua, che potete fare, a un po' di volte? Io, mi faccio già vedere, avete dei dubbi, su come si fa? Perché la soluzione, ve la faccio vedere, ma, non dovete avere dubbi, su come si arriva, quella soluzione. Ditemi un po', se avete dei dubbi, sapreste riapplicarlo, perché questa cosa, va applicata, un tot di volte, fino a quando, 1, lì, ma non è uguale, la 2. Quindi, ad un certo punto, si converge, si propaga, il valore, quindi, questo valore, 3, 4, 5, 6, fino a quando, mi chiamano di valido, adesso non mi ricordo, la soluzione, andiamo a vedere, allora, prendiamo, la soluzione, di questo esercizio, che era dove, allora, andiamo a vedere, eccole qua, allora, guardate, ah, questa miseria, non c'è, non ve lo dà, non ve lo dà, speravo che vi desse, la tabella, invece non ve la dà, quindi ve la devo fare, speravo vi desse, la tabella completa, invece non c'è, non ve la soluzione, c'è solo, quale è l'azione migliore, chiaramente, dell'azione migliore, sarà più difficile, sì, fino a qui, arrivano a sembrarsi, ora, voglio, farla con me, continui a rimanere, resto? per b invece troviamo per via che ho visto cosa interessante perché ci porta un valore allora per via abbiamo questo che con probabilità 1 possibilità una possibilità una possa andare a sinistra guadagno 10 ma arriva in attiva intero proprio benissimo non posso andare a destra che guadagna 0 sembra ma arrivano si che però vale 10 adesso giusto benissimo quindi entrambi casi valuti e si parla di 10 ok entrando in questa azione non è un po di fare il 10 questo è il più bello di entrambi queste azioni che vanno entrando i 10 quindi il suo valore non cambia cambia questo ovvero adesso in questa operazione io non lo so se devi andare a sinistra destra perché entrambe mi portano lo stesso vantaggio per certo adesso qui ancora non è vero perché non abbiamo converso ok quando convergeremo sappiamo che questa è la politica ottima quindi vuol dire che queste due azioni sono del punto più valente da un punto di vista di vantaggio finale quindi in quel caso trovo 50 50 ok per buddizzi invece invece e nel esempio left right se vado left vado in B quindi qui guadagna sempre 0 aiutatemi a prendere da 4 parzialmente quindi Vdx rimane uguale però ancora qui mi piace quindi qui mi piace andare a prendere da 5 parzialmente quindi la prima volta che cambia perché c'è lei allora io mi anticipo già a qualcuno che vi porteranno questo voi avvizzate o volete provare a fare quanti vi porteranno se non si può fare un po' di sicuro a questo se non si può lavorare a questa è stata un'ottima modibilità allora se vado in sinistra probabilità 1 finisco reward 0 se di tutto troppo vero ma Vdx vale 10 a questa operazione quindi cosa fare 10 se invece vado a destra con probabilità 1 finisco 2d e 2d e 2d vale 1 quindi che cosa ci vuoi fare in Vd? allora Vdx vale 10 e decido di andare a sinistra allora questo è il risultato io vi sto facendo i passaggi voi se allora basterebbe che scrivete questo ok questo basterebbe questa è la policy questo è in A rimane in A per forza l'abbiamo detto l'unica azione possibile se in B va in C se in C va in D io ho messo left right ma è un modo per dire la stessa cosa quindi se in B va in C per me è un vai ad extra ok? quindi basta quello come soluzione del problema quindi voi tecnicamente potreste anche scrivere solo quelli il problema è che se in C qualche errore io vado a vedere questa perché tanto voi per dirmi quella dovevano infatti vedere questo vuol dire che se in C qualche errore vado a vedere per vedere da che punto è arrivato l'errore ok? di base il mio suggerimento sarebbe quello di consegnare anche i vari passaggi che avete fatto per arrivare alla soluzione ok? allora volevo darvi una seconda finale per la questa grazie andiamo a cercare da qualche parte mi sembra che che lì avete solo la policy volevo darvi i numeri dell'ultima riga però non sono sicura come li potrei avere? in qualche momento della mia vita l'avevo calcolato tutto e ero giusto per darvi il riferimento alla soluzione e non stare qui prossimi bisogna andare avanti eh bisogna andare avanti perché lì adesso mi sembra che converga adesso me sto andando alle nuorie perché magari domani vi dà l'ultima riga che valore ha potrebbero volerci tutte e 15 perché ad un certo punto c'è questo valore più arriva, più ampiano ok? e questo qui scopperà il valore di un mare qui aumenta fino a non mi ricordo potrebbe essere questo eh ho due volte ho due volte il mio caso di un mare questo è una inglese? no no no no no no questo allora vi sfida questo provate a farlo e vedete più di questa con l'ultima riga guardate questa qua guardate questa qua e guardate questo non mi ricordo che mi fa un problema per cui qui io preferisco andare a destra e poi tutto quello che vedete già che sostanzialmente è l'ultima che è quella qua non si va che trovate ok e perché direi che i conti questi li potete provare se vi converge così lo fate oggi pomeriggio voi se vi viene bene se no domani me lo dite ok perché voglio dirvi un'altra cosa prima di lasciarvi andare non vorrei riprendere domani anche se comunque ripeto il reinforcement learning ce lo portiamo dietro cioè nel reinforcement learning ci portiamo dietro gli MDP pari pari come sono qui quindi non da allora vi volevo raccontare questa alternativa e poi voglio fare anche un esercizio su questo questo punto l'esercizio lo facciamo domani però questa cosa ve la racconto adesso allora potete capitare e questo per esempio è il caso che avete qua allora guardate un attimo questo esempio guardatelo con me allora noi siamo a K uguale a 5 io vedo che per esempio da questo stato tra questo e questo c'è un valore più alto in questo quindi altamente probabile che l'azione migliore anche se qui non ho più meglio è quella che da qui anzi lo vedete lo vedete con le freccesse è quella che da qui mi porta a qui è il valore più alto degli stati vicini quindi è evidente che è probabile che io voglio andare lì quindi guardate queste frecce di fatto ci mostrano esattamente la politica ottima la politica ottima la politica ottima vi ricordo quella che adesso non lo sto a rimosclare l'evento l'abbiamo visto diffuso tanto la settimana scuola a pafora e dai dai dai quindi abbiamo interessa anche l'inizio di questo argomento ci mostrano la politica ottima già qui non ancora qui nel senso che qui oggettivamente potrebbero essere qui mettere la frecce in alto però potrebbero essere equivalenti queste due azioni quindi è un po' presto però già qua io so che la politica ottima è questa e poi continuo a cambiare valori la convergenza mi arriva a k uguale a 100 ok la convergenza dei valori quindi ripeto il fatto che due righe ci sono stati meno gli stessi valori mi arriva molto dopo ma io la politica ottima la sapevo già a 95 per azioni prima ok questo è un caso che è molto frequente ovvero gli errori sui valori che non hanno un impatto reale se minimi su un errore nella politica ovvero io di fatto trovo la politica ottima anche con valori che non sono necessariamente già a convergenza ok quindi questo ci porta a pensare che un'alternativa non del tutto sbagliata potrebbe essere questa invece che ed è in parte quello che abbiamo fatto prima invece di partire da valori a zero come l'ho fatto qua tutti valori a zero e aggiornare l'iterazione per l'iterazione prima con la convergenza io parto da una cosa diversa parto da una politica definita random quindi i valori non li conosco non li conosco ma in maniera random qui vi dico se sei in A vai di Nato al posto se sei in B vai in C se sei in C vai in B per esempio eccetera eccetera eccetera ok quello un po' che abbiamo fatto prima cosa vi ho detto qui assumiamo di avere una politica fissa che è quella di andare sempre avanti quando abbiamo fatto tutti i due sattori che si fanno ok quindi in quell'esercizio là se ricorderete abbiamo calcolato i valori per lo stato A per lo stato B per lo stato C per lo stato D eccetera eccetera partendo da una sola azione che potevo fare e quella indicata dalla politica prima era se sei in C vai in D vai in D vai a destra eccetera qui in questo caso sono queste inizializzate random e indicate qui ok allora come avete visto questo porta sostanzialmente a rimuovere completamente nelle forzioni di Denman il termine max perché per calcolare il valore non devo calcolare il massimo su tutte le azioni possibili perché in questo caso l'azione che vado a valutare è solo una quella indicata da questo posto in maniera random ok quindi cosa faccio quindi sostanzialmente il valore come diceva prima il collega corrisponde esattamente al quveglio non è più il massimo tra tutte le azioni possibili ma il valore di fatto è il quveglio perché ho deciso che la politica mi dice di fare quell'azione A calcolo quel valore calcolo quel valore e e come abbiamo fatto prima posso calcolare tutti gli altri poi vi faccio vedere un esempio domani il punto in cui riesco in questo modo a ridurre il mio problema a un problema di amicazione n variabili che quindi sapreste risolvere tranquillamente anche voi in maniera aziendica ok era l'ultimo compito che ho fatto a settembre quindi lo dobbiamo fare insieme perché sappiate risolvere anche quel caso lì ok quindi a quel punto riesco a calcolare i valori per ciascuno stato ok siccome questa è una politica definita random non è detto che poi di fatto i valori che io vado a calcolare a partire da questa politica random siano effettivamente compatibili con questa politica quindi potrebbe essere che una volta che ho calcolato i valori scopro che in realtà da B non mi conviene andare in C ma mi conviene andare in A aggiorno quindi la politica con questi nuovi valori che ho calcolato quindi a partire da questi nuovi valori cerco di capire qual è l'azione migliore per farlo ok benissimo definita la nuova politica ricalcolo i valori con questa nuova politica che ho definito aggiornato ricalcolo i valori e di nuovo del 2 miglioro la politica quindi azzurro la politica guardando davanti cioè cercando di guardare questi valori utilizzando e utilizzandoli per definire l'azione ottima sulla base di quei valori lì questa cosa la faccio n volte fino a quando mi accorgo che questa politica non cambia più quindi la politica convergo come vuol dire che tutte le volte mi dice fai up up write write write write i valori magari non sono quelli finali tappo uguale a 9 non sono effettivamente quelle convergenti può darsi che se io faccio altre iterazioni cambiano i valori ma la politica non cambia più quindi a me basta non ho bisogno di fare 100 iterazioni me ne bastano 5 per capire qual è la politica migliore e questo è l'algoritmo di iterazioni delle politiche ok quindi l'algoritmo è questo stabiliamo una politica con una diciamo in maniera inizialmente arbitraria quindi ripeto può essere anche casuale ripeti questo algoritmo tante volte fino a quando la politica non cambia più e per ogni iterazione calcola le utilità le funzioni utilità data questa politica che hai definito per ogni iterazione e poi aggiorna di nuovo la politica utilizzando queste nuove utilità che è appena calcolato ok ah e basta nel senso che come vedete questo è un giusto molto più facile come lo stavo dicendo e che abbiamo fatto all'inizio di questa lezione per calcolare il valore di uno stato all'iterazione K di fatto non dobbiamo fare altro che utilizzare quella politica e calcolare esattamente il valore di quell'azione come sommatorio gli stati in cui posso finire eseguendo esattamente l'azione che ci viene suggerita da quella politica ok allora questa cosa magari domani ve la faccio vedere con un esempio e poi prima di mandarvi via mi raccomando facciamo così e oggi pomeriggio se riuscite o sicuramente ve lo chiederei per e poi se riuscite il valore di che non è un giusto e poi non è un giusto ma non è un giusto ma non è un giusto