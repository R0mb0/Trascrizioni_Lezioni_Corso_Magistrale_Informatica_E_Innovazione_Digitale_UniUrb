Salve ragazzi e benvenuti alla seconda video lezione di applicazioni distribuite cloud computing. Oggi parleremo in maniera molto superficiale e breve di interprocess communication. Perché questa lezione? Questa lezione è per dirvi che quando noi andiamo a programmare un'applicazione distribuita ci possono essere tante tante scelte di design e tanti problemi da affrontare. Vedremo che tutti questi problemi poi se uno utilizza un linguaggio di programmazione abbastanza evoluto come Erlang, non ha bisogno di fare tutte queste scelte perché queste scelte vengono già fatte dal linguaggio di programmazione dall'implementazione quindi vengono fornite opportune astrazioni tale per cui un programmatore deve solo concentrarsi di nuovo sullo scambio di messaggi tra i vari attori processi e non deve andare incontro a problematiche di implementazione tipo come mandare il messaggio, come fare un'invocazione e lo vedremo. Di nuovo in questa lezione parleremo di interprocess communication. Cosa vuol dire? Esistamente l'interprocess communication è quella serie di astrazioni o di metodi tale per cui un sistema operativo, un linguaggio di programmazione, un middleware, un framework forniscono affinché più processi possano comunicare tra di loro. Quindi, generalmente, uno quando definisce un processo, cioè quando definisce dei processi, possiamo avere dei processi indipendenti tipo il calcolo parallelo, l'abbiamo parlato HPC, generalmente il calcolo parallelo ogni processo fa il suo task, non interagisce con gli altri processi, quindi è un processo indipendente. Oppure possiamo avere dei processi che cooperano insieme per raggiungere un goal. Nel momento in cui abbiamo cooperazione, dobbiamo avere dei meccanismi per far comunicare questi processi. Questi meccanismi sono quelli che nel gergo vengono chiamati interprocess communication. Ovviamente, quando si parla di comunicazione si intende anche sincronizzazione, quindi quando due o più processi aspettano tra di loro per sincronizzarsi e per andare avanti. Come vedremo più in là nel corso, la sincronizzazione non va mai fatta attraverso dei timer. I timer sono da evitare. Sono da evitare nel locale, figurarsi nella distribuzione, quando il concetto di tempo tra varie macchine si fa molto più astratto. Abbiamo una macchina più veloce, una macchina meno veloce, e quindi i processi devono sincronizzarsi non attraverso dei timer, ma attraverso la produzione e il consumo di un particolare dato. Quindi, se uno vede quello che è stato fatto finora, vede dagli albori dell'informatica ad ora, abbiamo sicuramente il mondo, il mondo della concorrenza, il mondo dell'inter process communication si divide in due fette. La fetta che è di comunicazione attraverso variabili condivise, quindi attraverso memoria condivise, oppure la fetta che utilizza lo scambio di messaggi. Come abbiamo visto nella prima lezione introduttiva, quello che noi andremo a parare è lo scambio di messaggi, perché quando si va nel distribuito non esiste nessuna strazione che è non consideri lo scambio di messaggi. Vedremo anche una strazione che si chiama Spud Vituple, quindi una strazione a blackboard, ma quella banalmente è implementata sotto, implementata attraverso scambio di messaggi. Quindi Shared memory teoricamente l'avete già vista se avete programmato in java, concorrenza in java o concorrenza in C. Shared memory vuol dire che i processi si sincronizzano tra di loro attraverso una memoria condivisa, quindi se parliamo di java i thread si sincronizzano attraverso i monitor o attraverso dei metodi sincronizzati. Quindi si sincronizzano attraverso i metodi sincronizzati. Un sistema operativo può dare vari livelli di interprocess communication, quindi può implementare, cioè il sistema operativo, se è un buon sistema operativo, vi da varie opzioni, quindi Shared memory, pipe, sockets, quindi pipe e sockets parliamo di scambio di messaggi e non più di Shared memory. Quindi come vediamo la differenza essenziale è che Shared memory i processi comunicano attraverso l'accesso alla memoria condivisa e questo accesso avviene attraverso il kernel, mentre questa comunicazione avviene attraverso la memoria, quindi non c'è bisogno di passare attraverso il kernel, mentre nello scambio di messaggi bisogna passare attraverso il kernel. Perché il kernel? Perché il kernel vi da i driver necessari per poter inviare i messaggi. Quindi quando si parla di comunicazione a memoria condivisa, generalmente quello che succede è che un processo produce o sottrascrive valore di una variabile che è condivisa con un altro processo. L'altro processo aspetta finché un valore non quella variabile non viene settata per esempio ad uno. Questo comporta vari problemi di sincronizzazione, vari problemi di race condition, quindi se abbiamo più processi che accedono alla stessa variabile abbiamo dei problemi di race condition, quindi bisogna implementare dei meccanismi di sincronizzazione, quindi utilizzare dei monitor o dei semafori per evitare race condition, però porta anche ad avere quello che si chiama busy waiting, che vari processi aspettano aspettano aspettano per poter acquisire il lock e per poter accedere alla sezione critica o al valore della variabile. Con message passing invece i processi di nuovo comunicano tra di loro attraverso i messaggi, questi processi, mentre con sciarga di memory, siccome la memoria è condivisa quindi è unica, è la memoria del processore del computer quindi banalmente i due processi sono sulla stessa macchina, già con memory message passing possiamo avere due processi che stanno su due macchine diverse. Cosa è importante fare? Bisogna prima stabilire il link tra i due processi, quindi di nuovo posso stare sulla stessa macchina o su macchine diverse, stanno sulla stessa macchina o su macchine diverse, banalmente se noi utilizziamo la comunicazione a basso livello stiamo aprendo un socket tra i due processi e i messaggi, cioè scusate, i processi comunicano tra di loro attraverso delle send e delle receive. Send si manda il messaggio, la receive si manda il messaggio, scusate la receive si riceve il messaggio. Possiamo avere due tipi di comunicazione attraverso i messaggi, diretto e indiretta. Diretta dobbiamo per forza conoscere l'indirizzo fisico, quindi apriamo un socket diretto ad esempio con il processo con cui stiamo comunicando, mentre in diretta possiamo usare delle astrazioni tale per cui è come se noi avessimo una specie di proxy nel mezzo, quindi noi non conosciamo l'indirizzo fisico del processo, ma ne conosciamo una sua astrazione e questa astrazione fa sì che il processo sottostante possa cambiare e noi avremo sempre l'illusione di poter comunicare con lo stesso processo, è quello che fa per esempio un proxy. Anche la stazione per mailbox, quindi Erlang è un tipo di comunicazione per messaggi indiretti. Allora, quando andiamo in distribuito, quindi quando parliamo di comunicazione a messaggi, bisogna fare di distinguo. Quindi ogni operazione, quindi abbiamo una send e una receive, ogni operazione può essere bloccante o non bloccante. bloccante cosa vuol dire? Bloccante vuol dire che io mi blocco fino a che dall'altro lato non ho un hack di ricezione, quindi se io ho una send bloccante il processo non termina, cioè la chiamata la send non mi ritorna finché l'altro processo non ha affittato la corrispettiva read. Ora, questa generalmente è molto difficile da implementare e nessun... nessun middleware vi da questa strazione perché è veramente complicato garantire l'atomicità di questa operazione. Quindi, generalmente quello che uno ha è una... tutte le send sono non bloccanti perché se uno pensa a livello di kernel, la send non fa nient'altro che scrivere in un buffer. Questo buffer poi eventualmente verrà scaricato, quindi verrà flashato. Però, quindi la scrittura su questo buffer è sempre immediata, a meno che il buffer non sia pieno. Però questo è un altro discorso. Quello che uno ha, generalmente, è una resume bloccante. Quindi io ricevo un messaggio, se la mia mailbox è vuota, mi blocco. E questo è veramente ragionevole da implementare, perché è dal lato del cliente. Quindi dal lato dell'altro processo, legge su un buffer, se il buffer è vuoto, è ragionevole aspettarsi che io mi blocchi fino a che un dato non è disponibile. Questo non vuol dire che non possiamo implementare delle receive non bloccanti. Infatti, C, Java, qualsiasi linguaggio di programmazione permette di settare le receive non bloccanti. Anche Erlang. Quello che si fa è settare un timer. Quindi se io entro tot secondi, non tot millisecondi, perché stiamo parlando di applicazioni distribuite, parlare di secondi già vuol dire che introdurre un ritardo assurdo. Se io entro tot millisecondi e non ricevo un messaggio, allora mi sveglio. Quindi la procedura mi ritorna, mi ritorna con un valore null e quindi posso fare altro. Anche questo ci abbiamo un po' parlato, è un'altra decisione da prendere quando uno implementa un'applicazione distribuita o quando uno vuol dare una strazione di interprocess communication e deve capire se i messaggi vengono bufferizzati o meno. Essenzialmente quanto è lunga la coda, quanto è lunga la coda del buffer. Ora, se prendiamo Go, per esempio, uno può creare dei canali senza bound o col bound. Quindi canali di lunghezza potenzialmente illimitata o canali di lunghezza limitata. Questo cosa vuol dire che uno può scrivere fino a quanto ci spazio nel buffer. Nel momento in cui il buffer è pieno, allora sì che lì la send diventa bloccante. Quindi possiamo avere delle code bounded o non bounded. Generalmente quello che si utilizza sono delle code unbounded. Erlang utilizza delle mailbox che non hanno una dimensione. Ovviamente la dimensione esiste, esiste nel momento in cui consuma tutte le risorse della macchina su cui sta eseguendo e termina, termina in maniera anormale. Però stiamo parlando di code, di dimensioni, di 10, 20, 50 milioni di messaggi. Quindi veramente potenzialmente infinita per noi. Allora, tornando ai messaggi, abbiamo visto che il message passing communication è veramente basilare. un message passing communication è un message passato. Allora uno potrebbe, cioè quello che uno fa è utilizzando queste procedure, queste primitive scusate, costruisce delle astrazioni più complicate in modo tale che l'utilizzatore finale del framework non deve avere a che fare con dettagli implementativi, che è quello che è stato fatto in Erlang. In Erlang abbiamo sì delle sender receive, ma sono veramente ad alto livello, non sono tipo le sender receive DC con i socket. per esempio una strazione che uno può costruire usando messaggi passing è una strazione di chiamata procedura remota oppure invocazione a metodo remoto. Quindi stesialmente è come avere una procedura. La strazione è questa. Io invoco una procedura che penso sia locale, invece questa procedura è distribuita da un'altra parte. oppure se parliamo di metodo invocation in java, quello che fa j2e è questo. vi da la strazione, tra virgolette, strazione perché programmare quella roba è veramente complicato, vi da la strazione di un oggetto locale, quindi l'interfaccia, voi invocate un'interfaccia locale, ma a questa interfaccia corrisponde un oggetto distribuito. Quindi nel momento in cui voi fornite questo dovete anche fornire, se state implementando questa roba, dovete anche fornire dei meccanismi di marshalling un marshalling, di che semantica dare all'invocazione a metodo. Quello che si fa per esempio con una chiamata procedura remota, la strazione finale è quella di invocare un metodo, scusate, invocare una procedura che sia locale o che sia distribuita una somma, vi ritorna sempre la somma. Quindi se voi chiamate somma 3 più 2, 3 con due parametri, 3 2 vi da 5, sia che questa somma sia implementata localmente, sia che questa somma sia implementata dall'altro lato del mondo. Quello che succede, quello che il framework sottostante fa è questa serie di passaggi, che come potete vedere è veramente complicato. Quindi il client, la stazione che il client ha è 1, il client quello che vede è chiama, chiamata e ritorno di parametri. Quindi però tra 1 e 10 vedete che succedono tanti messaggi, che tanta roba succede. La prima cosa è se è una chiamata a procedura remota, bisogna localizzare dove è questa procedura, dove è implementato questo pezzo di codice, dove sta eseguendo. E quindi abbiamo bisogno di un servizio di naming globale, il name server. Il name server, pensate, ha un DNS. Quindi la prima cosa da fare è fare un look up, chiamare il name server, dire dove è la somma, chi me la implementa. Il name server ci ritorna l'indirizzo fisico di dov'è questa somma. Dopo mandiamo un messaggio tramite 4, mandiamo un messaggio al server. Esensivamente chiunque implementi qualcosa nel distribuito diventa un server dal punto di vista del client, perché l'invocatore è il client e l'invocato è il server. E quindi vedete che il server riceve, ma cosa riceve? Riceve una chiamata su un messaggio, su un pacchetto. Quindi deve spacchettare la roba, invocarla localmente, raccogliere il risultato, impacchettarla, quindi siamo al passo 7, mandarla a 8. Il risultato della somma, sperabilmente 5, perché abbiamo invocato somma di 3 e 2, bisogna spacchettarla di nuovo, quindi 9 e poi 10, ritorniamo al client. Quindi il client dell'invocazione e il risultato non vede tutto quello che c'è al contorno. E questo generalmente bisogna implementarlo, se uno utilizza rpc in c o rmi in java, questo deve implementarlo. Esensamente ci sono delle interfacce, ma deve implementare tutte queste interfacce. In RLang tutto questo viene proprio nascosto, viene astratto. Come vi dicevo, in RMI succede la stessa cosa, solo che non abbiamo il name server, abbiamo un registro, un registro su cui tutti gli oggetti distribuiti vengono annotati e client, quindi attraverso lo stub, lo stub sarebbe la parte di client, cioè l'interfaccia locale dell'oggetto distribuito. Attraverso lo stub invoca, passa sul layer remoto, quindi invoca in maniera remota qualcosa. Questo va su un layer di trasporto, banalmente su TCP che va su IP, che va sul livello 2, quindi va sulla rete, quindi ritorni su, viene spacchettato, viene fatto e torna giù. Ne più ne meno quello che abbiamo visto con il RPC. Quindi l'RMI consiste di tre layer, gli stub e gli skeleton, il remoto layer e il trasport layer. Uno deve implementare tutta questa roba, cioè deve implementare o deve predisporre tutte le interfacce per questi metodi, per questi layer. Esistono istanze di Remote Process Call e RMI, esiste CORBA, se abbiamo RPC, CORBA. RMI abbiamo per esempio J2I, Java Enterprise Edition, che vi permette di fare delle invocazioni remote a metodo, di avere degli oggetti remoti distribuiti. Oppure abbiamo le RPC su XML swap, ad esempio. Oppure abbiamo, uno potrebbe vedere le EPA come delle, essenzialmente le EPA sono una situazione di una invocazione a procedura a metodo remoto. Però la cosa più, c'è un distinto da fare, è che le EPA RESTful, essenzialmente le EPA fatte con HTTP, con GetSet, abbiamo tutta una stazione fornita da un livello alto di HTTP. Parliamo ancora di RMA in Java. Come vi dicevo, le invocazioni si hanno attraverso due interfacce, l'interfaccia locale e l'interfaccia remota. Quindi un oggetto a RMA vi dà la stazione di invocare un oggetto locale, ma questo oggetto può essere anche distribuito. Però cosa succede? Succede questo, che quando vanno distribuito magari il pacchetto si perde. Quindi la stazione, dal lato di utilizzatore finale, se io invoco, se io creo un oggetto e invoco questo metodo, che sia un'invocazione locale o che sia un'invocazione remota, dal punto di vista del programmatore, deve essere lo stesso. Quindi se chiamo somma, 3, 2 mi deve dare sempre 5, banalmente. Quello che diventa più complicato è gestire il livello di networking, il fatto che magari i messaggi si perdono, magari bisogna dare un ordine ai messaggi. Quindi le chiamate locali sono sincrone, non esiste un metodo, non esiste una chiamata asincrona a oggetto java. Quindi se voi invocate un metodo, la println di uno stream, quella è bloccante e vi ritorna subito. Quindi abbiamo una chiamata asincrona e affidabile al 100%, a meno che non crashi la java virtual machine. Mentre le chiamate remote sono asincrone, perché dobbiamo mandare un messaggio e bloccarci finché non ritorna il risultato. Magari questo messaggio si perde, quindi il problema è, oppure magari io ho mandato un messaggio, è caduta la linea, magari sto usando cellulare, la rete 5G, sto mandando il messaggio, passo in un tratto di territorio in cui non ho connettività e poi ritorno. Quindi magari io ho bufferizzato due volte lo stesso messaggio e questo messaggio viene mandato due volte. Però, quindi la semantica at most once cosa vuol dire? Che questi due messaggi, queste due invocazioni di somma, devono essere trattate come una. Quindi io devo garantire che almeno una volta il metodo mi viene invocato. Come vi dicevo, utilizzare, nonostante si dica che java vi permette di queste astrazioni, vi permette di utilizzare in maniera semplice gli oggetti distribuiti, è veramente complicato programmare la versione enterprise di java. Ed è veramente complicato e prono agli errori. in più uno deve tenere in conto anche di quello che nel gergo si chiama marshalling e unmarshalling. Generalmente con java viene fatto come serializzazione. Cos'è il marshalling? Il marshalling è quello che è la pacchettizzazione di una richiesta, come vi dicevo prima, mentre la marshalling è la spacchettizzazione. Quello che vi fa la java è serializzare e deserializzare, quindi esattamente trasformare tutto to string e da to string ricostruire la chiamata all'oggetto. Quindi vedete che è veramente complicata questa roba è veramente time consuming. Un'altra stazione che uno può costruire utilizzando sotto messaggi, però li nasconde, è quello che generalmente si chiama table space. Cos'è il table space? essenzialmente è una memoria condivisa distribuita. Quindi è una lavagna, mettiamo caso che noi siamo a lezione, è una lavagna in cui chiunque può scrivere e chiunque può leggere. Io scrivo qualcosa una lavagna e voi la leggete. La potete leggere, io cancello un messaggio, quindi l'avete consumato e così via. E voi rimanete in attesa finché io non scrivo qualcosa altro. Inoltre la cosa bella della memoria associativa con table space è che la lettura può essere fatta attraverso pattern matching. Quindi banalmente uno può programmare un programma che dica io rimango bloccato fino a che non mi arriva un messaggio che contiene nella prima posizione la stringa ciao e nella seconda posizione il valore 10. Vedete che è veramente un meccanismo potente di sincronizzazione perché quel messaggio è bloccato fino a che non gli arriva, scusate, quel processo è bloccato finquanto non gli arriva proprio quel messaggio. È veramente complicato, è veramente potente ma è veramente complicato da implementare. Perché vi parlo di questa roba? Vi parlo di questa roba perché il pattern matching lo vedremo in Erlang, perché Erlang vi permette di utilizzare pesantemente pattern matching sui messaggi. Quindi io posso ricevere una quantità abnorme di messaggi ma posso decidere di leggere solo quei link interessano a me, gli altri rimanano nella coda. Quindi quello che succede qui nel table space è che abbiamo questa enorme lavagna, ognuno scrive qualsiasi cosa, la cosa bella è che i dati generati sopravvivono al processo. Quindi il processo può essere stanziato, mandare un messaggio, quindi scrivere qualcosa alla lavagna e morire. Quindi tornando alla analogia della lavagna, uno di voi si alza, scrive ed esce dall'aula, quindi va altrove. Però quello che ha scritto sopravvive al fatto che il processo sia uscito e così via. Quindi per esempio vediamo che il processo P2 si aspetta una dupla di quattro elementi, quindi si aspetta si aspetta un dato di quattro elementi, in cui i primi due elementi sono la stringa array e la stringa primi, mentre gli altri due elementi sono le variabili che vuole leggere. Quindi lui rimarrà bloccato fino a che non verrà emesso un dato, un messaggio, una dupla che ha come primi due elementi, due stringhe in cui c'è scritto array primi 5-7 ad esempio. Quindi lui, nel momento in cui viene prodotto questo messaggio, lo leggerà. Però vedete che, di nuovo, qui abbiamo una race condition, perché tutti possono leggere e tutti possono scrivere. E magari due processi possono leggere lo stesso messaggio, quindi abbiamo il fatto che uno leggerà e l'altro no, e così via. Qual è il problema di questo? Il problema di questo è che questa strazione è veramente potente, ma è veramente complicata da garantire, perché quello che stiamo fornendo è una memoria condivisa globale, globale quindi su tanti computer, su tanti nodi, che è in più atomica. Quindi abbiamo tutti tutto un problema di replicazione, consistenza e così via. Di nuovo, quindi quali sono tutti i problemi quando uno vuole implementare una applicazione distribuita? E' che i processi sono sparsi nel mondo, il tuo processo, il vostro processo interaggerà con roba che sta altrove, con altri servizi. Bisogna gestire sia la concorrenza, quindi più processi sulla stessa macchina, che la distribuzione, più processi su macchine diverse, diversi nodi. E in più bisogna tener conto di tanti dettagli implementativi, quelli che vi ho detto finora. Quello che succede, quello che succederà con Erlang, è che tutti questi dettagli implementativi vengono astratti. Di nuovo avremo solo sender e serve, delle primitive per spawnare, quindi per lanciare più processi, e queste primitive mi permettono di lanciare un processo locale globale, quindi parallelo, o distribuito. In più queste primitive mi permettono di istanziare nuovi nodi, far sì che io mi accorga se un processo è morto, sia che sia questo processo locale o questo è distribuito, e così via. Quello che vi ho detto nella prima lezione è che bisogna mettiamo a caso che abbiamo un linguaggio mega cool, super cool, che ci permette di astrarre da tutti i dettagli implementativi, quale è Erlang, quello che noi rimane da fare è concentrarci sullo scambio di messaggi, concentrarci sul protocollo che questa applicazione distribuita deve avere. Quello che fanno le process algebra è proprio questo, ci permettono di, cioè astraggono da dettagli implementativi, e ci permettono di focalizzarsi sullo scambio di messaggi. Ad esempio, se prendiamo questo questo pezzo di codice C, è veramente pieno di dettagli implementativi, che dal punto di vista del protocollo a me non interessano. Questo è semplicemente un hello world, il client manda hello world al server, il server l'origine lo scrive. Quindi io mi voglio concentrare su questo scambio di messaggi, non voglio avere tutti questi dettagli implementativi. Quello che le processaggi però fanno, quello che le prossime lezioni, che sono già disponibili, fanno è ci permettono un modo di astrarre da tutti i dettagli implementativi e di concentrarci sullo scambio di messaggi e di ragionare sul fatto se due o più processi si comportano nello stesso modo. Quindi, preso questo codice, quello che io voglio veramente capire è lo scambio e l'interazione che questi fanno. Essenzialmente è questo. Il server aspetta un messaggio sul canale S che sarebbe il socket, riceve questo messaggio e risponde al client. Il client cosa fa? Manda il messaggio quindi il client manda il messaggio ciao server e aspetta un messaggio, la risposta, la reply dal server. Quindi vedete che quello che ho scritto è una maniera, una strazione di tutti i dettagli implementativi e mi permette di concentrarmi sul fatto che il server aspetta un messaggio al client e risponde al client. Client manda un messaggio server e aspetta un messaggio al server. Quindi vedete che tutte le sende delle resepi sono matchate, quindi non c'è problemi. Di nuovo, questa è una cosa che continuiamo a ripetervi. Al di là delle lezioni sul processaggio, ma che potrebbero essere più o meno complicate per voi, quello che a me interessa che voi fate è pensare. D'ora in poi dovete pensare a qualsiasi cosa come un oggetto distribuito, quindi una tazza, un cellulare, un distributore automatico, è un oggetto distribuito che espone un'interfaccia, quindi come io posso interfacciarmi, interagire con questo oggetto distribuito e questa interfaccia cos'è? Una serie di messaggi, quindi io devo capire che tipo di messaggi mandagli e che tipo di messaggi aspettarmi. E va bene, questo concludeva la lezione di oggi. Grazie.