Ho questo aspetto No No Sì, sai In Xbox c'era questa dinamica stranissima Perché io sono cresciuto Quando io ero superiore C'era ancora la guerra in Iraq E quindi tipo Letteralmente conoscevo questo Che era un vicino di casa mia Che lui faceva parte del Marine Corps E lui parlava sempre Dell Xbox Generation Di questi che volevano fare guerra Nel meravigliante perché giocavano a Halo E tipo, io ho sempre sospettato Che non è una coincidenza che diceva Xbox e non PlayStation C'è qualcosa di chi che giocava Xbox Allora Ok Ho appena sentito La segreteria Segretaria L'ufficio Segretaria è la persona, vero? Segretaria è la persona Ok, no Quindi segretaria è la persona Ok Ho sentito la segreteria Allora L'ufficio E Tentativamente Sono soggetto ai cambiamenti Verranno finalizzati il 21 aprile Però Io ho proposto Le seguenti Orari Suposti Questa è la persona, non è una persona che mi ha fatto in Italia Questa è la persona che mi ha fatto in Italia Questa è la persona che mi ha fatto in Italia Questa è la prima Il primo periodo Secondo periodo E E poi Quindi Allora Se qualcosa cambia Vi farò sapere Ma Speriamo Che questi vengano approvati Quindi Quindi Quindi Non dovrebbe essere Un gran case Un esame orale Come abbiamo deciso Quindi Spero che non c'è Non c'è qualche motivo Perché uno di queste non può funzionare Per meno una persona Però Ok Adesso Volevo ritornare a questa idea Questo È praticamente un modo generico Di scrivere un metodo iterativo Se vogliamo Forse sarà un esercizio Io chiederò a voi A derivare questa forma Dall'altro Quello del spiazzamento delle matrici Tanto che l'avevo già fatto Questa delegazione In un giorno Però Chiederò a voi A farlo nuovo Quindi praticamente Possiamo concepire un metodo iterativo Per la soluzione di un sistema lineare Come un costante Come un costante Un costante processo Adesso di prendere un indovina Di un'indovina di un'indovina Direzione Cioè la soluzione X Poi Di prendere una direzione Tipo Z In base al residuo Questo sistema del residuo e poi selezionare un magnitudine del passo dell'aggiornamento. Quindi qui possiamo concepire questo come... Adesso ne voglio fare un... Per fare un convegno possiamo immaginare che siamo qui. Ok, questo è dove siamo. A questo punto vogliamo essere qui. E quello che stiamo facendo, ora mi passo, e stiamo prima provando una direzione, diciamo, prima questa da una direzione, e questa dà un'idea di quanto dovremmo spostarci in questa direzione. E l'idea di un metodo non stazionario è che costantemente aggiorniamo questo in base a... l'interazione e lo stato della soluzione generale. Quindi... L'idea, allora... Quindi però per A faccia la direzione del passo, e la direzione... Z è la direzione, sì, perché sì, perché stiamo vedendo come la nuova soluzione, come una combinazione della vecchia soluzione, più qualche passo in questa direzione. E... Allora... Quindi c'è una questione di come potremmo in modo più ottimale, più informato, più intelligente, selezionare questa direzione. E... Parleremo in dettaglio di uno di questi metodi, due di questi metodi. Tutte e due sono molto comunemente usati, e... Strettamente parlando, si può garantire che funzionano per matrici SPD, e parleremo di matrici non SPD, quando finiamo di parlare di questi. Uno dei motivi perché parleremo di questi in dettaglio, e perché anche se funzionano solo per SPD, vedremo che per il caso generale possiamo un pochino generalizzare questo modo di pensare. Allora... L'idea che vogliamo dire qui, è... Guardiamo un attimo di questo... Se finiamo questo funzionale, si chiama una forma quadratica questo. Ok. Stiamo derivando un metodo non stazionario per risolvere un sistema lineare. Perché... Avevo detto che... L'idea qui... È che... Non siamo costretti a semplicire questa alpha di essere uguale a qualsiasi passo. Nemmeno questa Z, però... Ok, questa Z non è mai lo senso perché R cambia costantemente. Però... L'idea è che possiamo selezionare questo di essere più o più grande o più piccolo in base alla soluzione stesso. Quest'idea che questo è la lambda ottimale che l'abbiamo vista settimana scorsa, questa è vero solo se accettiamo queste idee che possiamo solo utilizzare uno per tutta l'interazione. Però in realtà è possibile selezionare questo diversamente a ciascun'interazione. Questa alpha che determina in quanto facciamo per questo passo. E quindi adesso quello che stiamo facendo è sto facendo vedere una cosa qui che stiamo assumendo che questo è simmetrico, definito e positivo. Ok. Perché facciamo questa assunzione? Perché? Immaginiamo un attimo che facciamo questa, la derivata rispetto all'X. Abbiamo già fatto una cosa simile a questo quando abbiamo fatto l'analisi ai quadrati, analisi ai quadrati minimizzati. E si può vedere che pure il grad alpha X è uguale a due, due. Perché come un ragionamento che abbiamo visto prima, possiamo concepire questo termine di essere un pochino analogo, si comporta come un termine di X quadrato, perché ci sono due fattori di questi, X trasposto e X. E poi questo qua, la derivata è un po', funziona come un termine lineare. quindi, quindi, maniamo con questo è la derivata di questa forma quadratica. Allora, questo è interessante perché questo è semplicemente il nostro residuo. quello che stiamo dicendo qui, se avessimo questo problema, la minimizzazione di questa forma quadratica, il gradiente di questa funzione viene dato sempre dal residuo per qualsiasi X, questa è la derivata rispetto all'X di questa espressione. Perché l'ho scritto così? Se avessi ok, sì, ah, ok, però possiamo dire che questo è meno che è diciamo che è analogo, lo tivo, possiamo immaginare questo come un quadratico, ma è multivariabile, quindi non è così facile, ma l'idea è così. Per questo tipo la derivata è 2 per AX, perché possiamo, è un analogo, ricordatevi, abbiamo fatto un calcolo simile per quadrati minimizzati, sì, che funziona in questo modo. E allora, avevo detto qui specificamente una cosa che A è simmetrica, definito positivo. E il motivo per questo, questa espressione, che questa sia il gradiente di questa espressione, è vero per un po' sia sia, questo non dipende all'A, però quello che significa questo, dipende proprio problema. Perché? Perché se A è simmetrico, definito positivo, significa in particolare che stiamo sempre cercando un minimo. perché se sia questa espressione sia un minimo o un massimo, dipende alla proprietà che A ha. Quindi se A fosse non simmetrico, definito positivo, questo ce n'è ben ancora vero, però però non ci punteremo necessariamente nelle direzioni che vogliamo, perché noi vogliamo trovare la direzione di cambiamento massimo che ci porta verso il minimo di questa forma quadratica. e quindi se minimizziamo questa espressione possiamo risolvere questo provando dove questa espressione è uguale a zero. ok? Quindi questo minimo quando questo è zero. Quindi il punto qui è per l'asservazione che min se è solo se l'X che minimizza questo è l'X che risolve questo. Allora quindi quello che facciamo è vediamo se non possiamo utilizzare questo per non cancellare questo però l'idea qui è che iniziamo con X iniziale quindi per adesso sì se non appartiene alle matrici e si può esatto tivo non puoi essere sicuro che questo perché quando a e spd l'idea qui è mi mi puoi dare in realtà tipo due minuti perché avrà più senso se faccio una cosa che scrivo la lavagna perché ho realizzato era troppo presto a dire queste cose di spd allora va bene l'idea adesso è questo se lasciamo stare questo precondizionatore al momento diciamo che facciamo sempre precondizionatore esplicito quindi facciamo ogni passo xk più 1 uguale a xk più lambda k rk ok quindi questo è semplicemente il residuo allora il residuo ricorda che questo che è questo il grande di questo allora sarà così quindi uguale a meno r e quindi ve lo ricordate l'interpretazione del grande quando abbiamo una funzione molti variato il gradiente ha una proprietà molto specifico punta sempre nella direzione di massimo cambiamento locale quindi questa direzione per un qualsiasi xk se valutiamo il residuo in realtà questo ci punta sempre verso la direzione di massimo cambiamento e questo è il punto di perché deve essere ASP sto provando a posso anche farlo due dimensionale allora il punto è questo se ASP di di in la direzione del massimo cambiamento ci porterà sempre verso questo punto qui dove vogliamo esserci perché la geometria dello spazio è questo parabola e quindi stiamo sempre andando verso questo punto quando proviamo la direzione del massimo cambiamento in realtà in base ad esattamente come costruito potrebbe essere un parabola con tipo un lato più lungo dell'altro eccetera ma l'idea è questo che la direzione del massimo cambio ci mette sempre verso questo punto qua parliamo un attimo di cosa succede se non è simmetrico definito positivo questo allora potrebbe essere che abbiamo una situazione in cui abbiamo questa punta alla sella ad esempio quando è così non è detto che questa direzione del massimo cambiamento ti porta verso la punta alla sella potrebbe essere infatti che ti porta qui molto fuori nell'altra direzione l'unico modo che possiamo essere assicurati che la direzione del massimo cambiamento è anche la direzione che vogliamo e quando questo a è strettamente simmetrico definito positivo in realtà definito positivo generalmente è abbastanza ma dobbiamo sapere che c'è solo un modo in cui possiamo spostarci quando facciamo massimo cambiamento che ci sta puntando sempre più e più verso nostra direzione ideale allora questo quindi cosa il concetto è questo ok immaginiamo un altro questo tensione ok quando a è simmetrico definito positivo questa cosa è sempre positivo perché tutti i autovalori sono positivi e quindi questa espressione è sempre determinata da a è sempre positivo se significa che allora se troviamo la direzione del massimo cambiamento di questo ci sta portando sempre verso un stretto minimo diciamo che c'è solo uno quando il simmetrico definito positivo questo punto del minimo questo punto critico è sempre un se fosse altro soprattutto in realtà anche funziona nel caso che è negativo definito perché è sempre un massimo però il problema proviene quando non è né positivo né negativo perché significa questo caso che il punto critico dove il gradiente è uguale a zero dove tipo ottimiziano è una punta alla senda questo succede quando questo non è definito è un problema qui e quando abbiamo una parabola guarda che se ti chiedo se siamo qui e vogliamo andare verso la direzione del massimo cambiamento sarà sempre puntata verso questo non non possiamo sbagliare seguendo questo percorso perché se pensiamo di una valle se prendi una palla e lo lanci da qualsiasi punto nella montagna d'intorno andrà sempre in fondo alla valle invece se pensi di un cavallo tipo una sella di un cavallo non è necessariamente così e soprattutto pensare non sono molto bravo a disegnare per questo bicchiero scusa ma un punto alla sella a questo tipo di aspetto quindi qui c'è una direzione che è un minimo e un altro che è un massimo e quindi è molto possibile che se ci troviamo qui sto provando a disegnarlo meglio ma non so se ci dicerò se ci troviamo qui su questo punto alla sella potrebbe essere che la direzione di cambiamento ci porta fuori da questo punto questa è l'idea che allora se fossimo fortunati di avere un indovina iniziale che garantiva che eravamo nel posto esattamente giusto potrebbe ancora funzionare questa cosa per una punta di sella ma in generale non abbiamo questo quindi se siamo qui potrebbe sbagliare perché non c'è questa proprietà di tipo a prescindere dove inizi se segui il gradiente arriverai a questo punto con un punto di sella potresti anche andare adesso questa è l'idea se la matrice è spd oppure npd oppure snd significa che siamo sempre in un massimo o un minimo quindi abbiamo una cosa monotonica abbiamo una valle quindi siamo sempre sulla montagna tipo andando invece se è indefinito che potrebbe che ha sia alto valore positiva che negativa non è sempre chiaro così come funziona avete fatto un corso di equazioni differenziali ve lo ricordate quel cosa dei punti di equilibrio in cui tipo si mette dal lato destro l'equazione a zero e poi si prova ad avere i punti di equilibrio e si può classificarli a punti stabili e punti instabili ok quindi questo punto di sella corrisponde all'uniquilibrio instabile quindi se siamo là e ci perturbiamo un pochino la posizione non non va bene potremmo anche scappare invece quando è definito siamo in un punto stabile questo è il punto e quindi questo ragionamento di selezionare di selezionare diciamo la strada la strada con il cambiamento più grande sarà sempre una cosa utile quando non è così il cambiamento più grande potrebbe portarti di qualche altra parte se non stai un pochino di e puoi sempre vederlo così questo la derivata tipo la concavità non cambia quindi siamo sì allora quindi la direzione di massimo discesa viene dato da allora quindi meno oppure meno diciamo la direzione di massimo discesa è meno questo iraniente invece la direzione di massimo cambiamento è così quindi meno r però se vogliamo allora discesa lo flippiamo il segno e quindi abbiamo nostro xk xk quindi abbiamo risulto problema 1 abbiamo risulto questa questione di quale direzione andiamo andiamo verso residuo va bene però rimane ancora la questione di cosa facciamo per questo alfa quindi abbiamo in quale direzione ma quanto è un altro discorso però adesso sì massimo disceso allora questo alfa è going щоб si per voglio questi resti questa cosa qua al nostro ex e cape più uno quindi abbiamo un mezzo e scappa più alta k rk trasporto a a scappa più alta k rk meno scappa più alta k rk ok se facciamo la distribuzione di queste cose ritorna allora questo che è ok dobbiamo fare un po di espansione e un pochino faticoso però ok si può fare quindi mezzo e scappa a tra po a scappa più mezzo ala k quadrata rk trasporto a rk poi più un mezzo 2 ala k rk trasporto a scappa poi questi termini che non sono così terribili meno xk trasporto b meno alfa k quindi questo è semplicemente l'espressione ridistribuito e voglio che facciamo una cosa qui che è importante che è qui qui che questo è il che davanti a tra il trasporto a quota si davanti c'è meno meno a meno a scappa trasporto me allora sto forse allora sto individuando alcuni termini perché voglio riorganizzare questa espressione in base a alla dipendenza del nostro parametro xk questi che sono stati inquadrati in blu hanno un termine che lineare ha scusino n ha veri e lineare in alfa k qui e questo qui individuato un giallo e quadratico in alfa k quindi non iscriviamo tutto come questi sono altri termini in alfa k poi vediamo che questo allora questi sono quelli lineari abbiamo alfa k alfa k rk poi trasporto a xk meno b che è una cosa familiare più i termini che non hanno nessuna dipendenza di xk di quindi me più meno più meno xk passporto b quindi rk 2 xk e xk e xk e xk quindi xk quindi xk xk xk xk ok allora questo e meno residuo intendo questo qui ok allora quello che vogliamo fare adesso è riscriviamo tutto adesso nella forma super semplificata e così ecco di io quanto vuoi prima ossol laughing e dimine un particolare condividendo LahiriOW II Qui penso che sia da qui. Più. ok allora ricordatevi cosa stiamo provando a fare perché abbiamo già determinato che vogliamo aggiornare nostro indovina per la prossima indovina prossima frazione di x abbiamo già provato la direzione e io esiluo poi ho detto che adesso vogliamo non provare solo la direzione ma anche questa alfa k quello ottimale che ci porta che è tipo anche la quantità di r giusto a lunga di qui ci dovremmo spostare quindi se vogliamo fare questo questo possiamo fare la di questo rispetto a ralpa e questo non è questo è veramente è un semplice derivato perché alfa è un numero non è un vettore quindi non è un gradiente che sto dicendo queste più o meno come un quadrato veramente quindi possiamo ok prendiamo questo 2 giù quindi questo quando prendiamo la derivata rispetto ad alfa k diventa rk a e poi questo è lineare in rk quindi va via e poi questi termini non nemmeno una singola volta quindi vanno via quando facciamo la derivata rispetto all'alfa k adesso cosa facciamo cosa facciamo lo mettiamo zero sul lato destra e lo risolviamo in modo che otteniamo l'espressione tra la pagata che è molto facile è molto facile quindi r è il nostro direzione ottimale e poi alfa k è nostra quantità di direzione ottimale quindi il metodo del gradiente o posso cancellare questo quindi il metodo del gradiente allora possiamo scrivere in modo molto molto metodo del gradiente a e di stiamo ottimendo quindi uno prendiamo x 0 r 0 uguale a p meno a x 0 1 quindi 2 poi per per k uguale a 0 1 2 bla bla bla allora prendiamo un 2 a alfa k più 1 uguale a rk trasporto rk rk rk trasporto a rk rk rk alfa k più 1 no non mi piace questo mi piace questo mi piace questo mi piace questo questo k terra alfa perché stiamo costruendo con rk quindi questo questo è meglio quindi poi diciamo riteria che potrebbe essere qualche raggiunta e quindi oltre questa cosa con sempre di verizzazione sale sotto un certo livello oppure se abbiamo qualche la io residuo va sotto qualche tolleranza predeterminata allora questo va bene sì sì allora il machine learning questo è leggermente diverso perché sta è questa è una funzione non lineare e quindi a quel punto è lo stesso è lo stesso di tema quello che sta facendo è stai linealizzando un corno a un punto localmente trovando questa cosa però la differenza è qui abbiamo un problema lineare quindi sappiamo la geometria del problema che è questo convesso cosa invece comunque i problemi nel machine learning diciamo che se qualcuno se ha mai trovato ad usare una rete neurale in bari che dico sì usiamo questo metodo ma devi giocare un sacco con parametri eccetera quindi non volevo fare un po' di modelli di neovare sì esatto questo è quindi sì esatto e questo è usato spesso in machine learning è usato spesso per i problemi non lineari in generale perché perché tipo diciamo ha un certo senso però quando lo usi per i problemi di machine learning e questo è un problema ben conosciuto spesso trova un punto ottimale locale quindi non puoi abbiarti così tanto di quindi quindi quindi sì però è lo stesso idea e lo stesso stesso algoritmo è solo che la differenza è che lo sta andando nella direzione dell'off function quindi anziché il residuo si può pensare allora spesso ai uomo che si chiama un loc function che di solito la differenza la è che il machine learning di solito quando stai facendo un training di un modello hai un insieme per cui sai veramente cosa dovrebbero essere i risultati quindi non usi un residuo invece usi vero errore tipo usi x model meno x però e questo è la funzione che usi quindi è simile però in questo caso diciamo lo scrivo questo perché è diverso ma anziché rk utilizziamo il vettore che risulta quando prendiamo la derivata rispetto a questo uguale a questo questo e machine learning queste non potrebbe essere il vivo è la norma dell'errore non il residuo però però quello si è la persona cosa identica stessa algoritmo allora c'è una cosa che adesso che abbiamo scritto questo che voglio osservare che è una cosa che non la che allora rkq1 è uguale a b meno alfa xk più 1 che allora è uguale a b meno a xk più rk più rk rk rk rk rk sì ok quindi questi sono le tue ok va bene vediamo un attimo partiamo una cosa vediamo guardiamo questa espressione qua quindi abbiamo b trasporto rk meno x trasporto a trasporto rk meno alfa k rk trasporto a rk ma questa alfa k sappiamo cos'è è di meno x k o altro rk meno rk trasporto rk diviso rk trasporto a rk rk rk rk rk rk e quindi come vedete c'è una cancellazione qui allora qual è il punto sì qual è il punto qui adesso lo riscriviamo il punto qui è che abbiamo adesso uguale a adesso faccio un'identità ricorda che A trasporto B è sempre uguale a B trasporto A quindi posso cliccare quei secondi qua per ottenere RK trasporto B meno RK trasporto A K meno RK trasporto RK faccio l'inno per RK trasporto tutto e abbiamo di meno A stappa meno RK uguale a RK tra poco RK allora 0 quindi cosa significa quando due vettori questo prodotto scalare è 0 sono ortogonali quindi quando facciamo facciamo un'inno gradiente e scegliamo una nuova direzione RK trasporto per RK è RK meno RK RK trasporto sì esatto perché questo sì stiamo facendo questa sostituzione scusa quindi sì questo è il punto allora ricordatevi che è un prodotto scalare quando il prodotto scalare tra due vettori sia 0 ciò che significa è che i due vettori sono ortogonali come nell'altro quindi quando facciamo il metodo del gradiente stiamo dicendo che ogni singola ogni singola direzione è ortogonale a quello precedente quindi adesso scriverò questo diciamo ok quindi questo c'è bello e brutto di questo se stiamo insistendo ortogonalità locale dice facciamo che siamo qui ok è nostro primo indovina è così adesso questo metodo di gradiente dice che dobbiamo andare così perché dà la direzione del massimo cambiamento che rispetto a dove sei e quindi sarà ortogonale rispetto a dove sei e quindi allora quindi io ridisegno questi per essere un po' più più chiaro questo è come arriva alla soluzione il metodo del gradiente ok allora è vero quello che hai detto la matrice è simmetrico definito positivo è vero che questa cosa ci arriverà ma tipo visto che me l'avete chiesto dei videogiochi qualcuno di voi avete mai giocato uno dei giochi di residentivo quello vecchio vecchio allora in questi giochi una cosa molto frustrante era un gioco tridimensionale ma avevi non c'era ancora inventato questo quindi avevi solo le frecce quindi potevi andare solo quattro direzioni quindi significava che guardavi il tuo personaggio e poteva spostarsi solo così esatto e questo è un po' come il metodo di gradiente nostra nuova direzione è così quindi stiamo dicendo che questo è il metodo il nuovo più facile raggiungere questo anno fai sempre questo cosa di se vogliamo andare da qui a qui lo fai così e potevi chiedere va bene ok ce la possiamo fare però sarebbe meglio se potessimo fare così quindi perché questo succede questo succede perché è quello che potremmo dire in inglese si dice greedy quindi è un algoritmo viziato quello che stiamo dicendo e c'è questo implicito assunzione che perché una cosa è ottimale localmente cioè rispetto alla nostra posizione attuale significa che è anche ottimale rispetto alla soluzione globale cioè significa che è necessariamente anche la direzione più intelligente che non è necessariamente vero e in generale algoritmi del genere non sono benedati ti faccio un esempio del problema di un algoritmo greedy ok quindi allora immaginiamo che vi chiedo attraversate questo partendo qui in modo che la somma accumulativa di ogni elemento attraversato e massimizzato il metodo gradiente ci porta qui perché dice ok localmente questa è la cosa migliore da fare e se lo facciamo così il massimo che possiamo raggiungere qui però se usiamo una cosa più intelligente possiamo forse massimizzare globalmente quindi nel caso specifico di una matrice simmetrico definito positivo possiamo garantire che raggiungeremo qui questo è più il problema quando è appunto alla sella che vivo veramente non c'entra un cavolo con la soluzione ottimale però c'è comunque un teorema quando a e se a std allora il metodo del gradiente converge con così in questo è il tazzo di cambiamento a che esimo che più unesimo interazione spiegherò con un attimo ok perché ho scritto questo a allora ho scritto a perché questa è una norma particolare questo è la norma di a quando a è simmetrico definito positivo possiamo indurre una norma particolare su un vettore che si definisce in questo modo quindi la norma della cosa tipo anziché la norma di x c'è questo a in mezzo si può dimostrare facilmente che questa è una norma e soddisfa tutta la proprietà che ne abbiamo bisogno e come avevo spiegato prima le norme sono tutte equivalenti in un certo senso quindi se converge in questa norma convergen tutte le norme e allora questo posso anche scrivere così quindi ma scusa e questo sì esatto è è l'errore ok quindi questo significa che chiaro questo garantisce la convergenza questo coso qua è sempre meno di uno quindi all'infinito convergerà sempre alla soluzione però non è detto che è molto meno di uno potrebbe se a se un numero di condizionamento qui è molto grande diciamo che a è mila allora la convergenza di questo localmente è questo fattore che non è esattamente rapido quindi se hai fino al giorno di giudizio ad aspettare questo è garantito funzionale per a spd ma anche quando a e spd non è mai necessariamente una cosa che converge velocemente è possibile che una cosa può convergere così lento che da un punto di visto pratico non converge veramente se dobbiamo aspettare due miliardi di interazioni non è molto ideale quindi cosa si può fare una delle cose che possiamo fare e come dire che è limitato da questo quindi ok è sempre importante capire che questi non significa che è per forza così lento in pratica lo è può essere anche meno diciamo che non può essere peggio di questo però sì in effetti hai ragione è un po' come dire che diciamo che anche se alfa k è localmente ottimale diciamo che localmente ottimale potrebbe essere comunque non molto non molto ottimo immagino che abbiamo vogliamo raggiungere il minimo e abbiamo questo diciamo che tipo andiamo verso ok vogliamo raggiungere questo punto qua direzione del massimo cambio comunque non è un cambio così grave quindi anche se localmente è ottimale questa alfa k ancora questo non potrebbe valere così tanto dipende dal problema quindi vuol dire che anche se possiamo dire che questo è il massimo localmente c'è sempre un vincolo in base al problema globale che determina quanto ottimamente possiamo portarci localmente sin dall'inizio potrebbe essere che comunque è lento e questo se a voi piace pensare geometricamente questo corrisponde geometricamente in un senso vero alla condizione di una matrice mal condizionato e perché se ve lo ricordate abbiamo detto che il numero del condizionamento può essere concepito come una certa misura di quanto la matrice è da essere singolare quanto non singolare lo è quindi anziché solo una questione di sì o no è invertibile o no ci dà un certo specie di grado di invertibilità quando una matrice non è invertibile significa in particolare che non c'è una soluzione unica e quindi se la matrice non è invertibile significa che tipo fino in fondo potremmo forse fino ad un certo punto fare spostamenti in direzione ottimo ma poi arriveremo in questo punto piatto in cui tipo siamo un po' in questo punto di caos perché non c'è questo ben definito punto e questo numero di condizionamento allora per una matrice spd ci dà una misura della curvatura di questo spazio e quando è più alto significa che la curvatura è molto basso quindi ok c'è un minimo ma tipo non è un minimo molto forte non è molto più meno rispetto ai suoi vicini quindi quando questo è grande significa che siamo così quindi anche se stiamo andando in direzione ottimale comunque fa schifo tipo quindi ci sono due cose che possiamo fare per remediare questa situazione non ideale la prima della quale sarebbe quella che abbiamo parlato stamattina la prima sarebbe opzione meno possiamo cercare di fare un precondizionamento e va bene è sempre sempre concesso il precondizionamento e questo potrebbe funzionare per rendere questo numero di condizionamento più piccolo però in pratica questo è un po' così poco vantaggioso che anche il precondizionamento potrebbe migliorare il condizionamento tanto ma ancora questa espressione non è fantastico quindi per un precondizionatore che veramente aiuterà tanto non è detto che non è detto che possiamo trovarne uno facilmente che è abbastanza facile costruire applicare che conviene farlo quindi forse il precondizionamento potrebbe funzionare per rendere questo fattibile ma probabilmente no probabilmente vogliamo fare un'altra opzione e quale sarebbe un altro modo se pensiamo del problema qui quello che ho fatto il disegno che ho fatto prima delle scaletti che questo dimostra come facciamo la convergenza il fonte di questo problema in effetti è che è totalmente locale il metodo non c'è nessuna conoscenza o nessuna memoria dalle direzioni precedenti invece se cercassimo un metodo che anziché semplicemente essere ortogonale alla direzione precedente adesso diciamo che utilizzeremo P perché questo potrebbe essere diverso rispetto al residuo se trovassimo direzioni tale che è semplicemente semplicemente essere ortogonale a questo singolo direzione precedente se insistessimo che questa era la direzione anche ortogonale non solo a quello ma tutte le direzioni precedenti in questo modo non faremmo questa cosa arrodondante in cui stiamo alternando tra due direzioni interpetuità questa condizione impedirebbe questo allora quindi l'idea è così in realtà questo è una condizione un pochino troppo forte da richiedere che sono ortogonale in questo senso ma c'è un altro tipo di ortogonalità che possiamo usare che è collegata ad una cosa che abbiamo appena visto l'idea è semplice ok l'idea è che 1 seleziona troviamo e spiegherò che è i o no i che è più 1 tale che è è è sì è è sì a i k trasporto i k più uguale a zero dovrei utilizzare cosa c'è j quindi vogliamo non solo che solista qualche condizione di minimizzazione del residuo ma anche che solista questa condizione qua che vedete non è esattamente la stessa cosa dell'ortogonalità classico perché c'è questo a in mezzo questo si chiama a ortogonale ortogonale se due vettori soddisfano queste proprietà allora quindi questo metodo la derivazione e quindi allora facciamo così un attimo come lo possiamo fare ok diciamo che insistiamo che diciamo che voglio solo perché è complicato questo non voglio sbagliarlo ok ok no non è la dimostrazione allora questo lo semplicemente scrivo qui è un nuovo metodo l'intuizione è che è una variazione del metodo del gradiente con questa condizione addizionale questo altro condizione a cui insistiamo per garantire che nostri nuovi direzioni non contengono questa redundanza e questo si chiama il metodo del gradiente congiudato e questo anche è una cosa che vale per a simmetrico definito così di giudato anche si sente spesso il termine cg congiugato conjugate gradient cg l'idea è questo è sì allora io scriverò il metodo e poi parleremo un attimo del metodo questo lo sto introducendo un pochino informalmente perché è più complicato l'argomento qui che richiede una ripensione e una dimostrazione dell'induzione matematica che pensavo che non era necessariamente il migliore uso del nostro tempo quindi prima è così k uguale a 0 1 2 cosa ma si l'iniziale direzione p0 stiamo dicendo è uguale a questo è tipo il nostro prima scelta di direzione è il residuo ok e allora facciamo così alba k è uguale a pk trasposto rk pk trasposto a pk poi abbiamo xk più 1 qui aggiorniamo allora la nostra soluzione pk poi abbiamo rk più 1 questo è il nuovo residuo che è uguale a rk meno alfa k pk che si può vedere manipolando questa espressione che questa è poi c'è un nuovo parametro pk che è così apk trasposto rk più 1 poi apk trasposto pk quindi questo qua e finalmente la direzione nuova dopo tutto questo è questo pk più 1 è uguale a rk più 1 meno pk pk ok allora non è banale la dimostrazione che questo funziona quindi mi dispiace se preferite che faccio la dimostrazione possiamo farlo domattina però intuitivamente spieghiamo cerchiamo di interpretare ciascuno di questi pk qui allora la differenza viene da qui e sto per spiegare questo sono entrambi i vettori ma sono diversi sono diversi perché r non ha anche se l'abbiamo scritto in questo modo questa è un'equivalenza r non è cambiata ok la differenza è che r come abbiamo visto quando continuiamo ad utilizzare r come nostro vettore direzionale abbiamo questa fetta delle scaletti quello che è p introduciamo questo parametro qua e poi facciamo questo pk più 1 che è una combinazione del residuo e questo parametro che dipende sulle vecchie direzioni ed è tramite questo che garantiamo questa condizione qua e si può si può vedere questo in base a questa espressione vediamo che se allora si può vedere vediamo sì sì rispetto a pk ci sta perché sì perché pk sì iq pk porto rk meno alfa k meno sì e poi ok oh mio dio ok è una rottura però lo facciamo ok penso che di averlo fatto non per l'uva qui ah no no no no no non funziona così però che stavo per scrivere non è vero perché questo garantisce che questa condizione è vero quindi stiamo dicendo che l'aggiornamento qui anche garantisce non solo non un'ortogonalità locale solo dovrei dire questo questa condizione garantisce che la nuova direzione rimane a ortogonale rispetto a tutte le tutte le direzioni precedenti e questo è un rapporto ricursivo che garantisce questo quindi ogni volta che facciamo una nuova direzione stiamo dicendo che questa direzione deve essere a ortogonale deve soddisfare questa condizione in modo che evitiamo la situazione in cui stiamo continuamente alternando o scegliendo in direzioni simili non spostandoci tanto è una seconda condizione di ortogonalità e può essere concepito come se abbiamo minimizzato per alfa k con il metodo del gradiente adesso lo stiamo facendo due volte una volta qui per trovare in direzione del cambiamento massimo è una volta qui per garantire l'ortogonalità questo calcolo in realtà forse che erro sarebbe un ottimo esercizio per voi per garantire e fa quello che è difficile quando dico che questa dimostrazione è difficile è abbastanza facile dimostrare questo per pk e pk più uno quello che richiede un argomento sofisticato è che questo anche convale non solo per quel cato precedente ma anche tutti quelli prima di quello però se mi prendete per la parola questo è l'idea e facciamo un attimo un'altra cosa sì dove questo è un p ok allora questo allora dovrebbe migliorare le cose e se guardiamo teorema teorema teorema teorema e e e tg converge allora la prima cosa è tg converge al massimo quindi il al massimo e in passi allora questo è una cosa importante questo sta dicendo che se ha spd al massimo un numero di interazione uguale alle sue dimensioni questo metodo dei gradienti congiugati è garantito a converte la disposizione che desideriamo quindi abbiamo già un limite massimo di quanto può quanto ci può volere questo quindi se lo implementi questi a matlab e metti su una matrice tipo tre per tre vedrete che tipo dopo tre interazioni già c'è convergenza perché questa proprietà esiste allora però comunque se hai molto grande se tipo qualcosa milla anche milione questo potrebbe essere ancora un numero molto grande quindi aspetta qui quindi l'errore è così anche l'errore perché esimo passo e scriverò tra un attimo cosa significa questo c dove c è questo ok e questo è anche l'errore a dove c è sì scusa sì k2 perché questo è numero di funzionamento 2 con la norma 2 ma sì abbiamo detto che numero di funzionamento se non è alternativamente specificato è sempre quello 2 allora questo se pensiamoci un attimo è un miglioramento incredibile che stiamo già dicendo che questo c'è un fattore che garantisce che questo sarà sempre molto meno di 1 perché c'è un potere di 2 qui e solo 1 qui ma anche qui questo fattore è tipo radice radice quadrato del numero di funzionamento anche anche del anziché il numero di funzionamento che è un ordine di magnitudine di miglioramento quindi è molto molto più veloce e l'idea è che c'è un disegno nel libro e anche cercherò di ristrutture se allora grazie come abbiamo visto va così allora grazie congiugato perché non sono ortogonale in questo stesso modo perché è quello che significa questa condizione di non è la norma o puoi vederlo come una una specie di norma non voglio abusare troppo ma una norma lagrangiana che tipo riorienta lo spazio in base a così tipo a ortogonalità può veramente ci porta sempre più verso nostra soluzione e per lo più garantisce che non ripetiamo la stessa direzione quindi non c'è questo problema con le scalette allora è ancora è ancora diciamo è molto meglio è molto meglio del metodo dell'organizzazione un grande miglioramento e ancora non è perfetto perché c'è ancora sempre questo problema qui che ok se questo è veramente enorme abbiamo ancora questo stesso problema amico siamo sempre lì quindi in questo caso l'unica alternativo è tipo fare un precondizionamento e l'unica va un po' modificato quando introduciamo il precondizionamento perché se ve lo ricordate ok scriverò questo qua quando c'è il precondizionamento se ricordate quello che dicevo prima l'idea è che facciamo introduciamo questo sistema dizionale per lo presivo quindi la versione precondizionata ccg clic con e così ora questo allora abbiamo il nostro indovino iniziale abbiamo il nostro residuo iniziale abbiamo il nostro residuo precondizionato iniziale e abbiamo il nostro direzione iniziale che corrisponde al residuo precondizionata allora a no questo dovrebbe essere e allora prima facciamo sempre questo non è diverso questo finora adesso è identico abbiamo questo c'è contraffata si c'èаци agli resistenza precondizionata è uguale si iniziamo 0 come un esempio e poi poi è uguale a meno ala k a tk e questo è dove la magia c'è qui iniziamo poi qui allora poi il resto è simile no vogliamo di carta e pagata quindi uguale perché si si perché di capa ok adesso prendiamo la nostra direzione ma la differenza rispetto a prima e che è tutto per questo residuo qui condizionato e anche questo viene rifletto nel nostro nuovo direzione e quindi quello che stiamo facendo qui è importante realizzare questo quando facciamo il precondizionamento questo gradiente non è più quando non è più condizionato abbiamo ok abbiamo questo che non diciamo non è con quando è più condizionato stiamo sempre considerando questo regino quindi per quello parliamo del residuo precondizionato bene qui otteniamo questo proprio pressione risolvendo questo poi quando scegliamo nostro nuovo fattore beta per incluire l'autogonalità non utilizziamo r residuo ma invece z il residuo preposizionata poi per l'aggiornamento della direzione utilizziamo questo residuo precondizionato non quindi abbiamo trasformato il problema di essere uno che funziona in base al residuo precondizionato poi c'è un pochino più di lavoro ma come vedete c'è una logica qui gli aluritmi sono sembrano identici fino a questo punto però qui otteniamo si sono bene si sono identici in realtà vabbè ok no sono leggermente diversi qui però comunque è un residuo adesso che è precondizionato e lo utilizziamo questo come la base della nostra decisione per la prossima direzione quindi sembrano molto diversi o sembrano un po diversi ma in realtà veramente è un algoritmo simile e lo stesso algoritmo applicato al problema precondizionata e si può ottenere con questo modo ma veramente questo il punto fondamentale che qui stiamo per funzionare un residuo per utilizzare quando selezioniamo sia la direzione nuova che il nostro passo nuovo movimento del basso allora c'è una cosa però che dobbiamo se ve lo ricordate cosa dicevo una cosa che è necessario per c'è un punto sottile qui perché potresti potresti pensare forse che dico ok questo metodo è perfetto se abbiamo un sistema SPD possiamo utilizzare qualche precondizionato sistema è risolto ma ricordatevi che questo è garantito solo per un situazione SPD quindi quando scegliamo il precondizionatore dobbiamo garantire che non rompe quella condizione che è compatibile con la spd quindi in generale cose generici come il l u eccetera non fanno questo in pratica però è sempre un motivo per stare attento perché potrebbe essere che se non sta attento se selezioni a caso un precondizionatore potrebbe rovinare questa condizione e poi non abbiamo più una cosa che funziona allora io voglio se mi potete concedere forse tipo tre minuti in più oggi voglio considerare voglio dire una cosa della situazione importante che cosa che cosa facciamo quando il sistema non è SPD perché non possiamo utilizzare questo l'idea però è simile in effetti esiste un algoritmo che funziona un metro iterativo non stazionario che funziona per un po' sia simile in effetti che non riceve nessuna proprietà particolare di a e sempre con del comunque l'idea quello si chiama algoritmo Jim res che significa generalize minimo minimo minimo risidual risidual risidual ok ok cos'è Jim res Jim res in effetti uno come CG garantito convergere in n interazione quindi per a una matrice n per n se diamo Jim res in interazione abbiamo sempre convergenza l'algoritmo è parecchio complicato però l'intuizione qui è che è molto simile a questo idea qua ma l'idea è questo idea utilizziamo direzioni utilizziamo diciamo di costruire dirò no non costruire certiamo costruire una base ortogonale di allora r il residuo questo è l'idea e assomiglia un pochino gradiente congiugati ma utilizziamo questo approccio con residuo iniziale sempre con questi poteri di a questo è l'idea di cercare direzioni che sono ortogonali a questi quindi cerchiamo un vettore che è ortogonale a questo insieme quello che è costruito viene costruito dai poteri di a e questo si chiama lo sottospazio privo e converge come è un po' diciamo è garantito convergere con questo tasso quindi si può dire che questo diciamo che è così questo è il tasso di convergenza quindi questo radice quadrato di k a a ogni passo allora e questo è l'algoritmo quello generale che funziona anche se non è spd e ovviamente vivo se vuoi migliorare questo dovrai fare qualche preconvezionamento che riesce ad abbassare tanto il numero di funzionamento quindi poi parlo con anche il diagonale qualcosa però questo diciamo quando abbiamo un generale matrice grande sparso e dobbiamo risolverlo e non possiamo usare gradiente gradiente congiugato questo è la cosa che sempre funziona come potete immaginare se potete utilizzare gradiente congiugato è superiore però quello non funziona sempre questo sì quindi basta che siete coscienti che esiste questo gym ed è quello che si fa nella situazione generale mi ricordo quando ero studente dottorato fare questo algoritmo c'era una settimana intera di un corso di scriverlo tutto quindi perdonatemi se non voglio fare tutto questo però abbiamo tante cose di parlarne quindi penso che per applicare questo metodo queste sono importanti e di nuovo quando hai il metro e devi farlo con vero c'è devi precondizionarlo perché c'è sempre questo numero di condizionamento che viene in gioco ok quindi ci vediamo domattina che se non sbaglio è il nostro ultimo lezione prima della pausa pasquale ok務 e