non è così importante questo lo che voglio è che tipo imparate un po l'idea dell'algoritmo e la sua implementazione quindi allora questo modo quindi oggi parleremo vedremo i metodi interazioni per ai sistemi lineari sicuramente i metodi stazionati quindi io non posso grazie ok quindi abbiamo i metodi interattivi per la risoluzione di a e x uguale a b l'idea di metodo operativo e vogliamo costruire una sequenza lì legal ingenio somete bild e come e abbiamo visto che metodi stazionali funzionano in base da prendiamo un P e N, tale che A uguale P meno N, e costruiamo queste sequenze, quindi è M sk1 uguale N skb e abbiamo visto che questo converge a se è solo se il raggio spettrale, quindi dirò max, il massimo alto valore di questo P, il valore assoluto, ho il raggio spettrale, se questo è meno di 1, questo è il valore assoluto, quindi meno di 1 significa valore assoluto meno di 1, e abbiamo visto che questo possiamo anche scriverlo in questo modo, non cambia niente dal punto di vista concettuale, è solo che è un altro modo di capire questo, ma questo rispetto all'idea che per funzionare bene e questo rispetto in qualche modo è un'approssimazione all'inverso di A, ma anche P dovrebbe essere facile risolvere, e da parte di A, dovrebbe essere molto più facile risolvere P rispetto a risolvere A, direttamente, altrimenti non c'è senso a perseguire queste strade di funzione, e poi abbiamo visto le due, diciamo, metodi classici e il modo di selezionare P, tale che P è facile, più ovvio sono Giacobi, Giacobi, in cui P è diagonale di A, e N è un resto di A, e poi abbiamo visto il gaussario, quindi P è il triangolare interiore di A, e N è un resto di A, e questo funziona per A diagonalmente dominante, e questo funziona per A SID, simmetrico positivo definito, e questo più o meno rassume quello che abbiamo visto ieri. Voglio avvertirvi che abbiamo fatto anche le varie ente ai blocchi, e ho messo un calcolo a gaussirello a blocchi sul sito Blended, e penso che Giacobi a blocchi, quello diagonale, non dava tanti problemi, principalmente Giacobbino, che era il ponte dei difficoltà. E quindi adesso concluderemo questo argomento. E allora c'è un altro metodo che voglio parlarne, che fa parte di questo classico di metodi generali. E questo si tratta del metodo di Richardson. di Richardson. Ok, cos'è il metodo di Richardson? Iniziamo con il nostro classico metodo stazionario generico, che quando si scrive al metodo stazionario generico, sempre partiamo con questa forma, PXK1 uguale a NXK più I, e l'idea qui è utilizziamo un'altra forma di questa espressione qua, che l'ho utilizzato quando ho derivato il metodo di rilassamento, che è un altro metodo meno fondamentale di questi due, in cui lo sostituiamo qui, questa espressione per N, e quello che proviamo qui, facendo delle inversioni di P qui, e abbiamo più P in versione A, P in versione B, e questo possiamo riorganizzare in questo modo. Questo abbiamo visto. si tratta del residuo. E la differenza... Allora, quando questo X è quello che risolve il sistema, abbiamo AX uguale a B. Quindi questo possiamo vedere come un indicatore di quanto siamo lontani dalla nostra soluzione desiderata. Questo è il termine per questo è molto importante, si chiama il residuo. Ok. Quando converge, se l'algoritmo converge, questo pezzo va a zero e siamo rimasti con, in effetti, XK più uguale a XK. E questo è quello che dovremmo aspettare, se la cosa converge, significa che dopo un certo punto non cambia più. Quindi non dovremmo avere più un contenuto da questo pezzo, perché è zero o vicinissimo a zero. Ok. Quindi riscriviamo questo in questo modo. XK. E' bene, questo è va bene. No, utilizzerò R perché dobbiamo abituarsi a questa notazione in cui utilizziamo R perché... Adesso parliamo di metodi stazionali, ma come avevo suggerito ci saranno anche i metodi non stazionali in cui utilizzeremo questo R spesso. Quindi voglio che già ci abituiamo a quello che significa questo R, perché se dobbiamo scrivere B meno E, XK, per alcune di cose che vedremo tra poco, che non usciremo mai da questo, da questa. Quindi l'idea qui è che se questa cosa converge, un po' come abbiamo visto con il classmate, se questa cosa converge, in principio non dovrebbe essere più una cosa che contribuisce al nostro soluzione. In questo senso possiamo anche utilizzare, diciamo, un po' di fantasia. Nel senso che immaginiamo per un attimo per ipotesi che questa cosa converge. In questo caso, allora, non stiamo creando nessun problema se facciamo l'introduzione di un altro parametro a cui possiamo giocare. lo chiamiamo adesso, alpha, alpha. Quindi se questa cosa converge, abbiamo questo R già va a zero. e quindi moltiplicando per un valore scalare qui, con questo alpha, non dovrebbe rovinare il nostro metodo. Allora, non stiamo cambiando tanto. L'unica cosa che possiamo dire rispetto a ciò che abbiamo già visto è prima abbiamo visto praticamente un singolo punto di variazione, tipo una libertà nella scelta di questo P e M. Adesso sto dicendo che possiamo anche mettere nel nostro, diciamo, nostri attrezzi che possiamo variare per raffinare o migliorare questi metodi. Un altro parametro qui, questo alpha, che potrebbe giocare qualche ruolo nella accelerazione, ad esempio, della convergenza di un metodo, forse. Ok. Questo non è incondruente con ciò che abbiamo già visto. Allora, se abbiamo P uguale al diagonale di A e A uguale a 1, questo sarebbe il nostro metodo di Giacomi. Quindi, in questo caso, abbiamo semplicemente Giacomi. Se avessimo scelto P uguale a la parte triangolare inferiore di A con altro uguale a 1, questo è il Giacomi. è un'altra scelta di A, quindi, che è un'altra scelta, però, che potremmo vedere come una scelta ancora più semplice. Cioè, se scegliessimo P uguale a l'identità, cioè non fa parte di A necessariamente, quindi P scegliamo con l'identità, però, adesso, selezioniamo. Selezioniamo. In modo. In. In. Diciamo. Questo è il metodo di Richard Son. L'idea qui è che possiamo. Scriviamo. Ok. Quindi, scriviamo questo come. Sì, ok. Sì, ok. È così, esatto. Adesso, scriviamo io residuo così. Questo è come. scriviamo il mio residuo. Questo è come. Sì, ok. È così, esatto. Adesso, scriviamo io residuo così. Questo è come scriviamo il Mere di. Quindi, immaginiamo che abbiamo già il K più unesimo iterazione di questo metodo. Allora, cosa era questo? Se utilizziamo questo metodo di Richard Son, allora, in questo metodo è questo. metodo. Allora, cosa era questo? Se utilizziamo questo metodo di Richard Son, allora, in questo caso abbiamo detto che questo P sarà l'identità e quindi abbiamo RK1 è uguale a P meno A XK più Alfa di inverso RK. Però, in questo caso, abbiamo visto che questo è semplicemente il residuo. Beh, ok. Ah, adesso vedo che vogliamo fare. Ecco, sì. Ah, no. Questo è, sì, P inverso. Ancora voglio avere questo. Ok. Quindi, adesso facciamo una manipolazione qui. A XK meno, sì, meno Alfa P inverso R A A A A A R K. Questo ha, perché abbiamo detto che questo è il mentale di ricerito. Ok. E quindi adesso abbiamo. È. A. A. Meno. A. A. A. A. U. 1. Meno. A. R. quindi questo è l'idea del metodo di l'idea è che vogliamo trovare una qui in modo che questo interazione a 0 se questo con me a 0 significa che abbiamo convergenza di questa è scala più uno alla funzione miserata ok in generale possiamo scrivere faremo un attimo è vero prima facciamo l'organizzazione letteralmente poi faremo un po modificato quindi l'idea di richardson e provare questo la questa alfa che ci da un modo facile con me a cero ok se abbiamo questo è un problema che forse ho una teorema qui no no no è un pochino ok teorema non lo dimostriamo e poi queste ricerci sono generale è una variante immaginiamo adesso che stendiamo questa idea per un generale scelto di chi verso l'idea qui e diciamo che questo si chiama di un residuo precondizionato abbiamo parlato dell'idea del condizionamento che ne guarderemo di qui l'idea qui è che possiamo allora scrivere tutto così rk più 1 è uguale a b meno a xk uguale a b meno a e adesso x più alfa p inverso rk questo è questo è il residuo e vedrai perché l'ho scritto così via non a Xk meno alfa a inverso quindi a zk utilizzando questa espressione con questa alfa a me a oppure meno l'idea qui è che veniamo come una sequenza in cui cerchiamo di tendere a zero questo risicolo in modo di corregge le nostre soluzioni piano piano a quello desiderato e questo è l'algoritmo generale che si consiste nei seguenti passi uno risolva chi la particolare avrei dovuto fare così prima però ok zk uguale a beto meno a xk uguale a rk quindi poi vediamo risolviamo il sistema per questo legittimo prepondizionato rk 2 calcola questo parametro di accelerazione questo parametro adesso abbiamo detto che possiamo cambiare sfruttare aiutare la convergenza 3 aggiorna la la in questo modo è 1 1 1 1 1 1 1 1 la in questo modo è 1 1 1 1 più au capo Z k la carta o a generale si ha fatto mettiamo una balla quindi qui se ricordiamo questo con di qui questi se ricordiamo l'espressione che abbiamo arrivato prima stiamo vedendo questo come il nuovo indovina è una combinazione di questo poi qualche modificata qualche termine che dipenda questo residuo che dovrebbe convergere a zero quando il residuo raggiunge zero e poi aggiorna il residuo ok e questo facciamo questo modo e a capo meno a capo a e questo e dove questa espressione centra quindi l'idea dell'albero in diritto sono e questo costante correzionamento del residuo in base al nostro possibilmente questo parametro e anche una matrice che potremmo utilizzare varie scelte nella forma più classica richardson prende una semplice forma in cui quindi diciamo che questo si chiama richardson classic richardson classic idea è il seguente in richardson classic assomiamo che questo è il verso e semplicemente identità e quindi quando abbiamo ho appoggiato abbiamo ex che ke c e e questo è semplicemente l'identità e quindi abbiamo quindi non c'è nessuna matrice che abbiamo applicato qui a questo di nuovo è semplicemente un numero moltiplicando questo residuo e allora questo numero come lo possiamo scegliere allora in questo caso idea è così non farò la dimostrazione di questo forse lo farò dipenderò se volete vedere la teorema posso farlo ma l'idea qui è c'è un modo ottimale di scegliere questo e la teorema così teorema se ha alto valori con parti reali positivi e quindi le parti reali di auto valori di ma sono positivi e ordinati in questo modo richardson converge e e e e e e e e e e e e e e e e e e sì e e f e il dö 스� end e che con questo scelta di alfa quindi questo è un'idea azione che ci dice richardson converge per tutte le matrici che hanno una struttura di alto valore che è così quindi se questo è il piano immaginario e questo è il piano l'asse reali se tutti gli autovalori sono così se hanno tutte le parti reali qui in questa parte positivo nel piano immaginario possiamo farlo come generi se scegliamo questo parametro in questo zona qui che grande di zero e meno di due diviso l'autovalore più grande e per lo più possiamo anche fare un po meglio e dire che c'è una scelta proprio ottimale possiamo scegliere alfa in questo modo tale che questa scelta è quello ottimale quindi possiamo far con me per tutti questi sistemi che hanno questa forma e se scegliamo proprio due diviso la sono tra i valori più grande la parte reale più grande e quello più piccolo dovrei dire per me non perché poi ti potrebbero le numeri complessi in generale quindi a queste sono le parte reali in generale non ci preoccupiamo tanto dei numeri complessi in questa corsa ma qui non si può evitare perché anche se è una matrice che ha tutti gli elementi reali come lo assumiamo sempre potrebbe essere il caso che gli autovalori sono complessi perché se ricordiamo gli autovalori sono le radici di questo polinomio caratteristica e questo potrebbe avere delle radici complessi anche se è un polinomio reale ok quindi questo è molto utile e quello che possiamo dire in generale come fosse immaginate e ho parlato c'è la ricerca in generale l'idea che abbiamo nostro sistema abbiamo questo esimo quindi questa evidente qui in generale possiamo dire ok generico possiamo dire anche che allora per l'innovazione non sfruttiamo tutte e due possibilità qui non abbiamo solo un parametro qui per la convergenza perché non anche inseriamo una macchina qui per condizionare il residuo che non è l'identità e in questo caso anche possiamo migliorare l'idea perché tutto questo è ancora rimane identicamente vera con la differenza che adesso richardson generale convergenza convergenza se sono diciamo questo gli autovalori di esattamente la stessa per l'emma qui con l'unica differenza l'unica tra l'unica generale è quello che ci sono in base e adesso anziché gli autovalori di a qui qui si parla di autovalori di qui si parla di autovalori di i i i inverso a quindi sono molto simili la differenza è che qui abbiamo solo questo numero questo parametro che è il nostro attrezzo per aiutarci con la convergenza qui anzi qui abbiniamo due cose abbiniamo sia l'idea che possiamo scegliere questo parametro per aiutarci con la convergenza e possiamo anche applicare una matrice che in qualche modo cercare di approssimare l'autovalori di a l'inverso di a cerchi di approssimare l'inverso di a allora sì sì sì sì esatto di a in questo caso tipo diciamo che classico possiamo capire come in casa speciale in cui scegliamo questo più inverso come l'identità se diciamo che più inverso e l'identità che questo poi capire il cedro classico come quello generale e solo che diciamo che usiamo questa scelta strana ma questa scelta è legale comunque di più inverso dell'identità e quindi allora più inverso a e solo a e quindi in questo caso abbiamo un spettro di a che è quello che ci ci porta qualche abilità di convergere se a solista questo se però bene che per funzionare nella forma classica a deve già essere ben comportato nel senso che dobbiamo avere questi condizioni dei suoi propriato valori invece invece ok invece a a a stiamo risolvendo un sistema a in guardia a questo quindi si ricordati che il residuo a non per cascolis quere es kamp si a però questa in Typically etc poi ciっち ci probabilmente prossima lezione che lo sfruttano questo è molto più utile e in dire questa notazione ma questo metodo allora forse se state già pensando possiamo utilizzare questo per sviluppare varianti di metodi che abbiamo già visto per forse farli migliorare ad esempio potremmo vedere tipo Giacobbi in cui utilizziamo il diagonale di A qui e possiamo anche aggiungere questo parametro per aiutarci con la convergenza questo lambda ottimale per perlopiù rendere più convergente più velocemente eccetera il il metodo quindi adesso voglio far vedere alcuni dimostrazioni non dimostrazioni nel senso matematico tipo voglio utilizzare matlab un pochino per così possiamo vedere l'implementazione e l'implementazione e l'applicazione di alcuni di questi metodi stazionari e quindi pensavo che questo sarebbe una buona occasione di farlo ho messo per il codice che tipo riempieremo oggi sul sito quindi se volete utilizzare qua ok cambio questo subito ok ok ecco e allora quindi adesso ho messo qui quindi se vai al sito blended e vai qui codicini c'è questo punto zip quindi salva ok poi andiamo a matlab online non penso che c'è già installato qui penso che devo andare online quindi matlab sign in ok adesso adesso apri matlab online ho scaricato questo qua quindi quindi dove sono ecco ci mangiρα google m hoologistin mi posso Smartlab, qui. Uh, mamma mia, comprehension, lente. Upload folders. Poi dove l'ho messo. Penso che è sabato di che. Home. Funcente. Se voglio fare l'estrazione qui, strato. Forse posso pure copiare e rincolare qui. No, sto pretendendo troppo. Ok, eccoci qua. Solo così non pierdo botto, sto barrando. Ho fatto anche una funzione compilato sul mio pc personale, quindi perdonami. Però non voglio perdervi nel mezzo della lezione. Quindi, ok, quindi questo lo chiamo template perché? Perché non è compilato ancora. Dobbiamo, cerchiamo di fare questo insieme. Quindi ci sono i commenti qui. Quindi come avete visto, ho incluso una qui. C'è la matrice A. Che quindi... Dove è il terminale qui? C'è... Non mi piace questo. Ok, va bene. Io preferisco vedere più anche... Non voglio vedere pure il terminale come di solito c'è. Perché qui c'è l'editor. Però voglio anche vedere... Non voglio questo. Voglio anche questo. Ok. Eccoci. Questo è il terminal. Di solito io preferisco questo. Dove è andato il terminal? Terminale. Ok. Ok. Ok. Ok. Ok. Adesso è peggio di più. Avevo il terminal. E adesso non più. Certo. Command window. Ecco. Ok. Questo. Ok. Questo è meglio di niente. Dai. Ok. E allora... Quindi adesso vediamo cosa significa quando carichiamo questo. Sì. Cerchiamo di... Vediamo se riesco a editor. Di solito c'è... Home. New. Ok. Questo è meglio. Ok. E adesso... Abbiamo... Abbiamo... Ok. Quindi A. E questa matrice qua. Non è enorme. È una matrice di... Ma questa è utile per il nostro... Per i nostri motivi didattici. Quindi. Allora. La prima cosa che facciamo qui... In generale non avremo questo. Però abbiamo un A e un B. Che... Ok. Così abbiamo un'idea di qual è la soluzione vera. Diciamo. Questo è... Corrisponde... Corrisponde... Al nostro... La cosa che vogliamo ottenere utilizzando un metodo iterativo. Quindi... Qui faremo qualche... Dichiareremo qualche parametri. Inizieremo con un numero di iterazioni. Piccolo. Diciamo cinque iterazioni. E... Vedremo... Cosa significa... Beh. Significa che... Applicheremo questi vari metodi iterativi cinque volte. E vedremo quanto avviciniamo alla soluzione. Per iniziare... Facciamo... Primo il metodo di Giacobi. Quindi ricordiamo come definiamo la nostra matrice P nel metodo di Giacobi. È scritto qui. L'estrazione del diagonale. Per fare questo... Vi farò vedere qui nel terminale qui. Quindi... Se facciamo il diagonale di A... Questo era A. Vediamo... Il comando Diag A... Ci ritorna... Un vettore. Perché prende il diagonale da matrice... E ritorna nella forma di un vettore. Però... Se diciamo Diag... Ad una cosa che è... Già un vettore. Quindi 3. Vedi che succede? Ci ritorna una matrice. Quindi Diag fa due cose. Se dai una matrice a Diag... Ritorna un vettore... Che consiste agli elementi diagonali. Se dai un vettore a Diag... Ritorna una matrice diagonale... Che consiste a quei propri elementi. E quindi se facciamo... Due volte Diag... Diag Diag A... Dovrebbe essere quello che... Che vogliamo... Adesso abbiamo un elemento... Che consiste... A è la matrice. Poi N... Questo è il resto della matrice. Abbiamo già A... Abbiamo già P... Quindi P-A... Dovrebbe darci... Quello che rimane della matrice... Per applicare questo. Qui... Iniziamo con un indovina... Per X... A caso. Come ho detto... Qui... Facciamo semplicemente... Un numero a caso. E... Io... Io... Lo risalvo qui... Non voglio sovrascrivere questo... Voglio tenere questo... X zero... Perché faremo una paragone... Tra vari metodi... E non voglio... Rovinare i risultati di convergenza... Utilizzando... Indovini iniziali... Che potrebbero essere più o meno adatti... Quindi... Ogni volta... Dobbiamo ritenere... Questo stesso... Inizio... Ok... Qui... Sto per fare una cosa... Che sembrerà molto strano... Però... È molto comunemente fatto... Questi... Cosi... Qui... Cosa significano? Adesso... Non sono niente... Però vediamo che... Matlab dice che... Questo è un sintax... Un sintax... Legale... Diciamo... E vedremo come possiamo sfruttare questo... Allora... Qui... Dentro questo... For... Faremo... Varie iterazioni... Del metodo Giacobbi... Quindi qui... X k... Dobbiamo aggiornare... Ad ogni passo... Nostra soluzione... Quindi possiamo... Questo è l'espressione... Che è semplicemente... La risoluzione di P... Al nostro sistema... N... Più... L'ultima iterazione... Che è anche la nome... X k... Più... B... Che B è il nostro... Anche il vettore su destra... Che abbiamo già... Allora... Cosa è questo... Residvec... Giacobbi? Questo... Sembrerà strano... Però si può fare così... E questo... È molto... Se provi a fare questa... For... Non... Non andrà bene... Penso che a Python... Più o meno... Si possono fare cose del genere... Questo... È una cosa... Matlab... Che è molto... Fressibile... Quello che possiamo fare qui... E... Quando facciamo questo... Quello che stiamo dicendo è... Possiamo dinamicamente... Aggiungere... Questo elemento... A questo vettore... Continuando a cambiare... Le sue proprie dimensioni... A ogni singolo passo... Con questa notazione... Matlab dice che... Questo non è consigliato... Perché dice che... È un po' lento... Che è vero... Però... È anche molto... Conveniente... Scrivere così... Soprattutto... Quando vuoi variare... Il numero di... Elementi o interazione... Che potremmo fare... E qui... Faremo due cose... Quindi qui... Sto ritenendo... La norma... Del residuo... Ah scusa... Non... XK... Quindi questa è la soluzione... Presente... Nell'iterazione... La norma... B meno A... Multiplicato per... XK... Qui invece... Voglio una cosa... Diverso... Perché abbiamo detto... Qualche lezione fa... Che questo residuo... È molto utile... Come usare... Per indicatore... D'errore... Ma non è esattamente... La stessa cosa... Dell'errore... È sensibile... A veri proprietà di A... In particolare... Su numero di condizionamento... Invece... L'errore di X... Cioè... Quello che quantifica... Quanto siamo veramente lontani... Dalla soluzione... Vero... Viene dato da questo... Abbiamo calcolato questo... X true... O X... Ok... E abbiamo... X... X... X... X... X... Vero... X... Vero... Ok... E abbiamo... Erebe... Quindi qui... Abbiamo... Come si può salvare... Save... Allora... Abbiamo qui... Tutto... Questo inizio... Quindi poi... Run... Section... Voglio solo... Fare questo... Non voglio fare più... Run... Section... Run... Section... Ah... Blind... Ah... No... Ma non volevo fare questo... Vero solo fare... Run... Ok... Metterò qui... Un... Breakpoint... Quindi si può fare qui... Si può dire... Che voglio... Ah... Ah... Ah... Ok... Qui... Voglio... Ok... Non... È un po' diverso... Delle versioni che uso localmente... Non voglio... Imparare... Quindi... Semplicemente... Copierò... Ok... Cosa vediamo qui? Vediamo che... Ah... Ok... Metterò qui... Questo... Vediamo... Quindi sto copiando... E incollando... E adesso... E... Er... Giacomi... Ok... Mmm... Stiamo vedendo che... Questo... A noi piace quello che sta succedendo qui... Non sembra che sta diminuendo l'errore o residuo... Vediamo... Vediamo... Se guardiamo... A... P... Inverso A... Questi sono gli alto valori... Di P... Inverso... No... Di P... Inverso N... Questi sono gli alto valori di P... Inverso N... Nostro matrice dell'iterazione... Vediamo che quello più grande è il magnitudine... Questo è... Più di... Più di uno... Ed infatti... In questo caso... Non abbiamo una matrice che funziona... Non abbiamo una matrice per cui... Ho un schema che per questo sistema funzionerà... Ok... Ed infatti... Se guardiamo A... Allora... Sum... Questa è... N... N... N... 2... Questo... Sarà la somma di tutte le righe... E... Diag... Diag... Diag... Diag... Diag... Diag... Diag... Diag... Diag... Diag... Vediamo che... Questi sono le somme... Di tutti gli elementi non diagonale... Di A... Tra tutte le righe... E questo... Sono gli elementi diagonali... Quindi vediamo che... Questo non è diagonalmente dominante... Quindi... A priori non c'è nessun motivo... Perché dovrebbe convergere Giacobbi... Quindi Giacobbi... Nonostante quanto è semplice ed elegante... Non è... Una cosa molto robusta... Necessariamente... Ok... Ok... Quindi adesso... Se non funziona Giacobbi... Cerchiamo... Gauss Seidel... Che... In teoria... Dovrebbe essere... Più robusto... Qualcuno ricorda... Quando dovremmo... Vedere... Convergenza di Gauss Seidel... SPD... Simmetrico definito positivo... Vediamo se A... È simmetrico... Sì... Sì... Ok... E vediamo... Sì... È simmetrico... E tutti i valori... I autovalori sono... Più... Sono positivi... Quindi sì... Dovrebbe funzionare Garzai... Allora... Se facciamo l'estrazione... Nelle parte triangolare... Trul... Così... Trul... Trul... A... Ci ritornerà... Questo... Ok... Poco... No... Vole indovinare... Come possiamo... Fare l'estrazione... La parte triangolare... Triangolare... Superiore... Triul... Esatto... Ok... Quindi adesso... Come ho detto... Non vogliamo... Influenzare... La convergenza... In modo... In giusto... Quindi... Adesso... Come ho detto... Non vogliamo... Influenzare... La convergenza... In modo... In giusto... Quindi... Ricicliamo... Questo stesso... Indovina iniziale... E facciamo... Facciamo qui... Esattamente... Come l'abbiamo fatto prima... Con questi... Cosi vuoti... Qui... Non cambia niente... Abbiamo cambiato... La definizione di P... Ma non una forma generica... Quindi... Per questo... Formi generici... Sono utili... Lo so... Che... È un po' una rottura... Quando sto sempre scrivendo... Questo... Generico... P... Generico... N... Eccetera... Ma... Quando... Facciamo... L'interpretazione... Vedete... Un po' il valore... Di questo... Ok... Resid... Vec... C... Poi... Norm... X... O... V... Se... Voglio essere veramente... A... Per... X... Scusate... Io non sono abituato... Alla pastiera italiana... Quindi devo... Trovare... Voter... Voter... Ok... Er... Vec... L'interno... Q... X... X... X... Vero... Dì... Adesso... Facciamo... Copia... Ringolare... Di questo... Qua... Ok... Ok... Quindi qui... Ok... Veriamo... è successo qui ok questo è il 10 quindi ok sembra che abbiamo convergenza qui sta diminuendo se guardiamo ai autovalori ok vediamo un valore assoluto di questo ok no qui ok quindi abbiamo tutti quanti meno di uno un valore assoluto quindi calzare dovrebbe funzionare non è necessariamente però superveloce ma sta funzionando ok bene adesso facciamo il metodo di richardson se guardiamo un attimo ai autovalori di a abbiamo visto che sono tutti quanti parte reale positivo quindi dovremmo riuscire a trovare alfa per cui richardson funziona quindi ok volevo che riempissi questo vuoi però un po forse ho nominato questo variabile non super non c'è tanto che rimane per voi a pensare qui ok alfa quindi come abbiamo visto alfa possiamo definire in questo modo quello ottimale max max abs no max real a e a più min in questo caso sono tutti reali diciamo comunque quindi è un po' radun dante ma in generale anche se ci fossero alcuni immaginari questo è il modo che dobbiamo definirlo le parti reali quindi questi lasciato stare allora qui lo scriviamo un pochino diversamente rispetto a quelli precedenti lo utilizziamo la forma che è più comodo quando sta stiamo facendo iterazione richardson cioè alfa meno infatti io per k uguale a b meno a per dove sei? xk no in realtà io faccio così così fuori e poi lo facciamo così rk e poi rk uguale a b meno a per xk e poi anche farò così così così allora possiamo fare questo stesso coso con la ripetizione poi norma adesso è un pochino diversi e poi anche faremo l'errore che sarebbe ok anche ho bisogno di fare un'altra simile qui norma x vero meno x ok adesso ok quindi adesso vediamo cosa succede qui voglio quindi questo è richardson che vediamo se è meglio dovrebbe convergere sì sta convergendo si vede che non è monotonico necessaria ma come è successo qui ok ok adesso ok adesso ok vediamo che sta convergendo non necessariamente velocemente quindi adesso abbiamo nostri tre cose abbiamo tre varie metodi quindi abbiamo jacobi abbiamo visto che jacobi non funziona poi abbiamo gauss-seidel che abbiamo visto che sta funzionando e abbiamo visto richardson che ok stava funzionando però abbiamo fatto solo cinque interazioni vediamo quale x k qui questa è adesso è l'indovinata attuale di richardson invece quello x vero è ancora ben lontano forse se guardiamo x k se vuoi veramente tipo essere super contiglioso si potrebbe dire che forse è un po' meglio ma non sembra così tanto meglio adesso allora aumentiamo il numero di interazioni concesso diciamo adesso che permettiamo 25 interazioni e adesso faremo solo gassari e richardson perché sappiamo già che giacobi è in qui ok ok e le esibere già covi a non giacobi gas quindi adesso ok vediamo qui che dopo un po' sembra che abbiamo raggiunto abbastanza una soluzione accettabile richardson ok come potete immaginare sembra che sta convergendo ma secondo voi è super veloce questo io direi proprio di no forse l'errore è meglio e meglio e non tanto e sai quindi questo però è un ottimo esempio anche del questa cosa è un rapporto tra il residuo e l'errore volevo far vedere questo allora sta per comparire una matrice qui sulla colonna sinistra questo è il residuo colonna destro è l'errore vediamo che stanno tutti e due convergendo ma questo un po' illustra il punto del residuo che a volte non può dare troppo retta perché vedi che tutti e due stanno convergendo ma se guardiamo solo il residuo abbiamo questa idea che la soluzione è super super preciso invece se guardiamo al vero errore è vero che stiamo avvicinando una soluzione meglio e meglio ma questo tipo 3% errore spesso non è accettabile non è tipo meno di 1% quindi importante vedere questo la formula dell'errore sarebbe questa iterazione è questa iterazione xk meno xvero che in questo caso solo per valutare le varie avvitini abbiamo anche tipo calcolato diciamo in generale non si ha questi errori il punto è tipo si deve si deve sempre ridurre il residuo ad una quantità più basso rispetto a quello che si vuole dell'errore quindi se vogliamo un errore che è preciso non c'è una regola super formale ma se vogliamo un certo livello di precisione nell'errore dobbiamo essere stringente con il residuo quindi non possiamo pretendere perché l'ordine del residuo è così che l'errore è simile potrebbe essere ancora molto più lontano finalmente c'è un'ultima cosa che volevo fare qui che è quello che richardson più garst seidel e quindi questo è l'idea che possiamo utilizzare tutte due cose qui quindi possiamo utilizzare garst seidel nel senso che facciamo l'estrazione del parte triangolare inferiore poi possiamo estrarre questi autovalori di non a stesso come abbiamo fatto per richardson normale ma di dell'a precondizionato di questo p inverso a quindi questo speriamo che da qualche miglioramento rispetto a questo richardson normale adesso utilizzerò un po' di copy paste哪裡 i PL ok qui ok e qui e adesso lo scriverò così allora possiamo scriverlo così uguale ad xk più dove cavolo? xk più alfa per adesso P grande poi questo B meno ok avrei potuto fare allora posso rispondere con il suo crank ok e poi di qualcosa di XTS di qualcosa di XTS e primo vc ok finalmente GTS norm x x vero ok ok e adesso voglio dire ok qui no perché non voglio in congruenza ma questo è ok ok ok allora adesso facciamo una cosa permettiamo 100 iterazioni e riparemo tutto così quindi adesso metodi iterativi stazionali template ok abbiamo rigenerato l'indovina iniziale quindi ci potrebbe essere qualche differenza tra ok adesso vediamo un plot dei risidori quindi questo faccio così quindi qui stiamo vedendo delle varie plot delle soluzioni quindi quindi ok figura compaia please ecco ok è un pochino strano ma va bene non so perché c'è questo spazio bianco però ok allora questo ok allora quindi guardiamo alle varie cose qui una cosa interessante di vedere è che in questo caso Gauss Seidel da solo è meglio di Richardson più Gauss Seidel però tutti convergeno si vede che Richardson però è super lento che dico dopo 100 iterazioni ok il residuo è stato ridotto tanto però non tanto dico sta lo stiamo riducendo però basso battiamo a un tasso molto basso invece Gauss Seidel siamo quasi al punto di tipo questa è quasi una soluzione perfetto e poi qui stiamo vedendo una soluzione con una richards in più Gauss Seidel non perfetto però ok se guardiamo invece l'errore cosa vediamo quindi adesso non è più il residuo stiamo per guardare l'errore proprio in questo caso vediamo la stessa cosa Richardson è ancora peggio no Richardson è un po' meglio dell'errore rispetto residuo però però ok c'è un motivo perché qualcuno riesce a pensare perché forse non vediamo perché potrebbe essere il caso che qui vediamo che Gauss Seidel è meglio di Richardson Gauss Seidel perché si penserebbe che Richardson Gauss Seidel immaginiamo immagineremo che questo dovrebbe essere meglio giusto stiamo facendo una cosa più sofisticata però non è necessariamente così c'è un buon motivo per spiegare quello che sta succedendo e questo è quello che parliamo soprattutto settimana prossima e rispetto al fatto che adesso chiuderemo questo per un attimo quindi metterò queste cose comprate oh mio dio ok ok ok ok questo come in lista vai sì come sì allora c'è qualche motivo per spiegare questa e è importante perché quando stiamo facendo questa procedura di qui cosa facciamo stiamo scegliendo una cosa questo parametro che è ottimale per la riduzione di questo residuo R questo è il vero residuo del vero problema quindi quando facciamo il definimento di questo stiamo facendo diciamo direttamente sul residuo del problema invece in generale nonostante che sembra che dovrebbe funzionare meglio in alcuni casi è così ma non stiamo facendo una riduzione direttamente su questo stiamo scegliendo un'alpha non quello che è ottimale di ridurre il residuo stiamo facendo la riduzione del residuo precondizionato cioè questo è l'alpha che è ottimale non per ridurre il residuo necessariamente ma questo alfa ottimale per ridurre non il residuo precondizionato e questo è importante perché questo potrebbe essere diverso molto diverso del residuo del vero problema e in alcuni casi questo aiuta ci dà una spinta in questo caso però quando abbiamo una matrice simmetrico del positivo in cui funziona bene quando facciamo la sua cosa direttamente stiamo facendo una riduzione di questo residuo di che ci porta in direzione che rispetta il problema originale invece quando abbiamo fatto insieme con Richardson anziché fare un'azione che è ottimale in base al residuo vero è ottimale in base a questo residuo qua quindi abbiamo fatto questo modifica qui che è ottimale se vogliamo ridurre questo però non è necessariamente il residuo del problema vero cioè abbiamo aggiunto questo passo dizionale che in realtà ci portava in una direzione più lontana dalla soluzione del problema vero cioè più vicino alla soluzione al problema precondizionata ma non è necessariamente la stessa cosa del problema vero quindi questo è quello che spiega la differenza che vediamo che Richardson è una cosa che è sempre ottimale rispetto a questa scelta di e questo residuo stiamo minimizzando questo quando deriviamo questo è possibile modificare Garzai nel direzione che farà diciamo si può correggere questo effetto però è questo il punto se veniamo adesso non però se l'avessimo visto invece il residuo precondizionato avremmo visto che il Richardson Garzai fa detrimentare più velocemente quello anche se il vero anche se il vero residuo forse non è necessariamente ridotto allo stesso livello quindi questo è una delle cose di un metodo che funziona con un residuo precondizionato perché questo è collegato al residuo però non è esattamente la stessa cosa quindi Gauss Se ve lo ricordate non era un metodo costruito in base alle minimizzazioni del residuo era calcolato un po' direttamente dalla matrice stessa del sistema e abbiamo pure fatto la dimostrazione non in base al residuo ma in base all'errore proprio invece questo è una modificazione a Gauss Seidel ma è una modificazione che vogliamo minimizzare il residuo precondizionato che non è la cosa che Gauss Seidel sta provando necessariamente a minimizzare quindi tutto questo per dire non è controintuitivo in realtà è esattamente quello che in alcuni casi dovremmo vedere che quando vogliamo a volte si pensa che questa cosa vogliamo minimizzare questo residuo che ho detto questo è ottimale questo è ottimale non per la minimizzazione dell'errore se guardi in quartarone penso che derivano questo ma vedi che tipo costruiscono l'espressione che minimizza questo residuo precondizionato questa è la cosa che questa alfa è scelta per minimizzare che non è sempre né l'errore né infatti il vero residuo è una cosa diversa è un residuo precondizionato nonostante questo spesso non abbiamo un'altra scelta per una cosa migliore di minimizzare però non è detto solo perché l'abbiamo fatto questa cosa ottimale in questo senso che sarà ottimale in generale in un senso tipo rispetto a soluzione vera ma come è stato detto un paio di volte di solito non abbiamo un problema come questo in cui abbiamo la vera soluzione a paragonare quindi in un certo senso questo discorso è un po' pantico ma pensavo che era un esempio interessante per vedere questo effetto allora quindi ci vediamo lunedì e metterò questo codice e anche la registrazione come sempre e ci vediamo alla prossima grazie