Allora. Dobbiamo affrontare l'ultima parte di questo corso di programmazione IoT, quindi il capitolo 5, dove nel capitolo 5 parliamo di machine learning dedicato all'IoT. In particolare vedremo tre sotto capitoli, come avevo già anticipato all'inizio, vedremo un po' cosa vuol dire fare machine learning centralizzato sul cloud, machine learning distribuito e machine learning sui nodi, quindi sui dispositivi low power e low performance. Quindi diciamo il machine learning che voi avete studiato il primo anno, avete fatto un corso dedicato al machine learning, sapete essere uno strumento sicuramente molto all'avanguardia, molto di moda in questo periodo, ma sapete anche essere uno strumento di una complessità non banale. Quindi nel mondo IoT è sicuramente un sistema fondamentale, utile e versatile, ma va declinato in certo modo, perché fin quando parliamo di machine learning classico su cloud, può essere semplice appunto immaginarsi il funzionamento, che è quello che è stato studiato, mentre andare a declinare machine learning in uno sistema IoT, fatto appunto di sistemi, di strumenti, hardware molto diversi tra di loro, con capacità computazionali, di comunicazione, tutt'altro che banali, ecco, diciamo, declinare machine learning è sicuramente qualcosa che merita un certo tipo di attenzione e di sensibilità. Allora, perché nell'IoT il machine learning è diventato importante? Il machine learning esiste molto prima dell'appelito dell'IoT, ma come abbiamo visto durante questo corso, una delle caratteristiche di base dell'IoT è quello di produrre una mole di dati stratosferiche, quindi non indifferente. Abbiamo parlato di valori, quantità di dati decifrabili e definibili nell'ordine di z byte, il prefisso z è un prefisso che difficilmente abbiamo usato fino ad oggi, perché è una quantità appunto di byte esagerata. Questa produzione smisurata di informazioni ha dato luogo a quello che viene tradizionalmente chiamato il big data, proprio per andare a raccontare appunto che questa dimensione di dati mai vista prima è diventata una cosa con cui confrontarsi. Quindi l'IoT è una delle sorgenti dei big data e la necessità poi di processare, analizzare questa quantità di dati smisurata si è presto collegata al mondo dell'intelligenza artificiale che appunto trae vantaggio da un'enorme quantità di dati. Voi avete visto nel corso di Machine Learning che i sistemi per apprendere necessitano di una quantità di dati, una quantità di iterazioni affinché i modelli possono modificare i parametri interni e quindi definire quello che può essere la conoscenza artificiale. Quindi questo matrimonio è un matrimonio riuscitissimo, big data e in presenza artificiale. L'intelligenza artificiale gode del fatto che la quantità di informazione smisurata permetta appunto un buon training del modello. D'altra parte i sistemi che producono questa grande quantità di dati traggono vantaggio dall'intelligenza artificiale per poter fare un'analisi e quindi per definire comportamenti e andare a estrarre informazioni significative da questa grande mole di dati. Abbiamo detto che questa esplosione di dati deriva dall'esplosione dei dispositivi. Vediamo in questa slide, parliamo ad esempio di 5 miliardi di smartphone venduti solo nel 2017. C'è questa quantità di dispositivi portabili che ognuno di questi in grado di generare una grande mole di dati ha dato luogo a questa rivoluzione. L'idea proprio dell'IoT è avere questi device sempre connessi in grado di fornire informazioni ma in grado anche di attuare e quindi generare delle azioni attive. Grazie a questi sistemi di big data, di intelligenza artificiale, si è cominciato a parlare di gemelli digitali, di digital twin. I digital twins non sono nient'altro che dei modelli digitali, quindi artificiali, della realtà. Quindi grazie a un sensing, una mole di dati grandissima, grazie al processing attraverso intelligenza artificiale, quindi alla creazione di modelli comportamentali, nasce il concetto di digital twin, che chiaramente è questo sistema che crea in qualche modo una copia digitale del mondo reale che ci permette di definire una forte ottimizzazione dei processi, soprattutto pensate nelle industrie, nella gestione di grandi infrastrutture, ci permette di amplificare appunto la nostra conoscenza e migliorare l'utilizzo delle risorse. Quindi nell'IoT il machine learning dove nasce? Nasce appunto associato a quello che viene chiamato il big data, che è la generazione di dati. I dati, come sappiamo, nascono dal basso, quindi partono dai dispositivi edge, dispositivi edge che si collegano attraverso router, attraverso dispositivi chiamati così nello strato fog, inviano dati al cloud, al server. E qua avviene la data analytics, quindi quella che è il processing dei big data che fa uso anche dell'intelligenza artificiale. Questa è la struttura classica, la prima struttura che ci viene in mente, la prima struttura che nel mondo IoT e big data ha integrato l'intelligenza artificiale. A livello cloud, quindi i dati vengono concentrati, analizzati e è possibile istruire modelli dell'intelligenza artificiale, grazie appunto a questa caratteristica di avere una quantità di dati elevata e la possibilità quindi di reiterare gli algoritmi di apprendimento su una capacità poi computazionale sicuramente elevatissima, perché il cloud ci offre in questa zona qua ci dà la possibilità di avere un numero più notevolmente ampio, una quantità di memoria ampissima e quindi ci dà la possibilità di eseguire sicuramente strumenti complessi. Ma nel mondo dell'IoT in realtà, come abbiamo detto all'inizio, l'intelligenza artificiale si può declinare in tre punti diversi. Possiamo declinarla a livello dell'edge computing, a livello del fog e, come abbiamo visto, a livello cloud. Ognuna di queste possibilità offre dei pro e dei contro, come possiamo immaginare. La classica struttura dove l'intelligenza artificiale è on cloud, abbiamo detto, è quella diretta che ci può venire in mente subito. Ma questo sistema in realtà è poco scalabile, nel senso che alla data attuale i sistemi cloud non sono più in grado, addirittura, non sono in grado di processare in tempo reale le informazioni che arrivano dalla grande mole di dispositivi. Quindi in realtà quello che si comincia a sperimentare è un collo di bottiglia in cloud, perché appunto il numero di informazioni prodotte dal numero crescente di dispositivi è talmente ampio che è difficile ad oggi riuscire a fare in tempo reale data analysis e quindi eventualmente fornire un feedback verso il basso, perché l'idea qual è? Ricevo informazioni da legge, le integro da tutti i miei dispositivi, le analizzo, estruisco modelli e poi torno magari con una attuazione sul mondo dei dispositivi. Bene, questo comincia a essere non più fattibile, perché essenzialmente per la stragrande maggioranza dei casi è dovuta all'aumento di dispositivi in tempo reale, all'aumento dell'informazione. Quindi se il classico, la classica intelligenza artificiale, soltanto i big data, è la prima versione che è stata in qualche modo implementata, utilizzata, oggi comincia a schicchiolare. Comincia a schicchiolare per questi obbiosi che vi dicevo, dovuti essenzialmente alla quantità di dati e alla quantità di dispositivi. Quindi si comincia a parlare, oltre al machine learning centralizzato, si comincia a parlare anche di machine learning distribuito e addirittura di machine learning on edge, cioè sui dispositivi low power, low energy. Vedremo cosa vuol dire declinare il machine learning in queste tre versioni, quali saranno i pro e i contro di ognuna di queste. Andando nell'ordine, partiamo dal machine learning centralizzato sul cloud, che è quello che magari avete già studiato nel vostro posto di machine learning, che come vi dicevo, è la prima versione che ci può venire in mente. Perché è la prima versione che ci può venire in mente? Perché dispositivi low power sono dedicati alla raccolta dati, produrre quindi una grande mole di informazioni che vengono centralizzate tutte on cloud, dove abbiamo a disposizione grande potenza computazionale, grande memoria, e quindi è naturale eseguire algoritmi di apprendimento all'interno proprio del mondo cloud in maniera sicuramente performante. E oggi sappiamo che il machine learning è andato pervasivo, mi dicevo, ma sicuramente di moda, ma non è semplicemente una moda, sicuramente è uno strumento molto potente che è in grado anche di risoluzionare la nostra vita. Avrete visto che negli ultimi anni i dispositivi tipo di smartphone hanno raggiunto una capacità di elaborazione non banale, tant'è che ormai è diffusissimo, ad esempio, lo sblocco del telefono attraverso il riconoscimento del volto. E riconoscere il nostro volto si basa sul politico di intelligenza artificiale. Così come il riconoscimento delle impronte digitali, oppure la classificazione delle immagini, all'attitudine sperimentato, come sono poste in fotografia scattate con lo smartphone, nella libreria, ad esempio, di Google Photo, vengono classificate in maniera automatica e in maniera anche, in qualche caso, molto precisa, in grado di separare le immagini che contengono dei volti, in grado di separare le immagini che contengono dei documenti, il testo, il cibo, in maniera totalmente automatica. Questo grazie a algoritmi di machine learning. Quindi, se vogliamo dire, i campi in cui il machine learning ha avuto e sta producendo risultati migliori sono sicuramente nel riconoscimento dell'immagine, nel riconoscimento della voce, del parlato. Gli assistenti vocali, ormai, sono una cosa diffusissima. se parliamo di Amazon Alexa, Google Home, ormai sono diffusi sistemi pervasivi. Anche uno smartphone è in grado di funzionare attraverso gli assistenti vocali. E questo, fino a una decina di anni fa, era un dolore, quindi era quasi impensabile. oggi, perché questi sistemi sono diventati così pervasivi e così ben funzionanti. Gli algoritmi di machine learning erano già noti vent'anni fa, trent'anni fa, non è che sono stati inventati nell'ultimo decennio. Cos'è che è cambiato nell'ultimo decennio? È cambiato la disponibilità di dati per fare training, quindi la qualità di dati smisurata che vi dicevo prima, e sicuramente la potenza di calcolo è un claudio, perché addestrare una rete come quella che troviamo all'interno di Alexa, Google Home, non è assolutamente una cosa banale, sia dal punto di vista della quantità di memoria disponibile, di cicli, di CPU, ma anche di energia. Anche se pensiamo solo dal punto di vista energetico, addestrare una rete come questa, come quelle che vi ho citato, è una cosa davvero dispendiosa. Fortunatamente, una volta addestrata, queste reti possono essere utilizzate poi in un numero di dispositivi totalmente indipendenti, quindi scalabili. Ma, tornando a noi, questi sono i campi, diciamo, dove l'evoluzione è forse stata la più rapida e la più evidente, no? Perché ha già modificato il nostro stile di vita, ma non sono gli unici campi, come potete immaginare, in cui il machine learning è stato declinato, no? nel campo dell'economia, di previsione, nel campo della diagnostica medica, pensate a tutti i sistemi di analisi, delle radiografie, delle immagini digitali, delle analisi, diciamo così, grazie ad algoritmi, il machine learning aiutano i medici forzino a trovare eventuali patologie e tali problemi. Abbiamo già parlato della data analytics, della robotica, eccetera. Quindi, oggi, effettivamente, il machine learning lo vediamo ovunque. E, giusto così, per fare un brevissimo ripasso, cosa vuol dire fare machine learning? Qual è la differenza fra un approccio standard della programmazione e un approccio basato su machine learning? Essenzialmente sono i dati, no? Perché un approccio standard il ricercatore, dopo aver fatto un'analisi diciamo così degli input, definisce un modello statico, come vedete qua, se fossimo nel caso del riconoscimento dei gesti di una persona tramite accelerometro, ecco, il ricercatore, dopo aver analizzato le tracce degli accelerometri XYZ durante il comportamento di una persona, andrà con la scrittura di un programma statico dove definisce delle soglie dei sistemi di processing a cercare di distinguere se la persona sta camminando, sta salendo le scale, è seduta, in piedi ferma o è stesa e questa andando a definire dell'equazione, dei modelli, delle formule. Questo è un approccio standard, che è un approccio sicuramente funzionale, ma che è un approccio che, diciamo così, lascia la capacità di funzionare essenzialmente al ricercatore, per colui che è in grado di interpretare questi input, è in grado di definire un modello e quindi produce un modello più o meno funzionale. L'approccio machine learning invece ribalta dal punto di vista perché la grande mole di dati viene usata per definire un modello, come sapete, in grado di essere addestrato, quindi in grado di modificare per se stesso un modello standard, generale, che attraverso un numero di interazioni notevoli porterà a definire dei valori, i parametri interni al modello tali che il modello possa, nel migliore dei modi, riconoscere appunto queste attività. Quindi l'idea è proprio che il machine learning usi tanti dati, tante interazioni per produrre che cosa? Per produrre un programma, un modello che poi viene eseguito come programma che servirà per fare qualcosa, per eseguire un task e normalmente i modelli di machine learning vengono usati o nella previsione o nella classificazione. Quindi in questo caso l'esempio che vi ho fatto vedere è proprio un esempio di classificazione delle attività. Si vuole capire partendo da valori di accelerometri e giroscopi si vuole capire se la persona sta seduta, sta camminando, sta correndo, eccetera. E questi sistemi sono oggi ormai una realtà talmente funzionante che tutti noi ad esempio abbiamo sullo smartwatch o sul telefono un'applicazione tipo Google Fit o tante altre applicazioni che durante la giornata tracciano i movimenti dello smartphone e vi dicono hai camminato per tot minuti hai fatto tot passi hai corso sei andato in bici sei andato in auto addirittura ci sono modelli che cercano di capire l'evolversi del vostro sonno durante la notte vi dicono quanti minuti avete passato il sonno leggero il sonno profondo semplicemente analizzando tutti questi sistemi la cosa il workflow classico degli sistemi a machine learning quali sono? sono proprio la fase di training dove si carichano dei dati i dati possono essere o meno preprocessati per produrre quelle che si chiamano delle feature sintetiche qui la differenza essenziale è fra deep learning e diciamo machine learning standard perché il machine learning standard prevede che il ricercatore in qualche modo analizzi faccia un preprocessing dei dati per estrarre da questi delle informazioni sintetiche che serviranno poi ad addestrare il classificatore per appunto produrre un modello il deep learning addirittura cerca di bypassare questa fase di preprocessing dove vengono state parte delle feature sintetiche ma lascia direttamente al modello la capacità di estrarre informazioni significative dai dati e questo questo sistema nel machine learning supervisionato ha bisogno sia dei dati di input sia dei riscontri cioè ok ha questo dato di input che etichetta se siamo nella classificazione corrisponde quindi nella fase di training come sapete diamo sia l'input che l'output l'input e l'output che servono a creare appunto il modello con algoritmi appunto di aggiornamento dei pesi del modello che ad ogni interazione modificano i pesi per far matchare al meglio l'output con l'input bene questa è la fase di training la fase di training come sappiamo è la più complessa la più onerosa sia dal punto di vista di cicli di cpu dal punto di vista di quantità di memoria necessaria memoria RAM ad esempio necessaria per la fase di training ma fortunatamente questa fase viene fatta una volta per tutte cioè una volta addestrata la vostra rete il vostro modello poi quello che interessa è l'utilizzo di questo l'utilizzo del modello che è la frase la fase di predizione o inferenza è molto meno complessa sia a livello di numero di cicli di cpu sia a livello di quantità di memoria occupata perché il modello una volta addestrato prodotto può essere utilizzato semplicemente fornendogli una mole di dati in questo caso ridotta perché un campione noi vogliamo sapere se questo campione corrisponde a una o l'altra etichetta quindi forniamo il campione il modello se si basa sull'estrazione di feature deve naturalmente prima prevedere l'estrazione di feature da questo campione una volta estratte le feature vengono passate al nostro modello addestrato il quale ci fornirà come output una delle classi di classificazione se siamo nell'ambito di un modello di classificazione questa seconda fase è la seconda fase vi dicevo meno onerosa rispetto alla prima se vogliamo l'unico onere grande della fase di inferenza è legato alla dimensione del modello cioè se il vostro sistema di riconoscimento ha generato un modello molto ampio perché magari le classi di definizione sono tante le classi di codifica sono tante eccetera l'unico onere della fase di predizione normalmente è la dimensione del modello cioè la quantità di informazioni che il modello contiene cioè la quantità di pesi che deve essere mantenuta chiaramente in memoria per poter essere generato appunto la fase di predizione perché normalmente i dati sono appunto ridotti il modello non offre una complessità troppo elevata a livello computazionale una volta che è stato addestrato ma diciamo normalmente il problema grande è ospitare modelli di grandi dimensioni perché vi dico questo perché quando studieremo la parte di machine learning on edge potete già immaginare che fare training sull'edge non ha molto senso quindi la fase di training viene mantenuta comunque un cloud oppure distribuita dove la capacità di calcolo è comunque elevata la capacità di storage è elevatissima e quello che normalmente verrà verrà fatto è una volta ottenuto il modello addestrato spostare il solo modello nelle varie modelle questo non sarà comunque banale perché vi dicevo alcune reti alcuni classificatori hanno modelli grandissimi la rete di riconoscimento locale di alexa o di google home non ci sta in uno smartwatch non ci sta chiaramente in un dispositivo edge ma non ci sta neanche in uno smartphone quindi come funzionano questi sistemi funzionano come machine learning basato su servizi on cloud quindi in realtà quando voi parlate con google parlate con alexa vengono state delle feature le feature vengono inviate in cloud ed è il modello che esegue in cloud che ritornerà verso google home verso alexa la codifica diciamo così di quella che è la parola che avete detto cioè questo per dirvi che il modello non esegue direttamente sul vostro dispositivo se le reti sono così grandi vedremo che ci saranno poi di riduzione delle dimensioni eccetera che vengono a rispettare quindi come lavora e la machine learning quando si trova sul cloud ecco qui vediamo la solita rappresentazione del mondo IoT dove abbiamo i sensori e i dispositivi edge all'in basso abbiamo i nodi nello strato fog e poi abbiamo il cloud il cloud che come sappiamo è accentratore tutte le informazioni vanno a finire sui server in cloud ha una grande capacità di storage una grande capacità di elaborazione quello che vediamo dalle frecce rosse è che i dati si spostano quindi dalle foglie fino ad arrivare nell'accentratore cioè nel cloud in cloud cosa avviene? avviene il training si definiscono i modelli una volta definito il nostro modello che sarà un modello più o meno grande appunto dipende dalla vostra rete il modello viene utilizzato come un servizio cioè per fornire un servizio agli utenti quindi la fase di inferenza è ancora eseguita sul cloud e gli utenti possono usufruire di questi modelli preaddestrati semplicemente sottomettendo con delle API delle query ci forniscono un'immagine ad esempio su Google Photo e Google Photo ti ritorna la classificazione di quelle immagini addirittura se fate se andate su Google Cloud ci sono dei test delle API caricate una foto con un volto non solo vi dice che sia un volto ma vi dice anche possibilmente l'etnia vi dice anche se il volto sta sorridendo è triste è felice quindi capite a quale livello di classificazione arrivano questi servizi il servizio poi viene propagato sul mondo vedete la freccia verde che torna giù viene propagato sul mondo edge quindi sull'IoT i dati dal basso vanno verso il cloud e dal cloud ciò che torna verso l'edge è il servizio quindi ci sono delle API e voi fornite i vostri dati il modello viene l'inferenza viene fatta in cloud e viene tornato il risultato e risultato verso il dispositivo qui c'è proprio in questa slide ho riportato proprio l'esempio di Amazon Alexa Amazon Alexa in questo caso chiude il cerchio l'IoT perché fungerà anche da attuatore in questo esempio quindi che cosa abbiamo abbiamo l'inferenza chiede qualcosa al device il device in questo caso registra l'onda audio quindi il segnale audio il segnale audio viene preprocessato in locale assolutamente il preprocessing cosa fa? fa una fase di future extraction viene in un'estate delle siti così che si possa ottenere anche una compressione dei dati che vengono inviati verso il cloud la future extraction cerca di una fase che cerca di estrarre le informazioni significative dal dato che è stato campionato in questo caso è stato campionato la voce si estraggono feature più o meno significative e ciò che vianda verso il servizio cioè verso il server di Amazon sono queste feature questo ha un duplice vantaggio vedendo il segnale perché trasferire solo le feature riduce sicuramente l'energia di trasmissione meno dati stiamo trasferendo trasferire meno dati garantisce anche una maggiore efficienza a livello di delay della comunicazione devo trasferire meno informazioni ho una banda necessaria molto inferiore l'altra cosa interessante è anche la possibilità di ridurre informazioni più o meno sensibili che vengono trasferite verso il cloud perché una cosa è trasferire l'intera traccia audio e una cosa è trasferire solo feature numeriche che la rappresentano in maniera sintetica se ti immaginare trasferire l'intera traccia audio può diciamo così avere anche risvolti in privacy cioè tutto tutto ciò che è contenuto nella mia richiesta viene viato verso il server quindi le strassioni di feature in questo caso ci dà un difficile vantaggio riduciamo la magna riduciamo l'energia ma riduciamo anche la quantità di informazioni sensibili che inviano in cloud il cloud espone appunto un servizio in questo caso Alexa Voice Service che è un servizio di interpretazione della vostra richiesta Alexa quindi risolve il contenuto vocale della vostra traccia e a quel punto attraverso gli skills di Alexa sarà possibile attuare cioè generare un'attuazione su dispositivi smart che avete installato in casa che sono comunque connessi alla vostra rete classico esempio Alexa accendi le luci in salotto la vostra traccia vocale viene interpretata signal processing si producono le feature le feature vengono inviate al servizio di Alexa viene riconosciuta sotto forma di stringa testuale il contenuto il contenuto della vostra richiesta attraverso il motore di matching di Alexa a identificare l'azione che volete svolgere e si torna indietro con un comando questo è il mondo IoT basato sull'interdizia artificiale e cosa c'è qua dentro c'è il servizio Alexa Voice che è una rete preaddestrata della dimensione molto molto ampia la necessità l'addestramento di questa rete è avvenuto in tempi davvero molto lunghi molto complessi ma una volta addestrata ecco la rete viene sì continuamente aggiornata magari mantenuta però diciamo il grosso è stato fatto a livello client le performance e le necessità hardware non sono così elevate perché abbiamo solo la necessità di estrarre feature in tempo reale e trasmettere dati la stessa cosa succede anche per assistenti di volum che vediamo la presentazione di questo slide l'utente effettua una query anche qui viene trasferito feature sintetiche di quello che avete detto vengono invocate le API di Google anche qui abbiamo una rete preaddestrata e diciamo si torna a interagire verso l'utente in questa slide si mostra una forma di conversazione con l'assistente locale quindi la vostra traccia audio viene convertita in testo il testo serve a fare delle query sulle API di Google Google ritorna un testo che verrà poi convertito in speech con il motore di conversione speech text to speech speech per fornire una risposta vocale all'utente questo è un sistema quindi torno un attimo indietro basato proprio su il machine learning on cloud dove sia il modello che l'inferenza sono centralizzati cioè stanno tutte in cloud voi fornite solamente dati e ricevete il servizio questo è il classico sistema in cui vi dicevo lavora l'intelligence machine learning con cloud e esempi diciamo così riuscitissimi di queste architetture oltre a Google Alexa e Google Home che sono un caso particolare di assistenti vocali sono tanti ad esempio prendete esempi di ottimizzazione energetica nella gestione di grandi grandi building ecco grandi edifici cosa si fa? beh anche qui nel mondo IoT fornisce una quantità di informazioni elevatissima sullo stato della temperatura umidità quantità di gas all'interno del vostro edificio e piuttosto che mantenere un sistema basato su termostati o basato essenzialmente su orologi temporali gli algoritmi di machine learning decidono come gestire la ventilazione l'umidità l'umidificatore la temperatura dell'edificio e grazie a questi sistemi normalmente si ottiene un beneficio di riduzione di consumo energetico che sta fra il 15 al 25% quindi la gestione basata su un'ottimizzazione dell'energia in maniera predittiva è ormai qualcosa di assodato che viene usato essenzialmente per grandi edifici ecco adesso ancora in Italia non è usatissimo ma l'idea è proprio quella di non gestire il condizionamento in maniera lineare con un semplice termostaco o un sistema assodido ma dotare il vostro edificio dell'intelligenza artificiale qualche tentativo edifici anche residenziali quindi non edifici in grandi dimensioni si sta facendo pensate ai termostati come sono dei termostati che oltre a essere remoti quindi avere la possibilità di essere attivati da remoto con un app o con un server hanno al loro interno algoritmi di intelligenza artificiale dedicati soprattutto a cercare di apprendere le nostre abitudini perché grazie al sistema di apprendimento si cerca di ridurre il consumo energetico in che modo magari non attivando il sistema di riscaldamento quando si prevede che l'utente non rientrerà a casa oppure fuori quindi si basano sia su informazioni di posizionamento tipo il GPS nel vostro smartphone che comunica con Nest e gli dice guarda che adesso sono fuori casa è utile che riscaldi la casa oppure appunto con sistemi di apprendimento dove non siete voi a dire quando il riscaldamento ma dopo un po' di settimane di training Nest cerca di capire che voi rientrate a casa bene o male verso le 5 di pomeriggio e in base alla temperatura esterna eccetera avvia il riscaldamento in anticipo nel tentativo appunto di farvi trovare sempre le condizioni migliori quando entrate in casa cercando di ridurre il consumo energetico quindi quando si parla di ottimizzazione energetica predittiva abbiamo sempre che parla con l'amborismo di machine learning se vogliamo parlare quindi di questo sistema dove il machine learning sta in cloud dove il modello e l'inferenza sono tutti centralizzati quali sono i pro e i contro che identifichiamo in questa architettura allora partiamo dai pro sicuramente avere il modello l'inferenza quindi il training in cloud ha sicuramente come pro l'elevatissima disponibilità di risorse come CPU disco energia abbiamo sistemi che non sono alimentati a batteria e quindi possiamo davvero usare tutta la potenza computazionale che vogliamo soprattutto nella fase di training abbiamo la possibilità di aggiornare il modello in continuazione pensate al nostro google home lo usiamo contemporaneamente il modello la rete può continuare il suo addestramento a livello cloud prendendo tante informazioni in continuo da tutti gli utenti che usano questi assistenti vocali e quindi il modello si perfeziona migliora grazie al fatto appunto che questi sistemi continuano a ricevere dati in grande quantità ma dati soprattutto variegati non è sempre lo stesso utente che utilizza lo strumento la rete ma tanti utenti nel mondo la utilizzano quindi si ottiene sicuramente un miglioramento del modello in continuo l'altro vantaggio è che i dispositivi Edge non necessitano di grandi risorse quindi noi possiamo eseguire oggi l'assistente google su uno smartwatch perché in realtà uno smartwatch che potete immaginare non avrà una potenza di calcolo elevatissima non avrà una quantità di memoria elevatissima ma in realtà utilizza il servizio di google assistant e quindi in realtà il vostro smartwatch deve estrarre le feature comunicare queste feature ad aspettarsi un valore di ritorno quindi in realtà i sistemi Edge in questa architettura necessitano di poca CPU poco disco e quindi anche poca energia computazionale l'ultimo vantaggio che possiamo riconoscere è una grande quantità di dati come lo dicevo prima utile per il training quindi tutti questi dispositivi inviano dati al sistema centralizzato il sistema centralizzato può godere di questa quantità smisurata di informazioni con la quale migliorare in continuazione i suoi modelli che sono gli svantaggi di questa architettura li abbiamo già visti un po' i sistemi Edge devono necessitare di una grande potenza di trasmissione questa volta una grande banda una grande energia perché trasferire in tempo reale da Amazon verso il server di Amazon serve tanta energia serve tanta banda anche qui gli assistenti vocali hanno preso diciamo un ripiede quando nelle case di tutti noi ormai è arrivata una commissione a banda larga ormai la fibra è qua nelle case di tutti e allora è possibile utilizzare un assistente vocale che in tempo reale riesca ad inviare le feature estratte verso il cloud il cloud chiaramente è in tempi brevissimi riesce ad eseguire l'inferenza vista la grande potenza che ha ma torna informazioni di nuovo verso il dispositivo quindi se avessimo una banda come se lo so 10-15 anni fa probabilmente noi avremmo acquistato Google Home Amazon Alexa perché fra il momento in cui fate la domanda e il momento in cui ricevete la risposta o l'attuazione sicuramente sarebbero passati diversi secondi che avrebbero reso poco attraente il servizio oggi invece si fa quasi tempo reale grazie anche alla banda ma questo è sicuramente un contro perché se potessimo eseguire l'inferenza direttamente sul dispositivo non dovremmo neanche trasferire nulla quindi i tempi sarebbero sicuramente brevissimi in tempo reale l'inferenza potrebbe avvenire non dovremmo trasferire una grande quantità di dati non dovremmo consumare una grande energia di trasmissione è chiaro che la coperta è un po' corta perché se io eseguo l'inferenza sul dispositivo è vero che riduco l'energia e la banda di trasmissione ma aumento l'energia computazionale perché mi serve una buona CPU per eseguire il mio modello oltre alla buona CPU mi serve anche una grande quantità di memoria RAM che come sappiamo consuma comunque tanta energia ecco perché vi dico la coperta è corta vedremo che poi non sempre sarà intanto non sempre sarà possibile spostare il modello sull'edge ma non sempre sarà anche vantaggioso dipende appunto la dimensione del modello e le risorse necessarie comunque ecco quello che qua diventa un pro cioè la necessità la non necessità di avere tanta CPU di memoria poi porta la necessità invece di avere tanta banda e quindi lo segniamo come un conto di questa architettura sicuramente spostare tutti i dati e le nostre feature dal dispositivo verso il cloud centralizzato può porre dei problemi di privacy dei problemi di sicurezza come vi dicevo tempo fa anche nel tentativo di riconoscere semplicemente un gesto se stiamo camminando correndo eccetera se trasferiamo tutte le tracce del nostro accelerometro verso il cloud in realtà è possibile con opportuni algoritmi di machine learning addirittura capire da quelle tracce se muoversi è una femmina piuttosto che un maschio stimare l'età della persona che si muove con un range di più o meno 5 anni quindi capite queste informazioni che a noi sembrano magari poco interessanti e che le forniamo tranquillamente al cloud per farci dire quanti passi abbiamo fatto durante la giornata in realtà il server centralizzato può anche sapere se fare quei passi che uno o che è una donna e stimare l'età capite che sono informazioni non banali a livello di privacy e quindi spedire tutte queste informazioni verso il cloud produce sicuramente un problema di questo genere diversamente torno a dire se potessimo fare l'inferenza sempre sul dispositivo non trasferiremmo nulla verso il cloud i nostri dati rimarrete sul dispositivo e quindi sicuramente avremo meno problemi in privacy un altro contro qual è? che i risultati dell'inferenza come abbiamo visto qua in cima dov'è che siamo i risultati dell'inferenza poi si propagano dal cloud verso verso il legge e quindi in questo caso non abbiamo solo i dati sensibili chiamati verso il cloud ma anche l'inferenza dei nostri dati che viaggia e torna verso di noi e quindi anche questa possibilmente potrebbe essere intercettata violare in qualche modo la nostra coronavirus ripeto di nuovo se facessimo l'inferenza ok se facessimo l'inferenza direttamente sul sul dispositivo questo problema invece non ce l'avremmo perché anche il risultato rimarrebbe vincolato all'interno del dispositivo stesso questo per chiudere prima di parlare di machine learning distribuito per chiudere quella che è il machine learning tradizionale quello che è il più diffuso in realtà quello che ha portato alla discussione e all'utilizzo di strumenti così evoluti che ci hanno davvero cambiato la vita con i nostri sistemi locali in un momento di riconoscimento appunto dei nostri comportamenti delle immagini eccetera cominciano a scricchiolare questi sistemi cominciano a scricchiolare come l'ho detto all'inizio sia per grande mole di dati grandi mole di dispositivi che ripienano servizi che inviano dati che quindi cominciano a limitare la capacità di eseguire anche inferenza in tempo reale sul cloud e quindi nasce quello che viene è chiamato il machine learning distribuito e poi il machine learning sull'edge adesso facciamo dieci minuti di pausa e poi riprendiamo a parlare appunto di machine learning distribuito vado a bloccare mettere in pausa la registrazione se avete qualche domanda non ho visto quanti sono diventati i partecipanti ah c'è Lorenzo e le valide che sto cercando di bloccarti di differenza allora riprendiamo si è bloccato questo si si no si è bloccato le slide un po' più avanti ecco ok quindi abbiamo visto machine learning on cloud e ora cominciamo a parlare invece di machine learning distribuito per far fronte appunto alla necessità di ridurre un pochino il carico on cloud che vi dicevo sta diventando davvero elevatissimo l'idea qual è allora l'idea è che i modelli come vedete da questa slide non siano più semplicemente centralizzati ma i modelli possono essere allocati non si parla di modelli su legge ma si parla di modelli su fog quindi su macchine che ancora hanno delle caratteristiche prestazionali elevate hanno la caratteristica energetica di essere sempre connessi ma hanno anche la caratteristica di essere più vicini e distribuiti più vicini agli legge più distribuiti allora come funziona il sistema il sistema funziona che ancora legge produce dati si raccoglie e li invia diciamo a dei nodi intermedi allora i dati in questo caso non arrivano più in cloud perché già nel nodo intermedio c'è un creazione di un modello che può in realtà essere un cosiddetto sottomodello nel senso che non è un modello generale ma è un modello ridotto questi modelli si trovano appunto su quindi chi deve chiamare normalmente worker cioè sono dei nodi che gestiscono la fase di training distribuita e sono loro che poi inoltrano al cloud modelli o solo i pesi dei modelli ottenuti e il cloud fa un'aggregazione di questi perché il vero modello generale rimane sul cloud ma l'addestramento avviene per pezzi su questi worker distribuiti questo comporta un grande vantaggio perché intanto i dati non arrivano mai in cloud i dati si fermano ai worker vedete in questo caso le frecce rosse che portano i dati non arrivano fino in cima questo comporta sicuramente la riduzione del problema della sicurezza che avevamo prima perché i dati di alcuni sensori arrivano solo a un worker arrivano a tutti gli altri quindi è più difficile avere una violazione della privacy così distribuita il modello si viene a comporre su worker in parallelo alleggerendo sicuramente il carico del cloud dove in quel momento il cloud funziona quindi da aggregatore dei modelli aggregatore delle informazioni di addestramento per produrre poi il modello generale la fase di inferenza la fase di inferenza avviene ancora come prima a livello di cloud ma finita la fase di addestramento in realtà il modello generale può essere distribuito sui worker è come se fossero dei proxy questi worker quindi a quel punto io non faccio l'inferenza direttamente sul cloud ma nel mio worker più vicino quindi a quel punto anche i delay e le performance scalano in maniera molto migliore come si può avere quali sono le tipologie di machine learning distribuito che oggi sono le più distrute allora il training del modello avviene in maniera distribuita può avvenire in tre modi diversi o col cosiddetto parallelismo dei dati o col parallelismo dei modelli oppure insieme parallelismo sia di dati che di modelli sono tre architetture diverse che ci permettono di addestrare modelli distribuiti e quindi torno indietro avere modelli locali sui worker normalmente nello stratoport o utile senza dover arrivare direttamente al cloud computing dove qua sopra invece abbiamo l'aggregatore ora vediamo il primo il parallelismo dei dati allora il cloud che è questo nodo grande qua chiamato anche job manager il job manager cosa avrà avrà alla fine il modello generale qui è una rete convoluzionale profonda è un esempio e avrà l'informazione del dataset ma cosa si fa con il dataset in realtà il dataset rimane vedete nel worker 1 c'è solo un sotto dataset non c'è tutto il dataset completo che addestra il modello completo quindi il modello in questo caso è completo su tutti i worker ma ciò che non è completo sono i dati perché un worker vede solo alcuni dati e l'altro worker ne vede altri è chiaro che quello che producono è un addestramento parziale della rete perché la rete è stata addestrata solo su sotto insieme di dati come se noi fossimo ottono indietro un attimo come se noi fossimo qua questo worker centrale si addestra solo sui dati provenienti da questi dispositivi quest'altro worker solo dai dati provenienti da questi altri dispositivi nessuno vede tutti i dati completi è chiaro che questo genera un addestramento parziale del modello quindi il modello che si viene a generare sul worker 2 e il modello che si viene a generare sul worker 1 sono stati addestrati solo su un sotto insieme cosa succede? succede che poi questi modelli vengono inoltrati al job manager e il job manager li deve aggregare per ottenere poi il modello come se questo fosse stato addestrato direttamente sull'intero training set cioè su tutti i dati ma in realtà i dati lui non li ha mai visti perché lui ha sfruttato l'addestramento sui vari worker questo ci dice parallelismo a livello di dati perché appunto sono i dati ad essere spezzati e parallelizzati una certa parte di dati va sul worker 1 una certa parte sul worker 2 eccetera e nell'architettura IoT questo viene da sé perché se il worker 1 è appunto un modo fog che connette 100 dispositivi IoT lui riceve i dati sull'intero il worker 2 gli altri 100 dispositivi IoT e così dicendo i dati si fermano come vi dicevo prima a livello dei worker quali sono le caratteristiche fondamentali dell'addestramento distribuito col parallelismo dei dati l'implementazione è molto semplice perché l'implementazione è molto semplice perché in realtà in ogni worker c'è il modello completo solo che è addestrato parzialmente quindi l'implementazione vuol dire semplicemente replicare il sistema di training e addestramento sui vari worker e dato che le piattaforme più utilizzate ad esempio usano container come docker o macchine virtuali in realtà nel worker viene passato direttamente il docker la macchina virtuale che è completa e quello che succede è che l'addestramento avviene con dei dati parziali quindi l'implementazione è molto semplice perché è una replica dell'implementazione globale ha una grande tolleranza agli errori certo perché il mio modello è replicato in tanti modi se anche un modello si addestrasse male pesera poco nel modello generale finale appunto perché viene mediato con tutti gli altri ha una forte capacità di utilizzo delle risorse prestializzate basta distribuire i miei docker su tanti worker che io ho a disposizione se ho a disposizione tanti worker li distribuisco questi ricevono i dati e addestrano il vantaggio che abbiamo già detto i dati sensibili non arrivano mai in cima alla piramide non arrivano mai al cloud ma rimangono localizzati all'interno dei worker e quindi abbiamo anche una riduzione della trasmissione dei dati se prima i dati passavano dal nodo foglia al fogo per arrivare in cima ora si fermano tutti i fogo quindi non c'è il trasferimento dei dati dallo strato intermedio fino allo strato in alto quindi abbiamo anche minor costi energetici quali sono i contro? che i modelli essendo totali perché abbiamo detto che il modello è completo all'interno di ogni worker può essere un modello molto molto grande siamo di nuovo alla rete di Amazon Alexa o Google Home sono reti grandissime modelli grandissimi con il parallelismo di dati questi devono essere allocati tutti noi quindi occupano sicuramente una grande quantità di memoria la scalabilità è ridotta perché vedremo aggiornare poi quello che viene fatto è che i modelli poi devono aggiornarsi io ho fatto un modello parziale qua un modello parziale là devo poi inviare questi dati in cima all'aggregatore e cambia notevolmente se i modelli sono 10 100 o 1000 il lavoro che deve fare l'aggregatore quindi la scalabilità non è all'ultima è chiaro che i nodi fog i worker devono avere grande quantità di memoria e grande CPU e poi c'è appunto questo problema di sincronizzare in questo caso i pesi dei modelli perché il worker 1 ha parzialmente addestrato il modello globale conoscendo dei pesi questi pesi devono essere mandati all'aggregatore ognuno di questi ogni worker fa la stessa cosa non solo consideriamo che l'aggregatore a un certo punto modifica leggermente il modello deve ridistribuire il modello globale quindi di nuovo è un problema di traffico generato e scalabilità qual è l'alternativa l'alternativa è la strategia di parallelizzare il modello questo è indietro quello questo vuol dire parallelizzare il modello come vediamo in questa slide il worker 1 ha un sottomodello il worker 2 ha un altro sottomodello che composti insieme danno proprio il modello generale quindi in realtà quando parliamo di parallelismo dei modelli quello che andiamo a dire è che i modelli sono spazzettati ognuno ha una piccola parte del modello a destra solo quella piccola parte e poi i modelli vengono ricomposti in questo caso però se parliamo di parallelismo di modelli puro ogni modello deve vedere tutti i dati e questo è un contro perché è vero che sono modelli piccoli quindi i miei nodi hanno potenza computazionale ridotta memoria ridotta ma devono vedere tutti i dati e quindi abbiamo di nuovo una grande circolazione di dati non circola il modello il modello è più ridotto ma circolano tanti dati e quindi siamo tornati un po' nell'esempio centralizzato dove i dati devono circolare per forza arrivare in cima qui se vogliamo la circolazione di dati è ancora maggiore perché i dati devono raggiungere tutti i worker mentre prima i dati andavano dal basso verso l'alto ora i dati andavano dal basso verso l'alto tornano indietro per andare a addestrare tutti i sottomodelli in tutti i worker quindi è vero risolvo un po' il problema di avere grandi modelli perché i modelli sono ridotti il problema di aggiornare tutti i pesi dei modelli perché alla fine uno addestra completamente il proprio sottomodello e lo spedisce addestra il sottomodello e lo spedisce è vero ma abbiamo una grande circolazione di dati quindi se vogliamo nell'uno nell'altro nel primo nel secondo sono effettivamente le soluzioni ottime la soluzione ottima vedremo che viene usata in realtà è al solito un ibrido cioè si spostano sia i modelli che i dati ma non completi quindi abbiamo modelli non completi dati non completi e quindi la fusione dei due diciamo quella che viene utilizzata ho saltato questi dispositivi di assidità e poi ti viene adesso li vado a far vedere perché abbiamo detto prima l'ultima riga i parametri devono essere sincronizzati ecco sincronizzare i parametri non è una cosa banale perché quello che si fa davvero è fare l'everaging di tutti i parametri di ogni worker quindi in realtà non è semplicemente un invio dei parametri dal worker 1 al cloud ma il worker 1 invia il cloud e il cloud rinvia i suoi parametri a tutti gli altri worker e questi man mano fanno delle medie quindi fanno ognuno deve fare la media di tutti i parametri di tutti gli altri worker non viaggiano i dati tra i worker ma viaggiano un sacco di parametri e gestire la sincronizzazione dei parametri non sarà per nulla semplice così come non sarà per nulla semplice nella strategia di trading avere un gradiente distribuito ricordiamoci il modello è sempre quello voi avete sicuramente studiato di algoritmi di gradiente per aggiornare i modelli ebbene quindi quando aggiorno i modelli condivido tutti i pesi fino a quel momento che sono stati fatti aggiorno il gradiente su tutti i worker e poi ricondivido di nuovo tutti i pesi ci genera davvero una quantità smisurata di informazioni che non sono dati ma sono appunto parametri che vengono sincronizzati fra uno e l'altro vedremo che ci sono delle strategie che hanno in qualche modo un pochino ridotto questo problema di trasmissione ma sono ancora non banali mentre il parallelismo di modelli come vi dicevo fa circolare i dati quindi sono due o tre o circolano i pesi e si sincronizzano o circolano i dati l'idea qual è se si mettono insieme entrambe le caratteristiche si cerca di far circolare meno dati possibile ma anche meno pesi possibile unendo le due cose lo svantaggio di quell'unione qual è la complessità gestionale perché abbiamo visto con il parallelismo dei dati era molto semplice bastava replicare dei container con le macchine virtuali e ognuno era identico all'altro l'unica differenza l'unico problema stava che ogni container poi doveva aggiornare i pesi inviandoli al cloud il quale poi li rimandava indietro in questo caso la complessità invece è maggiore perché andare a gestire sottomodelli all'interno delle installazioni produce installazioni non identiche quindi non posso semplicemente distribuire i miei container ma ci devo mettere le mie e quali sono i pro abbiamo detto che sicuramente ogni worker lavora sul modello ridotto e quindi ha bisogno di meno memoria meno CPU scala molto meglio quindi aggiungendo un numero di worker elevato diciamo così riducono ulteriormente i sottomodelli quindi migliora da quel punto di vista i contro sono di nuovo una grande circolazione di dati invece tra i nodi di addestramento quindi alta necessità di inviare informazioni alta energia per il trasferimento delle informazioni l'implementazione è più complessa rispetto a prima non basta applicare un container come vi dicevo e qua non devo sincronizzare i dati ma devo sincronizzare i modelli che come vi dicevo non sarà una cosa banale è vero che la sincronizzazione qui non è ricaduta nel senso che i modelli vengono tutti inviati al nodo principale che è lo scheduler dei lavori ma diciamo così è comunque non banale fare questa sincronizzazione nel primo paradigma si sincronizzano i parametri perché i modelli sono identici per tutti in questo caso invece si sincronizzano gli stessi modelli quindi a circolare sono effettivamente i modelli cosa c'è oggi di funzionale davvero implementabile nel machine learning distribuito cioè la domanda è se uno volesse fare davvero machine learning distribuito la tecnologia i framework a disposizione cominciano a essere maturi e cominciano a essersene diversi quindi non è solo un'esplorazione teorica ma effettivamente esistono esistono come vi dicevo diverse macchine virtuali diversi framework che ci permettono di fare essenzialmente training distribuito quindi se uno volesse oggi lavorare davvero in questo modo adesso vi faccio vedere ve lo dico un attimo in fondo quali sono sono per un flow sicuramente l'avete usato nel vostro esame di machine learning Keras e PyTorch che sono davvero dei sistemi che ammettono l'addestramento distribuito li potete usare già da oggi quindi in realtà sono già presenti sul mercato queste piattaforme che permettono di lavorare in maniera distribuita e ammettono diverse strategie di sincronizzazione ma oggi finiamo andando a vedere quelle che sono le sincronie le strategie di sincronizzazione soprattutto a livello di parallelismo dei dati le architetture che vi ho fatto vedere quindi Keras e PyTorch sono essenzialmente dei framework di addestramento distribuito con un parallelismo dei dati non tanto con un parallelismo dei modelli e abbiamo detto che il parallelismo dei dati genera questo problema di aggiornare i parametri su tutte le macchine ok io quindi aggiorno definiti i parametri del mio modello locale su un soppre insieme del training set e invio i miei dati i miei parametri scusate i parametri devono essere in qualche modo sincronizzati su tutti i worker e la strategia iniziale che è stata usata è quella di definire un nodo che viene chiamato server dei parametri quindi cosa succede? succede che abbiamo dei nodi del mio fog che sono i worker cioè quelli che addestrano e uno o più nodi che fungono da server dei parametri cosa vuol dire? vuol dire che il mio worker addestra con un training set parziale e produce dei parametri di addestramento benissimo questi parametri li invia al server dei parametri il quale a sua volta li recapita tutti gli altri quindi è un po' il gestore dei parametri questo vuol dire che i miei server di parametri hanno il compito di aggiornare questi parametri di addestramento su tutti i worker questo sistema fa sì che i nostri worker non siano totalmente indipendenti nell'addestramento perché ci sono queste temporizzazioni dei server dei parametri quindi facciamo fin perché ci sono un worker molto lento e purtroppo gli altri worker devono aspettare che il worker molto lento forniscia i suoi parametri per poter fare una sincronizzazione globale e quindi non è il massimo questo sistema perché si viaggia alla velocità del più lento questa è l'idea è vero che io distribuisco il carico di lavoro ma viaggio alla velocità del più lento e ogni volta che faccio appunto ogni aggiornamento ogni macchina poi deve riaggiornare i sottoparazzi questo sistema abbiamo detto che scala anche non molto bene cioè nel momento in cui aggiungo un numero elevato di worker in realtà vado a complicare notevolmente l'aggiornamento dei parametri allora l'alternativa qual è? è una strategia molto recente che è stata definita che si chiama all reduce e questa strategia fa sì che i parametri non siano condivisi come prima in maniera totale ma i parametri sono condivisi solo su alcune macchine che possono essere in un'architettura ad anello oppure in un'architettura ad albero cosa vuol dire? le macchine vengono mette in un'architettura ad anello la macchina in testa condivide i parametri di testamento solo con la successiva la successiva fa il suo aggiornamento che poi la condivide con la successiva ancora in un anello fino a tornare in cibo questo intanto fa sì che non ci sia più l'attesa per il più lento perché intanto ognuno lavora e c'è questa fase di sincronizzazione a cascata che si segue l'un dall'altro non si generano più invii di parametri verso l'unico serve che poi viene capita ma c'è uno step by step e quindi questo ha fatto sì che il miglioramento sia effettivamente non banale le architetture di solito come Keras e TensorFlow vi danno la possibilità di scegliere se volete un sistema di sincronizzazione dei parametri tipo il reduce con il ring o con la struttura d'albero ma diciamo che fra l'uno e l'altro non è che ci sia una grande differenza a livello prestazionale e l'ultima cosa ecco che abbiamo detto questo approccio è molto più scalabile rispetto al precedente perché aggiungo un nodo nell'anello non comporta una definizione come tutti i parametri come diciamo prima perché i parametri vengono aggiornati a volte cioè il primo con il secondo con il secondo con il terzo e via dicendo quindi è un sistema un pochino più intelligente per cercare di risolvere il nostro problema di sincronizzazione dei parametri nel parallelismo dei dati bene questo se ecco per vostra curiosità se vi interessa dare un'occhiata vedrete che sia TensorFlow che Keras che magari avete conosciuto nel corso di machine learning offrono comunque la possibilità di fare addestramento dei parametri e se addestramento dei modelli con parallelismo dei dati quindi lo potete sperimentare tranquillamente anche la piattaforma Azure mi sembra di Microsoft fornisce degli algoritmi di addestramento dei modelli di machine learning che vanno nell'optica di fare aggiornamento di modelli con parallelismo dei dati mi piacerebbe provare a fare un'esercitazione anche su questi aspetti qua ormai il corso è finito per riuscire a fare magari qualcosa di seminario più avanti tanto magari se siete interessati lo potete seguire perché intanto fare un'esercitazione su machine learning distribuito machine learning su edge ci vuole parecchio tempo anche non è una cosa da fare in due ore diciamo così però mi piacereva mi piaceva metterla su magari la facciamo come seminario perché soprattutto nella prossima lezione dove vedremo machine learning su edge vedremo che TensorFlow ad esempio ha già definito un sistema per addestrare un modello su cloud e poi calarlo su dispositivo edge per fare l'inferenza direttamente sul dispositivo con tutti i sistemi diciamo così di riduzione della dimensione del modello perché come vi ho detto il problema fondamentale sarà la dimensione del modello da farla mezzare all'interno di un dispositivo edge però insomma i maggiori framework ci stanno già lavorando ormai sta cominciando a diventare una realtà certo allora più che affidabile si può si è notato che si perde a livello di parità di complessità del modello numero di strati eccetera addestrando in maniera distribuita quando fai il parallelismo dei modelli si perde un po' di efficacia nel senso che ad esempio se parliamo di un classificatore l'accuratezza di classificazione è un pochino più bassa quindi per compensare cosa devi fare di creare un modello più grande ma se tu hai una rete si si è accettabile perché lo puoi compensare ma se tu hai una rete con tre strati e 109 a strato se lo fai in parallelismo dei modelli per avere le stesse performance invece che 109 a strato devi che 109 cioè devi compensare un pochino in questo modo però alla fine ci sta nel senso che tu hai comunque un sistema distribuito per l'attentramento che ti risolve le performance esatto assolutamente però puoi compensare incrementando un pochino più il modello quindi alla fine si riescono a ottenere le stesse performance con modelli un po' più grandi ma tanto li hai spezzati quindi alla fine diciamo non è che ci preoccupa più di tanto e ormai si va in quella direzione tant'è che vi dicevo se avete la possibilità di avere cloud di avere non distribuiti quindi potete far migrare che li lavorano con dei container far migrare tanti container con modelli e qui siamo nel pari con dei dati il modello è completo ma lavorano già a questo modo perché come vi dicevo sta cominciando a diventare un problema il trading centralizzato perché c'è tanta roba e anche l'interferenza comincia a essere un problema perché l'inferenza scusate la fase di inferenza perché appunto il modello è grandissimo tutti gli utenti chiedono lo stesso servizio e questi server sono davvero grandi cioè quando cominciano ad avere dei problemi allora la fase di inferenza non abbiamo ancora parlato ma la fase di inferenza è più efficiente nel momento in cui tu hai addestrato il modello distribuire l'inferenza è abbastanza ovvio nel momento che tu distribuisci il modello addestrato con modi parziali invece di chiedere a lui io chiedo a te lui chiede a lui il modello lo stesso immaginare l'inferenza la distribuzione dell'inferenza è più semplice sì ma ti dicevo il tensors flow è già mangiato in questa direzione il problema è che modelli si lo semplifichi ma la rete di Alexa e di Google è troppo grande però allora cosa fai prendi reti più piccole che magari ti danno una performance di accuratezza minore però la la la purtroppo lì è un 3DOS che se vuoi una performance eh sì se vuoi una accuratezza elevatissima non ci sta il modello non ci sta neanche nello smartphone con 8 giga di RAM non ci sta non so se avete fatto qualche esempio di rete reale deep learning che viene utilizzata appunto per il crescimento della voce ma se voi googleate un po' scrivete l'architettura della rete che riconosce appunto la voce e google google google e google e google e google e google e google e google e google c'era due trecento strati ogni strato c'era due trecento neuroni cioè è una cosa esatto è una cosa tossica e esatto quando reti funzionano ma sono grandissime quindi ancora calare quella rete lì con dispositivo non è possibile però per aspetti più semplici ad esempio di riconoscimento se stai camminando correndo quelle sono reti modeste che ormai è facile fare l'inferenza direttamente sul dispositivo adesso non dico più un contapassi da 12 euro ma in uno smartwatch ci sta una rete di quel tipo fortunamente ridotta perché poi ci sono tutte le politiche di riduzione dei pesi di dimensionamento però quelle ci stanno reti più grandissime come quelle che abbiamo citato ancora non ci stanno a dire anche perché occupano però esatto poi se se pensate quanto funziona va bene cioè se l'avete se l'avete se l'avete se l'hai in casa Google ho visto a volte sbaglia per carità è tanta roba cioè è una cosa è una cosa fantastica è davvero io io ho iniziato a lavorare ad esempio la certificazione delle immagini ma 15 anni fa ancora si parlava all'inizio di intelligenza artificiale così i primi algoritmi già ti dicevano ok c'è un taxi perché c'era il giallo dentro adesso a faccia cioè adesso ti riconosco cioè provate c'è proprio l'API di Google potete testare semplicemente facendo l'applode dell'immagine sul sito e vi ritorna in tempo reale tutti i tag tutte le cose che ci sono è fantastico sì esatto è una cosa in dieci anni io dico forse ho iniziato a lavorare cioè si passava da dire qui c'è del giallo e quindi c'è un taxi ma c'era una banana in primo piano quasi che lo beccava lo stesso a fare delle cose assurde altrimenti come dicevo prima per grazie alla disponibilità di dati che prima era così elevata e alla disponibilità di banda per trasferire e calcolo potenziali perché gli algoritmi erano già noti non si riusciva ad usarle così discusamente bene allora facciamo un terrompe la registrazione sì