Oggi facciamo quest'ultima lezione su machine learning nell'IoT sul dispositivo ENS, un po' quello che abbiamo visto nell'ultima esercitazione, solo che oggi lo affrontiamo più dal punto di vista teorico. Cerchiamo di capire quali sono le implicazioni nel trasportare machine learning sui dispositivi low-packer e low-performance, quindi poca memoria e troppo di prestazioni computazionali. Questa lezione proprio conclude il capitolo del machine learning e conclude il corso di di ottenere IoT. Abbiamo già visto l'altra volta cosa vuol dire fare machine learning nel mondo IoT. Avevamo visto che il machine learning nel mondo IoT nasce come sistema centralizzato, dove i sistemi IoT servivano essenzialmente a fornire una grande mole di dati al cloud. All'interno del cloud veniva fatto training dei modelli, i modelli venivano definiti e poi il modello generato veniva utilizzato per l'inferenza fornito come servizio. Quindi chiunque collegandosi al cloud da qualsiasi dispositivo poteva soffrire dell'inferenza di questi modelli così definiti. Questo è un po' ciò che avviene sui servizi, ad esempio di riconoscimento vocale, eccetera. Poi abbiamo visto la variante machine learning distribuita, dove invece il modello o i dati venivano ad essere sparati e spezzati sui vari attori per poi ricomporsi. Oggi invece vediamo proprio il concetto di machine learning sul nodo edge, quindi sulla foglia. Il presupposto di poter fare eseguire un modello sul nodo foglia che in realtà il modello venga solo utilizzato nel nodo foglia, cioè all'edge si vada a fare quella che è l'inferenza. La fase di training rimane circoscritta a livello cloud, perché come sappiamo la fase di training è la fase più onerosa, più pesante dal punto di vista computazionale e quindi non avrebbe senso portarla all'interno del modello, all'interno del dispositivo edge. Ma una volta creato il modello sul cloud, come vedete, questo vuol dire che comunque il cloud avrà bisogno di una fase di raccolta dati per poter effettuare il training. Quindi diciamo ci sarà comunque una prima fase, come chiamiamo una fase di protezione e implementazione, dove i dati arrivano al cloud per poter generare il modello e quindi addestrarlo, ma una volta che il modello è stato addestrato questo viene distribuito sui dispositivi. e ogni dispositivo quindi sarà in grado di contenere una copia del modello addestrato, che essenzialmente dovrà anche essere in qualche modo compressa per poter soddisfare le esigenze di capacità di calcolo e capacità di storage dei dispositivi. Ma poi la cosa interessante è che il modello lo esegue sul dispositivo, quindi la fase di inferenza viene eseguita da ogni dispositivo e i dati che vengono utilizzati per fare l'inferenza rimangono circoscritti al dispositivo. Quindi non c'è mai più il passaggio di dati dal dispositivo verso il cloud per poter fare l'inferenza. Questo vuol dire che tolta la prima fase di progettazione e creazione del modello, che naturalmente avrà bisogno di una fase di raccolta dati, poi i dati rimangono circoscritti sul dispositivo. Questo risolve tutti quei problemi che avevamo visto invece nei sistemi precedenti legati alla privacy, perché chiaramente il data non si sposta più verso il cloud, problemi di consumo energetico dovuti alla trasmissione, non ne abbiamo più, e problemi di latenza, perché è chiaro quando usavamo l'inferenza come servizio dal dispositivo i dati dovevano arrivare in cloud, il cloud processava e non si tornava i risultati. Tutto questo generava una catena di latenza non banale. Ora l'unica latenza che avremo sul dispositivo non sarà più una latenza di trasmissione, ma sarà la latenza di inferenza, cioè dovuta alle capacità computazionali del dispositivo che potranno eseguire il modello e generare appunto il risultato. Quindi i pro quali sono? Non dobbiamo più trasmettere dati con tutto ciò che ne deriva. L'inferenza, quindi i risultati dell'inferenza anche questi non si spostano, rimangono nel dispositivo. Un altro problema che abbiamo visto nel sistema centralizzato è che il mio dispositivo Edge forniva i dati in cloud, il cloud faceva l'inferenza e i risultati dell'inferenza stessa viaggiavano dal cloud verso il dispositivo, anche qui con eventuali problemi di privacy. Bene, l'inferenza rimane sul dispositivo, nessun altro, diversamente dal dispositivo, vede i risultati dell'inferenza. Quindi anche questo ci garantisce un miglioramento a livello sia di privacy che di latenza e di trasmissione. Tutto ciò comporta e comporterà sicuramente una riduzione del carico sui server, sui sistemi cloud, che come vi dicevo cominciano a spricchiolare, cioè cominciano a essere un pochino rallentati nella gestione appunto di questi modelli, dovuti al fatto essenzialmente della quantità di richieste smisurate. Bene, se l'inferenza rimane sul dispositivo il cloud ne potrà beneficiare. Parlo al futuro perché ancora, come avete capito, l'inferenza sul dispositivo è qualcosa di abbastanza primordiale e non molto spilizzato. Però ci sta andando proprio in quella direzione. Quali sono i contro di questo approccio? Beh, il contro è che a legge non abbiamo la necessità di trasferire tanti dati, ma abbiamo la necessità di eseguire calcoli. Quindi ci serve una quantità di risorse in termini di CPU e di memoria che diversamente non ci servirebbero. Cosa voglio dire? Il classico mondo IoT, se dovessimo solo campionare un dato di temperatura con una volta un secondo, molto molto povero, pochissima RAM e poco più. Se invece vogliamo provare ad eseguire un'inferenza sul dispositivo ci serve un microcontroller un pochino più evoluto, quantomeno un microcontroller a 32 bit. Qualche kilobyte di RAM ci serviranno, qualche megabyte di flash ci serviranno. Quindi diciamo il nodo foglia non può scendere sotto una certa dimensione, una certa caratteristica. Però diciamo ne guadagniamo comunque in privacy e in energia di trasmissione. L'altro famoso problema di questo approccio è che il modello, una volta distribuito, rimane statico. Cosa voglio dire? Voglio dire che l'aggiornamento dei modelli sui dispositivi diventa più oneroso, perché come abbiamo anche detto nell'ultima esercitazione, se salviamo anche il modello comunque in aetro, con le flash esterne, dobbiamo ogni volta che necessitiamo di fare un update del modello, siamo costretti in qualche modo a trasferire tutto l'intero modello che è eseguito da un array, una matrice di pesi, e trasferirlo verso i dispositivi, quindi distribuire il modello con di nuovo un dispegno energetico legato alla trasmissione. E non possiamo in ultimo citare il fatto che alcuni modelli non ci saranno. Per quanto adesso vedremo le politiche di compressione, segmentazione, eccetera, alcuni modelli proprio non ci stanno. Non ci stanno i modelli di riconoscimento locale degli assistenti come Alexa, come Google, non ci possono stare, perché sono dei modelli di dimensione spropositata, se si parla di qualche giga di dimensione. È chiaro che questi modelli non ci staranno. Però molte altre tecnologie e modelli utili nella vita attuale invece riescono a far fittare dentro i dispositivi tipo Edge al costo di che cosa? Al costo di una qualche riduzione della curatezza. Questo lo dobbiamo dire nel senso che addestrato il modello in cloud, la fase che vedremo di compressione del modello per farlo matchare dentro il nostro dispositivo alla fine toglierà qualcosa, e questo togliere qualcosa è chiaro che si ripercuoterà un pochino nella curatezza. Però una certa riduzione della curatezza può essere diciamo così utile se ci permette di fare inferenza sul dispositivo piuttosto che rimanere legati al cloud. Allora quali sono le principali idee per cercare di ridurre la dimensione dei modelli di machine learning? La prima cosa che si può fare a livello proprio di progettazione del modello è scegliere uno strumento, quindi un tool piuttosto che un altro. Cosa voglio dire? Voglio dire che ad esempio i support vector machine sono dei modelli di machine learning molto più snelli rispetto al deep learning e quindi se partiamo con l'idea di trasportare poi l'inferenza sul dispositivo è chiaro che il ricercatore, o come lui che dovrà implementare questi modelli, posso fare una scelta pulata e quindi dire ok affronto questo problema con un modello che io so che occuperà meno spazio perché altrimenti non fitterebbe all'interno del mio dispositivo. Questo vuol dire che i modelli di deep learning che sono più grandi come dimensioni sono quelli meno utilizzati diciamo in questo momento come scelta. l'altra cosa che possiamo fare quindi una volta definito il nostro tool appunto con un occhio all'occupazione di memoria e alla complessità anche qui andare a lavorare sulla riduzione dei componenti del nostro del nostro modello quindi se parliamo di un sistema cluster k-minc riduciamo il numero di cluster rispetto a un modello che magari avremmo voluto eseguire su un dispositivo classico. Questo di nuovo cosa comporterà comporterà una certa riduzione di accuratezza. Però ecco la prima scelta che si può fare per ridurre la dimensione del modello e scegliere lo strumento migliore. Magari quando facevamo un machine learning puro su cloud eccetera mirati solo al risultato cercavamo di prendere il modello che sapevamo meglio magari ci dava delle performance di accuratezza utili. Questa volta dobbiamo guardare anche alla scelta del modello non solo basato sul risultato ma anche basato sul fatto che poi ci può o meno stare dentro il nostro dispositivo. E questa è sicuramente la prima fase, la fase di progettazione. Una volta scelto il tool, una volta scelta la dimensione delle sue componenti, cosa facciamo? Andiamo a addestrare il nostro modello in classico dispositivo lì possiamo usare qualsiasi sistema cloud, GPU, grafico, quello che volete e la fase di riduzione del nostro modello poi la andiamo a fare nella fase di deployment, cioè nella fase in cui il modello lo prendiamo e lo trasferiamo sul nostro dispositivo. Ecco, nella fase di deployment possiamo fare diverse cose, possiamo fare quella che si chiama il pruning del modello, la compressione e poi vedremo anche la segmentazione. Ecco, questa è la slide di giustizia. Dove abbiamo pruning, quantizzazione e segmentazione o partizionamento. Sono le tre tecniche di riduzione della dimensione del modello legate alla fase di deployment, quindi dopo la fase di scelta del modello. Ognuna di queste comporterà sicuramente una perdita di accuratezza. non sono in dolore nessuna delle tre, però diciamo sono degli strumenti che ci permetteranno di in qualche modo di matchare il modello su delle caratteristiche di dispositivi low power, comportando la minor perdita di accuratezza possibile in qualche modo. Che cosa vuol dire fare pruning? Cioè, bene, prendiamo ad esempio come è una rete neurale, che si chiama una rete neurale, che stabilisce delle connessioni fra ogni nodo, quindi ogni neurone, diciamo così, dei vari strati, e queste connessioni sono identificate da dei pesi, no? Perché sappiamo che un neurone genera un output verso il successivo strato e l'out viene immediato da alcuni pesi. Se il peso è zero vuol dire che quella connessione è nulla, in qualche modo. Il concetto è questo, no? Ma le connessioni comportano l'allocazione a livello del medio debito di appunto parametri numerici dei pesi, che normalmente sono dei double, cioè 72, 72. Benissimo. Sappiamo che la fase di addestramento è proprio quella fase che va a modificare questi pesi, no? In base al match fra il input e altro. Dopo le varie epoche di addestramento abbiamo definito la nostra rete e i nostri pesi hanno assunto i valori addestrati, quindi stabili. Bene. Cosa vuol dire fare pruning? Vuol dire eliminare quelle connessioni e quindi cancellare dal modello i relativi pesi, quindi rimuovere questi double pesi. Di quale connessioni? Beh, le connessioni chiaramente non usate, cioè se un peso raggiunge zero è banale toglierlo, no? Per intanto non fa nulla quel peso. Ma in realtà non si cancellano solo quelli uguali a zero, che sarebbero davvero pochi. Si cancellano anche le connessioni poco usate, cosa vuol dire i pesi molto bassi. Anche lì si sceglie una soglia, questa è la soglia del roaming, e dico cancello tutti i pesi che sono per me poco significativi. Perché se sono bassi quei valori vuol dire che vanno a mediare l'output di un nodo che diventa l'info di quello successivo in maniera poco significativa. Chiaro che questo fa perdere accuratezza, perché magari quel poco che quel peso dava comunque non c'è più. però questo vi riduce notevolmente la complessità del modello, perché vi va proprio a tagliare le connessioni con la rimozione dei pesi. Quindi se la vostra matrice era totalmente connessa, era che ne so, un milione di pesi, cominciate a tagliare, da quel milione cominciate a calare. E' chiaro che dove mettete la soglia vi comporta una maggiore o minore rimozione di queste connessioni, però ricordiamoci che l'accuratezza sicuramente si modificherà e si abbasserà. Quindi qua c'è un compromesso da dire che quando faccio il pruning cerco di tagliare con una soglia ragionevole. e la cosa che si può fare è che questo pruning si può fare anche prima del training. Prima del training però è più pericoloso, perché dice ok, invece di fare una matrice totalmente connessa, io definisco un certo numero di connessioni e le sceglie io. E poi a destra quelle connessioni. Quindi vuol dire che la matrice non era totalmente connessa o ridotto la dimensione, però normalmente conviene fare il pruning a valle perché scegliete esattamente i pesi che nell'addestramento sono rimasti molto bassi. Però è un approccio che si può estendere diciamo così, in entrambe le circostanze. Quindi ciò che vediamo è che prima del pruning una rete totalmente connessa in questo modo, dopo il pruning appare una rete invece più sbilanciata chiaramente, dove sono rimaste solo le connessioni con un peso superiore ad una certa soglia. E questo ovviamente riduce la dimensione perché appunto ognuno di queste connessioni è rappresentato da un double, che è un parametro di peso. Quindi cominciando a ridurle io devo portarmi dietro meno double, meno una matrice di pesi molto più eccetera. Mentre quando abbiamo detto agiamo nella riduzione del modello a livello di progettazione, voleva dire ecco scegliere magari un numero di strati inferiore, scegliere un numero di strati inferiore, ma questo è a livello di progettazione. Qui invece una volta identificato un'architettura, la destriamo e da questa architettura andiamo a togliere come dicevo i pesi meno significativi. La successiva invece che cos'è? È la compressione del modello. La compressione del modello è come si ha, o anche questa è una compressione con perdita di accuratezza. La compressione è molto carina, molto intelligente, perché quando io faccio un modello classico in machine learning, come parametro i pesi normalmente ho un double, con tutta la sua capacità di raccogliere appunto anche le sfumature e la sua quantificazione. Un double è 64 b, ok? Quindi i miei pesi sono una matrice di double. li addestro, i miei pesi avranno ottenuto un valore, ognuno di questi non è l'addestramento, ogni peso è stato in qualche modo definito. Allora vado a fare quella che si chiama la quantizzazione successiva dei pesi, cioè i miei pesi che erano da 64 bit o 32 bit, vengono rimappati in una codifica più ristretta. invece che a 64 posso passare a 32, posso passare a 16. Normalmente quella che viene chiamata full integer quantization addirittura riduce i pesi a interi 8 bit. Come può funzionare questa cosa? Funziona perché si vanno a vedere i pesi della nostra rete, si stima il minimo e il massimo e si rifà la codifica del numero, cioè si vanno a spalmare i bit che usate solo fra il minimo e il massimo. Facciamo finta che i vostri pesi siano tutti fra 08 e 09 con tutte le sfumature, non vanno a codificare per niente un peso a 05, a 06, 07, perché ho cercato il minimo e il massimo della mia rete, se il range ristretto la codifica, la transcodifica nuova genera un peso molto ridotto, ma mantiene tante sfumature, perché comunque tutti i bit che avete scelto, se avete scelto 8 bit, se le 256 sfumature rimangono tra i bit e i massimi. È chiaro che più di tanto non si può fare, se passano da 64 bit a 8, hai voglia tu a cercare il minimo e il massimo, e poi se in una rete avete un range molto esteso tra minimo e massimo, sicuramente con l'8 bit fate un casino, magari passate sul 16, state sul 32, però capite che questo ve riduce notevolmente la dimensione. passarebbe da 64 a 16, no? Ristretto 2 volte, ok? Sì, sì, allora, io faccio proprio la transcodifica dei pesi e anche l'interfaccia del modello deve essere... assolutamente, assolutamente. Sì, sì, la proporzione la devono fare anche sull'Input, sull'Arte, perché è il migratorio. È quello che ne occupa tutto l'SdK, cioè, avete visto nell'esercitazione dell'altra volta, il modello in realtà è una macice di peso, come ho fatto vedere, ma tra i file che scaricavamo da Edge Impulse c'è un file H che contiene tutti i pesi, per esempio. E tutto il resto, però, tutti gli altri file erano il flashdK, no? Che usa quei pesi nella fase di inferenza e lì, quando fai appunto questa compressione o distillazione, come qualcuno la chiama, devi ritarare tutto l'Input sui nuovi pesi e l'alto. Altrimenti non corrisponderebbe a questo più un risultato significativo. Però ecco, questo è fatto ormai di routine, sia il pruning che questo è fatto di routine dai sistemi che conosciamo di deployment dei modelli sull'Edge. Quello che abbiamo fatto nell'esercitazione di martedì faceva proprio questo, perché una volta che abbiamo addestrato noi cliccavamo semplicemente su deployment, ma faceva tutte queste fasi di pruning e poi di compressione per poter ridurre la dimensione del modello. E addirittura il tool di ieri non ci abbiamo fatto caso, mi sono dimegato di ieri, mi diceva la dimensione del modello prima e dopo il building, che chiaramente veniva ridotto. Il modello che abbiamo usato martedì era molto semplice, quindi probabilmente non c'era neanche stato bisogno di fare questi passaggi, perché se non ricordo male occupava pochi kilobyte, una memoria di 4 megabit, però era un modello volutamente semplice. L'ultima che vediamo è invece il concetto di partizionamento. Il concetto di partizionamento è un concetto che, diciamo, tramalica un po' quello che è la normale inferenza sul dispositivo, perché in realtà l'inferenza non si fa in toto sul dispositivo. Partizionamento cosa vuol dire? Vuol dire il mio modello è grande, non ci sta sul dispositivo. Allora cosa faccio? Sul dispositivo ne metto solo un pezzo, una parte del mio modello, che normalmente è la parte iniziale, quella di pre-processing dell'input. Magari facciamo finta di avere una rete a tre strati, faccio il pre-processing dell'input, il primo strato lo metto nel modello, l'output del primo strato lo mando in cloud o in pod, cioè in un server un po' di più potente. Allora, fare questo, mi direste, ma che vantaggio ho? Il vantaggio è che io ho praticamente spezzato l'inferenza in vari punti. Allora, il vantaggio fondamentale è che se io non lo spezzassi, tornassi all'architettura principale dove io mando direttamente i dati agli altri a fare l'inferenza, avendo fatto una pre-computazione quantomeno di un primo strato, ciò che trasferisco è molto inferiore. Voi pensate come lavorano le reti. Prima soprattutto l'input, poi si comprime l'output del primo strato per arrivare al secondo strato. Io mando via quell'output, quindi mando via molto meno informazioni sul cloud e sul phone. Mandare meno informazioni mi riduce il consumo energetico di trasmissione. Problemi della privacy, ho tolto un sacco di informazioni, non mando più il dato grizzo, mando una pre-computazione che ha fatto il primo strato, addirittura se ci stanno due strati, il primo e il secondo strato. E quindi a livello di privacy ho tolto un sacco di cose. Consumo di energia, ho tolto un sacco di cose. posso comunque eseguire dei modelli abbastanza complessi. È chiaro che rimangono dei problemi, certo. È critico, come ho scritto qua, la scelta del punto dove il modello deve essere partizionato. Non è banale dire dove lo partiziono, poi dipende da modello a modello. Pensate all'aggiornamento dei modelli, per quanto sarà più complicato, perché il modello è tra una parte qui, una parte nel fondo, magari addirittura un'altra parte anche in cloud. Quindi io posso spargere i miei modelli, i pezzi, le partizioni dei miei modelli e soprattutto il tracciato di trasmissione. E capite che poi fare l'aggiornamento non sarà sicuramente banale. Però è una tecnica molto usata, soprattutto a livello di ricerca scientifica, dove questo partizionamento addirittura sarà dinamico. Cosa vuol dire? Sarà dinamico che, dato un modello, si sceglierà dove spettarlo, non in maniera statica, ma in base alle circostanze. Ho visto dei rambori scientifici da poco, anche in base alle circostanze di disponibilità energetica. Cosa vuol dire? Il mio nodo è alimentato da batteria, ha la batteria al 98%, allora partiziono poco, eseguo tanto in locale. Quando la batteria sta per finire, e non posso fare troppi calcoli lì, allora cosa faccio? Evito. Faccio solo un piccolo partizionamento e mando i dati in su. Quindi c'è addirittura una dinamicità di questo partizionamento. E dicevo, è un argomento un po' borderline, nel senso che è un argomento ancora in voga nella ricerca attuale scientifica. Quindi in produzione non c'è quasi nulla di tutto ciò, è solamente un di mello esperimentale. Per chiudere, perché la lezione di oggi è molto corta, ma in realtà poi parleremo di qualcos'altro, ecco, per chiudere ci chiamo uno dei framework più utilizzati proprio per portare il machine learning sull'edge, che è la versione light di TensorFlow. Il TensorFlow sicuramente l'avete usato, credo, con i testi freschi l'anno scorso per fare addestramento di modelli in cloud. Bene, esiste la versione light di TensorFlow che porta il modello sul dispositivo Edge. Quello che abbiamo fatto martedì, che si chiamava la piattaforma Edge Impulse, sotto lavorava con TensorFlow e TensorFlow light. Quindi quando addestravamo il modello lo addestravamo con TensorFlow e poi passavamo a produrre un modello con TensorFlow light. E vedete, questa piattaforma fa le fasi di training, distillazione, quantizzazione, encoding, compilation, che sono dei nomi diversi, ma delle fasi che abbiamo detto. Noi il training lo sappiamo, la distillazione è il pruning, la quantizzazione è la compressione che abbiamo detto, la ricodifica e poi la compilazione sull'architettura di, in questo caso, un Arduino, un Arno. E tutto questo è automatizzato, avete visto, ma tu l'imera scoperta da avere storia. Carichiamo i dati, addestramento, building, scariciamo la libreria Arduino e tutto funzionerà. E quindi questo è uno degli strumenti più utilizzati. Non è l'unico, ma a principio solo questo perché effettivamente è lo strumento ad oggi più evoluto da questo punto di vista, perché ci permette davvero di lavorare in questo modo. Sarà il futuro, probabilmente sarà il futuro del machine learning sull'IoT, non del machine learning in generale, ma del machine learning sull'IoT. Il machine learning in generale continuerà con tutta la sua necessità di enorme prestazioni di calcolo, memoria, eccetera. Purtroppo non si tornerà indietro, dico purtroppo, perché l'energia necessaria a addestrare i modelli sarà sempre maggiore, però se parliamo di machine learning on IoT, allora questo probabilmente sarà il futuro. La necessità di eseguire direttamente modelli addestrati sui dispositivi più varieganti e più produttivi. è vero, finiamo qua. Sì. Sì. Sì. Sì. Sì. Sì. Sì. Sì. È vero. Un'altra cosa. Sì. Sì… Sì. Sì. Sì. Sì. Sì. Lo sì. È vero. Sì. ... Sì. Sì. Sì. mi piacerebbe cambiare rutta, cioè cambiare i propri paradigmi l'idea è quella però nessuno sa come farlo diversamente ancora è una cosa bella che stimolava il pensiero, bellissimo però ancora nessuno sa come farlo perché tu dici entri lì dentro, cominci a vedere solo quello e continui a lavorare lì dentro per carità il pensiero divergente è sempre molto utile però non sempre riesce a portare magari, assolutamente secondo me l'altro problema è che se visto che così funziona e quindi la gente continua a lavorarci di brutto perché così davvero funziona è vero che energia, memoria, CPU sono un disastro, però funziona perché dieci anni fa non avevamo Alexa dentro casa, non avevamo un uomo dentro casa e adesso abbiamo visto quanto funzionano bene, no? sono a volte impressionanti come riescono a capire quello che diciamo diversamente non lo faremo però sì, il pensiero di provare una strategia totalmente diversa è sicuramente ben visto e speriamo davvero che ci sia la faccia perché a un certo punto arriveremo altrimenti alla nostra saturazione perché non possiamo accrescere in continuazione l'energia, le capacità di calcolo per fare questi strumenti bene, quindi questo chiudeva proprio l'ultimo capitolo del nostro corso di IoT ho interrotto la condivisione adesso diciamo che a parte i saluti magari andiamo a prendere un caffè insieme però visto che abbiamo ancora del tempo vi piacerebbe discutere con voi un pochino sull'esame, sui progetti perché questa è la prima edizione di questo corso anche per me io ho pensato di non fare un classico esame come potete immaginare con il compito scritto eccetera ho pensato invece di fare un progetto perché mi sembra che nella programmazione IoT poi come avete visto il corso IoT è veramente un corso basato sulla pratica perché tanto l'IoT è l'insieme di tante tecnologie non c'è nulla di rivoluzionario se non strumenti particolari da utilizzare quindi la mia idea è proprio quella di fare un progetto quindi mi piacerebbe con voi capire intanto a che punto siete e se avete l'idea di sostenere l'esame nei partelli di gennaio e febbraio che sono tre date se non sbaglio allora io se non sbaglio ho già ricevuto perché vi ricordo che avevo detto di condividere con me l'idea di fare il progetto per ottenere la mia approvazione allora ho già ricevuto diverse mail se non sbaglio voi me le avete mandate tutti no forse mandami la mail con i punti così ufficiale sì sì però magari se capito se me la mandi così rimane anche segnato io vi rispondo magari così la formalizziamo meglio ad esempio ecco visto Niccolò mi sente sì ok ecco con Niccolò ho visto che mi ha mandato qualche giorno fa la sua proposta anche questo è molto interessante perché lo dico pubblicamente ha trovato un sensore per l'analisi dell'equipamento a fare un sensore atmosferico diverso da quelli che abbiamo visto fino adesso perché misura il particolato quindi è un sensore abbastanza costoso mi sembra di aver visto su un sito dove si acquista costa una 60-70 euro insomma non è una cosa proprio banale non so quale sia la sua accuratezza ecco però mi piace che ecco dice purtroppo costa un po' sì mi piace effettivamente che abbia indagato Niccolò anche in altri cambi di misura però ecco dal punto di vista ricordiamoci sempre che facciamo dobbiamo fare un progetto IoT non semplicemente un progetto di misura perché fare un progetto di misura è un po' riduttivo rispetto all'IoT quindi magari quando cercate di formulare la vostra idea di progetto non pensate solo a come campionare il dato ma anche cosa farci dopo questo dato qua ecco abbiamo parlato anche di machine learning perché no se vi viene in mente anche di provare un piccolo modello sopra che poi prenda questi dati li analisi e ne faccia qualcosa oppure anche se non volete un machine learning una semplice correlazione che ne so invece di campionare solo il particolato campiono particolato insieme a l'anidride carbonica insieme magari a idrocarburi e poi cercare di vedere se gli andamenti hanno qualche correlazione perché questo un pochino serve l'IoT questo lo dico essenzialmente ecco per Niccolò che va benissimo a comprare quel sensore però dicevo magari pensi anche ad aggiungere qualche altro sensore al sistema così da dire ok il PM2, PM2.5 correla con qualcosa visto che sto misurando questo ma misuro anche aggiungo idrocarburi temperatura umidità vento probabilmente quando c'è la calma il PM2.5 sarà più presente quindi sarebbe interessante ecco lavorare un po' in questo modo per quanto riguarda i vostri progetti mi ricordo tu ad esempio campionare un sacco di informazioni vero Lorenzo su simpice sensore anche da te e poi interessante non semplicemente scriverli su Influgdb e li guardiamo ma pensare su Influgdb avete visto se è una valanga di tool sorta di analisi potete fare la correlazione facilmente cercare di capire che da te ci sarà correlazione tra temperatura umidità del suolo è probabile perché però è interessante anche vedere questi prodotti qua vi dico che dovete per forza metterci un modello di machine learning però ad esempio noi i martedì abbiamo visto che è facile fare un modellino di analisi delle anomalie perché no provate se campioni la temperatura è l'umidità dell'aria gli dai tre giorni di training e poi fai un modello di analisi delle anomalie poi vedi se effettivamente ti ritorna qualcosa di intelligente poi magari catturato un'anomalia pensi anche all'attuazione perché IoT è altra attuazione al momento in cui trovi l'anomalia accendi un led una sirena oppure semplicemente un relais che faccia qualcosa quindi ricordiamoci sempre che il progetto non è un semplice misurare questo modello di una volta ottenute le misure cerchiamo anche di farci qualcosa che sia una strategia casica strategia la CO2 super o 100 parte per migliori apro la finestra accendo il ventilatore ma quello che è strategia feedback diretto potrebbe essere oppure una strategia complessa studio la correlazione fatto un'analisi di qualche altro tipo un machine learning scottiamoci sempre che c'è anche quell'aspetto lì ecco Nicolò dice pensavo anche di aggiungere temperature umidità pressione atmosferica ecco per il PM se riuscisse anche la velocità del vento lo so che questo è un po' più particolare perché serve un anemometro in qualche modo perché secondo me per valutare correlazioni con appunto il particolato l'anemometro sarebbe fondamentale perché dice c'è calma allora ho questo PM non c'è calma ho quest'altro che non vuol dire che magari il particolato non c'è ma che viene portato via più facilmente perché c'è nel vento ok invece Paride tu mi ricordo facevi un progetto per la contabilizzazione energetica dentro ecco tu dal tuo punto di vista raccogli tante informazioni perché dopo anche pensa cosa facci cos'è per quanto Poi la correlazione tra i prodotti e ciò che davvero hai, per vedere se ci sono delle perdite, è interessante. Poi sarebbe interessante anche correlare con la temperatura esterna, perché la dispersione cambia se il gradiente tra interno e esterno è basso o se è molto alto, sicuramente la dispersione cambierà. Quindi ragionate sempre un pochino in quest'ottica di dire i dati li raccolgo, sì, ma il fine del progetto non è solo raccogliere i dati. Poi con i dati dobbiamo farci qualcosa, altrimenti torniamo un po' nel concetto primordiale di telemetria, piuttosto che a IoT e telemetria. E appunto leggo i dati, scrivo i dati, poi dopo la testa mia li guardo, però non vogliamo fare così, vogliamo generare un sistema, non dico esperto a tutti i costi, però un sistema che fa questi dati. Per capirmi? Sì, sì, sì, sì. Io ho due o tre comunicazioni, perché le auto moderne hanno questa porta OBD2, non so se avete mai sentito, perché è una porta di comunicazione verso il bus cam che c'è dentro, che collega tutte le CPU che ci sono in un'auto, ma ce ne sono centinaia di CPU. Cioè non c'è solo la centralina elettronica che esistisce tutto il motore, ma c'è la CPU nel terzo, ci sono le CPU nel pompa dei freni. E questo cambia. Purtroppo ogni auto, ogni fabbrica decide cosa rilasciare. Quindi tu fai un'app e dici, probabilmente non funziona per tutte le auto, no? Io ho provato sulla Citroen, sull'Alfa Romeo, dei miei amici, così cambiano le quantità di informazioni che hai. Però la cosa figa, scusate la parola, la cosa figa è che si danno informazioni del tipo, posizione dell'acceleratore, pressione del pianto frenante, gradi dello sterzo in cui sono girati, carico del motore, marcia inserita, numero di giri. Tutte queste informazioni qua, quello che mi è venuto in mente, come un mio tesista, ho fatto raccogliere una valanga di dati. Quindi abbiamo semplicemente fatto un'app, si prendeva da questa porta attraverso il Bluetooth e scaricava in un file di log tutte queste informazioni. Mentre lui guidava, andava a trovare amoroso, andava di qua, faceva la sua vita normale, no? E poi abbiamo provato a fare un'analisi con machine learning, questi dati, per capire se la sua guida era safe o non safe. Voi direte, beh, come faccio a capire se la mia guida è safe? Abbiamo cercato di capire appunto il regime massimo del motore, come di cambiare le marce, quanto l'era la pressione del freno nel momento in cui andava a frenare. Una cosa è fare una frenata dolce, una cosa invece è uno che guida, no? E abbiamo definito un modello in qualche modo che l'80-90% riconosceva il modo di guidare più o meno dolce di una persona. E poi l'abbiamo implementato in un modello per lo smartphone. Adesso siamo arrivati solo a livello di analisi più alto statistica, però l'idea cos'era? Mettere un dispositivo nella macchina che ti comincia a lampeggiare. Io non lo so, guarda, che stai guidando male, insomma. Male da che punto di vista? Si è energetico, perché a volte qui c'è l'accelerare, frenare, comporta consumi degli organi dell'auto, comporta tante cose. Però grazie a questo sistema, che è tutto gratuito, nel senso che questa porta OBD2 gli fornisce una valanga di informazioni, e addirittura si trova sul mercato, noi ancora non l'avevamo visto, una Raspberry Pi impacchettata per essere collegata all'OBD2. Se prende le OBD2 ci sono 15 volte, quindi l'alimentazione c'è, tu l'attacchi lì, poi poi lo vedi, scarica i dati, e addirittura se ci metti una scheda GSM, poi col tuo smartphone, perché lei li manda in cloud, col tuo smartphone riesci a vedere tutti i risultati, ma addirittura se il motore è da remoto. Si, si accende il motore, chiudi i finestrini, anzi si accende i trasigli cristalli, dopo è anche esagerato. Si, quello che si fa non è altro. No, nessuno di noi ce l'aveva. Si, esatto. Infatti per quanto riguarda l'economicità c'è già tanto, no? Dalle prime Fiat Tipo, voi siete giù, ma le Fiat Tipo che avevano i led che si illuminavano in base a quanto schiacciavi l'acceleratore, che passati, che ne so, io ho avuto una Ford, C-Max, anche lì gli dava le stelline, 5 stelle vuol dire che stai consumando poco. Quindi a livello di economicità c'è tanto, sulla sicurezza c'è un po' di meno. E lo studio che abbiamo fatto noi era, ad esempio, guardare anche l'accelerazione longitudinale e la posizione dello sterco. Nel senso, se tu affronti, scusate, l'accelerazione laterale, se tu affronti una curva di 25 gradi e c'hai 0,8 G, e dici, c'è qualcosa che non va, insomma. E anche lì abbiamo cercato di valutare questo, perché nella Raspberry Pi, ad esempio, c'era un accelerometro, si chiama Autopi, quella che si collega alle auto, c'è anche un accelerometro, perché, ad esempio, l'accelerometro non c'è anche delle auto nella centralina, quindi per avere l'informazione di accelerazione lo devi mettere esterno. E con questo si vede che magari tu affronti una curva di X gradi e c'hai 0,8 G laterali, e dici, guarda che lo stai guidando bene. Sì. Quindi poi soprattutto sapete cosa li usano per scovare i falsi incidenti, no? Perché tu dichiari, ho tamponato X a quell'ora, guarda, qui l'accelerazione, la tua macchina era ferma, oppure quando tamponi vedi un picco di accelerazione, non chiede, no? E lo usano soprattutto per scovare le false dichiarazioni. Sì. Ma poi addirittura, sì, c'è, esatto, se rilevano un incidente, addirittura ti possono anche telefonare per mandarti eventualmente per l'ambulanza, per l'attreste, insomma, sono davvero funzionali. Infatti c'è un lavoro IoT sulle auto, mi piace molto, perché un po' come Smartphone Sense, vi ricordate l'applicazione che aveva fatto Noboli, avevano fatto insieme, e il grafo sabella di quell'applicazione è che se tu riesci a farla usare tante persone, hai un bacino di raccolta dati assurde. Quante auto ci sono, no? Quindi sarebbe bello anche un progetto IoT sulle auto, perché basta comprare un traduttore da OVDU a Bluetooth e poi dopo scaricchi i dati. Ma costano 3 euro, su Wifi l'ho comprati 5-6 per fare queste cose, ma costa solo 2 euro l'uno. È molto facile estrarre dati da questo bar. E ci si può inventare tante cose. Noi abbiamo lavorato con l'intelligenza artificiale per cercare di definire un modello dello stile di guida. Abbiamo fatto due pubblicazioni su questa cosa. Bene, quindi, chi non ha mandato ancora la mail, ecco, si ricorda di mandarla. Si ricorda di mandarla se volete fare l'esame, se non è la sessione subito disponibile. Se la volete rimandare, se non sbaglio, abbiamo chiesto una relazione del progetto, la mandare una settimana prima della data. Quindi quando scegliete la data, sapete che una settimana prima dovete mandare la relazione. E poi dopo il progetto, l'esame consiste nella discussione del progetto con delle slide. Quindi preparate proprio una presentazione. Potete fare anche la demo durante l'esame. Se la demo non la volete fare, fate le slide, appunto, nel senso che registrate qualcosa, insomma, eccetera. E potete usare anche il QDB, quello che abbiamo messo a disposizione noi, io e Paolo, con l'università. Su Blended ho aggiunto un file ieri sera per le credenziali, per scrivere in un altro bucket che non è quello delle esercitazioni, perché quello delle esercitazioni si pentela periodicamente, l'avevo fatto apposta perché è molto sporco. Allora c'è un bucket invece proprio per gli esami, per gli testi. Si chiamano con il suo top, se volete usare quelli più di B, potete rimanere di farlo, e se avete le cadenziali, i dati non vengono cancellati. Quindi se avete la necessità di iniziare a campionare prima, per arrivare all'esame, a febbraio, potete già usarlo. Quindi magari a cortezza come tag, mettete il mio nome vostro, così avete un canale, visto che il bucket è unico, avete un canale se fare un canale. Quindi non vi succede in realtà. se usate il nostro, quello che ha messo a disposizione, lì non li cancelliamo mai. Allora tu sei in modo, perché il QDB è gratuito, il software, quindi tu lo scarico, lo scarico, lo scarico, lo scarico, e se usi invece di B, in cloud, loro, lì c'è l'attività del interesse. Allora noi cosa abbiamo fatto? Abbiamo comprato un server su Alumba, io e Paolo, Linux, abbiamo installato, abbiamo installato lì sopra e lì diciamo noi a decidere la politica di cancellazione dei dati. Però il bucket esercitazioni l'abbiamo lasciato che si cancella velocemente, perché lo sporchiamo durante l'esercitazione. invece il bucket test che abbiamo fatto, quello lì lo potete usare già da adesso, non cancelliamo niente. Sì, allora il server è sempre quello. Dopo vi ho messo le credenziali con un nuovo token, l'organizzazione, il nome del bucket. Quindi lo potete già usare. Sì, così se avete la possibilità di poter già scrivere. Poi avete visto, abbiamo usato tanti altri strumenti, la same speak, abbiamo usato il regime course, quindi potete scegliere quello che volete. Bene. La cosa che mi piacerebbe ritrovare nel vostro progetto è che facciate qualche riferimento alla nostra teoria, quando abbiamo parlato appunto architetture, politiche di programmazione super lupa, eventi, queste cose, mi piacerebbe che voi le accennate, se non le vedo accennate, ve le chiedo probabilmente, vi interrompo, vi dico sì, ma lì ho un codice, però non vi faccio una vera interrogazione sulla teoria. Poi mi piacerebbe che voi le riportate nella vostra relazione e quantomeno nella percezione di aspetti che abbiamo affrontato. Bellissimo. È il freddo perché mi dicevi che la palestra è freddo. modèle di gas cellulare qui nei 250 metri totali, miоловHAÏ è l'occhio di termine Euro metodoisk. Questa esistema della vostra è la percorso comunicazione non jakie sc Angstituosa niente. 55 acionistiche di구나Est. Non che pileLA Mitt Guthoff ecco grazie