Allora iniziamo l'esercitazione di oggi, l'esercitazione come avevo anticipato è dedicata alle reti neurali, quindi proveremo a creare una piccola rete neurale di tipo totalmente connesso, quindi una rete di tipologia densa che abbiamo visto a lezione e lo faremo grazie a una libreria che è una delle più note nell'ambito nel panorama sia accademico che industriale, che è TensorFlow, una libreria per il deep learning sviluppata da Google, è come vi dicevo una delle più note, più diffuse, non è l'unica perché ce ne sono altre, diciamo vale la pena menzionare sicuramente PyTorch che è un'altra libreria molto molto utilizzata, diciamo probabilmente PyTorch che è stata tra l'altro sviluppata da Facebook, oggi Meta, quindi diciamo è un progetto, sono ambedue dei progetti open per cui sono librerie accessibili, non sono prodotti diciamo che per i quali dovete per esempio pagare, commerciali possono essere utilizzati e ambedue hanno un'ampia diffusione, probabilmente PyTorch oggi giorno è un po' più utilizzato nell'ambito della ricerca accademica, TensorFlow è un po' più diffuso a livello industriale, probabilmente questa al momento è la tendenza, però sono tutte e due delle librerie estremamente sofisticate, sono notevoli da un punto di vista del livello di maturità che hanno raggiunto e insomma si possono fare come vedrete in pochissime linee di codice, si possono fare delle cose fino a qualche anno fa, erano veramente impensabili, avrebbero richiesto molto più sforzo da parte di chi si metteva a programmare e avrebbero richiesto molte più linee di codice, molte più ore, cose che invece qui vedrete in pochi minuti riusciamo a fare una piccola cosa, tutto sommato, ovviamente noi ci partiremo per cercare di risolvere un piccolo problema di visione artificiale, di computer vision, ma ripeto fino a 10-15 anni fa questo avrebbe implicato non le poche linee di codice, pochi minuti che noi ci dedicheremo, ma avrebbe significato uno sforzo molto maggiore, questo vale non solo per questo esempio ma per tutte le tipologie di reti appunto che potete costruire che supportano questi framework, sono dei framework che hanno raggiunto, ripeto, un livello di maturità notevole e sono una delle chiavi probabilmente del successo del deep learning negli ultimi 5-6-10 anni. Allora, molto semplicemente come possiamo utilizzarlo? Beh, se siamo in un ambiente appunto come Colab, dobbiamo semplicemente fare un import, chiaramente poi essendo piena compatibilità essendo strumenti sviluppati per l'altra azienda, comunque se anche lo scaricate e lo utilizzate su un nostro Jupyter locale oppure su un sistema Python locale, fate semplicemente l'installazione dell'ultima versione di TensorFlow e lo potete poi importare, tutto gira in maniera solitamente abbastanza semplice e liscia. Qui lo importiamo, lo chiamiamo Remote.f, TensorFlow, anche qui diciamo se andate a vedere le pagine di TensorFlow, c'è una documentazione ovviamente che si porta dietro su tutte quelle che sono le API, è un sistema che è diventato nel corso di questi anni anche abbastanza complesso, per cui non è banale entrarci così subito e avere la padronanza delle tante cose che ci sono, per come primo ingresso una cosa che è stata sviluppata, è stata sviluppata una sorta di sistema, diciamo front-end che si interfaccia con TensorFlow, che può essere utile, che è quello che noi utilizzeremo all'interno di questo notebook, e questo sistema si chiama Keras. Keras rappresenta appunto, è un qualcosa che sta sopra TensorFlow, in realtà è stato sviluppato anche adesso la versione che si interfaccia con altre librerie, comprese PyTorch, credo, se non leggevo male, e anche Jax, che è un altro ulteriore sistema, è un'altra libreria, per cui questo Keras è una sorta di wrapper, qualcosa che sta sopra e che avvolge, che potete utilizzare, e vi offre un'interfaccia semplificata per l'accesso alle funzionalità di TensorFlow. Chiaramente, come tutte le cose, quando avete delle interfacce che vi semplificano alcune cose, non avete il controllo di tutte le funzionalità che stanno sotto. Quindi potete lavorare o sul TensorFlow, tra virgolette, puro, quando siete abbastanza sicuri di dove andare a prendere le cose e che cosa fanno, o in prima battuta utilizzare Keras che vi mette a disposizione dei costrutti e delle funzionalità, con cui vedrete in maniera molto semplice, riusciamo a costruire una semplice rete neurale. Poi qui, come sempre, ci appoggiamo su NumPy e Matplotlib per fare un po' di cose, ovviamente di supporto a NumPy, come avete visto, è praticamente sempre necessario, e Matplotlib di supporto per la visualizzazione e i grafici. In questa cella alla riga 10 poi abbiamo settato un generatore di numeri pseudo-casuali di TensorFlow per garantire una riproducibilità e... E' fatto stampare con questa print la versione e basta, questo è quello che serve. Dopodiché, quello su cui andremo a lavorare, se lo lasciamo... io l'ho mandato prima in esecuzione, ecco, adesso ci vuole un po', probabilmente c'è un po' un rallentamento a livello di rete, ecco. Quello che potete vedere è che qui c'è, diciamo, subito un import che è in questa cella di un dataset che, esattamente come abbiamo visto per esempio per Scikit-Learn, ci sono un po' di dataset di prova che uno può utilizzare per prendere, per dimestichezza con lo strumento, e anche TensorFlow si porta dietro il suo dataset, in particolare si porta dietro questo dataset che è quello che vi avevo anticipato, che è il famoso MNIST, che è quel dataset di cifre scritte a mano, quindi sono delle immagini che sono scansioni di cifre scritte a mano, che sono state digitalizzate e sono rese disponibili, ed è quello di cui vi ho parlato più volte, vi ho detto insomma una sorta di benchmark, di riferimento per gli studi delle reti neurali a partire dagli anni 90, diciamo, fino a 15-20 anni fa era, poi il dataset da cui partiva chiunque voleva sviluppare un sistema di computer vision sulle reti neurali, poi è stato ampiamente superato da altri, ne abbiamo parlato tra l'altra lezione. Allora qui questo oggetto è il risultato dell'invocazione di questa chiamata, che vedete è TensorFlow, poi ci appoggiamo a Keras come interfaccia per lavorare su TensorFlow, c'è il modulo dataset, in dataset abbiamo questo MNIST. Questo MNIST è un oggetto che si porta dietro i suoi bravimenti, metodi tra cui load data, load data vi restituisce train images e train, diciamo, due coppie, se andate a vedere, la prima sono le immagini e le etichette da utilizzare per il training, la seconda le immagini e le etichette da utilizzare per il test, quindi non dovete fare nulla, è già tutto suddiviso, non dovete fare test, split, niente, e quindi il download qui l'ho già fatto, adesso non lo rifacciamo, giusto per darvi, vediamo un attimo, lo facciamo perché altrimenti non vi faccio i problemi, abbiamo il tempo di farlo, andiamo qui, eccoci qua. Cosa succede? Vedete, se andiamo a stampare la shape, quindi la forma del tensore che rappresenta train images, questo è un tensore di 60.028x28, cosa significa? Significa che questo è un array Numpy di fatto, che contiene 60.000 entry, ognuna delle quali è un'immagine di 28x28 pixel, quindi sono immagini piccoline, 28x28. Le etichette sono 60.000, quindi il dataset di training sono 60.000 immagini 28x28, ognuna con la sua etichetta. Il test viene fatto su 10.000 immagini, sempre della stessa dimensione, di cui abbiamo ovviamente altrettante etichette. È un problema di tipo chiaramente supervisione. Eccoci qua. Allora, questa è giusto una stampa di prova, a fare dire quali sono le etichette, vedete, 5, 0, 4, così via. Le etichette sono quindi le classi che vanno da 0 a 9, sono i 10 numeri delle cifre scritte a mano. Benissimo, allora, fatto questo, andiamo ad eseguire questa cella, che faccio vedere adesso, questa cella, volevo mettermi dove non parte. Ok. Allora, qui che cosa facciamo? Semplicemente generiamo una figura di matplotlib e invochiamo quella funzione image show che abbiamo già visto la scorsa esercitazione, in cui andiamo a visualizzare la prima di questa immagine del dataset di training. Ok? Quindi indice 0. Poi, evidenziamo anche quella che è una barra dei colori, che a lato della figura ci dice qual è, in qualche modo, il codice che vi ho utilizzato in questa barra di colori. e vedete che questa è la prima cifra che direi che è un 5, occhio a voce. Potete cambiare, quindi, per esempio, qui mettete un 1. Quindi cambia. Qui sarà il 0. Ok? Chiaramente. Ok. E queste le possiamo invece anche togliere, perché non ci serve. Va bene. E vedete, avete la barra che vi dice l'intensità, qual è il valore massimo del pixel, da 0 a 255. Dopodiché, alla riga successiva, viene fatta una normalizzazione, non è estrettamente necessaria, però, ognuna di quelle immagini viene riscalata, in modo che i valori non siano compresi tra 0 a 255 e tra 0 a 1. Ok. Poi, e andiamo alla riga successiva, abbiamo un, di nuovo, una cella per una visualizzazione, in cui andare, anziché andare a prendere in considerazione solamente una visualizzazione, viene costruito un piccolo for, in cui andiamo a visualizzare i primi 25, le prime 25 immagini del dataset di training. Questo ciclo for, quindi, che cosa fa? Crea, con la funzionalità di Matplotlib, un subplot, una griglia di 25 immagini, quindi, saranno 5 righe, 5 colonne, e ognuna di queste immagini, praticamente, viene presa, tramite l'invocazione della funzione image show, e gli viene passata l'iesima immagine, quindi, da 0 fino a 24, l'indice, con la relativa etichetta che viene poi stampata come stringa sull'asse X, e il risultato dell'esecuzione di questa, di questa cella del notebook, è questo, vedete? Vedete come ogni cifra poi è associata a quella che è l'etichetta effettiva, ok? Va bene, allora l'obiettivo è costruire un modello una rete neurale, nel nostro caso, per fare che cosa? Per fare in modo che, addestrando la nostra rete su queste immagini, la nostra rete sia in grado di produrre delle etichette che siano conseguenti, con il miglior tasso di errore possibile. Allora, per fare questo, diciamo, Keras e StensorFlow sono estremamente versatili e comodi, perché quello che potete fare è definire un modello tramite, lo potete fare in diversi modi, diciamo, due modi fondamentalmente. Noi utilizziamo oggi questo che è appunto la definizione sequenziale del modello. La definizione sequenziale del modello avviene attraverso la classe Sequential, che è una classe di TensorFlow, Keras, e questa classe Sequential è molto semplice, perché vi permette di definire il vostro modello come uno stack, come una pila di strati, di layer. E vedrete che qui all'interno, tra qui e qua, noi andiamo a definire tutti i layer che ci servono. In particolare, cominciamo dall'inizio, e vedete che, tra l'altro, la semantica poi sarà abbastanza intuitiva di queste cose, perché allora qui abbiamo sempre TensorFlow Keras, in cui andiamo a prendere un layer di tipo input. Questo è esattamente il primo strato di input come l'abbiamo studiato a lezione di teoria, ed è il punto in cui arrivano tutti gli input che la nostra rete si ritrova a dover gestire, e gli andiamo a specificare che il primo strato, che è lo strato di input, è uno strato che ha una forma 28x28. Cioè lui si aspetta a quel punto in input una griglia 28x28. Poi però, come sappiamo, queste griglie devono essere messe in una forma per cui che sia processabile dalla rete e quindi vengono vettorizzate, diventa un vettore. Ok? Come si fa a farlo diventare un vettore? Si invoca un altro tipo di layer che è la classe flatten. Flatten vuol dire proprio spianare, rendere piatto, e quindi l'effetto di questo layer è prendere un input che è organizzato in un array bidimensionale 28x28 e renderlo un unico array, un unico, scusatemi, vettore di 28x28 che fa, mi ricordo quanto, se fate i conti, dopo lo vediamo, quindi fa in grosso modo meno di 900, mi renderebbe di 784. Adesso lo vediamo e vedete quello che è, questo è l'effetto. Poi andiamo a vedere che cosa, che cosa succede qui. Ok, dico, dico, 28x28 con precisione, 784. ok, questo è giusto, a mente, poi abbiamo un ulteriore layer, quindi fino a qui cosa abbiamo fatto? Nulla di più che prendere l'input e metterlo in una forma vettoriale. Poi arriva il primo strato nascosto, il primo strato nascosto sta qui, quindi questa che andiamo a costruire una rete con uno strato nascosto, ok? Quindi, di fatto, non è neanche una rete deep, è una rete neurale che ha un solo strato nascosto, quindi diciamo non appartiene alla categoria del deep, però capite bene che se io volessi aggiungerne un altro e il mio invito è provare un po' a vedere cosa succede se cominciate a variare queste cose, quindi basta che aggiungete, vedete, sono separati da virgola gli strati, quindi voi basta che dopo questa virgola di questo strato di cui vi vado a parlare adesso ne aggiungete per esempio un'altra uguale, una stringa esattamente uguale a questa, avete creato una rete con due strati nascosti, chiaro? e in questo caso lo strato è un, vedete, un layer che appartiene alla classe dense, dense che cosa identifica? Quello che ovviamente possiamo immaginare è uno strato di tipo denso, cioè uno strato in cui neuroni prendono tutti gli input dello strato precedente, che in questo caso è lo strato di input. Se io ne aggiungessi un altro esattamente qui dietro, avrei esattamente un ulteriore strato che prende tutti gli input dello strato precedente e poi prosegue verso, diciamo, l'uscita. Il primo parametro che cosa rappresenta secondo voi? È un numero e rappresenta il numero di neuroni dello strato. Quindi in questo caso stiamo creando uno strato con 128 neuroni. È un parametro che potete cambiare, potete, anzi, di nuovo il mio invito è vedere che cosa succede se cominciate ad abbassarlo, aumentarlo, 10 neuroni, 5, quello che è. Qui partiamo con 128, che è un buon numero, ma non è esagerato. Secondo parametro, activation. Qui potete specificare l'attivazione non lineare. In questo caso è stata specificata la relu, ma potete specificare tangente iperbolica, sigmoidale, eccetera, altre tipologie. Poi se andate a vedere la documentazione ovviamente avete tutto quello che chiaramente può servire. Ok. Poi, ultimo strato è il layer di output. Il layer di output vedete che è di nuovo un denso. Un denso che è costituito da, vedete quali parametri gli passiamo? Gli passiamo solamente, allora, non gli passiamo il parametro di attivazione, il che significa che non facciamo un'attivazione non lineare, e ve l'ho detto a lezione, di solito l'ultimo strato fa solo combinazione lineare, e gli passiamo però il numero di neuroni. Il numero di neuroni in questo caso è 10, perché, se vi ricordate, anche questo l'abbiamo visto a lezione, vi ho detto negli strati di uscita il numero di neuroni è pari al numero delle classi, se è un problema di classificazione ed è pari a 1 se è un problema di regressione, questo è solitamente lo schema che viene utilizzato. Dopodiché, il neurone che viene attivato maggiormente tra quelle 10 alternative è quello che vi dirà la classe corrispondente. Ci siamo, fin qui, vedete, quante righe di codice? 6 righe di codice, e abbiamo costruito una rete norale veramente valorditivo, non è tutto qua perché poi adesso la dobbiamo anche andare ad addestrare, ma insomma, intanto, vediamo la sintassi molto semplice. C'è un metodo summary, vedete, quindi abbiamo definito in questo modo tramite la classe sequential l'oggetto model, questo oggetto si porta dietro alcuni metodi, tra cui il metodo summary, e vedete, vi dà appunto quella che è un riepilogo, questo metodo, della ci sta, leggiamo meglio forse, di quello che è il modello, vedete, vi dici che è un modello di tipo sequenziale, in cui abbiamo un primo layer di tipo piatto, ok, quindi, l'input ovviamente non lo non lo considera, nel senso che l'abbiamo già, lo mette in conto, lo dà chiaramente per scontato, abbiamo un input, abbiamo un layer di tipo flatten che vedete, produce un tensore, in uscita, un vettore di 784 elementi, che è 28x28, poi abbiamo un layer denso con 128 neuroni, e quindi la sua uscita sono 128 elementi, e poi abbiamo un secondo strato denso, che è il layer di uscita, che vi dice che sono 10 elementi, quindi la shape di uscita è 10, e vedete che il numero di parametri associati al layer flatten è 0, cioè quello non ha dei parametri da imparare, semplicemente prende una matrice bidimensionale e la spiana in un vettore da 784 elementi. Per questi altri vi dà anche il numero di parametri, che poi vi riepiloga qua facendo la somma, cioè se voi fate la somma di 100.480 più 1290, fa 101.770, che sono i parametri che lui deve affrendere. Allora, come li ha calcolati? Questo è un semplice esercizio che potete fare. Se voi fate, abbiamo 784 feature di input e le moltiplicate per 128, vedete che 784 per 128 fa 100.352. 100.352. Ok. Questi sono 100.480. Cosa ci manca? Non abbiamo tenuto costo. Esattamente. Ci sommiamo a questo tutti i termini di bias che sono 128, perché ognuno se ne porta dietro 128 e il risultato è esattamente 100.480. Poi, questi 1290 come li ottenete? Ognuno di questi 128 punta a 10 neuroni dello strato successivo, quindi 10 per 128 fa 1280, a cui sommate 10 ulteriori pesi di bias sono 1290. In totale 100.000 paranti. Sembrava una rete piccolina, però insomma cominciamo a dover addestrare, ricavare 101.770 paranti. Questo significa che la funzione, la loss function che utilizzeremo deve, che deve essere ottimizzata, deve andare a ottimizzare uno spazio a 101.770 variabili. lui dice anche se ci sono dei parametri che non sono trainable, io potrei anche in qualche applicazione voler avere dei parametri che setto fissi, quindi quelli non sono addestrabili. Bene, a questo punto il flusso di lavoro prevede una volta che abbiamo creato il modello, l'architettura l'abbiamo definita del modello, quel modello deve essere compilato. Compilato che cosa significa? Significa prepararlo per l'addestramento. Prepararlo per l'addestramento vuol dire che gli dobbiamo specificare due cose fondamentalmente. Il tipo di ottimizzatore che vogliamo e quindi il processo di ottimizzazione, quale algoritmo deve utilizzare e la seconda cosa quale tipo di funzione di loss vogliamo utilizzare. Anche qui se andate a vedere TensorFlow si porta dietro una serie di e Keras in particolare che utilizziamo una serie e di ottimizzatori e di funzioni di loss che sono già predisposte e che possono essere invocate. Quindi non dovete costruirvi voi il vostro ottimizzatore e se andate a vedere potete specificare con questa stringa diverse tipologie di ottimizzazione. Questa è una Adam è uno dei più utilizzati attualmente nell'ambito delle reti neurali non è unica tra l'altro appunto se andate a vedere poi magari ci guardiamo insieme la documentazione ne avete a disposizione anche altri e fondamentalmente è un metodo di discesa del gradiente di tipo stocastico con delle feature aggiuntive se vi ricordate vi ho detto a suo tempo quando vi ho spiegato il metodo del primordine vi ho detto ce ne sono delle versioni evolute che noi non abbiamo avuto modo di studiare ma per esempio momentum accelerated cioè accelerati con il momento con gradiente normalizzati sono tutte delle feature aggiuntive ecco questo è uno di quelli tipo se non ricordo male il momento accelerato però ce ne sono altri di tipo adattativo insomma che riescono a fare delle cose ben sofisticate ma fondamentalmente dietro sono metodi che sono quelli che abbiamo visto metodi di discesa del gradiente ok qui ne specificate uno avete altre stringhe dopo le guardiamo magari insieme la loss la loss potete anche qui scegliere tra una serie di loss quindi di funzioni di costo di perdita e vedete che viene invocata in questo modo tf.care spunto c'è il modulo losses e noi andiamo ad accedere a quella che è la sparse categorical cross entropy che è la cross entropy categorica qui abbiamo più categorie quindi le alternative sono le classi da 0 a 9 e è una loss di tipo cross entropy e sappiamo che cos'è perché l'abbiamo studiato la differenza c'è neanche una versione che non è sparsa che è quella per le categorie codificate one-hot encoding invece noi siccome ce le abbiamo codificate 0, 1, 2 eccetera così e allora utilizziamo questa sparse categorical qui settiamo questo from logis uguale a true significa dire che questa loss utilizza nella rete la rete all'uscita non l'abbiamo normalizzata con il softmax ma c'ha semplicemente dei valori che vengono chiamati logis serve solamente a dire prendi quei valori della rete di uscita per costruire la funzione di perdita qui gli specifichiamo nella compilazione anche la metrica che lui utilizza che ci farà vedere man mano che perché mentre fa l'addestramento lui ci dirà per ogni epoca dell'addestramento qual è il valore della loss function che lui utilizza per ottimizzare ma ci dirà anche qual è ad esempio l'accuratezza ok e poi insomma ci sono un sacco di opzioni che anche qui potrebbero essere esplorate ma insomma per il momento direi che questo è più che sufficiente se andiamo in esecuzione questa cella vedete abbiamo effettuato la compilazione a questo punto è tutto pronto per effettuare l'addestramento del modello noi abbiamo specificato il modello abbiamo specificato la loss function e l'ottimizzatore con il metodo compile e adesso il metodo fit il metodo fit come sempre come abbiamo visto anche per le classi di scikit learn che supportano ad esempio i classificatori lineari che cosa fa? a destra a destra il modello e come input gli passiamo train images che è tutto il lotto delle immagini di addestramento con le corrispondenti etichette quindi queste sono quelle 60.000 immagini del training set con le loro 60.000 etichette gliele passiamo sono già nel formato giusto perché sono 28 per 28 sono 60.000 per 28 per 28 e lui le andrà a prendere una per una e con le corrispondenti etichette 60.000 etichette poi gli diciamo anche vai a fare l'addestramento per un numero di epoche vi ricordate le epoche che cosa sono? lui suddivide qui farà un addestramento di tipo batch potete andare anche quello a variare la dimensione del batch in questo caso è quella di default e quella di default sono se non sbaglio 32 adesso non andrebbe verificato poi magari lo guardiamo insieme cioè lui prende 32 campioni del dataset di addestramento calcola il gradiente su quelli su quel batch e poi fa un passo di avanzamento di discesa del gradiente anche questo e poi ripete così via un'epoca è quando lui ha visto tutto il dataset facendo degli avanzamenti adesso vedremo diciamo con i valori di default cosa fa però anche qui sarebbe interessante vedere cosa succede se andate a specificare perché lo potete specificare un diverso tipo di dimensione del batch e vedete lui comincia farà noi gli abbiamo dato dieci epoche vedete ognuna consiste in una serie di step e vedete vi segnala qui l'accuracy e qui il valore della loss vedete che la loss comincia a scendere l'accuracy comincia a salire e lui ha suddiviso ognuna di queste epoche lui fa la passata di 1875 batch con quei 1875 batch lui riesce ad analizzare tutti i 60.000 campioni infatti se fate 60.000 diviso 1875 fa 32 che è esattamente la dimensione di default dell'otto però ripeto questo è qualcosa che potete anche andare a variare e lui ha terminato e ha terminato raggiungendo un'accuracy sul dataset di training del 99% quindi l'ha praticamente perfettamente ha ridotto la loss quasi ad dirlo praticamente perfettamente classificato però l'accuracy chiaramente va valutata sul test che è quello che abbiamo lasciato da parte sono quei 10.000 campioni che lui non ha mai visto allora per fare questo model si porta dietro un metodo che è valid anche qui non vi dovete costruire niente evaluate prende semplicemente i campioni di test le loro etichette quindi qui ci sono 10.000 immagini 28x28 quindi sono test images se vi andate a vedere è un tensore 10.000 virgola 28 virgola 28 test label è un tensore di 10.000 virgola un array di un paio di quel tipo gli facciamo stampare la accuracy e il risultato è che raggiungiamo una accuracy del 97% 97,7 che non è male per lui riesce quelle cifre scritte a mano a classificarle correttamente con un'accuratezza del 97,7% ok? a questo punto noi abbiamo un modello cioè qui dentro model e abbiamo che cosa? una rete neurale che è rappresentata da che cosa? noi la potremmo tirare fuori semplicemente e rappresentarla tramite questi 101.770 parametri che sono qualcosa come 400k con 400k di numeri noi riusciamo a sapendo come sono organizzati e come è costruita la rete riusciamo ce la possiamo portare fuori da qualche parte installarla da qualche altro qualche altro sistema mandarla per esempio in produzione eccetera e con 400k riusciamo a classificare al 97,7% delle immagini di questo tipo scritte a mano e lo abbiamo fatto con pochissime righe di codice questo è capite bene che è una chiave del successo di questi sistemi e anche nella facilità del loro utilizzo però come vi dicevo la volta scorsa è un conto utilizzare quasi chiunque abbia un minimo di dimestichezza con un po' di codice Python però farlo sapendo cosa c'è dietro c'è un altro discorso quindi diciamo andare a cambiare un po' di cose cosa vuol dire se io vado a variare un parametro se cambio qualcos'altro ecco quindi diciamo di nuovo l'invito a farlo con la consapevolezza di chi un minimo queste cose le ha studiate quindi far valere questa consapevolezza allora torniamo qui una volta vi dicevo allora stavamo parlando del fatto che abbiamo il nostro modello il nostro modello in cui pochi k di memoria ci permette di fare una predizione come facciamo a fare una nuova predizione ma semplicemente possiamo prendere dei nuovi input e darli in pasto al nostro modello una cosa che viene fatta spesso ecco che non abbiamo fatto qui nel potevamo già fare nel modello di partenza anche qui potete provare a farlo è costruire aggiungere diciamo un layer che è il layer che fa quell'elaborazione softmax vi ricordate la softmax non intesa softmax come funzione di loss vi ricordate vi ho detto c'è potenzialmente un diciamo un mismatch nel senso che può essere fonte di di fraintendimento che chiamiamo con lo stesso nome vengono chiamate sia la softmax diciamo come approssimazione della funzione max sia il layer nelle reti neurali che solitamente viene messo in uscita che fa la normalizzazione esponenziale la normalizzazione esponenziale vuol dire prende tutti i valori che ci sono negli ultimi nel layer di uscita che sono 10 neuroni e lo normalizza in modo da andare tra 0 e 1 con quella formula di normalizzazione esponenziale che vi ho fatto vedere poi nelle lezioni di teoria se le andate a prendere negli appunti le ritrovate allora questo vi crea una trasformazione dei valori di uscita che non sono più tra meno infinito e più infinito cioè dei numeri reali ma diventano dei numeri tra 0 e 1 che sommano a 1 tra di loro e che quindi possono essere interpretati come delle probabilità quindi a quel punto avete che un neurone si accende con poniamo 0,7 tutti gli altri sommeranno a 0,3 voi sapete che la probabilità che la vostra rete dà a quel neurone quindi che sia quella la classe effettiva è del 70% e quindi fa comodo spesso fare una cosa di questo genere e quindi qui viene creato quello che è stato chiamato probability model di nuovo utilizzando la classe sequential gli passiamo model e ci aggiungiamo un softmax un layer softmax io avrei potuto creare direttamente già nel modello iniziale il modello con questo layer softmax spesso quello che si fa però ve l'ho aggiunto adesso qui adesso ok quindi da adesso in avanti probability model è il model che abbiamo addestrato a cui abbiamo trasformato l'uscita ok infatti non lo dobbiamo riaddestrare è solo una trasformazione dei valori di uscita va bene ok allora se lo mandiamo in esecuzione lo aggiorna e poi che cosa facciamo possiamo prendere e fare una cosa di questo genere possiamo andare a effettuare delle predizioni cioè se gli passiamo tutto il lotto delle immagini di test lui produce quelle che sono delle predizioni andiamo a vedere che cos'è questo è un array quindi che sarà un array di 10.000 elementi uno per ognuna delle immagini di test se andiamo a stampare predictions predictions che cos'è vedete io le ho fatte stampare predictions di 0 ok predictions di 0 è questa cosa qua questo array che comincia qua e finisce qua se andate a vedere quest'array quanti elementi ha le contate sono 10 perché è la predizione che lui ha fatto sulla prima immagine del dataset di testing e la sua predizione è un vettore di 10 elementi e se voi fate la somma di questi 10 elementi vedete che adesso qui non si vede immediatamente ma la possiamo andare a fare guardate quanto fa 1 perché noi abbiamo fatto quella trasformazione softmax quindi ognuna di queste entry è interpretabile come una probabilità per cui lui dice la probabilità che questo elemento che poi gli ho fatto anche graficare che è questa cifra vedete che è il 7 ok glielo ho fatta graficare qui con l'image show la probabilità che questo input rappresenti uno 0 è 10 alla meno 13 cioè di fatto è nulla lo stesso 10 alla meno 14 perché sono tutte entry estremamente basse questa addirittura è 10 alla meno 15 però ce n'è una che è questa che è 9,99 per diciamo 1 che è praticamente 0,99 cioè lui è sicuro al 99% che quel che quell'input è che cosa qui abbiamo uno 0 un 1 un 2 il 3 il 4 il 5 il 6 il 7 lui è sicuro al 99,9% che si tratta di un 7 effettivamente così va bene adesso qui volendo insomma qui abbiamo terminato volendo un po' fare come abbiamo visto altre volte un po' di cose diciamo che a livello grafico ci aiutano un po' a capire meglio alcuni aspetti delle questioni che abbiamo detto potremmo andare a vedere anche queste altre queste altre celle adesso lo facciamo ma diciamo l'esercitazione potrebbe terminare anche qua qui in questa cella quello che viene fatto vengono definite due funzioni una si chiama plot image che serve per plottare l'immagine e quest'altra si chiama plot value array che serve per fare il plot dell'istogramma di quei 10 valori di quelle 10 probabilità che vi ho detto prima adesso poi magari lo andiamo a vedere insieme però non è adesso questo l'obiettivo vi faccio vedere solamente che cosa intanto guardiamo cosa prendono come input e cosa restituiscono allora molto velocemente questo prende un numero che è i poi prende l'array delle predizioni l'etichetta vera e l'immagine e vedete che viene utilizzato all'interno questa funzione di questa di questa cella cioè qui è stato selezionato un valore 321 è uno dei possibili valori delle immagini di cui abbiamo fatto le predizioni quindi sono quelle 10.000 immagini dei testi posso selezionare un valore qualunque gli facciamo stampare che cos'è l'array corrispondente gli facciamo stampare qual è il valore massimo cioè quale quale classe lui quindi a quel punto farà la predizione di nuovo gli facciamo stampare la somma di tutti i valori e controlliamo che sia effettivamente uno e poi facciamo una figura creiamo una figura in particolare con dei subplot una figura una riga e due colonne nella prima andiamo a invocare quella funzione plot image a cui passiamo questo indice la predizione per questo indice le etichette di test le etichette di immagini e poi facciamo anche un secondo plot in cui invochiamo la seconda di quelle funzioni a cui passiamo sempre l'indice le predizioni le etichette di test e vediamo cosa succede non vi ho detto ancora cosa fanno ma prima ve lo faccio vedere così almeno vi rendete conto c'è un errore vediamo un attimo cosa è successo plot image forse non l'ho mandato in esecuzione quindi direi che giustamente lui si lamenta vediamo un po' adesso dovrebbe essere tutto in ordine vediamo ok allora vedete lui ha stampato l'array della predizione per il punto abbiamo detto che era i uguale 321 che io l'ho selezionato perché prima sono andato a vedere sono andato a pescare un errore ok uno di quei pochi errori quel 3% di errori che fa ok poi viene creato un subplot con una riga e due colonne ok una riga e due colonne vedete che lui che cosa fa se andate a vedere l'array della predizione il valore più alto lui ce l'ha qua che è questo cioè per lui quello è un 7 ok la somma vedete fa 1 in realtà c'è anche questo ma sono gli errori chiaramente di approssimazione numerica per cui lui predice un 7 predicted label è 7 ma in realtà se andate a vedere la true label quello è un 2 e quelle due funzioni che ho scritto sopra che non siamo andati a vedere nel dettaglio fanno questo vi vanno la prima vi va plot image e la seconda vi va a fare questo istogramma plot array allora adesso non ci interessa andarci dentro poi magari se avete dei dubbi ci guardiamo insieme sono semplici non sono la prima che cosa fa fa il plot dell'immagine quindi vedete questo è un 2 però può essere scambiato anche per un 7 la rete l'ha scambiato per un 7 poi vi dice questa è la predizione questa è l'etichetta vera e questo vi dice guarda che secondo lui è un 7 al 97% andiamo a vedere se è vero andate a vedere qua guardate un po' 97% quindi lui è sicuro al 97% e vi fa vedere la distribuzione su tutte le 9 possibili alternative la distribuzione di probabilità ed effettivamente guardate e ve la evidenza in rosso quando lo sbaglia invece in blu quando lo fa correttamente quello fa quella funzione plot array non mi ricordo come si chiama plot c'è plot image e poi c'è plot value array ok plot value array fa questo e vedete questo si mangia quasi tutta la probabilità il 97% è sicuro di questo poi guardate c'è questo che è un piccolo valore molto più basso gli altri non ve li fa vedere neanche perché sono praticamente zero effettivamente se voi andate a vedere questo vettore c'è questa entry 10 alla meno 9 10 alla meno 8 però c'è questa che vedete è un 2,2 per 10 alla meno 2 quindi è diciamo un 2% un 2% di probabilità che sia un 2 effettivamente lui ha sbagliato era molto sicuro che fosse un 7 però la sua seconda alternativa sarebbe stata questa che era quella corretta quindi questo è molto carino perché vi fa vedere anche qui potete un po' giocare con gli indici andare a cercare altri errori di questo tipo e trovate che effettivamente le cifre scritte a mano insomma ognuno le scrive in modo diverso possono essere interpretate qui l'occhio direbbe che forse un 2 però ci sta anche che lui lo abbia scambiato per un 7 va bene? allora direi che i dettagli vi dico molto velocemente cosa fanno queste due funzioni ma direi che i dettagli non ci interessano più di tanto la prima proprio vedete prende un'immagine la va a plottare e poi vedete fa un if se avete indovinato la predizione ve la colore in blu altrimenti in rosso sotto la scritta e con questo colore va a scrivere quello che viene scritto qua vedete l'ha scritto in rosso 7,97% invece plot value array fa la stessa cosa ma va a invocare questa funzionalità di matplotlib scusate bar crea un histogramma a barre quindi gliele facciamo creare con 10 barre gli passiamo il prediction array gli diciamo qual è il colore poi andiamo a modificare anche qui l'etichetta a scrivere un po' di cose ripeto questo è un dettaglio secondario che non è l'oggetto dell'esercitazione per cui ci potete guardare se però avete qualche problema ne riparliamo ve lo spiego adesso direi che vale la pena utilizzarlo e andare avanti ok va bene però non sono cose particolarmente difficili se volete andare a scoprire quali sono gli errori potete semplicemente prendere il vettore delle predizioni andate a confrontarlo con il vettore delle label effettive dove c'è una difformità gliela fate stampare l'indice e trovate tutti gli errori in questo modo trovate altri errori come questo e vi accorgete quale dove la rete ha sbagliato ok va bene qui c'è un semplice un'ulteriore cella in cui vengono plottate 15 immagini di test quindi viene generato un plot con 5 righe e 3 colonne e quindi un numero di immagini che pari al prodotto tra il numero di righe e il numero di colonne e viene appunto generata una figura che ha una certa dimensione che è appunto il numero di righe e il numero di colonne poi viene generato appunto un subplot e per ogni elemento del subplot viene invocata quella funzione plot image in cui gli passiamo la predizione le vere label e lui va a prendere da lì l'immagine corrispondente e fa quella cosa che noi abbiamo visto per la singola immagine lì sopra e la stessa cosa facciamo per value array cioè facciamo invochiamo quelle funzioni che qui sopra abbiamo visto per una singola immagine per le prime 15 giusto per vedere che cosa succede e lui ci crea questa griglia guardate stampa un po' di cose e ha creato 15 immagini vedete che qui è di tutto e molto sicuro questo sono le prime le prime 15 del dataset di testing e vedete che è sicuro al 100% praticamente sempre qui è sicuro al 99% che sia un 1 vedete che gli histogrammi hanno una sola barra praticamente se fosse più indeciso avrebbe qui delle altre barre no? qui invece vedete subito che è praticamente sempre al 100% sono degli esempi ok? però se andate a selezionare delle immagini come quella sopra che ho appunto selezionato io prima e trovereste una situazione di questo genere ok queste sono tutte cose di visualizzazione che vi aiutano un po' a capire la natura del problema noi gli abbiamo passato al modello tutto il batch di 10.000 immagini di test uno potrebbe dire ma se me ne arriva una alla volta allora se arriva una alla volta voi potete chiaramente facciamo l'esempio appunto di prendere io uguale a 8 diciamo la nostra immagine e quindi la chiamiamo img è test image di 8 va bene? e la nostra etichetta è test label di 8 facciamo stampare la shape di image quindi è una singola immagine 28x28 ma lui gli abbiamo dato prima 10.000x28x28 allora come facciamo a dargli un singolo 28x28? allora siccome che i modelli di Keras sono ottimizzati per fare proprio anche delle predizioni in batch quindi su un insieme di esempi tutti insieme anche se usiamo una singola immagine dobbiamo andarla aggiungere a una lista a una collezione di immagini quindi quello che si fa per fargliela digerire diciamo è prendere la nostra immagine e aggiungergli una dimensione cioè come dire questa è la prima immagine di una collezione di un'immagine no? e vedete che infatti lui dice ok il tuo batch è anziché prima era 10.000x28x28 adesso è 1x28x28 questo lo facciamo con numpy expand dims si può fare in 10 modi diversi ma insomma questo è uno dei tanti va bene? quindi diciamo prendi l'array img e aggiungici una dimensione a questo punto possiamo fare la predizione sul singolo che in realtà è una predizione su un lotto di un solo elemento di un batch di un solo elemento invochiamo probability model punto predict su questo lotto da un solo elemento gli facciamo stampare che cosa che cosa ho ottenuto e quello che ottiene l'abbiamo mandato appena in esecuzione direi lo rifacciamo no? ecco perché appunto ci genera un errore vediamo un po' che cos'è che non funziona c'è qualcosa che mi riferisco un po' che cosa è successo? che lui ha fatto la predizione vedete ci ha stampato di nuovo il vettore della predizione chiaramente se andate a vedere la entry più alta è questa questa è l'etichetta vera questa è quella che lui ha predito e qui mi potete sbizzarrire di nuovo prendere una singola immagine e fargliela e fargliela predire questo è quanto ovviamente ci sono tante cose che varrebbe la pena andare a vedere una curiosità che potrebbe venire alla mente dire ok questa è una rete neurale quindi un classificatore di tipo non lineare cosa succede se noi proviamo a utilizzare un classificatore lineare per risolvere lo stesso tipo di problema ad esempio prendiamo la regressione logistica e in realtà la regressione logistica risolve questo problema abbastanza abbastanza bene non è così lontana è un dataset che ha delle caratteristiche di separabilità discrete per cui se andiamo a prendere il nostro scikit-learn per i nostri modelli lineari linear model importiamo la regressione logistica importiamo anche le metriche quindi accuracy score confusion metrics le confusion metrics le potete utilizzare chiaramente anche per le predizioni delle reti neurali ma qui diciamo la utilizziamo per andare a vedere un po' di cose come abbiamo fatto all'epoca per l'esercitazione sulla regressione logistica x-train sono le nostre immagini di training quante ce ne sono e il primo quindi saranno 60.000 e ognuna sarà 60.000 per 784 o meglio per 28 per 28 e quindi a questo punto quello che dobbiamo fare scusatemi questi sono 784 quello che dobbiamo fare è fare appunto andiamo andiamo le stampare le vediamo insieme lo mandiamo in esecuzione allora qui quello che viene fatto è questo viene creato il dataset di addestramento anzi prima ve lo dico così almeno ci ragioniamo insieme lo vediamo insieme dopodiché noi andiamo a prendere quelle che sono le nostre train images ok le nostre train images erano delle immagini 28x28 giusto ok qui viene andata presa viene presa la dimensione orizzontale e verticale delle nostre immagini 28x28 ok quindi quello che viene fatto viene creato un dataset di addestramento andando a fare un reshape di tutte le 60.000 immagini in cui andiamo a creare dei vettori 28x28 è l'equivalente del layer flatten della rete neurale stiamo facendo ok però lo facciamo noi a mano a mano tra virgolette lo facciamo fare a NumPy per poter dare in input quel vettore al modello di regressione logistica ok quindi se voi mandate in esecuzione questa cella qui ci sono un po' di print che vi aiutano adesso ci mette un po' perché sotto poi andiamo a vedere fa anche l'addestramento però vedete già ha fatto il print 60.000 28x28 diventa 60.784 cioè questa trasformazione che noi abbiamo fatto va da qui dove era 60.000 28x28 a qui che è 60.000 per 784 questo è quello che abbiamo fatto nel frattempo lui sta andando sta facendo l'addestramento della regressione logistica un pochino di ci vuole comunque e ci vuole anche perché diciamo la rete neurale deve ci vuole anche per addestrare la rete neurale però insomma il framework è ottimizzato è per fare l'addestramento quindi se poi sfruttate le GPU lo è ancora di più quindi sarebbe interessante dire qualcosa sul confronto dei tempi di esecuzione comunque adesso in attesa di vedere che lui che cosa ha combinato la stessa cosa la facciamo per il dataset di test quindi di nuovo questi passaggi servono a qualche cosa a spianare le matrici 28x28 in vettori e poi andiamo a creare il nostro modello il modello logistic regression gli abbiamo passato come solver quello che viene chiamato saga che è stocastic average gradient model fit a cui passiamo x train y train e le nostre predizioni le otteniamo con model predict questo l'abbiamo visto e pari pari a quello che abbiamo visto nell'esercitazione sulla regressione logistica che facciamo a suo tempo quello che cambia è il data safe va bene ecco lui ha terminato andiamo a vedere un pochino la matrice di confusione e andiamo a vedere qual è l'accuratezza ecco qua questa è la matrice di confusione vedete che riesce a fare tutto sommato un buon lavoro non male ci sono un po' di errori per esempio scambia parecchie volte un 5 per un 3 molte volte un 5 per un 8 anche c'è anche un 2 con l'8 spesso volentieri qui vedete un po' di cose che non riesce a risolvere e se andate a vedere l'accuratezza che è questa gliel'abbiamo fatta stampare con la funzione a che ora si scordi abbiamo passato y test e y predetto quindi la verità e quelle che sono le previsioni su quel dataset e l'accuratezza è del 92% che non è affatto male però la rite neurale ha fatto il 97,7% quindi comunque lì si comincia a vedere che la differenza c'è va bene vi volevo solo far vedere un attimo anche magari la documentazione di RSI tensorflow vediamo se la riusciamo a condividere benissimo allora questa è una delle pagine che se andate a cercare di Keras che vi dicevo appunto è diciamo da sopra poi ci sono altre pagine di documentazione di tensorflow però se andate a vedere Keras che è quello che abbiamo utilizzato vedete che ci sono varie nella documentazione spiega i modelli i modelli vi fa vedere per esempio tutte le API con cui potete accedere ai vari layer e se andate a vedere i vari layer vedete ce ne sono di vari tipi ci sono i layer di attivazione i layer che inizializzano i pesi i layer convoluzionali che servono per creare le reti convoluzionali di normalizzazione di vario tipo poi ci sono i core layers che sono quelli diciamo per esempio denso che abbiamo utilizzato noi input eccetera ma anche qui tante cose da andare a vedere quindi magari date un'occhiata se avete la curiosità se andate a vedere poi anche sotto la voce ottimizzatori trovate per esempio tutti gli ottimizzatori che sono disponibili e che sono implementati qui dentro e vedete sono tantissimi da un semplice tra virgolette stocastic gradient descent rmsprop o adam che sono quelli che vi dicevo i più famosi in letteratura sono o questo o questo anche adagrad e adadelta che sono degli adattativi più altri che francamente in un momento alcuni non saprei neanche dirvi di preciso bisognerebbe andare a vedere perché non me li ricordo o non li conosco ma se andate a vedere quello che abbiamo utilizzato noi vedete vi dice la classe adam che ha a sua volta tutta una serie di parametri vedete di default lui ha un learning rate pari di 10 alla meno 3 ma anche questo lo potete cambiare volendo quindi sono tutti i parametri che potete andare a modificare questi sono dei parametri suoi che servono appunto per effettuare quel quel calcolo di quelli che vengono chiamati momenti e tutta una serie di parametri vedete vi dice che implementa questo algoritmo che si chiama adam che è stato pubblicato nel 2014 quindi 10 anni fa ed è un metodo di disceso del gradiente stocastico e voi sapete a questo punto che cos'è basato su una stima adattativa di quelli che sono chiamati i momenti del primo secondo se andate a vedere altri altri metodi avete altre ottimizzazioni poi se se guardate qui sempre nella documentazione trovate i layer ma trovate anche tutte le metriche ma trovate anche le loss guardate quante loss ci sono già implementate la classe binary cross-entropy se avete un problema di classificazione binaria noi abbiamo utilizzato la la categorical cross-entropy function in particolare quella sparsa questa qui è quella per il one-hot encoding se avete delle classi codificate one-hot encoding questa è quella che abbiamo utilizzato noi ma ce ne sono tantissime ce ne sono altre specifiche per la regressione guardate cosa c'è vedete min square error e min absolute error questi sono quelli che noi abbiamo visto a lezione che vi ho detto sono i più utilizzati sono già implementati non dovete chiaramente fare nulla che andarli a specificare ma ce ne sono tantissime altre e ognuna si porta dietro ovviamente una letteratura conseguente e insomma questo è come sempre l'invito è andare a vedere oltre a questo c'è proprio la documentazione anche di TensorFlow che è ancora più ampia perché chiaramente qui si appoggia a questo strumento sopra a TensorFlow ma non solo perché se andate a vedere appunto vi dicevo prima vedete che vi dice che è anche disponibile adesso oltre a che per TensorFlow per PyTorch per Gets che sono altre due librerie di deep learning e anche qui insomma volendo potete approfondire e e vedere di capire qualcosa di più allora io se ci sono domande ovviamente rispondo volentieri alle domande intanto però direi che con questo concludiamo la panoramica sull'esercitazione anche qui abbiamo visto una delle tante possibili cose che ci sono ma è probabilmente uno spunto per approfondire per incuriosirvi se volete prepararci anche su questo con questo un progetto ovviamente capite bene che da qui potete fare diverse cose ma va benissimo anche qualunque degli spunti che abbiamo visto nell'esercitazione delle volte scorse e con questo concludiamo la panoramica delle esercitazioni perché poi rimane adesso una lezione in cui vedremo di terminare alcune cose sull'addestramento delle reti neurali che ancora di cui non vi ho parlato più in dettaglio ma fondamentalmente con questo concludiamo l'esercitazione siamo arrivati praticamente quasi in fondo alle lezioni poi la prossima settimana concluderemo anche la parte di teoria questo è quello che pensavo di farvi vedere insomma abbiamo visto parecchie cose se andate a riprendere i notebook di queste esercitazioni quindi penso che ci sia quello che vi serve per poter approfondire se avete qualcosa poi che vi interessa che vi incuriosisce o da riuscire come punto di partenza per preparare un progetto o magari per capire meglio alcune cose va bene intanto io direi che blocchiamo la registrazione poi se c'è anche se se c'è qualche domanda sia nel frattempo ma anche chiaramente dopo poi possiamo parlare