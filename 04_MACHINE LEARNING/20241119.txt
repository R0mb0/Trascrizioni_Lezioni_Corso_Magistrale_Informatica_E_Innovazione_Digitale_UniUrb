Benissimo. Allora, bentornati intanto. Oggi andiamo avanti sulla parte che abbiamo iniziato ieri sull'apprendimento non lineare. Quindi diciamo la strada intanto relativa all'apprendimento non lineare che abbiamo tracciato, vi ricordo, è quella di cercare di iniettare noi della non linearità. Quindi sia nel problema di regressione che in quelli di classificazione abbiamo detto se uno ha un'intuizione e ce l'ha perché per esempio è riuscito a visualizzare il dataset e quindi visualizzare il problema e come in questo caso rendersi conto che un confine decisionale di questo tipo, che è un'elisse, è quello che potrebbe risolvere il nostro problema, allora può cercare di trasformare le feature con delle funzioni non lineari, introducendo un certo numero F1, F2 fino a FB, quindi B funzioni non lineari, e componendo un modello come combinazione lineare di queste trasformazioni non lineari. Il prerequisito è ovviamente avere la possibilità, uno, di visualizzare per capire meglio il problema, visualizzare che cosa succede, due, anche quando lo avete visualizzato non è affatto scontato, vi dicevo ieri che riusciate a capire quali tipi di funzioni non lineari siano le più adatte. Però diciamo per il momento il setup è questo, ragioniamo su questo tipo di problema e abbiamo visto che se riusciamo a introdurre delle non linearità che siano ragionevoli si riescono a costruire delle cose che effettivamente non sono banali a prima vista e l'altra cosa che abbiamo visto è che comunque il framework teorico in cui ci siamo mossi è quello di trasformare queste feature in modo che una volta che lavoriamo nello spazio trasformato, nello spazio trasformato il classificatore di fatto mantenga un confine decisionale lineare, oppure il regressore sia un regressore lineare. Questo è quello che abbiamo visto ieri, sia per quanto riguarda la regressione sia per quanto riguarda la classificazione, quindi l'apprendimento con supervisione. La stessa medesima cosa può essere fatta sui problemi di tipo non supervisionato. Allora su questo vi anticipo, spenderemo solo adesso qualche minuto, adesso vi dirò un paio di cose, non avremo tempo di andare in profondità, vi dico semplicemente due cose, ma tenete presente che anche questo è un'area molto attiva sia della ricerca ma anche da un punto di vista delle applicazioni, è un'area che già è uscita se vogliamo dalla ricerca per andare nel mondo appunto delle applicazioni di diverso tipo, che diciamo vi dà anche l'idea del fatto che comunque c'è un certo spermento anche in quest'ambito, non solo in quello, perché solitamente diciamo l'ambito della classificazione è quello che è probabilmente il più immediato, per cui si sente molto molto spesso nominare, però anche anche l'ambito dell'apprendimento non supervisionato è un qualcosa in cui si sono fatti molti passi avanti e non avremo non abbiamo tempo per poterli andare a vedere, però vi dico giusto in linea di massima qual è la traccia che viene seguita. Allora noi quello che vi dico che è possibile fare, noi abbiamo visto se vi ricordate l'autoencoder lineare, come tecnica per andare a trovare una trasformazione, quindi un sottospazio lineare su cui proiettare i nostri dati, va bene? E l'abbiamo visto, no, con una serie di impostazioni che partivano dal concetto di identificare una base, poi da questa base utilizzarla per una rappresentazione perfetta oppure una rappresentazione non perfetta, quindi con perdita di informazione, con perdita di dimensione, va bene? La stessa cosa, lo stesso percorso possiamo farlo andando nella direzione di costruire un autoencoder non lineare, il che significa che andiamo a fare una proiezione che non è più una proiezione lineare, ma che utilizza delle trasformazioni non lineari, quindi andiamo a perdere dimensione, andiamo a ridurre la dimensione perdendo informazione con delle trasformazioni che non sono più lineari. Molto brevemente qui, intanto vi ricordo cosa abbiamo fatto con il modello lineare, avevamo una serie di punti in uno spazio a n dimensioni, che era questo, questo insieme di punti l'abbiamo chiamato x1, x2 fino a x con p grande. Quello che avevamo fatto, avevamo detto, beh, lo scopo è proiettare quei dati in uno spazio a dimensione k, dove k chiaramente è minore di n. Questo spazio, sottospazio a dimensione k, era un sottospazio che, diciamo, veniva spazzato, cioè identificato, costruito, a partire dalle colonne di una matrice c, che è una matrice di dimensione, vi ricordate, n per k, in cui mettevamo i vettori di quella base. A questo punto la codifica come l'avevamo fatta? Avevamo detto che c trasposto per x era la codifica, cioè la proiezione su quel sottospazio. Per cui partivamo da un vettore che aveva x, che aveva una dimensione che apparteneva a rn, e arrivavamo a ct per x, che chiaramente è un vettore che ha dimensione n per k, scusatemi, k per n per n per 1, cioè k per 1, cioè la versione di questo vettore appartiene a rk, dove rk chiaramente è una, k è una dimensione inferiore a n. Dopodiché abbiamo detto c'è anche la fase di decodifica, cioè io posso andare a ritrasformare uno spazio originale, di nuovo moltiplicando il vettore che ho ottenuto, che è il vettore v, lo potevo moltiplicare per c, premoltiplicare, e quindi di nuovo ottenere un vettore in cui avevo n per k moltiplicato per un vettore che ha dimensione k per 1, ci restituisce un vettore a n per 1, che di nuovo appartiene quindi a rn. Quindi siamo tornati nello spazio originario. Chiaramente ci torniamo perdendo informazione. Allora, la composizione tra il modello di codifica e il modello di decodifica, vedete queste sono tutte trasformazioni lineari, quando in algebra lineare si avvita una moltiplicazione di un vettore per una matrice, è una trasformazione lineare. E quindi lo vedete da qua. E questa, la composizione di questi due modelli, l'abbiamo chiamata modello dell'autoencoder lineare. Bene? Il modello dell'autoencoder lineare, quindi lo possiamo descrivere come la composizione di che cosa? Della fase di codifica, l'encoding, che è questa, e poi la componiamo con la decodifica e otteniamo il nostro modello totale. La composizione è proprio una composizione di funzioni, va bene? All'inizio eravamo partiti dal fatto di dire, ok, assegniamoci, quindi le basi che definiscono lo spazio a dimensione K, le assegniamo noi. Poi ci siamo anche domandati, non è che per caso troviamo il modo di ricavarle come parte del processo, una volta che abbiamo il dataset, come parte del processo di apprendimento non supervisionato. Per cui l'abbiamo messo insieme al vettore dei pesi come le variabili che vogliamo ricavare, minimizzando un'opportuna funzione di costo. Quindi la possibilità di fare proprio il tuning di questa matrice ci permette di produrre uno sottospazio lineare che ha lo scopo di rappresentare il dato. Cioè noi vogliamo il più possibile che questo che è il dato codificato, premoltiplicato per C, ci restituisce il dato nello spazio e nelle dimensioni, e vogliamo che sia il più possibile vicino al dato da cui siamo partiti. E questo implica, abbiamo visto che C per C trasposto deve essere circa uguale alla matrice identità, e abbiamo detto, beh, come riusciamo a farlo? Beh, prendiamo il nostro modello, lo andiamo a confrontare di nuovo con XP, di nuovo è una funzione di costa ai minimi quadrati, la sappiamo minimizzare, e questo è l'autoencoder lineare. Breve riepiloga anche dell'autoencoder lineare, e vi dico molto brevemente che cosa si può fare nel mondo della non linearità. Vengono chiamati autoencoder in generale, questi oggetti, sono degli oggetti che hanno come scopo quello di costruire non più un sottospazio lineare, ma di andare a identificare quello che i matematici chiamano, diciamo, una superficie in generale non lineare, una superficie che non necessariamente è un piano, ma può essere una superficie qualunque non lineare, quello che i matematici chiamano una varietà, manifold, solitamente viene tradotto come varietà, sulla quale andare a proiettare il dato. Cioè mentre nell'autoencoder lineare questo dato lo proiettiamo su un piano, su un iperpiano, quindi su una superficie lineare, nel caso non lineare lo proiettiamo su una superficie che lineare non è. E quindi rimpiazziamo i modelli dell'encoder e del decoder con una versione non lineare. E questo lo facciamo definendo un modello di encoder che prende come input il nostro dato x, ha un insieme di parametri che chiamiamo θE e va a fare che cosa? Vedete, prima cosa avevamo? Vi ho detto, avevamo un prodotto tra una matrice e un vettore, che vi ho detto è una trasformazione lineare. Qui invece abbiamo una funzione che è generica, F. Non sappiamo qual è. Se io so come è fatto, oppure l'idea che ho di dove voglio andare a portare la mia proiezione, quindi so che la miglior soluzione è una superficie di un certo tipo, allora posso codificare questa informazione qui dentro, andando a iniettare della non linearità, quindi scegliendo un'opportuna trasformazione non lineare di x, esattamente come abbiamo fatto per la regressione oppure per la classificazione. E allo stesso modo devo costruire una funzione non lineare che mi permetta di tornare dallo spazio non lineare allo spazio originale. Questo non è banale, motivo per cui anche andiamo, non andiamo ulteriormente nel dettaglio, ma in linea di principio è un qualcosa che può essere fatto. In generale, quindi il modello risultante, che è il modello dell'autoencoder non lineare, è questo, cioè la composizione di queste due funzioni. Io trasformo x secondo una funzione non lineare e poi ritrasformo il risultato per ritornare nello spazio originale con possibilmente perdita di informazione, andando a minimizzare, quindi quello che voglio andare a minimizzare è costruire tutti i parametri che definiscono queste funzioni in modo da minimizzare lo scarto tra questo e che cosa? E il punto di nuovo xp. Scusatemi, x mettiamo. Quindi ovviamente quello che posso fare è costruire come al solito questa e andare a minimizzare. Il risultato è che nel momento in cui, se vogliamo proprio sintetizzarlo, questo è quello che facevamo con l'autoencoder lineare, avevamo questa distribuzione di punti in due dimensioni e li proiettavamo su questa varietà lineare, su questa superficie lineare. Se io mi trovassi in una situazione di questo genere in cui i punti sono questi, questa è la distribuzione di punti, e riesco a capire che appunto la miglior superficie su cui proiettare questi punti non è una retta chiaramente, ma è un qualcosa che ha questo andamento e io riesco a iniettare della non linearità, quindi a trasformare questi punti in modo da finire su questa superficie, riesco a costruire questo autoencoder. Però capite bene che non è banale. Non è banale come non è banale andare nella direzione della classificazione non lineare o della costruzione di una regressione non lineare facendo tutte queste cose a mano. La cosa più interessante, vi dicevo prima della ricerca di questi ultimi anni, è che tutto questo in realtà, esattamente come anche nella classificazione o come nella regressione, si riesce a fare andando non a costruire noi manualmente queste funzioni, ma facendosi che, tramite l'utilizzo di opportuni strumenti, di cui cominceremo a parlare, a partire già da oggi, il sistema di apprendimento stesso riesca ad estrarre e a trasformare le feature. Quindi le riesca a trasformare secondo le non linearità. E questo è il principio di funzionamento delle reti neurali, ma non solo, anche di altri modelli non lineari, perché non esistono solo le reti neurali, ma ad esempio gli alberi. Gli alberi non avremo tempo di vederli nel corso, però se voi avete mai sentito parlare di un albero di decisione, forse ne avete sentito parlare, gli alberi di decisione fanno esattamente questo. Autonomamente riescono a costruire delle trasformazioni non lineari, a partire dal dataset di addestramento. Cercheremo adesso l'obiettivo, qui abbiamo concluso, diciamo, questa panoramica introduttiva. Da qui in avanti cerchiamo di costruire quelle che sono, diciamo, delle basi per poter capire alcuni aspetti di queste trasformazioni non lineari, che, ripeto, in questi sistemi avvengono in maniera autonoma, tant'è che si parla di, non più di feature engineering, ma si parla, come vi dicevo ieri, di feature learning, cioè l'apprendimento delle feature. La rete neurale o l'albero di decisione è in grado di apprendere quali sono le feature migliori e le trasformazioni di queste feature autonomamente, a partire dai dati. Ok, allora. Qui cosa c'era? Ah, un'ultima slide, che è questa. Un po'. Non riesco. Aspettate un attimo. Un po' un attimo, ricaricarla perché... Allora. Riluardo un attimo. Qui. Bene. Dovremmo esserci. Adesso. Allora, questa è l'ultima slide di questo blocco e, come vi dicevo, ve l'ho già scritto praticamente prima, abbiamo l'obiettivo di prendere il nostro modello di autoencoder che prima codifica e poi decodifica l'input e di fare in modo che questa, che non è più un'uguaglianza stretta, ma una similarità, sia il più possibile verificata per tutti i punti del nostro dataset. E quindi l'obiettivo è trovare quei parametri che rendono questa vera. Trovare questi parametri, prima devo definire una funzione di costo, una che viene molto utilizzata di nuovo, guarda, per caso l'abbiamo vista ormai comprata tantissime volte e a funzione di costi minimi fatati. Come vi dicevo, sia l'encoder che il decoder sono dei modelli che hanno dentro delle feature non lineari che devono essere determinate e possono poi essere composte e di nuovo concludiamo dicendo che l'ingegnerizzazione delle feature tramite analisi visiva è molto difficile anche su esempi semplici, quindi non è banale. Bene, allora detto questo adesso carichiamo un nuovo blocco di slide e cominciamo a parlare di appunto di apprendimento delle feature vediamo di capire un po' meglio cosa significa e come possiamo costruire un sistema che sia in grado autonomamente di andare a trasformare queste feature in modo da ottenere una non linearità. E quindi entriamo in questo blocco di slide che ci porterà verso la fase finale del corso. Allora quindi parliamo di apprendimento delle feature e intanto diciamo cos'è lo abbiamo già anticipato è il compito il task per cui un sistema è in grado automaticamente di apprendere delle feature che siano quelle adatte a partire dal dataset direttamente cioè anziché andare a ingegnerizzarle manualmente quindi iniettare noi della non linearità per cui vediamo l'andamento che assomiglia a una sinusoide diciamo ok trasformiamo con una funzione seno. Lì è il sistema è il modello che autonomamente riesce a stabilire le corrette relazioni non lineari. come fa? Beh è un compito che in prima battuta può sembrare tutt'altro che banale però è che si riesce a portare avanti e il punto di partenza è proprio al solito la scrittura di un modello generale che è questo modello che è riportato qua che è la combinazione lineare di una serie di funzioni f1 f2 fino a fb che sono funzioni non lineari ognuna con il suo vettore ognuna scusate con il suo coefficiente di peso w1 w2 fino a wb e un eventuale termine di bias. Questo è il modello che noi vogliamo costruire. Allora il punto è come possiamo scegliere la forma di f1 f2 fb e il loro numero e vi dicevo prima che ve l'ho detto più volte anche nella lezione di ieri ma ve lo ricordavo anche poc'anzi che se noi contiamo di ragionare semplicemente guardando qual è la caratteristica del problema andiamo un poco lontano andiamo un poco lontano perché anzitutto se viviamo in alta dimensionalità ci sono delle tecniche l'abbiamo viste alcune per proiettare in tre o due dimensioni però perdiamo informazione e quindi anche lì non è detta che sia un qualcosa che si riesce sempre a fare ma anche se voi riuscite a visualizzare perché il problema intrinsecamente è un problema che vive in due dimensioni oppure ce lo portate voi tramite una tecnica di riduzione della dimensionalità non è detta che avete risolto tutto guardate cosa succede in questa figura qua questo è una serie di punti e quella sotto tratteggiata è la funzione che diciamo per esempio può aver generato quellissimi se io vi chiedo questa che funzione è questa tratteggiata qualcuno me lo sa dire no cioè la non linearità tra tutte le infinite alternative che avete a disposizione è difficile dire di che tipo di funzione si tratta quindi è difficile andare a costruire una trasformazione non lineare direttamente ingegnerizzandola sulla base della solo diciamo della sola visualizzazione del problema la stessa cosa per un problema di classificazione supponiamo di avere una distribuzione di punti in due dimensioni e questa distribuzione di punti sia fatta così e anche qui andiamo un poco lontano perché come facciamo a dire qual è la funzione la funzione che descrive questi due confini decisionari queste due curve che tratteggiate che definiscono il confine decisionare di un classificatore che è in grado di discriminare correttamente tra queste classi chiaramente non ce la facciamo direttamente a partire da funzioni quindi a noi note matematicamente chiuse con una formulazione analitica ben precisa e quindi dobbiamo cercare qualche strada alternativa e per capire bene qual è la strada alternativa dobbiamo cominciare a introdurre dei concetti che ci aiutano e che piano piano diciamo ci permetteranno di definire alcuni aspetti del problema il problema è vi ricordo appunto la determinazione automatica della forma di un modello non lineare e del suo chiaramente tuning dei parametri perché poi di quel modello poi ne dobbiamo ricavare i parametri allora per capire questo concetto questo problema e poi altri concetti che ci porteremo dietro nel seguito nelle prossime lezioni è abbastanza utile utilizzare introdurre un concetto come una sorta di diciamo di metafora che qui è disegnata in questa figura questo concetto è quello della complessità di un modello allora la complessità di un modello che cos'è allora la potete vedere come una sorta di di manopola di disco su cui voi potete andare ad agire quindi spostandovi spostando questo selettore da sinistra verso destra quello che ottenete sono delle configurazioni di modello diverse ok e girando da sinistra verso destra corrisponde una configurazione a più elevata complessità cosa intendo dire con questo che se partiamo da minima complessità la minima complessità è chiaramente quella di un modello lineare intuitivamente cioè la complessità di un modello è la sua è legata a che cosa al fatto di essere anzitutto lineare o non lineare e nell'ambito del non lineare è legata ad esempio al numero di elementi che ci sono in questa sommatoria in questa somma pesata io andando ad aumentare b aumento la complessità del modello perché perché aggiungo termini non lineari il numero di parametri lo stesso parametri che magari sono interni a ogni singola funzione mi determina la complessità del modello cioè un modello è più complesso perché ha più parametri e riesce ad esprimere in maniera più accurata un certo andamento quindi intuitivamente possiamo dire che la complessità è un qualcosa che è legata al numero di parametri del modello e questo tra l'altro ci rende conto del fatto quando oggi non so se avete mai sentito dire si parla di modelli ad esempio quelli che vengono sono abbastanza sulla bocca di tutti che sono i modelli per esempio linguistici così diretti large language models non so se li avete mai sentiti nominare così ma sono di fatto quello che c'è dietro sono dei modelli di reti neurali che c'è dietro ai sistemi come appunto come chat GPT insomma o gemini o cose di questo genere sistemi di intelligenza artificiale applicati all'elaborazione del linguaggio naturale che generano del testo e lì dietro se voi andate a vedere un pochino quali sono le figure caratteristiche vi dicono se andate a cercare neanche per tutti perché per le ultime versioni non è neanche chiarissimo su quanti parametri su quali qual è il numero di parametri di questi di questi sistemi ma vi dicono qual è il numero di parametri della rete neurale che c'è dietro il numero di parametri è esattamente una misura della complessità del modello e per darvi un'idea questi sono parametri un modello lineare di una regressione lineare in una dimensione è un modello che ha due parametri che sono la pendenza e l'intercetta di questa retta ok chiaramente se aumentano le dimensioni aumenta ovviamente il numero di parametri e aumenta la complessità del modello ma è sempre lineare però rimaniamo in un ambito di pochi parametri se voi andate a prendere una rete neurale che lavora su un ad esempio un riconoscimento di immagine e anche lì ci sono modelli più piccoli modelli più complessi e il numero di parametri è collegato a questa complessità è una misura della complessità e lì cominciamo a parlare di sistemi che lavorano su input ad alta dimensionalità perché un'immagine è un input ad alta dimensionalità tra l'altro non l'abbiamo mai fatta questa riflessione perché un'immagine è un input ad alta dimensionalità adesso facciamo una piccola digressione perché se io devo dare un'immagine quindi ad esempio una fotografia in pasto a un sistema di machine learning che deve fare un riconoscimento ad esempio se lì dentro c'è un cane un gatto oppure un tavolino oppure un computer eccetera che cosa fa quel sistema è stato addestrato e prende come input l'immagine codificata come l'immagine come la vede un computer un sistema di elaborazione delle informazioni la vede come una matrice di pixel quello è il dato raw la matrice di pixel vuol dire che se voi avete la vostra immagine che è 100% pixel avete una griglia di 100% numeri in cui ogni singolo pixel ha un valore di intensità se la figura se l'immagine è in bianco e nero o toni di grigi avete un 0 fino a un valore massimo che può essere 255 in una scala solitamente perché sono 256 di solito viene codificato con 256 possibili livelli diversi per cui uno corrisponde il minimo corrisponde al bianco e l'altro al nero viceversa secondo me lo codificate e in mezzo avete tutti i vari toni intermedi ok e ognuno di quei pixel quindi avrà un valore e questo è come codificata quindi un'immagine al 100% è una griglia di 10.000 valori di questo tipo tra 0 e 255 se avete un'immagine a colori stessa cosa solo che anziché avere i toni di grigio quindi tutti gli intermedi tra il bianco e il nero avete tre possibili canali distinti rosso verde e blu componendo i quali ottenete un colore qualunque e quindi avete tre volte quella matrice quindi da 10.000 passate a 30.000 a quel punto avete un'informazione di potete costruire un vettore di 30.000 elementi ognuno di quegli elementi uno entri e appunto vi dà l'informazione su un pixel e la potete dare in input a un sistema che elabora quel vettore per fare il riconoscimento dice cosa in questi 30.000 valori numerici cosa c'è dentro riesco a capire se c'è un cane oppure un gatto o un tavolino o un paio di occhiali o una lampada ok e vedete che parliamo di sistemi ad alta dimensionalità questa è un'immagine 100% abbiamo 30.000 cominciate a fare un po' di conti che cosa succede se avete una fotocamera di un cellulare con un megapixel fa quasi ridere oggigiorno sono arrivate a quanto le ultime 12 10 megapixel fate un po' di conti moltiplicate per 3 e avete la dimensionalità del del dell'input di un sistema di riconoscimento automatico considerate che i parametri che stanno lì dentro in questi sistemi di riconoscimento ecco per esempio una rete neurale che fa image recognition quindi riconoscimento di immagini ha migliaia oppure milioni di parametri per poter gestire tutta quella che è il complesso meccanismo di riconoscimento di feature non lineare eccetera all'interno di questo sistema i sistemi di elaborazione del linguaggio naturale di cui parlavo prima scalano su livelli diciamo giganteschi perché hanno al loro interno miliardi di parametri queste sono le scale su cui stiamo ragionando tant'è che possono permettersi di addestrarli solamente poche grandi aziende al mondo perché hanno dei costi esorbitanti sia in termini di accesso ai dati ma anche in termini di elaborazione del potenza di calcolo dei centri di calcolo delle centri di calcolo è tutto un sistema che ovviamente deve poter gestire delle scale che sono decisamente enormi va bene ho fatto questa breve digressione perché insomma giusto per parlare dei numeri di cui parliamo dare qualche qualche punto ecco su alcune cose perché qui abbiamo introdotto abbiamo messo qualche numero perché abbiamo introdotto la complessità e la complessità è un concetto un po' astratto ma è sostanzialmente correlata al numero di parametri di un modello di machine learning ok crescendo l'uno cresce anche l'altro il che significa che io se mi posiziono con questa manopola tutto a sinistra sono nel caso più semplice ho un sistema che è a minima complessità cioè supponiamo di ragionare partiamo da un problema di regressione ok ho questa nuvola di punti se io costruisco un modello lineare è il modello a minima complessità cioè è il modello diciamo che fa l'assunzione più semplice se vogliamo per spiegare quella nuvola di t ecco perché ha complessità minima ed effettivamente è un modello che poi ha un minimo numero di parametri a parità di altri però io posso volendo cominciare a costruire un modello come quello che abbiamo visto nella lezione di ieri in cui vado a prendere ogni singolo punto e comincio a trasformarlo lo trasformo secondo una funzione non lineare ad esempio una funzione seno abbiamo visto ieri che funzionava era una cosa che funzionava abbastanza bene e lì introduco un'ulteriore funzione quindi aumento che cosa la complessità del modello perché comincio a trasformare le feature ora questo concetto di trasformare le feature qui non c'è qui le feature le prendo come sono faccio una proiezione un prodotto tra vettori e ho finito modello a minima complessità qui comincio a scalare in alto la complessità perché dico cominciamo a trasformare le feature cioè il mio modello comincio a trasformare le feature a questo punto che cosa succede? che io volendo posso ulteriormente andare a aumentare la complessità del modello perché posso cominciare a dire magari anziché una sola funzione sinusoidale ne posso mettere un'altra una funzione magari non lineare di altro tipo eccetera oppure vi ricordate ieri vi ho fatto l'esempio dei polinomi esempio di polinomi di grado 3 grado 5 eccetera aumentando il grado del polinomio aumentano vi ho detto il numero di termini del polinomio che avete e aumenta la complessità del modello e quindi aumentando la complessità del modello io vado a selezionare altri modelli che sono più alta complessità che ad esempio cominciano ad adattarsi ancora di più al mio dataset vedete? vedete come questa curva comincia a seguire questo dataset in maniera più più accurata fino ad arrivare a un modello di complessità diciamo massima rispetto a questo dataset che perfettamente o quasi perfettamente mi riesce a fare il fitting di questi punti e qui già evidenziamo una caratteristica fondamentale della complessità che quale? che non sempre aumentare la complessità significa fare un lavoro un buon lavoro da un punto di vista di quello che noi vogliamo ottenere perché noi vogliamo ottenere un modello del mondo che è rappresentato dai dati che noi abbiamo allora i dati che noi abbiamo sono potenzialmente affetti da rumore allora se il processo che ha generato quei dati è effettivamente una sinusoide supponiamo che sia se io vado avanti troppo con la complessità faccio un buon lavoro no no perché vado a schiacciarmi sul rumore cioè costruisco un modello che è molto accurato rispetto a nel descrivere il mio dataset di addestramento ma se il mio dataset di addestramento dietro aveva una distribuzione di questo tipo e poi magari c'è qualche rumorosità di rilevazione e io comincio a aumentare la complessità e a seguire qualunque scostamento perdo di vista quello che è effettivamente il giusto modello che mi descrive correttamente la distribuzione statistica dei dati e quindi è più aderente al processo che ha generato quei dati che può essere un processo di qualunque tipo ok questo concetto è quello alla base del concetto di overfitting overfitting e anche underfitting l'overfitting è di fatto che cosa è un processo diciamo è quella caratteristica di un sistema di machine learning per cui il sistema ha una complessità troppo elevata rispetto alla quantità di dati e alla tipologia di dati che ha a disposizione su questo ci torneremo però intanto ve lo anticipo cioè in questo caso è chiaro che questo modello è un modello che ha luogo overfitting perché o meglio qui non è ancora chiaro non ve lo dico non ve lo spiego diciamo ancora compiutamente ma ve lo posso anticipare perché se noi abbiamo fatto l'ipotesi che il processo che ha generato i dati corretto sia questo significa che se io addestro il mio sistema a una complessità troppo elevata ad esempio questa e vado a metterlo in produzione mi arrivano dei nuovi dati e magari non hanno questa rumorosità ce l'hanno di altro tipo comunque io voglio che effettivamente il mio modello aderisca a questo tipo di processo e mi arrivano dei dati di input qui queste oscillazioni così che lui fa per aderire per minimizzare l'errore quadratico medio e non è detta che quando mi arrivano dei nuovi dati di input facciano un buon lavoro anzi probabilmente lo faranno totalmente sballato perché sono qualcosa di molto lontano questo è il problema dell'overfitting quando la complessità è eccessiva e il vostro modello si schiaccia sui dati del training set perché da qualche parte nel test set vedrete che questo produce un errore troppo grande proprio perché voi avete cercato l'interpolazione perfetta qui esattamente è quello che si dice quando non riesce a generalizzare il concetto di generalizzazione è proprio questo è la capacità di astrarre dal dataset di addestramento un comportamento che sia adeguato a quello che si ritrova poi come test come nel mondo reale su tutte queste cose poi ritorniamo però intanto ecco le abbiamo le abbiamo cominciate a vedere e la metafora diciamo di questa di questa ideale selettore di complessità appunto che ci permette di capire di raffigurare proprio questo concetto di complessità io posso andare a aumentare la complessità aumentando ad esempio il numero di di parametri quindi per esempio il numero di termini di quella funzione di quelle funzioni f1 f2 fino a fb oppure il grado del polinomio se si tratta di polinomi il numero di parametri interni e aumento la complessità ma non necessariamente è detto che questo sia sempre un bene adesso su questo rifletteremo anche nelle slide successive nella parte successiva della lezione e poi anche la prossima ok il punto per capire bene questi concetti è che in linea di principio in realtà l'aumentare la complessità pagherebbe io non a caso vi ho parlato di rumore perché aumentare la complessità pagherebbe se noi disponessimo di un sistema ideale cioè di un modo ideale in cui abbiamo accesso a dati perfetti cosa vuol dire dati perfetti che sono completi cioè completo completi e privi di rumore allora cosa vuol dire completi completi vuol dire che ho accesso a una quantità infinita di dati quanti ne voglio e priva di rumore significa che noi ci possiamo completamente fidare delle coppie ingresso e uscita ingresso e uscita vuol dire problema di regressione input e risposta del sistema problema di classificazione input etichetta se noi siamo in questo caso allora sì effettivamente aumentare la complessità produce un modello migliore questo lo vedremo ok allora intanto quello che vi anticipo è che ci concentriamo senza ledere la generalità dei nostri ragionamenti sui problemi di regressione e di classificazione binaria non lineare però gran parte dei discorsi che faremo direi la totalità valgono anche per problemi di classificazione multiclasse e per il non supervisionato però noi per semplicità di trattazione ci concentriamo su regressione non lineare e su classificazione binaria però tenete presenti che questi stessi discorsi vengono anche per gli altri contesti di apprendimento automatico quindi di machine di nuovo partiamo dal nostro modello ipotizziamo di voler costruire un modello di questo genere e andiamo a analizzare cosa succede ok perché partiamo da questo perché è il modo in cui riusciamo più generale a gestire dei casi di non linearità ho tante funzioni di questo tipo e le compongo nella maniera più semplice tramite una una combinazione lineare di queste o questo è un modello in generale di regressione giusto ok se io voglio ottenere un modello di classificazione binaria beh basta che prendo il risultato di questo numero lo faccio passare attraverso una funzione sigmoidale o la tangente ipervolica al sigma e ottengo più uno o meno uno o meglio ottengo quella curva tra meno uno e più uno che è una curva s che posso usare mettendo una soglia a zero se sono tra meno uno e più uno e decido che tutto quello che sta sotto lo zero è classe meno uno tutto quello che sta sopra lo zero è classe più uno questo è il trucco che abbiamo utilizzato quando abbiamo costruito per esempio la regressione logistica e vale anche se noi parliamo di regressione non lineare e poi di classificazione non lineare ok adesso andiamo un pochino più in dettaglio su cosa significa avere dati reali e dati perfetti allora partiamo dalla curva sopra dalle tre figure che stanno diciamo nella parte superiore della di questa di questa slide quindi uno due e tre allora a sinistra partiamo dalla dalla uno che cosa abbiamo abbiamo un insieme di punti questo è un caso reale di punti che sono stati generati da un certo processo sono stati misurati e noi andiamo a fare ad esempio il fitting modello lineare ci spiega abbastanza bene questa questa curva questa scusatemi questa andamento di questa nuvola di punti e questo è il caso di un dataset reale ovviamente quello che vi ritrovate se andate a fare il fitting di un modello lineare su questo insieme di punti se noi assumiamo questi punti chiaramente se è vero che sono stati generati da un processo che ha un andamento di questo tipo sono affetti da rumore cioè per qualche motivo nel momento in cui siete andati a misurare la x e la corrispondente y c'è un qualche cosa che vi ha fatto saltare questo punto fuori da quella retta però voi avete ricostruito quella retta ok a partire da questi dati effetti da rumore in qualche modo disporre di dati non rumorosi significa dire che tutti questi dati tutti questi punti di fatto stanno su quella retta cioè il dato non è rumoroso siamo d'accordo? perché se il modello che l'ha generato è effettivamente la retta se i dati non sono rumorosi devono stare perfettamente su quella retta e quindi siamo nel caso 2 in cui tutti questi sono effettivamente si spostano da quei punti che prima a sinistra nella 1 erano verdi e qui a destra sono riportati in bianco e diventano verdi sulla retta cioè sono i punti che stanno sulla retta quindi questo è il caso privo di rumore dopodiché supponiamo di disporre anziché di 1 2 3 4 5 6 7 8 9 10 11 punti una manciata di punti di disporne non di 11 non di 100 non di 1000 ma potenzialmente infiniti punti e allora ci ritroviamo nel caso caso 3 vedete che avete un dataset che è perfetto potenzialmente infiniti punti voi potete fare mentre io qui non ho accesso a quanto vale la y per x uguale a x con 0 perché non ce l'ho quel punto ma qui ce l'avrò sicuramente e allo stesso tempo stanno tutti sulla retta per cui sono sicuro che non c'è rumore e la stessa cosa vale per il caso non lineare supponiamo di avere un andamento di punti che qui sta tracciato nella figura sotto figura 4 e questo andamento nel caso in cui noi riusciamo a ricostruire supponiamo di riuscire a capire qual è il processo che ha generato questo tipo di andamento ed è un processo che è descritto diciamo da questa funzione tratteggiata beh allora nella figura 5 se questi punti fossero privi di rumore starebbero tutti su quella curva e se fossero infiniti avrei la curva 6 e quindi significa che se io sto parlando di un dataset perfetto dal punto di vista della regressione questo è identificato da una funzione una funzione incognita perché io non la conosco che è una funzione y di x ed è una funzione continua continua perché ho infiniti punti senza discontinuità ed è priva di rumore perché di questi punti posso fidarmi assolutamente e quindi io vado in cerca di che cosa? di questa funzione y di x che è ignota noi non lo sappiamo quanto vale sempre questo è il problema attenzione perché poi non lo sapremo da un punto di vista analitico neanche se ci mettiamo su una rete neurale o un albero o un kernel o quello che volete però che sono tutti sistemi diciamo di tipo non lineare che riescono a ricavare quell'andamento ma in realtà non vi danno chiaramente la funzione non lineare che è questo arrendamento che è dietro a quest'andamento però sono dei sistemi per cui se voi gli date un punto x nuovo ignoto loro vi dicono qual è il valore del corrispondente di y ok perché abbiamo introdotto questo concetto di dati perfetti contro dati reali adesso si chiarirà tra poco ma prima di introdurlo vediamo anche oltre alla classificazione vediamo il caso di dataset scusatemi oltre alla regressione vediamo anche cosa succede nel caso della classificazione quindi prendiamo una classificazione binaria e quindi ad esempio abbiamo questo insieme di punti ok qui sono punti che sono evidenziati in rosso questi sono punti in blu e questo è il piano y uguale a 1 questo è il piano y uguale a meno 1 e in funzione di x1 x2 abbiamo questa distribuzione di punti ora questa distribuzione di punti supponiamo che sia separabile da questo confine decisionale lineare che cosa significa vedete qui in questo caso cosa significa questo è un dato reale questo è un dataset reale e cosa significa rimuovere il rumore da questo dataset vuol dire che questo punto rosso che qua stava dall'altra parte chiaramente deve scendere e essere assegnato vedete alla classe meno 1 viceversa questi due punti che stanno nella classe meno 1 dovrebbero appartenere alla classe più e quindi se io rimuovo il rumore mi ritrovo in una situazione di questo tipo questo è un dataset finito ma non rumoroso se io di questi punti ne disponessi di un'infinità sarei in questo caso vedete abbiamo queste superfici che sono continue questa rossa e questa blu perché sono con infiniti punti e queste due superfici sono separate da questi confini decisionali che sono delle rette in questo caso se fossi nel caso non lineare ragioniamo allo stesso modo abbiamo questa distribuzione di punti e chiaramente questo punto dovrebbe stare qua questo punto dovrebbe stare qua e avrei a questo punto scusate la ripetizione in questo caso un dataset non rumoroso per arrivare a un dataset perfetto dovrei avere infiniti di questi punti non rumorosi e questi mi definirebbero questo confine decisionale non lineare a partire da queste due superfici costituite da infiniti punti quindi nel caso del dataset per la classificazione binaria perfetto noi abbiamo un confine decisionale che è una funzione continua esattamente come nel caso della regressione y di x che non conosciamo che non conosciamo di ricordo ma che fatta passare attraverso un'opportuna funzione segno a quel punto vi dice più uno meno uno per ogni se riusciamo ad addestrarla perché parliamo di dataset perfetti perché vedremo ve l'ho anticipato prima che la complessità di un modello riesce a descrivere potenzialmente in maniera perfetta un dataset perfetto ma poi i dataset non sono perfetti del motivo per cui abbiamo i problemi di overfitting e generalizzazione questo è l'anticipo però intanto supponiamo di aver disporuto di un dataset perfetto ci torneremo sopra e vedremo quali sono le conseguenze di questa assunzione teorica puramente teorica e poi invece cosa succede in pratica ma su questo ci torniamo sopra più avanti dopo aver fatto un po' una digressione tra un parallelismo tra un insieme diciamo quello che che vi è noto dall'algebra lineare e che in qualche modo abbiamo già anche visto quando abbiamo parlato anche di riduzione della dimensionalità quindi sotto spazi lineari eccetera e invece una cosa che forse non vi è nota immediatamente ma che diciamo per analogia dovrebbe venire fuori in maniera abbastanza semplice facendo queste considerazioni allora supponiamo di avere un insieme di vettori ok abbiamo un vettore e lo vogliamo descrivere in termini di un sistema quello che abbiamo chiamato una base uno spanning set questo l'abbiamo visto a proposito della riduzione della dimensione questo spanning set sia composto da questi vettori che chiamiamo f1 f2 fino a fb va bene ok allora io quello che posso fare è dire posso costruire un vettore y che è la combinazione lineare dei vettori della mia base ok perché questa combinazione lineare mi restituisca in maniera esatta il vettore no abbiamo detto l'algebra lineare voi sapete che ci devono essere delle condizioni devono essere anzitutto il numero di vettori della base deve essere pari alla dimensione del vettore che voi volete rappresentare se io voglio rappresentare un vettore come in questo caso in tre dimensioni ok quindi un vettore che vive in uno spazio a tre dimensioni e ho bisogno di almeno tre basi cioè devo avere la componente lungo la x lungo la y e lungo la z se ho solo le componenti x e y descrivo dei vettori che stanno su questo piano non descriverò mai un vettore che ha una componente verso l'altezza giusto e l'algebra lineare ci dice non solo questo ci dice anche che i tre vettori che compongono la base devono essere linearmente indipendenti giusto? ok e quindi io posso costruire questo vettore y vedete questo qui che vi ho appena rievidenziato come opportuna combinazione ad esempio di f1 f2 f3 che rappresentano tre vettori del mio sistema di riferimento che rappresentano uno spanning set sono linearmente indipendenti perché sono ortogonali tra loro tra l'altro e facendo un'opportuna combinazione lineare per cui moltiplico per un peso w1 f1 per un altro peso w2 f2 per un altro peso w3 f3 che sono le tre coordinate ottengo il vettore y bene questo diciamo è noto dall'algebra lineare lo abbiamo brevemente rivisto anche quando abbiamo fatto la riduzione della dimensionalità e fin qui ci siamo tutti fin qua che un vettore lo possiamo descrivere tramite le sue coordinate ok è come quando io dico in uno spazio euclideo sponiamo su un piano io questo vettore sponiamo che questo valga 2 e questo facciamolo 1 e 1 qui le coordinate x y sono 1,1 ok e questo è un vettore a quindi a lo posso descrivere in questo modo sono le due componenti lungo x e lungo y quindi sono i due pesi w1 e w2 se io avessi quest'altro vettore ok ad esempio qua questo è 3 questo vettore b il vettore a b lo posso rappresentare come un vettore che ha componenti 3 e 1 cioè mi dice 3 lungo la x e 1 lungo la y in questo caso tramite queste due sole componenti io riesco a descrivere qualunque vettore del piano x y qualunque sono uno spazio a dimensione 2 e le due x e y sono associati dei vettori che sono ortogonali sono detti dei versori e va bene con quei due vettori riesco a esprimere quel sistema di coordinate qualunque tipo di vettore ci siamo fin qua stessa cosa se vado in tre dimensioni se io volessi andare in tre dimensioni è chiaro che se con solo con f1 e f2 rimango su questo piano e quindi non descriverò mai qualcosa che va ad esempio in basso ok tant'è che abbiamo detto perdiamo la riduzione della dimensionalità è proprio questo cioè significa che io vado a proiettare ad esempio solo su un piano e perdo delle informazioni bene ah fin qui nulla di nuovo riepilogo di cose che dovreste aver già visto o che comunque abbiamo richiamato quando abbiamo fatto la riduzione della dimensionalità ci siamo fin qui ok la cosa interessante è che un discorso del genere io lo posso fare anche a proposito di una funzione cosa vuol dire? vuol dire che io ho una funzione y che dipende da un insieme di variabili x1, x2 fino a x con n e posso domandarmi diciamo meglio se riesco a descriverla in termini di un insieme e chiamano insieme ricoprente di funzioni non lineari vedete che già qui si capisce dove vogliamo un po' andare a la strada che vogliamo intraprendere dove vogliamo andare a parare perché vedete questa è la scrittura che abbiamo utilizzato per costruire il modello quando abbiamo detto iniettiamo noi la non lineareta e non è un caso cioè la domanda che ci facciamo è possibile esprimere y di x come combinazione lineare di un certo numero di termini f1, f2, fb ognuno con il suo peso w1, w2, wb più eventualmente un termine che viene aggiunto per spostare questa funzione diciamo in alto e in basso è il termine di Bayer e quindi questa domanda è che ci facciamo ha un parallelo con il discorso dei spazi vettoriali cioè c'è un'analogia l'unica cosa che cambia è che qui abbiamo questo termine w0 in più di Bayer ma eventualmente lo potremmo anche andare a diciamo assorbire dentro quell'espressione adesso non non ci interessa quello so quello che ci interessa è notare un'analogia tra queste due rappresentazioni e guardate che se io vado a prendere ad esempio questa è un'analogia che è vera se io vado a prendere una funzione f1 ok e questa funzione f1 sia ad esempio questa sinusoide che moltiplico per un opportuno valore moltiplicativo di peso e poi ci sommo un'altra funzione f2 questa funzione f2 in questo caso x diciamo è un vettore con una sola componente siamo in una dimensione e ci sommo un'altra sinusoide ok che magari vedete ha una frequenza diversa quindi ha delle caratteristiche diverse però un'altra funzione f2 sempre sinusoidale con un opportuno valore di peso e poi ci aggiungo un'altra terza funzione sempre sinusoidale ma che ha una frequenza ancora più elevata e magari c'ha una fase anche diversa per cui parte in questo caso partono tutti da zero e tutti la stessa fase però potrebbe non necessariamente dover avere quella fase e la moltiplico per w3 io se voi andate a fare la composizione punto per punto su questo dominio di esistenza della x ottenete una curva di questo genere e quindi vedete che anche qui riusciamo ad esprimere una y di x che è questa in questo caso questa curva azzurra a partire da che cosa? Da tre elementi base allora il problema dell'apprendimento non lineare è tutto qui capire quali sono gli elementi base che ci permettono di arrivare ad approssimare una qualunque funzione a partire da un insieme di elementi base di funzioni non lineari perché se io riesco a fare questo ho risolto qualunque problema di regressione ma anche di classificazione perché classificatore binario basta che lo faccio passare poi attraverso questa y di x attraverso una funzione segno e ho fatto se ho fatto un classificatore binario ho fatto anche un classificatore multiclasse non lineare perché applico per esempio un one versus all ma se ho fatto questo ho risolto anche dei problemi di tipo non supervisionato perché ovviamente tramite questo riesco a costruire una proiezione su un sottospazio più bassa dimensione eccetera eccetera quindi il problema dell'apprendimento non lineare è questo trovare il modo di capire se è possibile che una cosa del genere avvenga cioè data una combinazione lineare di funzioni non lineari trovare il modo di approssimare con questa combinazione qualunque tipo di funzione ora questa che può a prima vista sembrare una cosa non non possibile di fatto lo è cioè la risposta è affermativa c'è il modo di arrivare a una formulazione di quel genere e che questo sia possibile ve lo dovrebbe dire un qualche cosa che nasce ben prima del machine learning ma nasce diciamo almeno un paio di secoli fa dalla fisica perché e non a caso qui ho parlato anche delle siglose perché se voi pensate ai segnali nel dominio pensate a un segnale nel tempo vedetelo come un segnale del tempo questo segnale del tempo lo possiamo rappresentare come la somma ad esempio di tante sinusone questo se avete fatto il corso un corso di elaborazione di segnali non so se l'avete fatto avete comunque se anche non l'avete fatto se avete mai sentito parlare per esempio di una scomposizione di un segnale con la serie di Fourier la serie di Fourier vi dice proprio questo che voi potete prendere un qualunque segnale e lo potete approssimare come una serie con una sommatoria infinita di segnali sinusoidali ognuno con la propria fase con la propria ampiezza eccetera il concetto di approssimazione di una funzione è un concetto che esiste quindi da tempo e che è stato in qualche modo messo a sistema nell'ambito del machine learning perché per andare nella definizione appunto di queste funzioni che rappresentano la regressione di una funzione arbitraria a partire da il punto chiave è che non lo vogliamo fare prima dicendo ma questa funzione ha questo andamento ma lo vogliamo fare in maniera automatica a partire dal dataset che abbiamo questo è il grosso traguardo a cui ci si vuole arrivare va bene allora per fare questo dobbiamo mettere insieme un po' di mattoncini quindi con un po' di pazienza questa lezione le prossime li cominciamo li cominciamo mettere insieme ok allora qui abbiamo un esempio di cosa significa nel caso ad esempio qui sopra partiamo ecco di nuovo perché ho introdotto i vettori perché sono un parallelo dell'algebra lineare che abbiamo più o meno tutti più chiaro e che ci aiuta a capire meglio poi cosa succede quando passiamo dal mondo dei vettori al mondo delle funzioni ok quindi torniamo un attimo sopra al mondo dei vettori supponiamo di voler trovare i pesi w1 w2 wb che rendono vera questa questa espressione ok e quindi noi vogliamo abbiamo questo bel vettore che vi ho appena evidenziato il rosso che vive in un spazio tridimensionale abbiamo f1 f2 f3 che sono tre vettori che rappresentano la nostra base il nostro sistema di riferimento che sono questo rispettivamente e poi abbiamo questo e poi abbiamo anche quest'altro quindi ci muoviamo lungo le tre dimensioni e se io voglio utilizzare solamente una dimensione chiaramente descriverò solo vettori che stanno qui nel momento in cui aggiungo anche una seconda dimensione come vi dicevo prima andiamo a fare a ricoprire tutto questo spazio se aggiungo anche la terza dimensione mi muovo riesco perfettamente a fare a identificare la combinazione di pesi ad esempio minimizzando la funzione di costo minimi quadrati che mi permette di ricostruire questo vettore bene la stessa cosa la posso voler fare nell'ambito dell'approssimazione di una funzione arbitraria y di x che io vado ad approssimare lo vedete perché questo non è un uguale ma è un circa uguale come combinazione lineare di quanti termini b grande termini e di nuovo qui supponiamo che la funzione da approssimare sia questa io comincio a metterci un termine non lineare in questo caso una sinusoide perché diciamo siamo partiti da qua poi vedremo che tipicamente non saranno i sinusoidi che permettono di ragionare diciamo in maniera semplice solo su una dimensione ma vedremo quali sono questi blocchetti elementari però intanto ci aiuta a introdurre il concetto una sinusoide e una sinusoide chiaramente è questa nera vedete che è un'approssimazione abbastanza povera di questo segnale di questa funzione qui parlo di segnale perché mi viene a parlare appunto di approssimazione in serie di corriere esattamente come questo vettore è un'approssimazione povera di questo però se comincio ad aggiungerci una seconda componente come qui andavo a ricoprire questo spazio qui vado a ricoprire uno spazio più ampio e comincio ad approssimare meglio questa funzione passare da qui a qui significa muoversi in quel selettore della complessità di una tacca verso destra aggiungere un ulteriore componente significa vedete arrivare a un risultato che comincia a essere migliore allora la differenza tra il mondo dei vettori le analogie sono chiare una delle differenze tra il mondo dei vettori al di là del path e il mondo delle funzioni al di là di questo termine u00 è che qui riesco a fare una ricostruzione perfetta con un numero di basi che è quindi di vettori appartenenti al mio spanning set che è pari alla dimensione del mio vettore questo è un vettore a tre componenti mi bastano tre basi nel caso delle funzioni per arrivare a una ricostruzione chiamiamola perfetta o pressoché perfetta potenzialmente cosa mi servono? infiniti termini ecco qui la differenza grande tra questi due tra questi due mondi cioè io posso voler dover aggiungere un numero infinito di termini ciò nonostante questo è uno strumento molto potente per fare una serie di ragionamenti e nel momento in cui io voglia trovare questi pesi nel caso dei vettori che cosa potrei fare? potrei andare a costruire una funzione di costo chiamiamo g appunto sarà la funzione di w1 w2 fino a wb che ad esempio può essere che cosa? f1 per w1 più f2 per w2 più fb per wb da cui togliamo y e andiamo a prendere la norma di questo vettore la leviamo al quadrato e questa funzione di costo è minimo i quadrati e abbiamo e ricariamo i pesi nel caso delle funzioni nel caso delle funzioni possiamo fare una cosa analoga quindi possiamo costruire anche qui una funzione di costo che possiamo chiamare g dipende da w0 w1 fino a wb in realtà qui dentro ci potremmo mettere anche eventuali parametri interni a queste funzioni adesso supponiamo che non ci siano parametri interni per semplicità questa funzione di costo sarebbe che cosa allora intanto vi vado a scrivere il termine di scarto quadratico quello è abbastanza semplice io prendo w0 più f1 di x per w1 più f2 di x per w2 più fb di x per wb meno y di x ok scarto quadratico significa che prendo questa differenza al quadrato bene allora mentre qui è chiaro che ho una somma discreta di termini il problema qui sotto è che trattandosi di funzioni che cosa devo minimizzare questo è come se fosse un singolo punto giusto quindi ho questa funzione valutata su un singolo punto però io devo fare in modo che non sia solamente questo punto ma anche questo questo e questo che io devo andare a minimizzare quindi in realtà questa non è più una sommatoria discreta di termini ma cosa diventa quando non ho più una sommatoria discreta diventa in matematica un integrale esattamente quindi quella funzione di costo diventa se la volessi scrivere un integrale dove x appartiene al dominio di esistenza della vostra funzione di questa roba qua in dx ok adesso questo lo lasciamo qui non non non vi preoccupate più di tanto è giusto per per farvi vedere non andremo oltre nel dettaglio su questo però di fatto per farvi vedere l'analogia dove dove si fino a dove rimane dove ci sono anche chiaramente delle dissimilarità quindi nel momento in cui aumentiamo il numero di di componenti di base dell'insieme ricoprente il vettore viene approssimato fino ad arrivare in maniera perfetta all'approssimazione perfetta per le funzioni potenzialmente devo spingermi un numero molto elevato di termini però in linea di principio lo posso fare costruendo appunto questo tipo di di funzione di costo ok allora ci ho detto vediamo un altro no no ancora è ancora il grosso punto interrogativo che ci portiamo di ok allora adesso vi dicevo facciamo un passo avanti e parliamo di quella che viene chiamata la capacità di un semre ricoprente allora cos'è la capacità di un semre ricoprente è un qualcosa che vedrete la complessità è legata a questi fattori che andiamo a introdurre uno è la capacità come avevo detto prima se aumento il numero di termini di quella sommatoria aumento la complessità infatti quello che faccio nella fattispecie è aumentare la capacità del nostro insieme e la capacità è una delle componenti vedremo della complessità allora che cos'è la la capacità quindi di un insieme ricoprente la funzione diciamo partendo dal mantenendo il parallelo con i vettori l'approssimazione di un vettore dipende da che cosa dalla diversità dei vettori cioè i vettori devono essere indipendenti da un punto di vista lineare ok guardate cosa vi dicevo prima io partendo da un solo vettore riesco a dire a esprimere quest'altro vettore ma anche questo che ho appena evidenziato ma anche quest'altro quello che cambia è semplicemente il coefficiente moltiplicativo se però aggiungo anche quest'altra componente questa componente è ortogonale quindi è massimamente diversa perché è indipendente linearmente indipendente dall'altra io posso andare vi dicevo prima nel piano e descrivere qualunque vettore che sta nel piano se aggiungo un ulteriore componente arrivo alla rappresentazione perfetta in questo caso quindi la diversità e il numero mi descrivono la mia capacità di andare a descrivere qualunque vettore oltre a quello c'è la capacità di fare il tuning corretto dei parametri che chiaramente se sbaglio e quei parametri li assegno a caso e non riesco ad approssimare perfettamente questo vettore la stessa cosa vale per le funzioni la capacità di approssimare la funzione dipende da questi tre fattori primo fattore la diversità la diversità che nel caso dei vettori era il fatto che fossero linearmente indipendenti e qui è un concetto diciamo che lasciano un po' sospeso cosa vuol dire la diversità delle funzioni beh è la possibilità di andare a raggiungere dei termini che sono il più possibile diversi l'uno dall'altro questa sinusoide e questa sono diverse perché per esempio cambia la frequenza nel periodo della funzione e se aggiungo una terza funzione questa deve essere applicabilmente diversa dalle prime due ma non solo andando da sinistra verso destra partiamo da sinistra cambiando il coefficiente moltiplicativo io ottengo questa oppure questa oppure quest'altra se già ne metto due con i loro coefficienti moltiplicativi riesco a comporre questa oppure questa oppure anche questa solo cambiando vw1 e vw2 se aggiungo una terza funzione riesco a ottenere queste tre quindi aumentando non solo la diversità delle funzioni ma chiaramente anche il numero delle funzioni più sono meglio riesco ad aumentare quella che appunto viene chiamata la capacità dello spanning set ma non solo anche la possibilità di fare il tuning di quei pesi mi dà una misura di questa aumentata capacità quindi questi tre fattori mi determinano la possibilità di approssimare in maniera corretta la funzione quindi diciamo i primi due definiscono la capacità dell'insieme ricoprente il terzo è un qualcosa che invece ha a che fare con l'ottimizzazione quindi i primi due fattori diversità e numero di elementi mi definiscono la capacità di quello che poi sarà un modello e il terzo è legato all'ottimizzazione cioè la quanto noi siamo in grado poi di trovare l'insieme dei pesi ottimale come vi dicevo prima le funzioni che utilizziamo possono essere loro stesse parametrizzate e il fatto di parametrizzarle ci dà un'ulteriore possibilità di diversificarle e questo l'abbiamo visto implicitamente nell'esempio di prima se io prendo il seno di x quando vi ho detto una seconda componente la prendiamo diversa se aumentiamo la frequenza e per fare questo basta introdurre un parametro un parametro possiamo chiamare w e il seno di w x ottenete queste se w vale 1 è quella originale questa nera se w uguale 2 ottenete questa in azzurro se w uguale 3 aumentate la frequenza ottenete la curveggia il fatto di introdurre dei parametri interni quindi sono dei parametri oltre che vanno anche questi poi diventano oggetto dell'ottimizzazione vi aumenta però la diversità e quindi vi permette di ottenere di avere più carte da giocare nel processo di approssimazione la stessa cosa in qualche modo in qualche misura la potete fare nei vettori ma i vettori già sono un pochino più rigidi da questo punto di vista perché se voi avete un vettore x se volete utilizzare una versione parametrizzata di questo vettore x dovete moltiplicare per una matrice di rotazione adesso non so se queste le avete fatte nei corsi di geometria però se voi prendete una matrice fatta così questo non ci interessa per il machine learning più di tanto ma è giusto per vostra curiosità aspettate che la faccio tempo perché altrimenti ecco non volevo semplicemente spassarla faccio prima cancellarla scusate ok sperimento fallito allora se voi prendete una matrice di questo tipo una matrice 2x2 qui stiamo parlando di vettori nel piano quindi due componenti e fissate un valore di vw se vw uguale a 0 se andate a vedere questa è una matrice che ha sull'antidiagonale degli 0 e sulla diagonale degli 1 quindi vi fate la matrice identità e riottenete il vettore di partenza se fate rw per x se invece mettete vw uguale a π o qualunque altro angolo che voi vogliate ottenete delle rotazioni di π o di qualunque angolo voi vogliate di questo vettore nel piano quindi introducendo un parametro qui dentro riuscite anche lì a diversificare maggiormente il vettore però a noi non interessano tanto i vettori quanto le funzioni ovviamente ok negli spazi vettoriali se voi avete quelle che vengono chiamate le basi standard ok e ne avete un numero maggiore o uguale della dimensione del vettore che volete rappresentare quell'insieme viene chiamato approssimatore universale cioè se voi avete uno spazio tridimensionale i tre versori che vi dicono ortogonali x y e z vengono chiamati rappresentati anche approssimatori universali di quello spazio vettoriale le funzioni hanno degli approssimatori universali cioè hanno l'analogo di questo concetto allora ogni funzione y funzione di vettore x può essere in linea di principio l'abbiamo detto approssimato a una precisione arbitraria da un insieme ricoprente che abbia capacità massima la capacità l'abbiamo detto che cos'è legata determinata da questi fattori sono la diversità e il numero quindi se io riesco a identificare un insieme di funzioni che hanno una buona capacità sono molto diverse l'uno dall'altra e hanno la possibilità di essere composti insieme in un certo numero di termini chiaramente qui ne ho una varietà potenzialmente infinita di queste funzioni abbiamo preso l'esempio di quelle sinusoidali ma ne abbiamo cioè ne possiamo inventare quanto vogliamo però a quel punto abbiamo la possibilità di costruire se siamo stati bravi quello che viene chiamato un approssimatore universale poi va dimostrato matematicamente che è un approssimatore universale e potenzialmente l'abbiamo detto prima noi potremmo avere la necessità per arrivare a una precisione arbitraria cioè io fisso un certo valore di precisione per cui la mia funzione la voglio ricostruire a meno di un certo valore di precisione dove quel valore di precisione è il risultato di quell'integrale che vi facevo vedere prima possono volerci potenzialmente infinite funzioni mentre per i vettori vi dicevo prima ne basta un numero almeno uguale a dette da un punto di vista del machine learning ci sono un'infinita varietà di approssimatori universali quindi di insiemi di funzioni a partire dalle quali noi riusciamo a costruire un'approssimazione arbitrariamente precisa ma questi vengono diciamo categorizzati raccolti in tre macro categorie che si può dimostrare hanno appunto ci sono dei teoremi che dimostrano proprio il fatto che sono degli approssimatori universali cioè che riescono dato un numero sufficiente di termini ad approssimare in maniera arbitrariamente precisa una qualunque funzione questo è il concetto di approssimatori universali e queste tre categorie sono i cosiddetti approssimatori in forma fissa vengono chiamati e vi dirò poi cosa sono le reti neurali guarda caso e gli altri quindi qui abbiamo tre teoremi quindi c'è un rigore matematico dietro a queste a queste a questa categorizzazione perché abbiamo altrettanti teoremi in questo caso tre che vi dicono che ognuna di quelle categorie di funzioni a partire dalle quali vengono costruite includendo più termini di quelle tipologie di funzioni delle approssimazioni arbitrariamente precise ebbene questo è possibile farlo grazie appunto a queste teoremi la garanzia che abbiamo è di quei teoremi quindi questo vi dice che cosa che se voi prendete ad esempio una rete neurale avete la possibilità di costruire in maniera arbitrariamente precisa qualunque tipo di funzione che voi vogliate potenzialmente dato che cosa un numero di termini che non sappiamo ancora cos'è una rete neurale è composta da tanti blocchettini di un certo tipo di cui vi dirò qualcosa a partire dalla prossima lezione stessa cosa vale per gli alberi o per l'altra tipologia noi partiamo da un piccolo elemento che rappresenta l'equivalente di quella funzione sinodidale e ne andiamo a comporre tanti e le reti neurali di fatto funzionano così c'è un piccolo elemento che è un neurone vedremo e componendo tanti neuroni di questo tipo insieme riusciamo a costruire delle approssimazioni arbitrariamente precise di qualunque funzione questo vale per le reti neurali ma per esempio anche per gli alberi per l'altra tipologia che sono cosiddetti approssimatori in forma fissa di questa lezione di oggi la sintesi sono due cose che mi preme che siano adesso ci fermiamo qui e la prossima volta andiamo avanti a partire da questo punto però ci sono due cose che mi preme che vi siano chiare quindi vi chiedo di rifletterci e se così non è di provare a ragionarci su e magari ne riparliamo uno è il concetto di complessità che poi vedremo legato alla capacità intrinsecamente e due quindi complessità come misura della capacità di approssimare bene un dato comportamento di una funzione e due il concetto appunto di approssimatore universale cioè il fatto che io possa approssimare una qualunque funzione in maniera arbitrariamente precisa quindi provate a riflettere su questo e a riguardare un po' di queste cose la prossima volta da qui partiamo per poi dire qualcosa in più su andare appunto dalla teoria dell'approssimazione al machine learning che su questa può essere costruita non so se avete delle domande no? non so da casa se avete domande altrimenti direi che possiamo intanto fermarci qui siamo? ok allora intanto per oggi blocchiamo qui la registrazione