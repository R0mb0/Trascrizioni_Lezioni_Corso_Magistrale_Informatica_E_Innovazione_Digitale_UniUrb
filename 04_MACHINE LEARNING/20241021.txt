Benissimo, allora intanto bentornati. Oggi questa lezione, inizieremo tra poco il nuovo blocco di slide e quindi il nuovo argomento. Però prima concludiamo, ero in debito, in particolare con una piccola parte dell'argomento della scorsa lezione che vi ricordo riguardava la regressione lineare. Vi dicevo ero in debito, avevamo lasciato da parte la formulazione matematica di quelle che vengono chiamate equazioni normali, che sono null'altro che la formulazione analitica della soluzione del problema della regressione lineare quando la funzione di costo è quella ai minimi quadrati. Abbiamo visto che la funzione di costo è minimi quadrati, vi ho detto, è una funzione di costo che per risolvere il problema della regressione lineare si presta molto bene per tutta una serie di motivi. Ovviamente con le sue, abbiamo discusso anche ovviamente quelli che sono i limiti di questa tipologia di funzione di loss function. Abbiamo anche detto che si può dimostrare che è una funzione di costo di tipo convesso e quindi può essere minimizzata, per esempio, con metodi del primo, del secondo ordine in maniera abbastanza agevole. Però vi ho anche detto che è uno dei pochi casi in cui abbiamo la possibilità di formulare in forma chiusa una soluzione. Siccome non capita spesso di avere per i modelli di machine learning questa possibilità di avere direttamente la soluzione in forma chiusa, è il caso di darci un'occhiata, non vedremo tutti i passaggi matematici, vi accenno un po' quelli che sono i principali punti attraverso i quali si può arrivare a questa soluzione, che ripeto la formulazione in forma chiusa, prende il nome di normal equations, cioè equazioni normali, ed è un'equazione matriciale, vedremo. Il tutto parte semplicemente dalla formulazione del gradiente della funzione di costo, funzione di costo che vi ricordo è una funzione del vettore dei parametri e che noi abbiamo visto essere formulabile come 1 su p, poi c'è la sommatoria per p piccolo che va da 1 fino a p, ah qui non ci sta, vediamo un attimo di scrivere. Allora, così partiamo da qua. Come dicevo, 1 su p grande, p grande lo scriviamo così, sommatoria per p piccolo che va da 1 fino a p grande di xp trasposto per vw, che è il nostro modello lineare a cui togliamo il valore yp elevato al quadrato. Questa era la funzione di costo ai minimi quadrati. Allora, per la derivazione delle equazioni normali, il primo passo è ricavare il gradiente. Allora, questo è un passo che, diciamo, si può andare a dimostrare in maniera abbastanza agevole, che può essere scritto in questo modo come la somma di una serie di termini che portano a queste... questa formula in particolare. Allora, qui ci sono delle regole di derivazione. Allora, non entriamo nel dettaglio perché bisognerebbe andare a studiare le regole di derivazione applicate a vettori o matrici, per quello che non vi faccio vedere tutti i passaggi. Però intuitivamente vedete che come nel regolo di derivazione per funzioni di una variabile, quindi diciamo delle normali funzioni, non andando a tirare in ballo appunto matrici o vettori, la derivata di una funzione quadratica si porta dietro un 2 e poi c'è la derivata di quella che è la funzione composta all'interno. Quindi non è un qualcosa di molto diverso da quello che ci potremmo aspettare. Però, diciamo, questo è un passaggio che potete anche prendere per buono. Dopodiché quello che si fa è si pone questa quantità uguale a 0. Mettendo questa quantità uguale a 0 e mettendolo in forma più compatta, cioè si riesce ad aggiungere, scusatemi, a una formulazione più compatta, ma il tutto passa attraverso la definizione di una matrice che viene costruita a partire dagli input ed è una matrice in cui andiamo a mettere qui il primo input, quindi il vettore con il primo input e con l'1 in testa. Quindi praticamente come se avessimo x1 trasposto con il cerchietto sopra organizzato in riga. Dopodiché qui andiamo a mettere x2 fino all'ultima in cui andiamo ad organizzare il piesimo input. Ok? Allora, questa matrice prende il nome, spesso e volentieri, di design matrix, matrice di progetto. È semplicemente un modo di organizzare in forma matriciale quelle che sono gli input del problema. Va bene? Ok. Perché si fa questa operazione? Perché scrivendo gli input sotto forma di questa matrice, quello che si può dimostrare, è anche qui un altro passaggio, che risulta abbastanza semplice se si ha un po' di dimestichezza con regole di arrivazione applicate a vettori matrici. Non è necessario, per cui non le faremo, non è, diciamo, strettamente indispensabile per il nostro corso, per cui, diciamo, possiamo saltare questo passaggio. E vi dico semplicemente che il risultato è che il gradiente, quello che abbiamo scritto prima in quella forma, in maniera molto compatta, utilizzando la matrice di design x, può essere scritto come due volte x trasposto, dove x è appunto la matrice di design che abbiamo appena introdotto, per x meno, scusatemi, per w, meno due volte x trasposto, per y. dove y è il vettore che contiene tutte, invece, tutti gli output. Vi dicevo, imponendo che il gradiente sia uguale a zero, quello che si ottiene è che deve essere chiaramente x trasposto per x per w uguale a x trasposto per y. Questa è un'equazione matriciale, in cui abbiamo diversi componenti, abbiamo l'incognita, che è w, e abbiamo invece questi termini, che sono noti, perché la matrice di design l'abbiamo costruita a partire dall'input, quindi questo è l'input del nostro problema di regressione, e questo è chiaramente l'outro del nostro problema di regressione. quindi significa che noi, imponendo la condizione di ottimale del primo ordine, cioè imponendo che il gradiente è nullo, arriviamo a un'equazione di questo tipo, in cui sappiamo esattamente quali sono gli input e quali sono l'output, e sappiamo che qui c'è l'incognita, ma questa è un'equazione che di fatto può essere risolta, nel senso che io posso andare a scrivere, cose di questo genere, posso andare a moltiplicare per l'inversa di questa matrice, il prodotto tra queste due matrici è una matrice, anche questa che ha dimensione n più 1 per n più 1, e posso moltiplicarlo per l'inversa, e quello che ottengo è, avrei questa matrice, cioè x trasposto per x, premoltiplicata per la sua inversa, e questo per definizione della matrice identità, che è moltiplicata per w, restituisce semplicemente w, anche qui ci sono un po' di passaggi di algebra lineare, prodotto vettore matrici, su cui andiamo un pochino lisci per andare direttamente al risultato, il risultato che a questo punto è premoltiplico, vi ripeto, per l'inversa di questa cosa qua, questa è l'inversa, e quindi a destra mi rimane solamente, andate a portarmi dietro, x trasposto per y, quindi ne discende da quella sopra immediatamente questa, quindi imporre il gradiente uguale a 0, per la funzione di coste minimi quadrati, significa giungere a questa, questa è l'equazione, sono appunto note come equazioni normali, di fatto si vede meglio anche in quella sopra, questo equivale, allora qui potete prendere la matrice di design, la premoltiplicate per se stessa trasposta, e poi ricavate la matrice inversa, poi moltiplicate di nuovo per x trasposto, e poi per il vettore y, e ottenete il vettore di peso e w. Qui sopra vi dicevo, nel passaggio precedente, che è questo, si vede meglio come questo è equivalente a risolvere un sistema lineare. Sistemi lineari, vi ricordo che in algebra lineare, possono essere scritti come qualcosa del tipo, matrice A per vettore incognito uguale vettore noto. Questa è la formulazione più generale di un sistema di equazioni lineari, n incognite, n variabili, e vedete che se andiamo a considerare, appunto vi dicevo, questa, la struttura di questa equazione, è esattamente di questo tipo, dove A è questo termine qua, questa è la nostra incognita invece, e questo è quello che è il vettore di termine cosi. Allora, tutto questo, ripeto, lasciando un po' sotto traccia alcune questioni matematiche, ci dice che cosa, arrivando direttamente al dunque, ci dice che noi possiamo, risolvendo un sistema di equazioni, a partire dalla matrice di design, che è quella che abbiamo costruito nei passi precedenti, e a partire appunto, quindi dall'input e poi dall'output, ricavare il vettore dei pesi ottimali, che va a minimizzare quella funzione di costo. Qui andiamo in cerca semplicemente dove il gradiente si annulla, perché sappiamo che la funzione di costo è convessa, quindi ammette un minimo, che non solo è locale, ma è anche globale, quindi possiamo tranquillamente operare in questo modo. Vi faccio anche notare che questa è appunto qualcosa che è molto simile a quello che abbiamo visto, quando abbiamo visto, parlato dei metodi del secondo ordine, in particolare il metodo di Newton, e vi ricordo che un singolo step del metodo di Newton conteneva al suo interno l'identificazione appunto, tramite lo studio della curvatura della funzione, l'identificazione del passo successivo di discesa come un qualcosa che coinvolgeva l'inversione di una matrice, se vi ricordate, della matrice siana. In particolare avevamo anche detto all'epoca che quello era equivalente a risolvere un sistema di equazioni lineari. Vedete che qui avviene la stessa cosa, abbiamo un'inversione di una matrice, solo che in questo caso è la matrice che deriviamo dall'input, quindi quella che abbiamo chiamato matrice di design, che viene premoltiplicata per se stessa trasposta, e questo equivale a risolvere un sistema di equazioni lineari, e tutto questo non è assolutamente, ovviamente, frutto del caso, ma di fatto le due cose coincidono, cioè un singolo passo del metodo di Newton conduce esattamente a questa soluzione. Quindi le equazioni normali di fatto rappresentano equivalentemente quello che è la soluzione di un singolo passo del metodo di Newton imponendo un metodo di tipo ottimizzazione locale su una funzione di costo che è questa. Allora, questo è quello che esattamente si fa quando c'è una regressione lineare con una funzione di costo di tipo ai minimi quadrati. Se non ci sono particolari vincoli, appunto, in termini di occupazione di memoria, si utilizza di fatto questa formulazione analitica, cioè si va a risolvere questa equazione. Se ci sono, diciamo, se si è in alta dimensionalità e, diciamo, ci possono essere problemi, perché vi ricordo che, appunto, il metodo di Newton aveva come caratteristica, come punto, diciamo, di potenziale fragilità, di potenziale debolezza, proprio l'occupazione di memoria dell'essiano. Allora si ricorre, in quel caso, invece, al metodo di scesda del gradiente. Queste sono le due soluzioni che vengono utilizzate. Tipicamente, in dimensionalità contenute, si va direttamente per la formulazione analitica, che è estremamente compatta ed è esatta ed è questa che vi dicevo. Ecco, quando vi dicevo in forma chiusa, vuol dire che riusciamo a ricavare direttamente il vettore ottimale dei pesi in funzione dei parametri del problema. Va bene? Ci siamo? Siete riusciti a seguire? Sì. Ok, perfetto. Allora, adesso quello che facciamo è passare invece al, come promesso, al nuovo blocco di lezioni e cominciamo a parlare di... di modelli lineari per la classificazione binaria. Eccoci qua. Dovreste riuscire a vedere le slide. Modelli lineari per la classificazione binaria che appunto rappresentano il nuovo argomento e qui, diciamo, per capire dove ci troviamo, ci troviamo sempre nella costruzione di modelli di tipo lineare, il che significa che ci sarà una relazione lineare ben precisa tra i parametri che caratterizzano il nostro modello e l'input e in particolare vedremo come questo si traduce nella linearità di quello che viene chiamato un confine decisionale. Su questo torneremo e avremo modo di tornare nel seguito della lezione e anche delle lezioni poi che seguiranno. Però è bene tenerlo fin d'ora presente, cioè noi quello che facciamo vogliamo fare, innanzitutto costruire un classificatore. Quindi il problema è un problema di classificazione, non è più un problema di regressione. Il problema di classificazione significa che dobbiamo costruire un modello che è in grado di effettuare delle predizioni su un target, su un obiettivo, che ha valori discreti. A oppure B, 0 oppure 1, gatto oppure cane, stiamo cercando di risolvere un problema oppure un messaggio di mail, spam o non spam, ecco stiamo cercando di risolvere un problema in cui l'insieme dei possibili valori di uscita è binario. Cioè abbiamo un problema di risolvere una possibile scelta tra l'appartenenza tra due possibili classi dell'oggetto che stiamo cercando di classificare, oggetto che può essere appunto stabilire se un paziente ha una determinata malattia o no, date alcuni sintomi specifici, classificare un'email come spam oppure non spam, fare la predizione del fatto che una transazione finanziaria possa essere o meno fraudolenta, analizzare un documento di testo e stabilire se questo rappresenta una recensione, se a quella recensione di un determinato prodotto può essere associata un'opinione positiva oppure negativa, quindi fare quella che viene chiamata analisi del sentiment, fare la rilevazione delle facce, abbiamo un'immagine, dobbiamo identificare un riquadro e dire per ogni riquadro se questo contiene oppure no un oggetto e così via. Quindi un problema di detection. E ce ne sono tanti altri, come ben immaginate, quelli da cui partiamo sono appunto i problemi di classificazione binaria. Poi vedremo come estendere e cercare di risolvere il problema della classificazione a modelli che devono fare classificazione tra più di due classi. E vedremo anche poi più avanti eventuali modelli non lineari. Però partiamo appunto da questo perché ovviamente è il setup più semplice ed è il setup che ci permette anche di mettere insieme tutta una serie di concetti base che poi ci porteremo avanti anche per diverse lezioni. Allora, in questa slide abbiamo due immagini. Partiamo dall'immagine di sinistra. In realtà poi ecco appunto sono delle immagini, se vogliamo vedere, ognuna possiamo vederla sdoppiata, nel senso che poi possono essere quattro, se vogliamo, perché c'è una parte superiore e una parte inferiore, identifichiamo questa come la prima immagine e poi abbiamo l'altra come la seconda immagine. Ok, il primo è un caso di input unidimensionale, il secondo è un caso di input bidimensionale che poi è quello che c'è scritto qua. Ok, allora, quello che c'è nella parte superiore, allora questi sono due modi diversi, diciamo due impostazioni equivalenti, ma diversi di vedere allo stesso problema, il problema della classificazione. Allora, prima partiamo da quello che c'è nella parte superiore, partiamo dal caso unidimensionale. Allora, qui abbiamo la x, è un valore unidimensionale, abbiamo una serie di punti, che vedete, sono questi distribuiti qui. Quelli che sono distribuiti in questa parte appartengono a quella che è la classe 0. Diciamo, partiamo assumendo, e questa non è un qualcosa che le dè, in alcun modo, la generalità del discorso che faremo, che le due classi, siano delle classi a cui possiamo associare due valori numerici di uscita, cioè y, che è il nostro valore di uscita, può avere il valore 0 oppure il valore più 1, ok? E questo capite bene che non è necessariamente una limitazione, perché associamo semplicemente alle due classi dei valori simbolici che sono dei valori numerici. Per gli altri valori di x di questa distribuzione di input vedete che il valore di appartenenza di quei valori è alla classe più 1, ok? questa vista è quella che viene chiamata vista di regressione, cioè proprio da un punto di vista geometrico si rappresentano i dati fin dove è possibile rappresentarli chiaramente, cioè fino a 1 o 2 dimensioni perché sopra non si va come dimensioni dell'input, però si vedono esattamente come quella che viene chiamata vista di regressione, questo si chiarisce meglio anche nelle slide successive, ma se ci pensate un attimo è quello che di fatto abbiamo fatto nel momento in cui abbiamo cominciato a ragionare su un problema di regressione lineare e abbiamo rappresentato la variabile risposta in funzione degli input e abbiamo fatto esattamente una cosa di questo genere, però nulla vieta di vedere lo stesso problema anche a livello proprio di rappresentazione visiva secondo un altro punto di vista che è quello della parte inferiore, cioè possiamo andare a rappresentare un unico asse in cui andiamo a riportare i nostri valori di input che sono esattamente gli stessi, vedete, che sono questi e questi, quindi la nostra distribuzione e andare a diciamo discriminarli, quindi qui non abbiamo più l'asse y in cui avevamo prima 1 e 0, ma qui li andiamo a discriminare visivamente associando un colore, quindi associando arbitrariamente per esempio alla classe 0 il colore blu, alla classe 1 il colore rosso e questo è un modo di rappresentare il problema, è quello che viene chiamato a vista di percettrone per motivi che diciamo si chiariranno quando studieremo il percettrone tra un paio di lezioni circa. È un modo diverso di vedere il problema, ripeto, a livello proprio visivo. Quello che conta è che vedete qui è abbastanza chiaro come in questo esempio qui abbiamo una serie di punti distribuiti a sinistra di un certo punto specifico che appartengono a una certa classe, a destra di quel punto specifico ci sono i punti che appartengono all'altra classe. Vedete che questo punto specifico è identificato con questa x anche nella seconda nella seconda vista mentre qui sarebbe questo punto. Quello che conta è che qui è un punto è un punto specifico che ci permette di discriminare perché sappiamo che a sinistra ad esempio di quel punto avremo tutti i punti di una classe e a destra di quel punto avremo tutti i punti dell'altra classe. Chiaramente i discorsi non saranno sempre così semplici nel senso che non sempre sarà possibile mettere tutti i punti di una classe da una parte e tutti i punti di una classe dall'altra altrimenti vorrebbe dire che avremmo trovato un modello che ovviamente riesce perfettamente a discriminare. In questo caso ci riusciamo non sempre ci si riuscirà. Questo che è uno specifico punto viene chiamato confine decisionale. il confine decisionale è il luogo geometrico dei punti che vi permettono di separare una classe dall'altra in base al vostro modello. E questo si capisce bene in questo caso il confine decisionale quindi è un confine molto diciamo che si riduce a un punto quindi molto specifico. La cosa è ancora più evidente se guardiamo all'esempio bidimensionale in cui la parte superiore corrisponde alla vista di regressione vedete avete una distribuzione di punti secondo le due dimensioni x1 e x2 questi punti in questione tutti questi che giacciono qui appartengono alla classe 0 questi che hanno questi altri valori di x1 e x2 quindi sono valori che stanno diciamo per valori di x2 che sono negativi ad esempio o inferiori a questo valore che potrebbe essere un valore che potremmo chiamare x2 sopra segnato oppure x2 start ok hanno la caratteristica di appartenere alla classe 1 quindi anche qui abbiamo una perfetta separabilità tra queste due classi ed è una separabilità che riusciamo a fare a costruire semplicemente con un confine decisionale che di fatto è una riga cioè se noi guardiamo una linea scusatemi cioè qui nel plot tridimensionale avremmo una sorta di scalino quindi la figura della evidenza in questo modo sarebbe questa diciamo giallo per non ecco questo è giusto per farvi vedere a livello visivo si vede un po' meglio che c'è una sorta proprio di scalino per cui a partire da questo valore qui si sale si va a finire su quell'altra su quell'altra classe e se io voglio guardare la vista quella che abbiamo chiamato di percetrone quello che devo fare ecco una cosa che non vi ho detto che vale per il caso unidimensionale ma è ancora più evidente nel caso bidimensionale quello che devo fare è semplicemente guardare da sopra dall'alto la stessa figura quindi di fatto proietto tutto sul piano y uguale a 0 quindi mi colloco come osservatore guardando da sopra quindi guardando da da qui questa figura geometrica e ottengo questa mi devo ricordare ovviamente di evidenziare con un colore diverso le due classi e la cosa interessante è che oltre a questo quello che posso evidenziare è proprio questa transizione che qui avveniva appunto in questo scalino e questa è la proiezione di quello scalino cioè è quel luogo geometrico dei punti che mi permette di dire al di qua di questo luogo geometrico ci sono i punti della classe 0 al di là di questo luogo geometrico ci sono i punti della classe 1 questo luogo geometrico viene chiamato appunto confine di decisione e non è un caso che questo confine di decisione sia una retta perché appunto è proprio quello che caratterizza i classificatori lineari classificatori lineari hanno dei confini decisionali che sono delle rette in due dimensioni è un punto in una dimensione sola e se andiamo in più dimensioni diventano dei piani o degli iperpiani ovviamente io avrei potuto separare questo insieme di punti secondo altre rette altri piani altri iperpiani sempre magari separandoli perfettamente oppure no e avrei potuto anche separarli secondo invece un confine decisionale che era del tutto arbitrario quindi una cosa di questo genere e questo ovviamente non sarebbe stato più un confine decisionale lineare sarebbe stato un modello non lineare bene ci siamo fin qui quindi avete capito qual è il problema di cui stiamo parlando e se avete domande ovviamente potete potete intervenire e fino adesso quello che abbiamo visto è semplicemente una visualizzazione di un problema di classificazione in una o due dimensioni però ci è permesso di introdurre alcune nozioni introduttive tra cui appunto quello di confine di decisione che è il luogo geometrico dei punti che separano le classi che separa scusatemi le classi il luogo geometrico dei punti che separa le due classi in questione se è un punto una retta un piano un iperpiano parliamo di classificatori lineare altrimenti parliamo di classificatori non lineari bene facciamo un piccolo passo avanti quello che vedremo è di anticipo giusto il cammino che faremo perché abbiamo introdotto queste due viste perché sono equivalenti ma ci permettono di vedere il problema appunto da due angolazioni diverse e queste due angolazioni corrisponderanno poi due modalità di introdurre appunto il problema delle classificazioni che condurranno poi a dei modelli che saranno appunto il primo che era chiamato regressione logistica il secondo vedremo il percetrone e poi le svm che sono dei modelli lineari differenti e che corrispondono appunto a che vengono naturalmente introdotti se vediamo il problema come un programma particolare di regressione oppure direttamente come un problema di classificazione con la vista di percetrone questo giusto per dirvi qual è il cammino che traccieremo e che faremo e diciamo ci serve per fare questo tracciarlo appunto secondo quelle che sono i due modi questi due modi di vedere appunto il problema e partiamo quindi dal primo problema dal primo approccio diciamo che quello che vede e prova risolvere un problema di di classificazione come se fosse un problema di regressione cosa significa significa che se noi abbiamo di nuovo un problema partiamo dal caso unidimensionale ovviamente che ci permette di introdurre una serie di di concetti e di ragionare in maniera semplificata su un po' di cose abbiamo il nostro valore di input abbiamo una distribuzione di punti che vedete questa questi punti appartengono alla classe 0 questi appartengono alla classe 1 ok allora la prima cosa che potremmo fare è dire ok io posso vedere un problema di classificazione come un problema di regressione fino adesso i modelli che abbiamo imparato a costruire sono modelli di regressione lineare quello che potremmo fare è dire ok cosa succede se provo a fare una regressione lineare cioè se questa serie di punti provo a farne il fitting tramite una regressione lineare nulla vieta di farlo lo potrei fare e e otterrei ad esempio questa questa retta verde qui quindi sarei in questo caso qui sotto otterrei quella retta verde capite bene che quella retta rappresenta ovviamente cioè quella che viene chiamata una rappresentazione diciamo dei dati abbastanza povera cioè mi spiega male quei dati quei dati hanno una caratteristica molto diversa da quella retta lo si vede hanno quell'andamento a scalino e questo è un primo problema è un problema intrinseco perché ovviamente noi stiamo cercando di risolvere un problema in cui l'output è discreto tramite un output tramite un modello che ci fornisce un output invece che discreto non è mai continuo allora uno potrebbe dire ok prendiamo questa retta e poi facciamo passare il risultato diciamo che ci dice questa retta quindi se io ad esempio fossi con questo input x uguale a 3 vado qui qui sopra ho un valore che mi viene proposto dal mio modello se questo valore come in questo caso è superiore a una soglia ad esempio 0,5 perché è ragionevole perché sono tra 0 e 1 allora restituisco il valore 1 altrimenti restituisco il valore 0 e già qui siamo molto vicini ad aver di fatto costruito un classificatore cioè stiamo di fatto utilizzando la regressione lineare e la facciamo passare poi attraverso una funzione che è una funzione a gradino una funzione a scalino che è una funzione tipicamente non lineare e abbiamo che dichiariamo che il nostro punto appartiene alla classe 1 se la soglia è superiore ad esempio a 0,5 e appartiene alla classe 0 se la soglia è minore di 0,5 qui chiaramente già ci muoviamo meglio rispetto al caso precedente ed equivale ad avere questa questa retta cioè il modello che di fatto è rappresentato da questa retta tratteggiata rossa vedete questi sono tutti i punti che fanno parte di questo modello qui c'è la transizione a scalino verticale quindi chiunque sta al di sotto appartiene alla classe 0 e chiunque sta al di sopra di questo punto che rappresenta il confine decisionale appartierà alla classe 1 e anche qui vedete il problema è che ci giochiamo ovviamente questi due questi due punti i due punti verranno classificati come punti appartenenti alla classe alla classe 0 perché vedete se io vado a vedere questo punto di input interseca qui il mio modello e quest'altro lo interseca qua e sono ambedue due punti che sono minori della soglia 0,5 allora questo non è ovviamente un il fatto che qui vengano sbagliati due punti su 10 non è appunto un caso il problema da dove nasce nasce dal fatto che noi stiamo cercando di forzare un modello di utilizzare un modello di regressione facendo prima il fitting e poi facendo passare il risultato attraverso questa questa funzione a gradino un lavoro migliore può essere quello di dire ok anziché fare il fitting e poi farlo passare attraverso quello che otteniamo di modello di regressione lineare attraverso la funzione a gradino cosa succede se io tengo il quindi non faccio subito il tuning dei parametri ma lo tengo diciamo come come variabile del problema e costruisco il mio modello come regressione lineare concatenata con la funzione a gradino e poi vado a cercare di ottimizzare quel tipo di modello e lì il discorso comincia a farsi invece più interessante perché quello che si ottiene è effettivamente un modello decisamente migliore ed è il modello di cui andiamo a discutere nella slide successiva vi faccio altresì notare che fin qui appunto stiamo utilizzando un approccio basato sulla regressione lineare infatti il primo modello di cui parleremo è proprio quello che viene chiamato regressione logistica che non è un caso che corrisponde proprio a questo nome ma su questo torniamo tra poco allora quindi la domanda che con cui ci siamo lasciati nella slide precedente ok abbiamo un nostro modello un nostro modello lineare che è il solito modello lineare x trasposto per w che è quello che abbiamo utilizzato per per fare regressione questo è lo sviluppo di quel prodotto scalare ok che cosa succede se anziché fare prima il tuning dei parametri e poi farlo passare attraverso la funzione gradino se facciamo il tuning dei parametri dopo aver introdotto una funzione gradino funzione gradino che ha esattamente questa struttura step di t vale 1 se t è maggiore di 0 mentre step di t vale 0 se t è minore di 0 questo corrisponde a una funzione che ha esattamente questo tipo di andamento è una funzione che ha questo tipo di ma scriviamo va bene che cosa succede nello 0 è indefinito è un comportamento indefinito potremmo per i nostri scopi arbitrariamente dire che vale 1 che vale 0 che vale qualunque cosa ovviamente è un qualcosa che non non ci interessa neanche se vogliamo definire è un qualcosa che potremmo assegnare arbitrariamente a uno o l'altro dei due possibili valori che cosa rappresenta il punto 0 il punto in cui questo valore non è definito rappresenta quello che è il confine decisionale qui siamo in una se siamo in una dimensione abbiamo una funzione unidimensionale avremo un punto che è quello che poi corrisponde al luogo dei punti che definiscono il confine decisionale in particolare però vi faccio notare che noi la scrittura che abbiamo qui è una scrittura che è una scrittura che vale per più dimensioni e tante che appunto ci portiamo dietro la notazione di vettori e però quello che vi stavo dicendo è proprio quello che trovate scritto nel punto successivo cioè il confine decisionale tra quelle che sono la parte inferiore e superiore di questo scalino sono tutti i punti in cui questo prodotto scalare vale 0 cioè tutti i punti x per i quali il prodotto scalare di x trasposto con w vale 0 sono i punti che caratterizzano il confine decisionale perché sono i punti in cui io rivado a cadere qui nel punto indefinito punto indefinito e caratterizzato da questo ok ora questo è di nuovo qualcosa che abbiamo già incontrato perché questa è esattamente l'equazione di un hiperpiano cioè quando vado a mettere questo uguale a 0 ottengo di fatto l'equazione di un hiperpiano e se sono in una dimensione chiaramente x è unidimensionale se sono in due dimensioni e quindi avrò un punto se sono in due dimensioni x è bidimensionale quindi avrò una retta se sono in tre dimensioni avrò un piano e così via a questo punto una volta che io ho definito il mio modello che è questo è questo qui questo è il mio modello quello che posso fare è andare a fare il tuning dei parametri ormai abbiamo capito che il meccanismo è definire il modello e questo è un modello ecco si chiama modello lineare proprio perché c'è un confine decisionale che è definito da un'equazione lineare che è questa un fine decisionale è un hiperpiano un piano una retta quello che volete ma è sempre lineare cioè il rapporto tra parametri e input è una relazione lineare e questi si chiamano classificatori lineari proprio perché il discrimine tra una classe e l'altra stiamo parlando di due possibili alternative è un discrimine classificatore lineare è un ente geometrico caratterizzato da un'equazione lineare ecco perché si chiamano classificatori lineari ed è il luogo dei punti in cui non so prendere una decisione se io sono esattamente sul confine da una parte o classe 0 o da l'altra classe 1 lì sul confine non so quanto vale il mio modello e potrò rispondere 0 1 in maniera del tutto arbitraria però appena mi muovo da una parte o dall'altra ricado in un semispazio oppure nell'altro e so dire se i punti di quel semispazio appartengono alla classe 0 oppure alla classe 1 chiaro? oh a questo punto una volta che ho costruito il modello che ripeto è quel funzione gradino di x trasposto w quello che facciamo è fare il il tuning dei parametri cioè trovare il vettore w che minimizza quindi l'insieme di parametri che minimizza un'opportuna funzione di costo e quella che abbiamo visto finora è abbiamo visto anche la funzione della deviazione assoluta che è una variazione di questa però abbiamo visto che sicuramente il primo tentativo che potremmo fare è quello di costruire ad esempio una funzione di costo ai minimi quadrati e allora proviamo a ragionare e vedere che cosa succede se costruiamo una funzione di costo ai minimi quadrati allora noi vi ricordo partiamo dall'avere il nostro modello che è questo ok questo è il nostro modello questo vale per un insieme di punti che sono da 1 fino a più grande quindi mille punti del mio dataset di addestramento per ognuno di quei mille punti costruisco quello che è appunto il mio modello e vado a costruire per ognuno di quei mille punti la funzione di costo siccome abbiamo detto costruiamo una funzione di costo ai minimi quadrati la costruiamo come la media di che cosa dello scarto che c'è tra quello che mi restituisce il mio modello che è step di x trasposto per v scarto vi dicevo rispetto quindi prendo il meno rispetto al valore yp che può essere 0,1 classe 0 classe 1 quindi primo punto appartiene alla classe 1 vado a vedere quanto vale step di x trasposto v se vale 1 il contributo chiaramente sarà 1 meno 1 che fa 0 se invece vale 0 vuol dire che abbiamo sbagliato la previsione il contributo vale 1 e quello lo prendiamo elevato al quadrato o meglio meno 1 appunto che al quadrato diventa 1 e quindi mi incrementa di 1 la funzione di costo quindi tutti i punti che vado a sbagliare quelli per cui commetto un errore di classificazione mi contribuiscono per un valore pari a 1 ora questo pur essendo un modo del tutto ragionevole di affrontare il problema è una funzione di costo che in realtà risulta particolarmente difficile da ottimizzare perché se voi andate a fare un'analisi di questa funzione di costo e qui avete un esempio di un'ipotetica funzione di costo di questo tipo con diciamo due soli parametri da ottimizzare quindi ci muoviamo in uno spazio bidimensionale da un punto di vista dell'ottimizzazione e vi accorgete che ottenete una delle figure di questo genere che è del tutto ragionevole cioè sono delle figure che sono scusate delle funzioni che hanno delle superfici che sono molto piatte in diversi punti hanno degli scalini assolutamente ripidi in altri punti quindi sono particolarmente ostiche da un punto di vista dell'ottimizzazione perché sono completamente piatte in diverse zone della superficie e hanno delle discontinuità in altri punti della superficie quindi utilizzare metodi del primo e del secondo ordine diventa decisamente proibitivo proprio per questo perché gradiente è piatto quindi è nullo in tantissimi punti ci sono un sacco di discontinuità e quindi la funzione non è derivabile in quelle discontinuità è possibile si può fare però di meglio e da qui appunto si parte per arrivare verso funzioni di costo che sono quelle che poi vengono utilizzate però qui è un buon punto di partenza proprio perché ci permette di evidenziare quelle che sono le caratteristiche di questi modelli caratteristiche appunto che ci portano a rivedere alcuni aspetti quindi siamo arrivati al punto in cui abbiamo detto ok se partiamo con un approccio che è quello tipo di fare una regressione a cui mettiamo in cascata una funzione a gradino e utilizziamo la funzione di costo ai minimi quadrati finiamo in una situazione che non è comodissima da un punto di vista dell'ottimizzazione e quindi cosa si fa? beh si prova a modificare la funzione di costo cioè quello che si fa tipicamente è dire ok che cosa succede se anziché pretendere appunto di ottimizzare direttamente quella che è la lo scarto tra le previsioni del mio modello e quelli che sono i valori effettivi elevato al quadrato io approssimo quella funzione a gradino cioè se ad esempio comincio ad approssimare la funzione a gradino con un altro con un altro valore che cosa succede? ecco quello che succede è una cosa interessante perché si fa una approssimazione si rimpiazza la funzione a gradino con una sua approssimazione continua questa approssimazione continua viene chiamata funzione logistica sigmoidale il nome deriva dal fatto funzione logistica perché vabbè deriva dalla statistica e dove le funzioni logistiche hanno tutta una serie di significati sigmoidale è abbastanza semplice come significato invece perché vedete è un suo grafico e questa è una di fatto una S S sigma in greco allora la funzione sigmoidale logistica sigmoidale ha questa questo questo andamento cioè è definita come sigma di x uguale 1 diviso 1 più e alla meno x quindi è un ha un andamento funzionale ben ben preciso che è quello che è riportato in questo grafico qui nei grafici di destra avete la stessa funzione in cui l'argomento viene della funzione sigmoidale viene premoltiplicato per un fattore w e vedete che cosa avviene con le tre curve quando w vale 1 vale 2 o vale 10 vedete che diventano via via più ripide e questo vi fa vedere come possiamo in maniera sempre più fine andare ad approssimare quello che è di fatto uno scalino ora è uno scalino ma in realtà quindi diciamo siamo in grado di approssimare uno scalino con la funzione sigmoidale ma in realtà questa transizione non è mai così brusca nel punto di transizione e questo è un qualcosa di interessante perché questa funzione è una funzione appunto liscia che sarà differenziabile che avrà determinate proprietà che da un punto di vista matematico si traducono poi in una funzione di costo che comincia a essere più gestibile da un punto di vista dell'ottimizzazione e allora quello che di fatto stiamo stiamo facendo è proprio il vero è proprio un vero e proprio primo esempio di modello di classificatore lineare che ha una sua spendibilità pratica che viene chiamato appunto regressione logistica e si chiama regressione logistica proprio perché parte diciamo dai modelli di regressione come ispirazione questo è un nome che può essere fonte di fraintendimento si chiama regressione logistica ma di fatto è un classificatore però siccome parte dal concetto della regressione per costruire il classificatore ha conservato questo suo nome il fatto che si chiama logistica è perché viene utilizzata la funzione sigmoidale logistica per costruire il modello e quindi la funzione di costo ai minimi quadrati e il nostro modello di fatto non è più step quindi funziona gradino di questo argomento ma è sigma di questo argomento quindi quello che si fa è si prende x trasposto per v doppio ok e si fa passare all'interno della funzione sigmoidale a quel punto io sarò a seconda del valore di x trasposto per v doppio quindi dello scalare che ottengo a seguito di quel prodotto scalare ad esempio in questo punto supponiamo che x trasposto per w mi faccia finire qua questo significa che io sarò su questo punto questo può essere 0.7 e a quel punto potrò mettere un'opportuna soglia e dire ma se fissiamo la soglia a 0.5 sono sopra la soglia e quindi restituisco un valore che è 1 per il mio classificatore se invece mi ritrovo supponiamo di ritrovarci facciamolo in x qui con x trasposto per v doppio e questo supponiamo che sia ad esempio 0.28 sono sotto la soglia del 0.5 e restituirò in questo caso 0. mentre nel caso precedente restituiamo questo è come funziona il classificatore basato su regressione logistica e quello che rimane da capire è se quello che vi dicevo prima che cosa succede cioè verificare che cosa succede alla funzione di costo se effettivamente rispetto a usare step di questo stesso argomento abbiamo dei vantaggi ok quindi abbiamo sostituito step con sigma creando appunto quello che viene chiamato regressione logistica la funzione allora quello che noi vogliamo è che il nostro modello sia il più possibile una volta che andiamo a fare poi il tuning dei parametri restituisca quindi vogliamo aggiustare w in modo che per i vari punti del mio dataset di addestramento sigma di x trasposto per w sia il più possibile vicino a yp è chiaro che non lo posso imporre a priori non diciamo non posso sapere che vale esattamente yp per ognuno dei punti ma è contento di arrivarci nei pressi quindi di avere per esempio 0,7 oppure 0,8 quando yp vale 1 e di avere magari 0,28 quando yp vale 0 allora la nostra funzione di costo diventa questa diventa questa g di w in cui abbiamo la media di che cosa? dello scarto quadratico rispetto a yp del mio modello di quello che mi restituisce il mio modello ora quello che si può dimostrare non lo facciamo ma si può dimostrare che questa funzione di costo in generale è non convessa e quindi di nuovo abbiamo un alternarsi potenziale di minimi e massimi locali chiaramente però ha delle caratteristiche anche qui si può vedere insomma dal punto di vista matematico che la rendono più facilmente minimizzabile tramite degli algoritmi di ottimizzazione delle tecniche di ottimizzazione di tipo locale perché? perché se torniamo alla figura da cui eravamo partiti qui abbiamo visto che la funzione invece minimi quadrati che utilizzava lo step il gradino era era era quella più a sinistra diciamo delle tre elli se vedete ce ne erano tre che sono queste ecco il caso della funzione che sta nel mezzo è proprio il caso di una funzione di coste minimi quadrati regressione logistica con quella funzione di coste minimi quadrati quindi con quella appunto funzione sigmoidale e vedete che di nuovo conserva questo è un semplice esempio bidimensionale ma queste caratteristiche valgono anche in generale su più dimensioni conserva il fatto di essere diciamo piatta in diversi posti quindi anche qui diciamo abbiamo un po' di problemi con il gradiente però in realtà si sono smussati un po' di quei gradini cioè non abbiamo più quelle quelle discontinuità così brusche e questo rende di fatto il gradiente calcolabile in diversi punti della funzione mentre prima non lo erano e il fatto che sia piatto diciamo può essere un qualcosa che riusciamo a ovviare come problema bypassare nel momento in cui introduciamo delle caratteristiche degli algoritmi che hanno delle caratteristiche particolari sono delle versioni particolari di discesa del gradiente di cui vi avevo accennato qualcosa quando avevamo studiato il metodo di discesa del gradiente in particolare c'è una versione del metodo di discesa del gradiente che si chiama discesa del gradiente normalizzata in cui si va a dividere il vettore gradiente per la norma del gradiente stesso e questo di fatto permette di mitigare appunto il problema di dell'attraversamento di queste zone della funzione di costo che sono piatte quindi questo significa che tramite ad esempio un disceso del gradiente normalizzato riusciamo a minimizzare una funzione di costo di quel tipo e questo è già qualcosa che ci piace di più da un punto di vista della progettazione ma non è finita qui perché c'è non a caso un'ulteriore un'ulteriore parte che è quella a destra del grafico che è quella a cui arriveremo adesso e qui arriviamo con l'ultima diciamo tipologia di funzione di costo che vedremo per il momento che ci permette appunto di arrivare a quella terza situazione che vi anticipo è quella più favorevole tra le tech che abbiamo visto però già con questa seconda riusciamo a fare delle cose assolutamente ragionevoli e qui avete l'esemplificazione grafica di quello che vi sto appunto dicendo andiamo regressione logistica utilizzando minimi quadrati quindi quella funzione sigmoidale che vi ho detto prima meno yp tutto elevato al quadrato normalizzata con discesa del gradiente normalizzato giusto per capirci cosa significa il disceso del gradiente normalizzato significa che noi andiamo a impostare un algoritmo in cui il punto all'iterazione k è come sempre il punto all'iterazione precedente meno alfa gradiente di g del punto precedente e questo è il classico all'iterazione del gradiente però viene in più diviso per un termine che è la norma del gradiente nel punto precedente norma di tipo L2 allora costruendo un algoritmo di discesa del gradiente che opera in questo modo e minimizzando quella funzione di costo partendo da un punto arbitrario per esempio il punto 20 meno 20 ok che è questo questo punto iniziale che è quello evidenziato in verde da lì si parte e si segue un percorso che attraverso una serie di passi vi porta se voi andate a seguire nelle curve di livello questo tracciato vedete che va a convergere verso questo punto che è un punto di buco e quello che riuscite a fare è riuscite a costruire una fare il fitting di questa serie di punti con questa curva che nel grafico disegnato in rosso che vi ho appena rievidenziato in verde e quello rappresenta di fatto il fitting della funzione vedete che è un fitting proprio sigmoidale perché abbiamo utilizzato una funzione di quel tipo come sostituto della funzione a scalino che vi dice che il vostro modello adesso mentre quando siamo partiti all'inizio dall'utilizzo di una semplice regressione di cui facciamo prima il tuning e poi la facciamo passare attraverso la funzione gradino non lineare poi abbiamo introdotto il gradino abbiamo detto che quello però non è minimizzabile il risultato è questo vedete che questi punti che all'inizio però venivano sbagliati dal primissimo modello da cui siamo partiti adesso in realtà non vengono sbagliati perché nel momento in cui si tratta di prendere in considerazione questo questo valore questo chiaramente va sull'uno ma anche quest'altro risponderà a un valore che è superiore a 08 quindi siamo sopra soglia verrà classificato come 1 correttamente bene mi hanno domande su questo fermo un attimo se avete qualche domanda a fare ci potete ragionare io non ho capito bene qual è il problema con il gradino cioè perché siamo passati dal gradino all'approssimazione è perché il problema del gradino è questo torno un attimo di litro il problema del gradino è che quello che viene fuori è questa cosa qua vengono fuori delle funzioni di costo che hanno ampi ampi porzioni dello spazio dei parametri che sono piatte e tante altre porzioni in cui abbiamo delle discontinuità allora se tu ragioni in una variabile funzioni di una variabile le funzioni che hanno una discontinuità in un punto x sono funzioni per cui in quel punto x non esiste la derivata giusto? ora se non esiste la derivata tu non puoi costruire qualcosa che lavora sul sul gradiente sul gradiente o meglio lo puoi fare solo nelle parti della funzione in cui questo gradiente esiste non nelle parti in cui questo non esiste e qui sono tante i punti dove questa non esiste se invece sostituisco la funzione step con una funzione sigmoidale sono in questo caso e qui vedi che ci sono sempre dei punti piatti però quelli li risolviamo utilizzando il gradiente normalizzato questi altri punti invece vedi che le transuzioni sono più sono più docili e questo è un fatto vero non è solamente un artefatto di questo esempio è una caratteristica proprio di quella tipologia di funzione le funzioni a gradino hanno delle transizioni brusche e quindi si portano dietro questa discontinuità da una dimensione a due fino avanti e le funzioni invece come la sigmoidale hanno questo andamento più docile e non si portano dietro alcuna discontinuità e questo da un punto di vista dell'ottimizzazione è decisamente migliore anche se sono non convesse tutte e due poi alla fine va bene? va bene quindi il motivo è questo è un motivo legato all'ottimizzazione mi faccio notare che nel se voi andate a fare nel prendete l'espressione che ho scritto prima per il capesimo passo e andate a vedere che cosa succede alla differenza tra il vettore w al passo k meno il vettore w al passo k meno 1 e prendete la norma di questa differenza che vi dice qual è l'ampiezza del passo che abbiamo fatto tra l'iterazione k meno 1 e l'iterazione k dall'espressione precedente trovate che questa è esattamente la norma di questo vettore e il gradiente moltiplicato per alfa fratto la norma del gradiente stesso ora chiaramente questo è un vettore questo è un vettore diviso la sua norma quindi un vettore diviso la sua norma è un vettore che ha norma unitaria quindi porto fuori il meno alfa e il risultato è che quello che ottengo è che l'ampiezza di questo passo è proprio alfa cioè il gradiente normalizzato fa ad ogni iterazione un passo di ampiezza esattamente pari ad alfa non dipende più dal gradiente ok allora detto questo quindi siamo riusciti a costruire un qualcosa che riesce magari lavorando un po' sull'algoritmo di sceso del gradiente a trovare un minimizzatore che è del tutto ragionevole e questo appunto è un primo esempio di regressione logistica con funzioni di costo ai minimi quadrati ma come vi dicevo è ben intuibile si può sempre fare meglio e in particolare qui si arriva alla formulazione che è quella diciamo più utilizzata probabilmente della regressione logistica che è quella che fa utilizzo di quella che viene chiamata funzione di costo funzione di perdita di entropia incrociata o cross-entropia in inglese la funzione di costo cross-entropia è una funzione che ha delle caratteristiche ancora più interessanti da un punto di vista della poi vedremo della minimizzazione e quindi dell'ottimizzazione della funzione stessa e viene introdotta in questo modo cioè si parte da una considerazione che noi abbiamo definito la funzione di costo scusate abbiamo definito la funzione di costo indipendentemente dal valore di y però yp sappiamo che ad esempio vale 0 oppure vale 1 e questo possiamo sfruttarlo per costruire una funzione di costo un po' più se vogliamo un po' più intelligente un po' più smart in particolare se ragioniamo a livello di di funzione di costo cosiddetta point wise cioè che ragiona sul singolo punto poi diciamo la estendiamo e infatti scriviamo g con il il pedici p proprio per indicare che stiamo ragionando sul generico p esimo punto tra quei mille ad esempio ponendo che siano mille del mio dataset di addestramento allora quello che si può fare è dire ma cosa succede se introduco ad esempio una funzione di costo che dipende dal logaritmo del da quello che viene chiamato il log error cioè che cosa succede succede che se io vado a fare una suddivisione logica tra il caso in cui yp valga 1 yp valga 0 cioè sono sul piesimo punto e vado a vedere intanto sono in fase di addestramento so esattamente quali sono le etichette vado a vedere qual è l'etichetta di quel punto è 1 oppure 0 cosa succede se definisco una funzione di costo in questo modo cioè la vado a definire tramite il log error cioè vado a prendere meno logaritmo di sigma di xp trasposto v doppio se yp uguale a 1 oppure meno logaritmo di 1 meno sigma sempre dello stesso argomento se yp uguale a 0 allora questa è anzitutto andiamo a vedere se è una funzione di costo ragionevole allora che cosa a noi interessa ricordiamoci che cosa interessa andare a scriviamo magari qua sopra a costruire un qualcosa tale per cui sigma di xp esposto per v doppio ci restituisca qualcosa che sia il più possibile vicino a yp allora vediamo che se y ragioniamo appunto sul caso yp sia uguale a 1 che cosa succede se yp è uguale a 1? beh se il mio modello se io ho fatto un tuning dei parametri e per cui il mio modello è tale per cui quell'insieme di parametri v doppio mi porta dal diciamo verso l'1 sono più ovviamente contento allora vediamo che cosa succede se effettivamente ho fatto un tuning corretto vuol dire che sigma di xp trasposto per v doppio sarà un valore presumibilmente vicino a 1 08 09 e a quel punto che cosa succede? che ho un numero vicino a 1 e il logaritmo di qualunque numero vicino a 1 è vicino allo 0 il che significa che pago un penalty 0 ok e questo è un qualcosa che noi vogliamo è un buon comportamento per una funzione di costo che mi faccia pagare poco nel momento in cui ho indovinato un insieme di parametri corretto viceversa mi faccia pagare di più se ho sbagliato quell'insieme di parametri e io quando è che lo sbaglio se y più uguale a 1 se la mia sigmoide mi porta verso l'altro lato cioè vi ricordo che questa è la sigmoide no? ma la caratteristica l'avevamo visto prima di di avere un andamento che ok era aspettate che ve la faccio vedere meglio torniamo qui un attimo allora la disegniamo esattamente come questa centrata nello 0 tra 0 e 1 ok? a quell'andamento la voglio riportare qua bene? allora cosa succede? succede che nel momento in cui yp vale 1 e dovesse essere che invece sigma di xp trasposto w è tale da portarmi verso questa zona qui ok? quindi un valore basso ad esempio 0.1 ok? noi sappiamo che il logaritmo di un numero vicino allo 0 non esiste tanto è che c'è un asintoto e diventa un numero negativo molto molto grande potenzialmente va a meno infinito moltiplicato per il segno meno davanti vuol dire che io ho un qualcosa che è un numero abbastanza grande il che significa tanto più grande quanto più mi avvicino allo 0 come argomento quindi quanto più se y vale 1 quanto più sigma di xp trasposto w è vicino allo 0 tanto più quel punto verrà penalizzato e quindi fin qui diciamo regge perfettamente una logica di questo genere la stessa cosa devo andare a verificare che accada per y più uguale a 0 allora se ipotizzo di essere in un punto che ha etichetta 0 cosa cosa vuol dire devo andare a vedere che effettivamente il mio modello mi restituisce un valore ragionevole supponiamo oppure no allora se me lo restituisce ragionevole cioè mi restituisce qualcosa che mi porta vicino allo 0 come risposta allora non lo devo penalizzare viceversa lo devo penalizzare e vedete che questa funzione è esattamente questi requisiti perché che cosa succede se il risultato è ragionevole se il risultato è ragionevole vuol dire che questo sigma ok è circa vale circa 0 ok ma questo sigma vale circa 0 vuol dire che 1 meno sigma vale circa 1 logaritmo di 1 è 0 di fatto non aggiungo nulla alla mia funzione di costo ed è giusto perché il mio modello mi ha proposto una un valore assolutamente plausibile se invece y fosse stato 0 e il mio modello mi avesse detto guarda che sei ad esempio da queste parti ok quindi sei qualcosa che va vicino all'1 allora che cosa succedeva beh vuol dire che qui sono qualcosa che questo è circa 1 quindi vuol dire che tutto questo è circa 0 ma di nuovo il logaritmo di un numero circa 0 è un numero che diventa rapidamente molto molto grande negativo quindi molto molto piccolo se vogliamo ma è un numero in valore assoluto molto grande moltiplicato per il segno meno è un numero quindi meno per meno fa più che è un numero grande quindi vado a penalizzare un qualcosa di questo tipo e lo vado a penalizzare tanto più quanto più sono lontano dal quindi quanto più sono diciamo verso lo 0 quanto più sono lontano dalla soglia invece per esempio di 0,5 allora questo è un comportamento quindi ci dice che cosa è tutto questo che una scrittura di questo genere cioè introdurre una funzione di costo puntuale che vale per quel generico punto P piccolo che ha quella struttura è del tutto ragionevole perché se noi stiamo facendo se il nostro modello dei pesi ci dà una previsione che è abbastanza vicina a restituirci il valore corretto noi di fatto tanto più siamo tanto più diciamo siamo vicini alla all'1 tanto più siamo contenti perché se yp è uguale a 1 abbiamo dato una risposta corretta viceversa tanto più siamo lontani tanto più saremo non contenti di quel valore dei parametri W e quindi dovremo penalizzare quel punto stessa cosa ribaltata con yp uguale a 0 il risultato è che questa funzione di costo è una funzione di costo decisamente assolutamente legittima e plausibile e che risponde a quelle che sono le nostre le nostre esigenze per il momento questa funzione viene chiamata funzione di costo di tipo cross entropi anche qui diciamo sulla nomenclatura sul nome diciamo ci sono delle ragioni ben precise ma non ci interessa entrare in questo dettaglio quello che ci interessa notare è che questo logaritmo nonostante venga comunemente scritto come log in realtà sarebbe più corretto scriverlo come ln perché è un logaritmo in base e solitamente non cambia nulla perché poi i logaritmi possiamo passare da una base all'altra con un semplice cambio di base appunto che è un fattore costante di scala giusto per dirvi però trovate nei libri spesso log ma il sottinteso è che è un logaritmo in base naturale quasi sempre anche solitamente il logaritmo in base naturale in matematica si indica con ln ma questo è giusto una notazione ok allora perché dobbiamo fare una cosa detto che questa funzione di costo è una funzione perfettamente legittima poi io quello che posso fare ovviamente è il solito discorso di dire ok la mia funzione di costo è g di w la costruisco a partire dalla funzione di costo point wise andando a prendere la media su tutti i punti quindi p piccolo che vada 1 fino a p grande di gp di w quindi vado a prendere la somma di contributi di questo tipo e ci vado a dividere per w e vedo mediamente come il vettore w se l'è cavata andando a classificare correttamente punti 1 e punti 0 oppure non se l'è cavata e quindi a quel punto dove ha sbagliato la classificazione verrà penalizzato quel vettore e questo è l'obiettivo della costruzione di una buona funzione di costo la cosa bella è che una funzione di costo di questo tipo si può dimostrare non lo facciamo ma si può dimostrare ed è un teorema matematico quindi è un fatto che è una funzione convessa e le funzioni converse abbiamo imparato a riconoscerle come delle funzioni che da un punto di vista dell'ottimizzazione sono decisamente desiderabili perché qualunque tecnica di ottimizzazione locale ci permette di andare a minimizzare queste funzioni e non solo ci permette di minimizzarle ci permette anche di ottenere un minimo che sappiamo essere un minimo globale e questo è il motivo per cui la formulazione più utilizzata dell'algoritmo di regressione logistica del modello regressione logistica è quella con la funzione di costo a entropia incrociata la funzione di costo cross entropia è una funzione che introduce il modello di costo puntuale basato su quello che viene chiamato log error che è null'altro che il logaritmo con valore negativo di sigma di xp trasposto w se la risposta l'etichetta del punto era 1 mentre vale meno log di 1 meno sigma se l'etichetta del punto era 0 e questo decisamente si traduce in un vantaggio netto dal punto di vista dell'ottimizzazione questo lo vedremo lo vedremo diciamo tra poco torno un attimo un secondo su una cosa che diciamo io sulla quale ho sorvolato ma è bene un attimo puntualizzare poi torniamo alla nostra coscentropie che è bene puntualizzare qui nel momento in cui noi abbiamo introdotto la funzione gradino quindi torniamo indietro noi abbiamo dato per scontato una cosa che qui vada perfettamente bene andare a prendere la funzione gradino di x trasposto w in realtà in realtà qui per essere proprio corretti avremmo dovuto prendere step di x trasposto w meno 0.5 ok e andare a studiare quella in realtà perché appunto è quando questo argomento diventa o maggiore o minore di 0 che introduciamo andiamo a vedere che cosa accade alla funzione del gradino cioè quando x trasposto meno w è maggiore o minore della soglia di 0.5 ok ponendo la soglia di 0.5 in realtà poi è sufficiente andare a considerare questa perché perché io posso se vi ricordate nel vettore w c'è un primo termine che non va a moltiplicare le varie feature ma è quello che viene chiamato termine di bias quindi posso andare a costruire un nuovo w 0 che costruisco a partire da quell'altro a cui tolgo meno 0.5 e quindi di nuovo questo significa andare a studiare la funzione di costo step di x trasposto w e basta e quindi da qui siamo partiti per tutto il discorso che abbiamo fatto era una cosa che prima non vi ho detto però per andare avanti è opportuno chiarire perché altrimenti la matematica potrebbe non tornare per qualcuno insomma l'occhio di qualcuno più attento potrebbe potrebbe appunto soffermarsi su questa incoerenza che in realtà ripeto non è non è un'incoerenza bene detto questo invece torniamo alla nostra funzione di costo cross-entropi prima di andare a che di fatto vi anticipo ci permette di ricadere cioè vi anticipo ve l'ho già detto ve l'ho già anticipato ricadiamo in questa parte del grafico cioè questa è una funzione che è convessa l'ammento insomma tipo appunto a scodella ed è vero per qualunque dimensione n ed è vero per qualunque dataset quindi è un teorema appunto che vi permette di concludere questo per in modo del tutto generale non è un caso specifico che si verifica solo appunto per questo esempio prima di di appunto di concludere con quelle che sono e riempilogare quelle che sono le caratteristiche delle funzioni della regressione logistica con la cross-entropi è però opportuno fare fare un confronto un confronto tra la funzione di costo ai minimi quadrati e quella cross-entropi perché un'altra oltre da un punto di vista dell'ottimizzazione un'altra caratteristica della funzione di costo basata sul log error quindi quella cross-entropi è quella di penalizzare in maniera ma in qualche modo l'abbiamo visto prima cioè il logaritmo quando ci avviciniamo allo zero diventa un numero molto molto negativo quindi un numero molto grande in valore assoluto con davanti chiaramente il segno meno questo fa sì che le deviazioni il log error è una funzione di costo che ci penalizza gli scarti le deviazioni rispetto a quella che è l'etichetta che noi vogliamo che il nostro classificatore sia in grado di predire correttamente quindi da quella che è la vera etichetta in modo decisamente più marcato rispetto all'errore quadratico e questo si vede se andate a fare il grafico di queste due funzioni allora qui in questo grafico che cosa abbiamo abbiamo proprio questi andamenti che sono sono stati graficati quindi abbiamo gp di w in funzione di quello che è il valore di sigma di xp trasposto w che significa che se sigma di xp trasposto w mi restituisce 0.5 sono nel punto esatto di transizione nella soglia ok sono nel mio confine decisionale praticamente e quanto vado a pagare quando io sono sono su una utilizzo la funzione di costo ai minimi quadrati che è quella in blu oppure una funzione di costo cross entropy la log error cosiddetta che è quella in russo la differenza tra il tratto continuo e il tratto tratteggiato è che se vi ricordate la log error fa distinzione tra y più uguale a 0 e y più uguale a 1 allora y più uguale a 0 significa che mi muovo su queste curve qui y più uguale a 1 sono sulle quote tratteggiate ok allora qui vedete che nel momento in cui ipotizziamo che l'etichetta vera di un punto sia y più uguale a 0 se andate a calcolare per un ipotetico modello sigma di xp trasposto w e quello vi porta a un valore che po' prima per esempio dello 0.5 vedete che pagate per quel singolo punto un valore che è qualcosa come 0.65 ok che quello che succede se andate a vedere nel se applicate appunto la funzione di costo log che è esattamente appunto meno il logaritmo di di sigma che è 0.5 man mano che voi vi allontanate da siete poniamo su questa su questa curva che prosegue ovviamente qua scusate e chiaramente andate a pagare un valore sempre più grande perché perché y più uguale a 0 e siete addirittura sopra la soglia del del valore 0.5 e la cosa la cosa interessante è che tutta la curva rossa vedete al di sopra della curva che significa che pagate molto di più per lo stesso valore che vi viene fornito nel vostro vettore dei parametri se utilizzate la funzione di costo log error rispetto alla funzione di costo list square stessa cosa nell'altro nell'altra parte del grafico dove avete le curve relative a y più uguale a 1 vedete che c'è sempre una distanza tra quello che è il valore che viene fatto pagare io dico pagare perché appunto è una funzione di costo per cui ogni questo è il contributo di un singolo punto se quel singolo punto voi vi avvicinate e vale 1 e vi avvicinate a questa zona vedete che l'errore tende a 0 tant'è che quando è perfettamente 1 il log di 1 vale 0 e lo scarto quadratico ovviamente vale 0 e i due coincidono man mano che ci allontaniamo vedete queste idealmente sono le curve logaritmiche che tendono a dare un contributo sempre più grande e qui si vede come appunto quello che vi dicevo prima cioè il log error penalizza delle deviazioni da quella che è l'effettiva etichetta in maniera più marcata rispetto all'errore quadratico che è un'altra caratteristica che che noi diciamo possiamo ritenere come un un desiderata cioè noi vogliamo penalizzare dei punti che sbagliamo in maniera più marcata rispetto a un'altra ipotetica funzione di costo perché perché nella speranza che questo ci guidi nel processo di ottimizzazione verso dei modelli che appunto dovendo ottimizzare la funzione di costo vogliono evitare il più possibile questo tipo di situazione benissimo allora un'ultima un'ultima diciamo cosa che potremmo notare su questo su questo argomento della della della cross entropi è che se noi andiamo a scrivere per esteso la funzione di costo g di w come 1 su p per la sommatoria dei vari contributi delle funzioni di costo pointwise che abbiamo chiamato g con p la possiamo scrivere come un'unica come un'unica espressione anche se siamo partiti da un gp che aveva vi ricordo abbiamo visto nella slide precedente aveva queste due condizioni a seconda che yp fosse 1 oppure 0 però diciamo proprio per il fatto che yp può valere 1 oppure 0 noi possiamo scrivere questa doppia diciamo equazione come un'unica equazione e adesso lo andiamo a fare ok lo mettiamo magari qui ok qui abbiamo g con p che è per scrivere era meno log di sigma di x trasposto w nel caso di yp uguale a 0 scusatemi a 1 e meno log di 1 meno sigma di x trasposto per w doppio nel caso di questa p in valenza ok benissimo allora è esattamente questa va bene è esattamente questa ora qui si può notare che quello che vi stavo dicendo prima è che questa la possiamo scrivere come un'unica diciamo ops scusate ho collegato il pennarello è questo come un'unica diciamo equazione un'unica formula che possiamo scrivere esattamente in questo modo log di sigma di x p trasposto per w più 1 meno yp questo moltiplicato per yp scusatemi più 1 meno yp che moltiplica log di 1 meno sigma di xp trasposto per w vedete che qui quello che facciamo è semplicemente sfruttare il fatto che yp vale 0 oppure 1 perché? perché quando yp vale 0 mi si annulla questo termine e mi sopravvive solamente log di 1 meno 5 esattamente questo con davanti il segno meno segno che non abbiamo messo ma che ovviamente dobbiamo mettere allora facciamo così facciamo spazio scriviamo meglio questo magari il colore mettiamo il verde e questo chiaramente allora mettiamo un meno anche qua o mettiamo il meno davanti con la parentesi oppure mettiamo un meno anche qui vi dicevo cosa succede nel momento in cui yp uguale a 0 si annulla questo termine e quello che mi rimane è solamente meno logaritmo di 1 meno sigma che è esattamente quello che avevamo scritto qui sopra se invece yp non vale 0 ma vale 1 mi si annulla questo termine e vale 0 e rimaniamo solo con meno log di sigma che è esattamente questo quindi questa è esattamente la formulazione che c'è sopra quello che qui è spezzata in due e qui è su un'unica riga a questo punto posso andare a metterla dentro nella formula finale e ottengo questo vado a fare la sommatoria su tutti i punti e otteniamo esattamente quella formula lì che è la formulazione più generale che trovate per la regressione logistica con il modello di costo di tipo cross-entropietto con questa concludiamo la lezione di oggi e quindi da qui ripartiamo la prossima volta perché poi insomma continueremo a fare un po' di considerazioni in merito ma direi che intanto per oggi possiamo fermarci qui non so se avete delle domande ovviamente se ci sono sono più che benvenute altrimenti ci fermiamo qui allora intanto fermo la registrazione qui abbiamo