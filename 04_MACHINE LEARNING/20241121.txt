benissimo iniziamo la lezione di oggi oggi vediamo un'esercitazione in cui vediamo come implementare tramite scikit learn un classificatore basato su svm quindi support vector machine quindi macchina vettore di supporto ok quindi in particolare quindi è l'occasione per riandare a vedere se non le avete ben presenti perché abbiamo fatto un po di tempo fa le lezioni di teoria riguardanti questo questo argomento per cui invito è senz'altro riandare a dare un'occhiata è l'occasione giusta e quello che vediamo oggi invece come come appunto implementarle vedrete che con poche poche linee di codice python sfruttando la libreria scikit learn si riesce a fare una cosa che non è banale perché appunto le svm sono il ricordo si basa sul concetto di margine quindi dobbiamo ricavare il classificatore lineare che a margine massimo per fare questo c'è una formulazione che può essere duplice quella che abbiamo chiamato con diciamo vincoli cosiddetti hard quindi hard margin può essere solta tramite programmazione vincolato oppure c'è una formulazione con regolarizzazione che è quella cosiddetta soft margin comunque per fare tutto questo quello che dobbiamo fare è come al solito importare un po di librerie quindi cominciamo a importare le librerie e quelle che importiamo sono le solite intanto nampai matplotlib e va bene anche seaborn per ulteriormente arricchire i grafici e poi ovviamente importeremo scikit learn in particolare cominciamo ad importare da scikit learn quello che il pacchetto che ci permette di di andare a ad analizzare a scusatemi a creare dei data set o comunque importare dei data set allora un secondo che volevo annotare il eccoci qua la prima cosa facciamo è vi dicevo importare il modulo data set da scikit learn e in particolare importiamo questa funzionalità questa funzione make blobs che serve proprio per creare un data set artificiale sintetico che fa esattamente appunto quei quei blob che avevamo visto anche la volta scorsa quando abbiamo fatto l'esercitazione che per la regressione logistica quindi quello che facciamo è generiamo con questa funzione una distribuzione di punti in particolare generiamo 50 e qui andiamo a specificare il numero di campioni che sono generati in maniera casuale in due centri sono generati secondo una distribuzione gaussiana cosiddetta isotropica cioè uguale in tutte le due direzioni quindi è una gaussiana centrata sui due centri che vengono anche loro generati casualmente questo è il il seed del generatore di numeri pseudo casuali che ci serve per garantire la riproducibilità degli esperimenti quindi se cambio il seed cambiano i punti se ripeto con lo stesso seed i punti generati sono sempre gli stessi e questo è un questo parametro questo parametro il standard del cluster ci dà una misura della dispersione di queste nuvole di punti poi facciamo stampare la dimensione dei dati e con il comando scatter di matplotlib andiamo a stampare tutte le righe della prima colonna versus tutte le righe della seconda colonna il colore è quello dell'etichetta perché il dataset viene generato e a ognuno di questi due nuvole di punti viene associato vi ricordo un un'etichetta prima classe o seconda classe 0 o 1 qui associamo un colore a seconda della classe quindi avremo quelle due nuvole colorate questo l'abbiamo fatto anche l'altra volta questa è la dimensione dei punti e questa è la mappa di colore che viene scelta e il risultato è questo questo l'abbiamo fatto anche la volta scorsa e quello che andiamo a fare adesso è operare intanto su questo dataset sintetico e la prima cosa che facciamo è generiamo tramite il comando la funzione link space che vi genera dei punti distribuiti in maniera uniforme su una retta in questo caso gli specifichiamo il valore minimo e il valore massimo e lui di default potremmo specificargli un terzo parametro che è il numero di punti qui di default la funzione prevede di generare 50 punti quindi con questo comando noi con questa funzione generiamo 50 punti equispaziati tra meno 1 e 3.5 va bene dopodiché questa variabile questi vettori lo chiamiamo x fit andiamo a vedere qual è la shape di x fit vedete l'ho fatta stampare se lo rimando in esecuzione questo perfetto vedete l'abbiamo fatto stampare sono 50 punti ok cosa ci facciamo con questo 50 punti qui viene intanto viene rigenerato il grafico della nuvola di punti come abbiamo fatto prima dopodiché andiamo a prendere un punto ok un marker che chiamiamo x e lo andiamo a plottare esattamente nel punto del piano che ha queste coordinate 0.6 e 2.1 gli specifichiamo che lo vogliamo di colore rosso gli specifichiamo la grandezza del del marker e vedete il risultato è questo punto qua ok va bene allora tutto questo è un arricchimento per diciamo scopi didattici per far per farvi inquadrare il problema e andare a richiamare poi alcuni concetti di base che ci servono abbiamo una nuvola di punti e queste che vi ho tracciato adesso vi faccio vedere come si generano sono delle rette che rappresentano degli ipotetici confini decisionali di altrettanti classificatori lineari per farle ho semplicemente fatto una cosa del genere esattamente come ho plottato quel punto quella croce con quelle coordinate qui c'è un ciclo foro in cui quello che viene fatto è viene fatto varia vengono fatti variare due parametri m e b m e b prendono dei valori che appartengono a queste tre possibili tre possibili liste alternative di valori quindi alla prima iterazione quindi abbiamo una lista di coppie di valori scusatemi in cui alla prima iterazione m vale 1 e b vale 0,65 alla seconda iterazione m vale 0,5 e b vale 1,6 e alla terza iterazione m vale meno 0,2 e b vale 2,9 a questo punto quello che facciamo con questo comando ad ogni iterazione prendere il vettore di 50 punti tra meno 1 e 3,5 e andiamo a calcolare quanto vale m per x fit più b cioè l'equazione di una retta per cui per ognuno di quei 50 punti andiamo a vedere con quel valore di pendenza e con quel valore di intercetta qual è il valore corrispondente della y e il risultato è questo sono tre rette ognuna di queste è stata plottata come vi ho appena descritto cioè come il risultato di prendere 50 punti vedete tra meno 1 e 3,5 per ognuno di questi 50 punti siamo andati a calcolare il valore della y corrispondente in questo caso è una di quelle tre poi per la seconda e poi per la terza la stessa cosa l'abbiamo ripetuto per quindi abbiamo calcolato questo punto questo punto e questo punto per questo, questo e questo per tutti i 50 punti chiaro cosa abbiamo fatto? quindi abbiamo in questo ciclo for fatto nulla di più che andare a creare tre rette con tre equazioni diverse come ho scelto i parametri? sono stati scelti arbitrariamente quindi uno è chiaramente la pendenza che corrisponde alla bisettrice del primo quadrante poi ne abbiamo altre e così via ok? quindi per intenderci abbiamo scusatemi, non è la bisettrice del primo quadrante perché ovviamente uno è la pendenza sarebbe la bisettrice del primo quadrante se avesse intercetta nulla invece c'è intercetta pari a 0,65 1,6 o 2,9 quindi questa che è quella per esempio la identifichiamo perché è l'unica che ha la pendenza negativa quindi questa retta qua ok? gli altri due hanno pendenza positiva e in particolare quella che ha la pendenza maggiore che sarà vediamo un po' questa perché è 1 che è maggiore di 0,5 sarà la retta con pendenza maggiore cioè sarà questa ok? sono quelle tre rette quindi le abbiamo semplicemente graficate va bene? perché vi ho fatto questo? perché è semplice diciamo una semplice visualizzazione del concetto che abbiamo espresso quando abbiamo cominciato a ragionare sull'SVM cioè con il classificatore a margine abbiamo detto che se abbiamo un dataset linearmente separabile questi due questo dataset è linearmente separabile io lo posso separare in infiniti modi questi tre confini decisionali corrispondono a tre classificatori sono tutti e tre corretti perché tutti e tre vi classificano correttamente questo insieme di punti del dataset di addestramento giusto? vedete se io scelgo questa diretta tutti i punti che stanno sopra verranno classificati come blu tutti i punti che stanno sotto come verdi e non sbaglia nessuno del dataset di addestramento stessa cosa per quest'altra retta e per quest'altra ma quando andiamo a prendere altri punti quindi un ipotetico test set in cui abbiamo questo punto che è questa croce che vi ho appena disegnato cosa succede? succede beh che questo sta sul lato positivo di questo classificatore e di questo classificatore che lo classificheranno quindi come blu ma se noi avessimo questo classificatore questo punto verrebbe classificato come verde quindi di nuovo è vero che ne esistono infiniti non tutti sono allo stesso modo uguali nei confronti di variazioni rispetto alla distribuzione dei punti del dataset di addestramento e quindi di nuovo il concetto di margine diventa importante cioè introduciamo il concetto di margine proprio per poter sceglie tra queste infinite alternative quella che ci separa queste due nuvole di punti con il massimo margine e qui per farvi richiamare il concetto di margine quello che viene fatto nella nella nella cella successiva è proprio un plot di nuovo di queste due nuvole di punti di quelle tre rette e dei relativi margini che sono stati calcolati in questo caso diciamo sono valori inseriti manualmente quindi diciamo creati ad hoc ma giusto per farvi vedere come vengono disegnati così vedete anche un'altra funzionalità di matplotlib c'è una simpatica funzione che si chiama fill between che vuol dire rienti in mezzo e qui guardate quello che viene fatto è di nuovo fare un ciclo fori in cui ai parametri m e b che sono i parametri pendenza intercetta di quelle tre rette viene aggiunto un terzo parametro che è la dimensione del margine di quel buffer ok quindi sarebbe il margine di ognuno di quei classificatori questo è stato fatto ad hoc quindi vi dico io è 0.33 oppure 0.55 oppure 0.2 poi quello che viene fatto viene generato il valore di y corrispondente come avevamo fatto prima viene plottata la retta in colore nero il k è colore nero come abbiamo fatto prima e poi viene utilizzata questa funzione fill between in cui gli si dice guarda prendi quei valori della x poi prendi tutti i valori tra y meno d e y più d dove d è il margine che è diverso per ognuna delle tre rette e lui va a riempire con un colore che gli potete specificare qui c'è il codice e gli potete specificare anche il livello di trasparenza va a riempire quello spazio quindi praticamente qui sono tre rette e ognuna di quelle tre lui va a disegnare vedete con un certo livello di trasparenza vedete va a riempire tra questo e questo vedete questo è il margine più alto che corrisponde alla retta con diciamo la pendenza positiva meno grande la retta con la pendenza positiva meno grande è questa e vedete a quella corrisponde un valore di d che rappresenta il margine che è il più alto tra i tre e lui infatti va a riempire quella fascia con un valore più alto il margine più piccolo ce l'ha 0.2 la retta con pendenza negativa e infatti lui è andato a disegnare vedete qui un margine molto piccolo questo ancora non ci dice nulla se non ci richiama alla mente è utile per andare un attimo a rinfrescare il concetto di margine perché a questo punto tra questi tre classificatori beh ovviamente avrà senso andare a scegliere quello perché l'obiettivo delle macchine a vettore di supporto è proprio quello di scegliere il margine massimo tra tutte le infinite alternative fin qui semplicemente semplicemente l'utilizzo di matplotlib e di funzionalità per generare il dataset di punti ma ancora fino a qui nulla che abbia a che vedere con l'SVM come codice e allora cominciamo a metterlo invece in campo e allora per per mettere in campo appunto il codice relativo al support vector andiamo a prendere il modulo SVM da scikit-learn quello è il modulo dedicato alle support vector machine vi permette di fare anche regressioni noi ne l'abbiamo visto con le support vector machine vi permette di fare tutta una serie di cose quindi tutta una serie di funzionalità relative a questo e definiamo il modello il modello si definisce in questo modo è un oggetto in questo caso che viene creato con la classe SVC SVC sta per support vector classifier e qui poi magari andiamo a vedere la documentazione prevede che voi specifichiate una serie di parametri altri sono ovviamente questi possono essere anche questi di default hanno dei valori di default cioè se non li specificate qui diciamo ne andiamo a specificare due un primo parametro si chiama kernel e può prendere più valori allora come vi ho detto a lezione vi ho detto guardate che le SVM hanno una notevole flessibilità perché vi permettono di costruire dei classificatori lineari ok ed è la formulazione che noi abbiamo visto ma vi permettono anche di costruire dei classificatori non lineari con determinate tecniche che non abbiamo avuto modo di vedere ma oggi adesso magari vi faccio vedere qualche cosa però che adesso possiamo un pochino diciamo capire forse meglio come funziona no? alla luce delle cose che abbiamo fatto a lezione sulle trasformazioni non lineari non l'abbiamo vista a proposito dell'SVM però il concetto è sempre lo stesso cioè trasformare i dati iniettando una non linearità e poi applicarci un'SVM di fatto lineare nel dominio trasformato quindi questo parametro kernel sfrutta qui dietro c'è di più di quello che vi sto dicendo quindi sto iper iper semplificando però questo parametro kernel sfrutta di fatto questo concetto cioè voi potete andare a specificare una trasformazione in tipo non lineare e costruire una SVM su quella trasformazione non lineare poi nel notebook c'è un esempio di come funziona questo ve lo faccio vedere non è banale perché non è mai banale cercare di capire le trasformazioni allora qui ve ne mette a disposizione se non sbaglio tre c'è una trasformazione di tipo polinomiale e poi ci sono dei kernel cosiddetti RBF si chiamano che sta per radial basis function che non le trasformazioni tipo come se fosse trasformato secondo una gaussiana adesso vi faccio vedere poi l'esempio molto velocemente però l'obiettivo non è entrare nel dettaglio di questi che sono ripeto dei dettagli sulle trasformazioni non lineari che non abbiamo visto non avremo modo di vedere però vi segnalo che ci sono giusto per dire cosa vuol dire che qui andiamo a specificare kernel uguale linear vuol dire che non facciamo nessuna trasformazione vogliamo costruire l'SWM esattamente come l'abbiamo vista a lezione un classificatore lineare però sappiate che se cambiate questo parametro potete fare anche delle trasformazioni tipo non lineare come sempre il presupposto è che sappiate che cosa andate a fare nel momento in cui fate quelle trasformazioni dovete avere le idee chiare su come andate a trasformare le feature di ingresso va bene? a noi per il momento interessa costruire un classificatore lineare ci siamo? secondo parametro secondo parametro è un coefficiente c che è vi ricordate quando abbiamo visto la formulazione soft margin che aveva quel termine di regolarizzazione noi andavamo a minimizzare la funzione di costo e poi c'era lambda per il termine di regolarizzazione ok? questo c lo potete leggere come l'inverso di quel termine lambda per cui vedete qui è un valore molto alto perché 10 alla 10 questo significa che quel termine di regolarizzazione è circa 0 quando quel termine di regolarizzazione è circa 0 vuol dire che cerchiamo una soluzione di tipo hard margin vi ricordate? più alziamo quel valore lambda quindi più diminuiamo c più andiamo nella direzione del soft margin quindi questo è un parametro che vi permette di giocare su quel termine di regolarizzazione ok? quello che fate nel momento in cui avete definito l'oggetto model a partire dalla classe svc e poi potete fare il fit che è il vero e proprio addestramento quindi gli passate l'insieme di vettori x che rappresentano il vostro il vostro dataset di addestramento tutte le etichette y e lui fa l'addestramento come con una serie di algoritmi anche qui volendo potete specificare l'algoritmo che usa ma utilizza delle librerie poi andiamo a vedere la documentazione anzi questo può essere senz'altro utile ma diciamo in prima battuta utilizzare ovviamente le l'algoritmi che vi propone di default è sempre una buona una buona scelta e vedete qui vi da un riassunto vi dice questo è il valore dc ripeto l'inverso è il parametro di regolarizzazione quindi è quasi zero praticamente nullo ed è un svm di tipo lineare adesso magari andiamo a vedere anche la documentazione perché può essere utile vedere come funziona o Grazie. se voi andate sulla documentazione di scikit-learn vedete l'SVC avete tutta una serie di indicazioni di come funziona questa classe questa è una classe appunto che appartiene al modulo SWM ed è una classe in cui definite passate diciamo una serie di parametri e noi li abbiamo passati vedete il default per C per quel parametro di regolarizzazione è 1 e noi gli abbiamo detto invece un numero molto grande poi vedete il default per kernel è RBF non è quello lineare questo fa una di quelle trasformazioni che vi dicevo poi gli potete passare tutta una serie di parametri che adesso non andiamo a vedere neanche nel dettaglio e niente definite appunto questa classe poi vi dà una serie di informazioni che potete andare a leggere andiamo a vedere un pochino i parametri un po' più nel dettaglio se scendete sotto quello che avete vedete che C è un floating che è un parametro di regolarizzazione cioè la forza della regolarizzazione è inversamente proporzionale a questo valore e il termine di regolarizzazione vedete è un termine di tipo L2 e a questo punto noi sappiamo che cosa significa introdurre un termine di regolarizzazione di tipo L2 vi ricordate L2 è la norma L2 quindi la norma Euclidea standard che abbiamo introdotto vedete come avete già cominciato ad acquisire tutta una serie di informazioni che vi permettono anche di decifrare più in dettaglio una documentazione di questo tipo che fino a due mesi fa chiaramente non vi avrebbe detto molto e questo fa probabilmente la differenza tra chi queste cose le utilizza senza sapere bene cosa sono tutte queste questa serie di dettagli che sono delle manopole che avete a disposizione per costruire un sistema e chi invece queste cose un po' le ha studiate e comincia a essere consapevole di che cosa vuol dire andare a cambiare certi valori numerici in certi parametri questo è il grado dei polinomi nella funzione Kerner se voi specificate una trasformazione di tipo polinomiale gli potete dire qual è il grado del polinomio della trasformazione questo fattore gamma vi permette invece di regolare quelle RBF o altri parametri di trasformazione quindi questo per costruire quelle trasformazioni non lineari poi avete altri parametri che adesso non ci interessa più di tanto andare a vedere per esempio c'è il numero massimo di iterazione cioè potete dire ok siccome il solver l'ottimizzatore è un ottimizzatore che lavora in maniera iterativa li abbiamo visti no? potete scegliere se fissare un numero di iterazioni massimo oppure dire il default meno uno no? vai avanti fino a che non trovi la soluzione e quindi insomma ci sono altre cose per esempio vedete qui ci sono le opzioni quando avete un problema multiclasse se restituire una funzione di decisione tipo one versus rest e vi specificate questo è il default oppure one versus one e anche qui l'abbiamo visto abbiamo detto che cos'è qual è la differenza e quindi sapete che cosa significa tutte cose che vi invito magari andare a vedere con un po' di calma perché vi aiutano a capire tanti aspetti e a coniugare la teoria poi con delle cose che sono gli strumenti pratici per questo è uno strumento di utilizzo professionale che viene comunemente utilizzato e l'altra cosa interessante è che al cambiare questi parametri cambiano non è detta che cambi sempre ma cambiano chiaramente le prestazioni del sistema che andate a costruire e questa è un'altra cosa che secondo me è un argomento potenziale parlavamo la volta scorsa dei dei progetti andare a esplorare diversi parametri di un modello su un dato problema cioè fissate un dataset e cominciate a fare un'esplorazione dei parametri o degli iper parametri del sistema è sempre un buon modo per farsi un'idea di che cosa di come funziona un per esempio un modello e produce avete tanti dati a quel punto a disposizione potete anche costruire perché no un progetto in cui esplorate diversi diverse modalità di funzionamento di un determinato modello questo può essere un'idea quindi fare un'esplorazione di questo tipo ok va bene random state serve per fissare il il seed dei generatori numeri pseudo casuali andiamo a vedere quali sono questi sono gli attributi volevo far vedere anche qualcosa sul questo è l'esempio ecco su il i metodi vi permette di restituisce la funzione di decisione quindi esattamente per ogni campione una misura di quanto vedete della pro vedete anche qui ci sono se specificate one versus one i valori di questo metodo vi permettono di ricavare praticamente sono proporzionali alla distanza dei campioni dall'iper piano che separa che li separa dai punti quindi sapete esattamente anche quanto quanto sono distanti quei punti dall'iper piano in termini di distanza poi c'è il metodo fit il metodo fit fa quello che abbiamo detto cioè gli potete passare x e y noi non gli abbiamo passato un terzo parametro il terzo parametro guardate si chiama sample weight che vuol dire il peso del campione significa che voi potete passare un array in cui per ogni campione gli specificate qual è il peso che ha e anche qui potete andare a vedete vi dice che pesi più alti forzano il classificatore a mettere più enfasi su quei punti anche questa è una cosa che abbiamo visto a lezione quando vi ho parlato della regressione della classificazione pesata vi ricordate e anche questa è una cosa che volendo potete provare a esplorare cioè andare a vedere che cosa succede se alcuni punti li pesiamo di più o li pesiamo di meno come cambiano le prestazioni del classificatore che cosa succede ha senso in alcuni casi farlo sì se assumo che alcuni punti siano meno affetti da rumore di altri può avere senso assolutamente oppure se ho un dataset sbilanciato può essere un modo per dare più peso ai punti della classe meno presente queste sono tutte cose che anche queste possono essere eventuali spunti da esplorare se qualcuno è interessato per un eventuale progetto questo è il metodo fit ma poi avete altri altri metodi a disposizione ripeto potete avere chiaramente avete il metodo predict una volta che l'avete addestrato vi fa la classificazione cioè gli passate un vettore di campione una matrice appunto dove ci sono i campioni e vedete per un modello vi dice o più uno o meno uno per un modello diciamo di classificatore binario cioè vi restituisce le predizioni allo stesso modo potete accedere a tutte quelle che sono le probabilità sia il logaritmo di queste probabilità ma anche le probabilità stesse perché c'è il metodo predict prova che vi calcola la probabilità degli outcome degli esiti di una classificazione cioè non vi dice solo più uno o meno uno vi dice più uno no vi ricordate alla nozione di margine di distanza è associata una valutazione che viene trasformata con la sigmoide in un numero tra zero e uno che possiamo interpretare come probabilità cioè se zero otto vi dice ok sono sicuro all'ottanta per cento che questo sia una classe più uno se ho un punto più vicino al margine e quel valore è zero quattro sono sicuro al quaranta per cento che è di meno chiaramente lo avevamo visto a lezione ma qui avete anche qui la possibilità di andare a esplorare questo tipo di meccanismo più nel dettaglio poi direi che altre cose al momento non vale la pena andarle a vedere ma insomma ok mi sembra che nel momento non ci sia vediamo un po volevo vedere se c'era qualcosa sui solver vediamo un po se troviamo vengono utilizzati direi dei solver che sono sì qui ci sarebbe un confronto anche vedete vi mette a disposizione anche un altro un'altra un'altra tipologia un'altra classe che si chiama linear svc che specifica per i classificatori lineari proprio support vector quindi non è quella generale che vi ho fatto vedere finora ma dovrebbe essere se non ricordo male più efficiente nel senso che vi permette di specificare allora ecco vedete le differenze principali sono in termini della funzione di loss che viene utilizzata pensavo ci fosse qualcosa sul solver solver sempre lo stesso quindi evidentemente ok vedete ci sono un po di differenze tra svc e quest'altra e linear svc per esempio qui vi dice che allora per esempio vi dice che lsvc può essere più adatto in caso di se avete problemi di occupazione di memoria dati sparsi quindi dunque sparse data will steven call memory copy quindi comunque comunque se i dati sono sparsi c'è un problema di fatto che fa la copia di memoria va bene adesso questi sono dei dettagli che probabilmente non ci interessano più di tanto però vi volevo ecco vedete c'è anche un sgd classifier che avete la possibilità di utilizzare ma che dovrebbe essere più efficiente allora c'era qualcosa adesso non lo ritrovo poi ve lo rivirò potete specificare dei solver più efficienti vediamo un po' se è questo no non era questo quello che volevo far vedere va bene lasciamo adesso un attimo il sospeso qui diciamo che allora diciamo che lasciamo il sospeso un attimo perché volevo farvi adesso se lo recupero mi sembrava che ci fosse l'indicazione anche di quali tipi di di solverve tra quali tipi di solverve come esattamente c'era per il la regressione logistica ma non lo riesco a ritrovare vediamo un po' qui non c'è perché direi che non c'è tra i pesi e in quest'altro neanche ok no vedete come sempre esistono più modi diversi di fare le stesse cose la formulazione è sempre la stessa direi che noi torniamo al nostro esempio però mi piaceva no non c'è va bene ve lo dico quando lo ritrovo qual è il tipo di comunque lui praticamente va a risolvere un tipo di problema che è esattamente quello che abbiamo indicato a lezione cioè se vi ricordate è la trasformazione di quel problema no il margine massimo ci impone di risolvere un problema di tipo vincolato lui va a trasformarlo in una formulazione soft margin e risolve quel tipo di problema con dei metodi di ottimizzazione del primo ordine volevo ritrovare i metodi per vedere se potevate specificarli ma vedo che nella classe non c'è la possibilità di specificarli come come c'è nella regressione logistica quindi probabilmente qui dovete prendere il solver che vi dà e questo utilizzate però tra svc e lineare svc secondo me cambia anche il solver volevo recuperare quello ma non sono sicuro di poterlo fare però magari anche qui potete andare a studiare un po' la documentazione magari vedere voi se queste due che sono ripeto due formulazioni due implementazioni lineare svc e svc che trovate all'interno della libreria o anche vedete vi dà anche la possibilità di usare questa che differenze hanno da un punto di vista per esempio del se utilizzano lo stesso ottimizzatore metodo del primo ordine di scese del gradienti questo utilizzerà un metodo di scese del gradiente stocastico per esempio che dovrebbe essere più efficiente quindi come cambiano sullo stesso dataset ad esempio i tempi di esecuzione questo volevo andare a ricavare cambieranno leggermente anche i risultati fondamentalmente saranno sempre gli stessi va bene adesso torno al nostro notebook e cerchiamo di andare avanti abbiamo fatto questo excursus perché come vi dicevo andare a vedere la documentazione è una cosa utile quindi vi invito senz'altro a farlo perché è un qualcosa che nel quale vedete molte cose che abbiamo visto a teoria come in teoria come vengono messe in pratica oh qui c'è la prossima cella di questo di questo notebook adesso ci sono un po' di cose allora questa questa cella potrebbe essere tranquillamente saltata quindi adesso non vorrei spenderci più di tanto tempo per cui c'è un po' di codice vi faccio vedere rapidamente che cosa fa però l'obiettivo dell'esercitazione non è costruire una funzione che permetta di fare il plot del confine decisionale perché chiaramente è un qualcosa che è utile finché siamo in una o due dimensioni come in questo esempio da cui siamo partiti e non farete se avete invece dei dati ovviamente in 15 20 vettori da 50 100 feature o quello che è per cui io ve lo lascio perché secondo me è significativo da un punto di vista proprio didattico farvi vedere come il confine decisionale che sono cose che abbiamo già visto poi quando abbiamo fatto le lezioni di teoria nelle slide ci sono appunto questi queste figure vi faccio vi lascio il codice di come può essere generata questo confine di decisione non andiamo troppo nel dettaglio vi faccio vedere un po' velocemente perché ripeto questo step lo potreste tranquillamente saltare cioè una volta che avete fatto il fit mi seguite non ci interessa più di tanto andare al grafico potremmo andare direttamente a fare il predict ok però giusto per farvi vedere che cosa succede qui c'è una funzioncina che è stata chiamata plot svc decision function che prende come input il modello che abbiamo di cui abbiamo appena fatto il fit e poi una figura che potete anche non specificare se non la specificate viene generata è un oggetto di tipo grafico che viene generato all'interno eventualmente del di questa funzione e poi potete dire se volete che vengano evidenziati i vettori di supporto oppure no ok è anche un parametro di tipo verboso per stampare la shape delle figure allora che cosa fa questa questa questa funzioncina va bene se gli avete passato un oggetto grafico va a prendere i valori minimo e massimo della figura se no genera lui un nuovo oggetto grafico lo chiama ax e poi crea una griglia di valori per valutare il modello allora cosa vuol dire che crea una griglia di valori dopo vi faccio vedere come come lo fa ma intanto andiamo a vedere lo fa in questo modo guardate guardate questo codice ok infatti qui vi ho detto guardate la cella sotto che cosa fa questa funzione allora qui generiamo x e y che sono due vettori tra il valore minimo e il valore massimo del grafico e ne generiamo 30 ok dopodiché invochiamo questa funzione mesh grid che crea una griglia di valore cioè fa questa cosa qui sotto guardate ve l'ho rifatta vedere vedete qui gli ho detto tra 0 e 5 10 punti tra 0 e 5 10 punti e lui genera sia sull'asse x e sull'asse y vedete 3 6 10 punti e poi va a creare una griglia a partire da questi 10 è qui spaziata va bene quindi mesh grid crea questa griglia di punti ci siamo ok allora come viene utilizzato questo all'interno di questa funzione che fa il plot del confini decisionali semplicemente viene generata una griglia di punti come vi dicevo prima che vengono poi con questa istruzione adesso non entriamo nel dettaglio vstack sta per concatenare concatena esatto questi vettori e li mette tutto insieme quindi qui genera sono 30 per 30 sono 900 punti e li mette in un unico vettore dopodiché con quei 900 punti va ad accedere alla funzione decision function ok che vi avevo fatto vedere prima della classe svc quella funzione vi restituisce dei valori che sono proporzionali alla distanza dal margine cioè per ognuno di quei 900 punti in p vi dice quali sono la distanza dal margine o comunque vi restituisce un valore proporzionale alla distanza dal margine dopodiché con quello viene invocata una funzione di matplotlib che vi permette di creare delle curve di livello gli passate x y e gli passate il valore in questo caso in p della distanza dal margine ok gli specificati i livelli in questo caso meno 1 e 1 e lui va a creare delle curve di livello che saranno delle rette che stanno al livello meno 1 al livello 1 rispetto al livello 0 su questa griglia dei punti con questo valore p che rappresenta i livelli di valore della re cioè la distanza dal margine si fa prima a me andarlo in esecuzione che a spiegarlo perché il risultato è questo guardate guardate guardate cosa succede io qui plotto la mia nuvola di punti e poi invoco questa funzione a cui passo il nostro modello ok il nostro modello ci abbiamo anche i vettori di supporto glielo fatti stampare sono questi il risultato è questo qua allora scusate aspettate ecco se no lo devo rimandare in esecuzione perché se l'ho fatto più volte ha ripreso i valori di prima scusatemi eccolo qua questo questa è la griglia di punti e questo è il risultato dell'esecuzione siamo qua in questa cella guardate l'esecuzione di questa cella che cosa ha fatto lui vi ha creato con il modello con quella funzione plot decision function vi ha creato queste sono le curve di livello cioè a più 1 e a meno 1 cioè lui ha preso su quella griglia di punti lui ha creato una superficie in cui per ognuno di quei 900 punti è andata a vedere la distanza dal margine e poi ha creato delle curve di livello con quella superficie in cui gli abbiamo specificato meno 1 scusatemi più 1 e meno 1 e 0 che sono i tre livelli in cui lui è andato a tagliare quella superficie e dopo gli è andati a graficare qui e il risultato è questo allora tutto questo come vi ho detto non è strettamente necessario al notebook quindi prendetelo se volete se avete tempo e voglia provate andare a capire come funziona questa funzioncina plot svc decision function ci sono andato molto veloce volutamente ripeto se volete avete dei dubbi o vi interessa ne riparliamo più con calma ma è una cosa che potete guardare con calma voi se non ci riuscite però non importa perché sapete la utilizzate come una sorta di black box a cui passate il modello e gli fa e sapete che farà la il plot del confine decisionale ovviamente lo fate solamente in due dimensioni di più non ci riuscite ok però non ci interessa questo perché è semplicemente per farvi vedere che cosa che quello che lui ha plottato è il confine decisionale e questo che cos'è il buffer con il margine più ampio ok quindi quella funzione plot decision function serve per generare queste tre reti questo è il succo del discorso va bene come lo fa sarebbe interessante ma dovremmo stare qui diciamo un po troppo tempo e non è non è non è l'obiettivo del dell'esercitazione però mi piaceva metterla perché comunque ve la lascio perché abbastanza esemplificativa fa un'altra cosa quella funzione plot decision se voi andate a vedere torno un attimo su torno un attimo su questa funzione plot decision function che abbiamo appena specificato vedete che c'ha come supporto cioè scusatemi come parametro questo parametro plot support che di default è uguale a true se plot support è uguale a true quello che fa è qui entra qui dentro e semplicemente vi va ad accedere a i vettori di supporto che sono degli attributi del vostro modello l'oggetto modello si porta dietro che cosa i vettori di supporto cos'erano i vettori di supporto ve lo ricordate da lezione erano quei punti del dataset di addestramento che passavano esattamente sui confini del margine e quindi voi a quel punto se conosceste quelli potete anche buttare via tutto il resto del dataset perché esatta il vostro modello rimane sempre lo stesso e qui glieli facciamo vedete va a prendere questi sono proprio dei vettori definiamo la dimensione 300 e li andiamo a a plottare in nero edge color su uguale k e sono questi questi cerchi qua 1 2 e 3 quindi sono tre vettori di supporto cioè voi potreste tranquillamente liberarvi di tutti questi punti qua e con solo quei tre definire questo oggetto chiaramente qui ci arrivate solo dopo che avete fatto tutto l'addestramento quindi diciamo però è per farvi capire anche qui di nuovo cosa l'avevamo detto a lezione da dove deriva il nome ecco perché è interessante ok allora come vi dicevo prima tutto questo potremmo anche farne a meno questo è semplicemente per ritornare a agganciare la teoria all'esercitazione di oggi ok ecco anche perché sono andato un po' veloce su quella cella prima che fa il plot della funzione di decisione ripeto se vi interessa la potete approfondire andate a guardarla e sicuramente male non fa però diciamo se non riuscite adesso a capire tutti i dettagli ci sta è solo per fare questo collegamento che mi sembrava comunque significativo andiamo invece avanti e vediamo un attimo cosa succede allora qui vi ho scritto che punti che stanno lontano dal margine ma sul lato corretto non modificano il modello cioè non contribuiscono alla funzione di loss che è il concetto proprio di vettore di supporto qui c'è un'altra funzioncina che fa una cosa di questo tipo questa funzione andiamo a vedere che inizia qui alla riga 1 e finisce alla 14 fa questa cosa allora viene chiamata plot svm prende di default 10 e un valore di punti uguale a 10 questo qui è sempre un oggetto grafico e quello che fa al suo interno è generare il dataset con 200 punti due centri e questa deviazione standard x e y sono appunto i valori corrispondenti andiamo a prendere tutti i punti del nostro dataset quali? allora qui in realtà definiamo il nuovo x come l'x che è stato generato 200 punti in cui andiamo a prendere in realtà i primi 10 punti oppure se gli specifico qualcos'altro i primi qualcos'altro punti e lo stesso è il valore della y cioè vado a selezionare da questi 200 un sotto insieme ok? e poi con quello vado a creare l'oggetto a partire dalla classe svc e vado ad addestrarlo cioè vado ad addestrare sui primi punti su un sotto insieme di punti e vediamo che cosa succede poi qui c'è il plot di questi punti e di nuovo invochiamo quella funzione che plotta la decision function ok andiamo a vedere come può essere utilizzata questa funzione adesso si capirà perché abbiamo fatto questo giro me la mando in esecuzione questa qui che cosa viene fatto? guardate viene creato un oggetto grafico a partire dalla funzione di matplotlib subplot subplot vi crea in questo caso due figure possiamo specificare anche la dimensione una riga e due colonne va bene? c'è una figura e ci sono gli assi di questa figura ok? quindi questo è l'oggetto grafico dopodiché facciamo un ciclo for in cui gli passiamo alla prima iterazione la prima figura alla seconda iterazione la seconda figura quindi alla prima iterazione siamo qua e alla seconda iterazione siamo qua e gli passiamo un secondo vettore un valore che alla prima iterazione è 60 e alla seconda iterazione è 120 e poi andiamo a invocare quella funzione che abbiamo appena scritto con 60 e 120 che significa che andiamo a selezionare 60 punti dei 200 alla prima iterazione oppure 120 punti alla seconda iterazione ci siamo? ok? alla prima iterazione infatti se andate a vedere quella funzione che abbiamo appena scritto che cosa fa? genera 200 punti ne va a selezionare un certo numero 60 e va a fare l'addestramento o siccome in quei punti vedete sia nel primo che nel secondo comunque sono compresi questi nel primo e secondo caso nel secondo caso lo facciamo con 120 il primo con 60 vedete che non cambia nulla cioè tra questo grafico e questo dove qui avete più punti non cambia niente perché non cambia niente? perché avete sempre quegli stessi vettori di supporto questo è quindi un esempio per farvi vedere di nuovo il concetto di vettore di supporto il vettore di supporto è tale per cui se io ho questi punti sono quelli che definiscono il margine io potrei come vi dicevo prima anche non avere questi punti e sarebbe sempre lo stesso modello infatti qui ne ho presi 60 qui ne ho presi 120 ma non cambia il modello questo è un esempio per farvi vedere di nuovo il concetto di margine, vettore di supporto e così via però di nuovo ancora non è che abbiamo fatto abbiamo fatto il fit del modello ma ancora non l'abbiamo utilizzato per fare chissà quali cose nel senso che fino adesso abbiamo girato intorno all'azione di margine abbiamo creato delle funzioni con matplotlib eccetera eccetera ma siamo sempre riagganciati un po' la teoria ma non siamo andati molto avanti perché non abbiamo fatto il predicto ah qui c'è un'altra cosa che va bene qui c'è un'altra cosa che vi ho voluto lasciare vi faccio vedere anche questa perché allora questi sistemi sono sistemi molto carini anche per la presentazione dei dati se uno lavora nell'ambito data science eccetera anche far vedere i dati secondo me non solo secondo me è in generale un aspetto critico importante e allora vi faccio vedere anche al volo quest'altra cosa poi chi vuole la approfondisce allora c'è una libreria si chiama IPy Widgets IPy sta per Python Interattivo quindi è qualcosa che vi permette di gestire dei widget degli oggetti che interagiscono con l'utente quindi sono delle librerie in cui avete la possibilità di costruire come in questo caso un grafico che ha vedete un menu a tendina importate interact e fixed che sono due moduli e per esempio la funzione interact prende una funzione che è quella che abbiamo utilizzato noi sopra prende due parametri 10 e 200 e vi genera un menu a tendina in automatico in cui 10 oppure 200 sono i parametri che vengono dati come input in automatico al plot svm quindi il risultato è questo vedete qua sotto cosa succede se n è uguale a 10? se n è uguale a 10 lui seleziona 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 punti di quei 10 punti vedete ci sono due vettori di supporto lui ha invocato la funzione plot svm che permette di disegnare a partire da quella nuvola di punti questo è il confine decisionale e vedete il confine decisionale questo è il margine la cosa carina di questo IPy Widget è che possiamo fare una cosa di questo genere vedete lui mi ha messo qui questo in automatico questo menu a tendina io gli ho specificato 10 e 200 se gli metto 200 guardate cosa succede sotto mi ha generato un altro grafico vedete i punti ci sono tutti ovviamente se io vado a cambiare i parametri cioè se vado a eseguire questa cella anziché 200 ci vado a mettere 50 e la mando in esecuzione ovviamente l'opzione che ho a disposizione è 50 o 10 ed ecco questo con 50 punti e quindi è uno strumento molto carino ripeto lavora sulla funzione che abbiamo scritto sopra ok detto ciò andiamo un po' più nella sostanza perché fino adesso abbiamo lavorato sull'aspetto grafico che poi in realtà è sostanza anche quella perché ripeto l'ho fatto per farvi collegare al meglio la parte di teoria con la parte di laboratorio però torniamo a noi supponiamo di generare di nuovo con questa funzione make blob dei punti in questo caso vedete ho generato 100 punti e ho leggermente aumentato la deviazione standard prima era 06 adesso 1.2 il risultato è che questa nuvola di punti è più sovrapposta questo significa che questi punti non sono più separabili linearmente se voi tracciate un confine decisionale lineare chiaramente sono separabili linearmente ma non più in maniera perfetta ok quindi quindi significa che se noi cerchiamo di risolvere questo task con una formulazione hard margin non ci riusciamo dobbiamo ricorrere a quella soft margin perché vi ricordo che quella hard margin cerca proprio di massimizzare il margine invece quella soft ammette degli orrori ammette una separabilità non perfetta e l'abbiamo visto prima tutto questo viene controllato da un parametro di tuning che viene chiamato nella libreria C che è inversamente proporzionale a quello che noi a lezione abbiamo chiamato lambda quindi valori grandi di C vanno verso una formulazione di tipo hard margin e i punti non possono stare dentro il margine una formulazione invece di tipo soft vi permette di andare a costruire un classificatore lineare in cui ci sono dei punti che cadono all'interno del margine questo era il concetto che c'era dietro e se noi andiamo a questo punto a fare a creare un modello della classe svc di tipo lineare passandogli 10 alla seconda 100 come parametro C quindi è una formulazione abbastanza ancora hard margin non troppo ma abbastanza facciamo il fit di quello che succede e andiamo poi a fare il plot sempre di tutti i dati e andiamo a invocare quella funzione che abbiamo definito sopra per vedere qual è il confine decisionale cosa vi aspettate che succeda? succede una cosa di questo tipo vedete è una formulazione che con C uguale a 100 va verso la direzione hard margin ma non è hard margin è una formulazione soft margin cioè ci sono dei punti che comunque lui ha creato un confine decisionale che è questo questi sono i chiaramente i margini del buffer quindi corrispondono a più o meno uno ma qua dentro ci stanno dei punti ora se io avessi specificato 10 alla 10 come ho fatto prima questa è una formulazione hard margin cosa succede secondo voi? cosa vi aspettate? si stringe il margine fino a in realtà a diventare di fatto un qualcosa che non è un margine perché lui non ci riesce perché lui cerca di trovare il margine che in modo che non ci stia nessuno di questi punti che ha dall'interno ma siccome non è perfettamente separabile lui non lo trova quindi quello che comincia a fare è cercare una soluzione cercare una soluzione e lui va avanti ma non la trova non riesce a trovare la soluzione non riesce a minimizzare quel problema in modo da vincolare il sistema a non avere punti dentro il margine vedete? lui va avanti lo dobbiamo fermare ecco guardate oppure se non lo fermiamo guardate quello che ha trovato niente margine a un certo punto chiaramente esce dal numero di iterazioni perché avrà comunque un valore fissato o comunque quello che trova è un margine nullo questa è la sua soluzione se invece io rigenero questi punti abbassando qui a 0.6 creo un dataset linearmente separabile ritorno a una formulazione di tipo hard margin ritorno chiaramente vedete a un margin di questo tipo e a questo punto a maggior ragione potrei introdurre un soft margin vediamo cosa succede e vedete qui l'ha trovato comunque quindi la formulazione soft margin è abbastanza utile questa insomma torniamo 1.2 ok ah beh qui vi fa vedere la stessa cosa che vi ho fatto vedere adesso a mano mi ricordavo che c'era anche questa questa cella allora qui vengono generati mi fa vedere cosa succede al variare proprio di un po' di questi parametri vengono generati 100 punti con una certa deviazione standard vengono create due figure una riga e due colonne qui va bene questo è semplicemente un aggiustamento a sinistra a sinistra a sinistra per creare un po' di spazio tra le due e poi c'è un ciclofor questo ciclofor che cosa fa prende la prima figura e gli passa 10 come parametro la seconda figura gli passa 0 1 come parametro questi parametri sono quel valore di c cioè viene creato la prima interazione un modello di tipo svm lineare con valore di vergolarizzazione 10 e viene fatto il fi poi vengono plottati quei punti e viene stampata graficata scusatemi la funzione di decisione vengono anche plottati i vettori di supporto viene dato un titolo agli assi eccetera eccetera alla seconda iterazione rifaccio la stessa cosa con c uguale a 0 1 risultato è questa cosa qui vedete c uguale a 10 e c uguale a 0 1 c uguale a 10 è una regolarizzazione un pochino più nella direzione dell'hard margin e vedete che lui riesce a trovare un margine stretto all'interno del quale non c'è niente questa rilasso quella condizione quindi aumento il peso del termine di regolarizzazione e lui qui dentro ci butta cioè crea un margine più ampio mettendoci dentro comunque diversi punti del dataset di destra ok questa è la versione diciamo automatizzata della cosa che facevo vedere prima ok poi vediamo anche come andare a misurare accuratezza del caso lineare quindi quello che si fa tipicamente in maniera molto andando dritti all'obiettivo è fare il fit quindi addestramento del modello e poi si fa predict e poi si vanno a misurare gli errori che uno ha fatto qui vi ho fatto vedere un po' di cose però ecco per fare fit e predict che vi governano quelli che sono i parametri del sistema cioè i parametri che governano il funzionamento del sistema e alcune caratteristiche adesso vi faccio una breve digressione sul fatto della non linearità che è quello che vi dicevo anche all'inizio della lezione cioè l'SVM le cosiddette kernel SVM allora qui ci sarebbe tanta teoria dietro quindi stiamo facendo un triplo salto mortale in avanti e indietro però diciamo che anche qui cercate di prendere quello che un po' che vi serve o che per avere un'intuizione della cosa ok quindi è giusto per farvi vedere secondo me è significativo perché si ricollega a diverse cose che abbiamo detto in questi giorni a lezione a proposito dell'iniezione delle non linearità ok però ci sono dietro un po' di cose che chiaramente andrebbero specificate in maniera più dettagliata però diciamo prendiamola prendiamola un po' così come un excursus in cui vi faccio vedere come di fatto questi modelli supportino anche la non linearità però senza andare troppo nel dettaglio allora qui quello che viene fatto viene importato un'altra funzionalità di Psychiclare che permette di creare dei dataset che sono praticamente vedrete disposti su dei cerchi sono sempre delle feature bidimensionali quindi riusciamo a graficarle la funzione che vi permette di crearlo è una funzione che si chiama make circles gli specificate 100 punti cioè gli specificate scusatemi il numero di punti in questo caso è 100 poi c'è un fattore che vi dice quanto questi punti sono disposti su dell'ellissio su dei cerchi e anche un fattore di rumore cioè quanto sono rumorosi e dopodiché se fate il plot adesso lasciate stare la retta viene fuori una cosa di questo genere cioè lui ha generato due classi disposte su due cerchi se io avessi messo il livello di rumore a zero qui quei punti vedete sono perfettamente disposti su dei cerchi quindi questo fattore governa quanto questi punti stanno più o meno su quei cerchi e l'altro fattore se lo vado ad aumentare vedete vi genera dei cerchi che sono scusate non ellittici ma come ho detto prima perché sono cerchi non sono ellissi ma sono più o meno vicini l'uno all'altro cose di questo tipo è un fattore che governa quel tipo di di parametro ok ci siamo poi anche qui se andate a vedere la la documentazione trovate quello che vi serve allora quindi ci siamo cosa fa quello questo oggetto make circle questa classe make circle permette di generare questo dataset un dataset in cui generate due classi di punti disposte su dei cerchi più o meno rumorose e più o meno ravvicinati ok a questo punto alla riga 4 che cosa abbiamo fatto qui abbiamo invocato la classe svc per creare un oggetto che chiamiamo clf quella classe è una classe di tipo classificatore lineare abbiamo fatto il fit di quei punti e poi abbiamo fatto il plot di tutti quei punti e abbiamo invocato anche quella funzione che abbiamo scritto sopra che si chiama plot svc decision function che fa il plot ovviamente del suo bravo confine decisionale e vedete che il suo confine decisionale in questo caso è questo ovviamente lui crea un classificatore lineare tutto quello che sta qua sotto sarà una classe quello che sta qua sopra un'altra classe e qui in mezzo tra l'altro ci cascano un sacco di punti quindi è facile vedere che fa un mezzo disastro chiaramente il problema è un problema altamente non lineare allora le svm vi dicevo se voi specificate quel parametro non più come linear ma con un parametro di tipo non lineare quindi avete la possibilità abbiamo visto di specificare o dei polinomi e il grado del polinomio oppure questo RBF che sta per radial basis function vi trasforma i dati di input come li trasformano? per farvi capire come li trasforma qui sotto c'è una cella che fa una cosa di questo genere li trasforma anzitutto aumentando la dimensione del problema cioè aggiunge una dimensione ulteriore a quelle due va in una terza dimensione e quello che fa è calcolare questa appunto funzione che viene chiamata funzione a base radiale radiale vuol dire raggio ed è una funzione che proprio lavora diversamente a seconda che i punti stiano più o meno lontani da un centro che è il centro della distribuzione quindi voi pensate a quei due cerchi concentrici i punti che stanno vicino al centro verranno immaginatevi centrarci una gaussiana quindi una sorta di campana tridimensionale i punti che stanno nel centro vengono trasformati verso l'alto e i punti che stanno a lato invece rimangono schiacciati verso il basso questo è quello che fa questa trasformazione e fa una cosa di questo genere vedete è un esponenziale e alla meno x2 adesso questo qui non serve neanche più di tanto entrare nel dettaglio perché poi lo fa lui in automatico però giusto per farvi vedere è una trasformazione di questo tipo in cui viene aggiunta una funzione esponenziale di tipo e alla meno x al quadrato che significa che i termini che stanno più lontani dal centro non subiranno nessuna trasformazione perché è un esponenziale quelli che stanno vicino al centro invece vengono aumentati adesso lasciate stare quello che succede qui in questa cella che è puramente a scopo così numerico qui vi faccio vedere anche questo ho utilizzato quell'iPy widget anche qui non ci focalizziamo su cosa fa cioè su come fa il dettaglio focalizziamoci su cosa fa quello che fa vi fa vedere uno scatter in tre dimensioni guardate in cui scusate lo devo rimandare in esecuzione perché prende quel valore di r che io ho calcolato prima vedete r è la terza dimensione e vedete che cosa fa cioè io prima il mio dataset è solo distribuito su x e su y io ho aggiunto una terza dimensione r che fa qualcosa di tipo e alla meno distanza dal centro al quadrato per cui punti che stanno più vicini al centro in questa terza dimensione vengono elevati quelli che stanno lontani dal centro in questa terza dimensione vengono pesati per e alla meno distanza al quadrato quindi e alla meno qualcosa al quadrato capite bene che va rapidamente a zero quindi tutti questi altri punti in blu nella terza dimensione hanno un valore nullo però quelli vicino al centro in questo caso vengono elevati allora perché questo trucco in questo caso cioè che cosa qual è l'obiettivo è arrivare a un trucco di questo tipo cioè aumentare di una dimensione il problema e trasformarlo iniettando di fatto della non linearità perché a questo punto cosa succede? succede che se io taglio qui con un piano questo è perfettamente separabile con un classificatore lineare è chiaro che io devo sapere la geometria del mio dataset di punti devo sapere un sacco di cose per cui però qualche volta diciamo questo non sempre funziona però qualche volta quello che si fa è provare a comunque dare dei valori a quelle trasformazioni e vedere cosa succede se uno trasforma non linearmente un dataset di punti e queste funzioni radiali spesso e volentieri anche quelle polinomiali vi permettono di ragionevolmente qualche volta ottenere migliori risultati se avete un dataset di tipo non lineare è chiaro che come ci si muove è si prova perché ci provano diversi diversi si prova l'RBF oppure si prova il polinomio grado 3 oppure grado 5 l'RBF a sua volta ha dei parametri quindi si entra in un ambito che è quello proprio dell'artigianato del provare trial and error perché o uno sa esattamente come funzionano le cose e allora fa una cosa di questo tipo a ragion veduta oppure è molto difficile capire dove si va con queste qual è la direzione che queste trasformazioni vi permettono di applicare ed è il motivo per cui in realtà quei sistemi che abbiamo cominciato a studiare che fanno questo in automatico da soli come appunto gli approssimatori universali diciamo hanno su questo un notevole vantaggio perché non dovete stare voi a dire tante cose della trasformazione ma è il sistema che poi alla fine la costruisce da sola da solo ok comunque andando a noi se andiamo a utilizzare la classe svc e non specifichiamo più linear ma RBF con tutti i parametri di default del caso qui specifichiamo un certo valore abbastanza elevato di c creiamo questo oggetto e guardate che cosa succede a questo punto se facciamo il plot della nostra nuvola di punti che è quello che c'è alla riga 1 il plot della funzione confine decisionale che abbiamo scritto all'inizio del notebook e andiamo a fare lo scatter plot dei vettori di supporto che è questo ok che saranno dei pallocchi di dimensione 300 quindi dei cerchi di dimensione 300 che cosa succede a questo punto secondo voi abbiamo la nostra nuvola di punti che ripeto sono quelle due nuvole di punti concentriche guardate il confine decisionale se utilizzo quel kernel RBF cosa è diventato lui ha fatto esattamente quella trasformazione che vi dicevo prima questi punti li ha ha aumentato di una dimensione li ha messi su un ha introdotto una terza dimensione e quelli che erano al centro li ha alzati quelli che erano ai lati sono spianati secondo una gaussiana siamo ai bordi di questa campana tridimensionale quindi non hanno una terza dimensione che è zero praticamente e lui lì taglia questo punto concettualmente come se costruisse a quel punto un un un un classificatore lineare su questo dominio trasformato quindi un po' quel gioco di iniezione della non linearità e qui vedete che questi sono i vettori di supporto li abbiamo plottati noi con dei cerchi di dimensione più ampia questo è il confine decisionale tutto quello che sta all'interno di questo cerchio verrà classificato come classe 1 tutto quello che sta all'esterno classe meno 1 e questo è il buffer questa zona qui queste definiscono il buffer ok questo era diciamo dovuto per quanto riguarda la non linearità torniamo però ai classificatori lineari e completiamo un attimo l'esercitazione nella direzione che vi dicevo di andare quello che si fa di solito che non è andare a fare tutti questi grafici perché solitamente si lavora con dati che sono a più alta dimensionalità e di solito quello che si fa è prendere i dati costruire il classificatore fare l'addestramento con il metodo fit e poi si va dritti all'obiettivo con il metodo predict e allora qui facciamo un po' di import di funzioni che abbiamo già visto la volta scorsa a proposito della regressione logistica c'è il pacchetto la sottolibreria metrics che è un modulo dedicato al calcolo delle metriche abbiamo visto c'è la 0 1 che vi permette di dire quanti errori avete fatto nella vostra predizione c'è anche un modulo che si chiama model selection l'abbiamo visto anche l'altra volta da cui importiamo train test split che vi permette di fare in automatico suddividere il vostro dataset lasciando una porzione da parte per il test e una porzione che invece è il vostro training set e lui lo fa in automatico gli specificate la frazione e lui automaticamente vi va a selezionare casualmente all'interno del dataset dei punti da dedicare al training e altri da dedicare al test qui di nuovo rigeneriamo il nostro insieme di punti 200 punti con una certa deviazione standard anche qui vi potete divertire a cambiare questi parametri e vedere che cosa succede di nuovo riga 7 che cosa facciamo lo scatter plot li andiamo a visualizzare e qui niente di nuovo mostriamo questo grafico e il risultato è questo sono questi i punti che abbiamo generato ok bene sono abbastanza sovrapposti vediamo adesso cosa facciamo con questa nuova di punti creiamo un modello dalla classe un oggetto modello chiamiamo model dalla classe svc torniamo nell'ambito lineare perché abbiamo capito che quello non lineare diciamo ci vuole bisogna muoversi con un pochino di cautela parametro c uguale a 1 quindi abbastanza basso per cui formulazione tipo soft margin addestramento con il metodo phi gli passiamo x train e y train x train e y train che cosa sono il risultato sono andato un po' sotto scusate allora qui va bene gli avevo fatto semplicemente stampare per prova dei valori tutte le righe fino alla 9 e tutte le i primi 9 elementi ed erano questi guardate glielo avevo fatti stampare vedete erano questi questi sono le prime i primi 6 9 punti del dataset e le relative classi ok vedete questi sono della classe 1 questi sono della classe 0 per esempio il punto 3 con coordinate 3.1 e 1.3 sarà sarà occhio croce abbiamo detto 3 e 1.3 sarà un punto che sta da queste parti sarà questo probabilmente ed è la classe 1 insomma giusto per avere il controllo di quello che stiamo facendo ecco fate sempre qualche print quando fate delle prove per vedere insomma di capire bene che cosa fate se andate a prendere il penultimo che è meno 0.2 e 1.5 è un qualcosa che sta da queste parti meno 0.2 e 1.5 probabilmente questo qui dovrebbe essere questo punto blu oppure 1 e 1 e 6 se andate a prendere 1 e 1 e 6 siete sicuri di andare a pescare questo per esempio direi questo giusto per un ulteriore controllo e poi che cosa facciamo con questo dataset di punti quindi siamo qui abbiamo fatto questa suddivisione tramite la funzione train test split a cui abbiamo passato x e y gli abbiamo detto 33% lo dedichiamo al test set il restante 67% al al training set facciamo il plot e a questo punto si facciamo il invochiamo la classe svc andiamo a creare l'oggetto e facciamo il training su x train e y train che sono quelli che abbiamo ottenuto nella riga nella riga 13 è chiaro fin qui come funziona questa è proprio la pipeline tipica di scikit learn prendete i dati fate train test split decidete una certa percentuale la lasciate per il test una certa percentuale solitamente più ampia per il training e poi andate a fare il training del modello una volta che avete fatto il training del modello si punta dritto al predict prendete il il metodo predict del vostro oggetto model gli passate tutti i punti del dataset di test e andate a vedere cosa succede a quel punto su dei punti che lui non ha mai visto il vettore risultante è un vettore di predizioni 0 1 0 1 eccetera a quel punto quello che potete fare è andare a calcolare se voi andate a fare la print vedete adesso qui vi faccio fare anche la print dei primi dieci punti predetti facciamo dieci punti e se mi basta stampare allora un attimo sono scriviamoci qualcosa perché altrimenti non sappiamo più che cosa fa e vedete qui queste sono le predizioni che fa dice 1 0 1 0 per ogni punto del test e tutto perché ok questo è il nostro dataset questi sono i punti ok scusate ok qui viene a ristante perché c'è un nuovo plot vedete un attimo ah sì sono fatti due scatter plot perché il primo è tutto il dataset il secondo è il dataset di a destra scusate viene stampato che cosa andiamo a leggere nel secondo scatter plot il primo scatter plot x test quindi è il dataset di testing questo è il dataset di testing li ho fatto anche stampare dopodiché e andiamo nel codice quindi torniamo a noi abbiamo fatto il fitting abbiamo fatto la predizione abbiamo fatto la stampa delle varie predictions quello che otteniamo che cos'è è un vettore di predizione che possiamo andare a testare andare a vedere dove sono gli errori quindi confronto y prend con y test questa è la verità perché io lo so quanto vale sono le etichette del dataset vado a vedere dove sono diversi creo un vettore che chiamo errore vado a fare la somma quello è un vettore booleano 0 1 0 1 0 1 vado a fare la somma degli 1 e ottengo il numero di errori e a quel punto posso stampare hai messo un certo numero di errori sul totale su queste specifiche istanze questo l'abbiamo fatto anche l'altra volta sulla regressione logistica cioè numero di errori la lunghezza totale del numero delle predizioni e anche invocando il metodo where di NumPy vi dice anche dove esattamente quali sono i punti che avete sbagliato potete andare a fare a quel punto una print degli errori e dell'errore corrispondente e andare a vedere anche quanto valgono i punti che avete sbagliato il risultato sarà una cosa di questo tipo vedete vi dice hai fatto sette errori su 66 predizioni che hai fatto su queste istanze di questo array hai sbagliato il punto 4 il punto 23 il punto eccetera eccetera e queste sono di questi sei errori di questi sette errori vi dice esattamente vedete la vostra predizione è quello che lui ha fatto cioè la vostra predizione è quello che lui doveva fare scusatemi ovviamente e vi dice anche quali sono le coordinate in questo modo perché io sono andato a fargli stampare guardate nel codice siamo qua io fado stampare confrontare la predizione con l'effettiva etichetta negli indici dove abbiamo sbagliato e poi sono andato anche a prendere le coordinate cioè i campioni con gli indici sbagliati i campioni che abbiamo sbagliato di cui voglio sapere quali sono le feature quali sono le feature glielo fate stampare ci siamo riusciti a seguirmi? ok glielo fate stampare e sono queste ha sbagliato questi sette punti e uno può andare anche a vedere in questo caso cosa è successo per esempio cosa è successo al punto con coordinate 2.5 2.8 andiamo a vedere quale sarà il 2.5 2.8 occhio e croce 2.5 qua 2.8 sarà uno di questi due uno di questi due a cui ha segnato la classe sbagliata e effettivamente vedete saranno tutti i punti che stanno da queste parti cioè vicino al margine tanto sono quelli che lui sbaglia di questi lui è più sicuro questi che stanno qua è difficile che li sbagli quindi potete andare anche a fare un controllo di questo genere che è anche questo interessante last but not least ultima cosa ma non la meno importante potete andare a calcolare anche qui nelle modi diversi la percentuale di errori quindi l'accuratezza quella che abbiamo chiamato accuratezza potete invocare questa funzione 0.1 loss a cui passate test e prediction cioè i valori veri e i valori predetti questo sia sul test set ma anche sul train set quindi potete andare a vedere queste sono le predizioni queste sono le predizioni sul dataset di addestramento cioè voi potete andare a misurare l'accuratezza sia sul dataset di testing che su quello di addestramento su quello di addestramento per vedere il vostro modello come si è addestrato se già fa dei disastri nel dataset di addestramento è bene cercare di migliorare qualcosa se invece sul dataset di addestramento avete delle buone prestazioni allora a quel punto potete andare a vedere cosa succede sul dataset di testing e a quel punto potete stampare l'errore il tasso di errore con tre cifre dopo la virgola sia sul train set che sul test set quindi passate o il primo o il secondo di questi due valori e il risultato è questa stampa qua vado giù se lo eseguite error rate sul train è 0.104 e sul test 0.106 qui vedete che sono più o meno equivalenti buona cosa quindi abbiamo fatto un tasso d'errore del 10% quindi un'accuratezza che è quasi il 90% 89,6 stessa cosa avreste potuto ottenere vedete questi due numeri qua e l'accuratezza 0.89 sul training set sul test set l'avreste potuta ottenere con la funzione accuracy score a cui passate di scikit-learn a cui passate y test o y predetto e vi restituisce l'accuracy e poi dall'accuracy chiaramente ottenete che cosa quella quella print che abbiamo visto prima allo stesso modo è lo stesso valore che avreste ottenuto se fate 1- questo 1- un error rate di test e chiaramente vi deve restituire l'accuracy e infatti quei due valori vengano gli stessi esattamente ok quindi questi modi diversi per fare la stessa cosa ma il flusso di progetto è dataset con le feature creazione dell'oggetto quindi definizione del modello in questo caso svc lineare con tutti i suoi parametri per cui decidete che è lineare decidete il parametro di regolarizzazione eccetera train test split quindi suddividete il dataset in training set e test set fate la lanciate il fitting quindi l'addestramento del modello testatelo quindi utilizzate il predict per vedere cosa avete fatto cosa siete arrivati come accuratezza sul train set ma soprattutto anche sul test set che lui non ha mai visto questa stessa pipeline poi può essere ripetuta tipicamente viene ripetuta più volte nel senso che quello che si fa questa cosa di lasciare da una parte il una frazione del vostro dataset per fare del testing serve per valutare quella che poi è la generalizzazione lo vedremo tra l'altro le prossime lezioni nel concetto di generalizzazione e quello che si fa è questa procedura significa lasciare una porzione fuori del vostro dataset si chiama hold out in inglese cioè lascia fuori un pezzo del dataset e verifica lì per dare robustezza statistica questo quello che si fa tipicamente è dire ok l'ho fatto una volta di nuovo rigenero a caso un training set un test set lo faccio più volte e vado a prendere per esempio la media delle accuratezze ok quindi io a caso se dico il 67% lo dedico al training set e il restante al test set diciamo 70-30 per semplificare 70% training set 30 67-33 non è che c'è molta differenza tra training set e test set è ovvio che se io vado ad estrarre la funzione train test split estrae a caso dei campioni magari la prima volta mi hai estratto dei campioni di una certa tipologia se lo rifaccio a partire da un altro seed di generatore di numeri pseudo casuali non è detta che vado non andrò tipicamente a pescare gli stessi campioni e quindi non è detta che le prestazioni i numeri siano gli stessi quindi a quel punto riaddestrate i modelli 1, 2, 3, 10 volte e potete fare la media questo è come funziona diciamo l'hold out con una validazione statistica che rende tutto più robusto perché a quel punto potete dire l'accuratezza media su 10 run è stata 88% potete dire anche qual è la varianza qual è la variabilità di questo dato questo è quello che di solito si fa quando si presentano dei lavori che hanno degli studi in cui si vuole introdurre un po' di robustezza non è l'unica tecnica perché poi queste ce ne sono diverse oltre all'hold out c'è la cross validazione quella che viene chiamata k-fold ma magari ve ne parlo a lezione un'altra volta quando parliamo un po' di questi problemi di generalizzazione però ecco sapete che insomma ci sono diversi modi di valutare l'accuratezza di un modello e deve avere anche un minimo di robustezza statistica il più diciamo intuitivo è questo hold out in cui ne lasciate una fetta fuori dal vostro dataset su quella andate a testare il vostro modello però tipicamente viene fatto più volte perché ovviamente l'estrazione è casuale quindi va bene detto questo la lezione di oggi direi che può terminare qua non so se avete qualche domanda siete riusciti a seguire? su alcuni dettagli sono andato volutamente più veloce se no non riuscivamo a chiudere magari provate a riguardarlo da voi tenendo presente che le cose veramente che dovete avere ben presenti sono ripeto questo flusso di progetto che vi ho riepilogato quindi è dataset selezione training e test addestramento del modello dei suoi parametri con la scelta lineare anche se vi ho fatto vedere qualcosa che può essere fatto nell'abito non lineare ma non è mai banale sull'SVM e la verifica dell'accurtezza e poi il resto plot della funzione di decisione sono cose accessorie che in due dimensioni ci possiamo permettere di fare ma non sono così strettamente indispensabili chiaramente come quando si va poi nella pratica di tutti i giorni però lo sono per capire bene questi concetti quindi magari provate a riguardarlo poi se c'è qualcosa mi chiedete la prossima volta va bene? ok? allora direi che a questo punto intanto possiamo fermare qui la registrazione ringaporeronigamber literal laure laure