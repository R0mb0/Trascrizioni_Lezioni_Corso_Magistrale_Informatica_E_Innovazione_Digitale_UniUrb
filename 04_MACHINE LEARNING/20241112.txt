benissimo allora bentornati intanto alle lezioni oggi iniziamo un nuovo nuovo argomento un nuovo blocco di lezioni abbiamo terminato la parte relativa all'apprendimento non supervisionato con tecniche con modelli lineari e oggi iniziamo una parte che poi ci porterà al ai primi modelli non lineari ma prima di fare questi facciamo delle considerazioni generali che valgono sia per i modelli lineari che non lineari delle considerazioni relative a delle tecniche di pre processing di pre elaborazione che sono valide ripeto per indipendentemente dai modelli e che spesso vengono vengono applicate ovviamente non sistematicamente perché poi come sempre sistemi di apprendimento sono molto dipendenti dal dominio applicativo quindi ogni dominio applicativo si porta dietro il suo insieme di tecniche di processing di pre processing però quello che hanno in comune daremo un po di definizioni e informazioni riguardanti alcune tecniche più generali che vengono applicate e parleremo in particolare di ingegnerizzazione delle feature e di selezione delle feature ok quindi vi ricordo le feature sono quelle caratteristiche dei dati che noi abbiamo come input al nostro al nostro sistema di apprendimento e quindi abbiamo il nostro modello di apprendimento abbiamo un input con delle feature quindi torniamo all'esempio da cui siamo partiti all'inizio abbiamo uno degli esempi da cui siamo partiti era per esempio un problema di regressione che riguardava il cercare di fare la predizione di quello che è il prezzo di un immobile sulla base di alcune caratteristiche devono essere il numero delle stanze dell'immobile metri quadri la distanza dal centro e così via ognuno di questi rappresenta una feature quindi rappresenta un punto del nostro sistema di addestramento e eventualmente rappresenta anche un punto incognito da andare sul quale andare a fare predizione o inferenza ok una volta che abbiamo queste feature quindi che abbiamo i nostri punti di input rappresentati come vettori numerici quello che possiamo fare è applicare delle trasformazioni a questi punti di input e queste trasformazioni possono essere molto diverse tra di loro ripeto molto dipende anche dal dominio applicativo se io ho un un input per cui sto lavorando su delle immagini le trasformazioni che posso andare ad applicare al miei vettori di input sono tipicamente diverse da quelle che posso avere la necessità di fare se il mio input non rappresentano dell'immagine non rappresenta dell'immagine ma rappresenta ad esempio dell'audio perché devo andare a classificare dei dei segnali audio però diciamo posto che quindi c'è questa dipendenza dal dominio applicativo alcune diciamo caratteristiche sono abbastanza generali e possiamo darle indipendentemente quindi dal dominio applicativo alcune di queste tecniche di trasformazione del dei vettori di input vengono applicate in maniera abbastanza trasversale e cercheremo di dirle insomma di riportarle e di descriverne le principali le principali caratteristiche ok allora diciamo anzitutto una cosa anzitutto facciamo una distinzione tra quella che sono le tecniche di feature engineering quindi le tecniche di ingegnerizzazione delle feature e quelle di feature selection ok allora la selezione delle feature è un problema diverso dalla problema dell'ingegnerizzazione delle feature problematiche di ingegnerizzazione delle feature è applicare un insieme di tecniche tipicamente queste tecniche vengono applicate prima del training prima dell'addestramento dei modelli che possono essere modelli di tipo supervisionato o non supervisionato quindi io applico una serie di tecniche per cui trasformo il mio input e poi una volta che l'ho trasformato lo trasformo perché voglio ottenere che cosa delle migliori prestazioni dei modelli perché dopo che l'ho trasformato poi su quello vado a fare l'addestramento dei miei modelli quindi l'obiettivo è estrarre ad esempio delle informazioni utili da quello che viene chiamato il dato grello e questo è quello che richiede tipicamente la conoscenza del dominio applicativo perché tipicamente che cosa si vuol fare si vuol prendere l'input che è in formato ripeto formato può essere formato così detto un'immagine che è una griglia di pixel di cui abbiamo delle informazioni numeriche che ci dicono solamente l'intensità dei colori dei tre canali rosso verde e blu e lì sono dei numeri ma possiamo volerli trasformare in maniera che se io devo distinguere un'immagine che contiene appunto un gatto da quella in cui c'è un cane posso andare a identificare ad esempio oppure se voglio identificare se c'è una persona all'interno di di quella di quell'immagine posso andare ad applicare delle tecniche di trasformazione dell'immagine in modo che magari immagini più simili mi finiscono in un nello spazio trasformato più vicine da un punto di vista della similarità e viceversa diventano dissimili immagini più dissimili immagini che devono stare separate e quindi mi facilita il compito di costruire un classificatore ad esempio allo stesso modo se io ho un qualcosa che è relativo a un ambito di dominio del audio ok quindi un segnale audio e devo costruire un classificatore che mi dice quel segnale audio che cosa succede se c'è per esempio che cosa sta dicendo la data persona quindi riconoscere per esempio una parola una parola chiave posso avere o il segnale così com'è quindi l'effetto del campionamento del segnale con la frequenza di campionamento e quindi semplicemente la trasformazione numerica dell'intensità di quel segnale per ogni istante di campionamento ma è quello il segnale raw grezzo ma posso anche andare ad applicare delle trasformazioni di quel segnale quindi fare un'analisi in frequenza quindi applicare per esempio un filtro posso andare a andare a estrarre delle frequenze facendo un'analisi di Fourier eccetera eccetera tutte queste tecniche possono servire per rendere la vita più semplice poi al processo di apprendimento che sta a valle quindi fino ad ora abbiamo lavorato sui modelli adesso diremo qualcosa su alcune tecniche stiamo dicendo qualcosa su alcune tecniche che possono facilitare l'apprendimento di quei modelli va bene ci siamo e di solito quando si tratta di estrarre informazioni che solitamente fanno leva sul fatto di avere conoscenza del dominio applicativo dal dai dati così le tigreze si parla di trasformazione delle feature invece altro tipo di operazione di processing che viene fatto e questo invece è trasversale indipendente dal dominio applicativo è quello dello scaling delle feature lo scaling delle feature che cos'è è una tecnica che permette di andare a normalizzare i dati ok è una delle più tipi che l'abbiamo anticipata nella scorsa lezione l'abbiamo detto anche in altre occasioni se non ricordo male è quella di andare a non l'abbiamo visto sicuramente però per la pca a normalizzare a centrare i dati rispetto alla media cioè ogni feature vado a prendere nel mio data set di addestramento i valori di ogni feature lungo ogni dimensione vado a calcolare la media lungo ogni dimensione e sottrago quel valore medio dalle occorrenze di ogni singola feature più tipicamente quello che si fa poi è non solo centrare rispetto alla media ma dividere per la deviazione standard per la deviazione standard più per sigma ovviamente stiamo parlando di medie campionarie e di deviazione standard campionario io quel campione di dati vado a vedere qual è la media vado a vedere qual è lo scarto rispetto alla media e ottengo la deviazione standard e un tipico modo di operare è quello di andare per appunto a sottrare la media dividere per la deviazione standard questo si chiama standard scaling se voi andate su scikit learn poi in un'esercitazione magari lo faccio anche vedere c'è proprio un ci sono diverse tecniche di pre processing lì è implementato proprio quello viene chiamato standard scaler che fa esattamente questa cosa vi ho descritto prendi i vostri dati va a cercare la media lungo ogni ogni ogni direzione lungo ogni feature vi sottrae la media vi sottrae dalla feature il valore medio quindi ve le centra praticamente questi dati e divide per la per sigma questo perché si fa beh anzitutto perché il fatto di normalizzare significa rendere tutto più omogeneo se io ho qualche feature che per qualche valore che ha qualche valore numerico ordini di grandezza maggiore rispetto a un'altra feature questo può andare a influenzare pesantemente il processo di apprendimento avate noi vogliamo che queste feature stiano tutte nello stesso range possibilmente prima cosa seconda cosa cioè che siano confrontabili numericamente seconda cosa tipicamente se noi facciamo lo scaling l'ottimizzazione ne risente diciamo ne trae beneficio questo perché quello che si può vedere che generalmente le funzioni di costo che ottenete a partire da dati normalizzati con lo scaling sono funzioni di costo che hanno dei profili migliori per esempio se vi ricordate quando all'inizio del corso abbiamo fatto le tecniche di ottimizzazione abbiamo visto no abbiamo detto va se abbiamo visto delle funzioni di costo no che sono magari convesse ma che comunque hanno quei profili no in due dimensioni che sono delle lissi no con le curve di livello che sono delle lissi no e vi ho detto vi ricordate quando sono molto schiacciate crea dei problemi alla discesa del gradiente ecco se voi fate feature scaling tipicamente quello che ottenete è che i profili sono più tondeggianti quindi da un punto di vista quindi meno schiacciati da un punto di vista dell'ottimizzazione sono più semplici delle funzioni più semplici da ottimizzare quindi solitamente è buona norma fare scaling delle feature questa è una regola solitamente che si dà e che generalmente vale per tutti i modelli di apprendimento quindi è trasversale ed è indipendente dal dominio applicativo quindi tutto questo sono tecniche ecco come dicevo di ingegnerizzazione delle feature trasformano il segnale poi ci sono le tecniche di selezione delle feature le tecniche di selezione delle feature corrispondono invece a una a una logica diversa rispondono a una domanda diversa e non vengono applicate prima di costruire il modello ma servono per analizzare il modello cioè io voglio andare a selezionare una volta che ho costruito un modello voglio andare su un certo numero di feature voglio andare ad analizzare la prestazione di quel modello e voglio capire quali sono le feature che hanno avuto la maggior influenza nella prestazione del modello perché se c'è un sotto insieme di feature che riesco ad estrarre a selezionare e quelle mi determinano la prestazione del modello quindi per esempio supponiamo di parlarvi un classificatore ok supponiamo che quel classificatore ha da un'accuratezza del 90% e supponiamo di avere 20 feature di input supponiamo che in qualche modo io riesca a stabilire che ci sono 5 feature che da sole mi spiegano tra virgolette la prestazione del modello cosa vuol dire che se io avessi utilizzato solamente quelle 5 avrei magari ottenuto un classificatore corretto all'89% anziché 90% vuol dire che quelle 5 sono responsabili per l'accuratezza all'89% e poi tutte le altre mi aumentano l'accuratezza dell'1% quindi sono sicuramente le feature più importanti allora questo è importante nel momento in cui io voglio anzitutto semplificare volendo il modello perché posso decidere a quel punto anche di ridurre il numero delle feature di input e lavorare con un modello con 5 feature anziché 20 che può avere senso per motivi di efficienza in qualche caso se anziché essere 5 e 20 i numeri fossero 1 milione e 150 magari il discorso avrebbe un senso da un punto di vista della potenza computazionale ma soprattutto una cosa molto importante soprattutto oggigiorno è che a quel punto io ho uno strumento per dire il modello su che cosa lavora e quindi quali sono le caratteristiche dell'input che il modello va a vedere per determinare il suo valore di output perché se io estraggo quelle 5 feature e le seleziono e le identifico come le più importanti sto dicendo che il modello ha una sua interpretabilità alla luce di questo cioè vado a vedere prevalentemente a utilizzare quelle feature e quindi questa questione dell'interpretabilità del modello è un qualcosa che oggigiorno sta diventando sempre più importante perché nel momento in cui voi andate a costruire un sistema di machine learning e lo mettete a disposizione di qualcuno cercare di capire perché quel modello ha dato quel determinato risponso per esempio perché quella classificazione ha dato sì oppure no quindi pensate ad esempio delle applicazioni in cui questa particolare importanza e particolare peso quindi applicazioni di carattere biomedico oppure dove avete ad esempio delle decisioni a valle che hanno impatto sulla vita delle persone e allora volete cercare di capire qualcosa di più del perché il modello di machine learning vi ha dato quella risposta non soltanto il modello mi ha detto sì oppure no Si concedo il credito, il mutuo a questa persona oppure no, oppure si questa persona secondo il modello è affetta da questa malattia potenzialmente oppure no. Si vuole cercare di capire di più. Ecco, fare feature selection è un modo per cercare di capire qualcosa di più di quello che è il processo di apprendimento e quindi dire che sono alcune le feature coinvolte, quelle che hanno più peso, può aiutare a costruire dei modelli che hanno un livello di interpretabilità. Poi più avanti vi ho un paio di esempi in cui questa cosa la cerchiamo di sviluppare meglio, va bene? Qui abbiamo un grafico che esemplifica un po' dei discorsi che stavamo facendo e lo mette in relazione a due tecniche che abbiamo già visto in realtà, due modelli di tipo non supervisionato che abbiamo visto, che sono rispettivamente PCA e Clustering. La PCA, l'analisi dei componenti principali, la possiamo vedere, questo l'abbiamo anticipato nell'ultima lezione, come una trasformazione vera e propria del segnale. Qui ritroviamo esattamente questo concetto. Quindi la PCA è di fatto un qualcosa che a tutti gli effetti rientra nell'ingegnerizzazione delle feature ed è una tecnica che viene usata per fare due tipologie di cose. Però adesso ci arriviamo tra un attimo. Prima andiamo per ordine, guardiamo, partiamo da sinistra verso destra. Ora qui abbiamo feature selection, qui abbiamo la nostra matrice dei dati di input, quindi è una matrice che vi ricordo ha n righe e p colonne, dove n è il numero delle feature, quindi questo è il numero delle feature, p è il numero dei punti del vostro dataset. Va bene? Per pensare le idee. Allora, che cosa vi dice questa esemplificazione grafica? Questo grafico vi dice che cosa? Se voi applicate una tecnica di feature selection, dopo ve ne descriverò alcune, alla vostra matrice di dati di input, quindi leggiamo questa matrice di dati come ho il singolo punto, iesimo, il punto iesimo chiamiamo così, anzi jiesimo. Questo punto iesimo sarà un punto che ha un certo numero n di feature, ok? Allora, fare feature selection significa di queste feature andarne a isolare un sotto insieme, e ad esempio questa, c'era la freccia, quindi non dovevo inventarmi niente di nuovo, è questa, e andarlo a riportare qui sotto, perché per esempio mi sono accorto, messo in atto appunto queste tecniche in maniera sistematica, che questa feature, la seconda e la penultima, queste due feature sono le più significative. Allora io vado a trasformare il dato e non mi porto dietro solo quelle due. Questo è il partice of selection. Se io faccio la PCA, se applico l'analisi dei componenti principali, arrivo a una, posso arrivare a una cosa anale, la PCA la posso fare applicando una trasformazione agli stessi dati, senza perdita di dimensionalità, e lì è una trasformazione. E anche qualche volta si fa anche quello, e lì diventa una sorta di scaling, perché si fa per anche di facilitare, ha lo stesso effetto sul processo di ottimizzazione dello standard scaler, in alcuni casi funziona anche meglio. E questo si chiama, nell'anticipo si chiama anche, viene chiamato anche PCA whitening, o PCA scaling. E questo, se non lo sbaglio, c'è poi nella slide successiva, viene richiamato. Però io posso usarla, l'abbiamo vista, anche come tecnica di riduzione della dimensionalità, quindi andare a prendere questi dati, questi punti, che vivono in uno spazio a n dimensioni, e li proietto tramite una trasformazione lineare in uno spazio a più bassa dimensione. Il che significa che se passo, ad esempio, supponiamo che qui i nostri punti vivono in uno spazio a 6 dimensioni, potrei dire, proiettiamo con la PCA in uno spazio a 2 dimensioni. E il risultato è questo. Io mi porto dietro solo due coordinate, due sole feature lungo le quali andare a descrivere i punti. Però vedete, qui sono delle barre che hanno dei colori, gli stessi colori, ma, diciamo, dei pattern diversi sopra, perché sta ad indicare che cosa? Che sono delle feature che vivono in un dominio trasformato. Io le ho trasformate, non sono chiaramente le feature originali. Però ne ho due, anziché 6, esattamente come a sinistra. Quindi qui la PCA svolge un duplice ruolo perché ci permette di fare scaling, senza riduzione della dimensionalità, oppure trasformazione. Però è diverso da feature selection, perché feature selection sopravvivono le feature originali. Qui trasformo le feature e riduco la dimensionalità. Ci siamo, fin qua? Ok, poi siccome abbiamo visto ieri anche l'algoritmo K-Means per fare clustering, uno potrebbe dire, beh, anche il clustering è una forma, se vogliamo, di riduzione della dimensionalità, però non è di riduzione della dimensione del singolo dato, ma vi permette di ridurre il dataset. Perché se io identifico che nel mio dataset, vedete, ci sono una serie di punti, ok, e identifico che ci sono per esempio due cluster, due raggruppamenti, io posso andare a prendere il baricentro di ognuno di quei due raggruppamenti e utilizzarlo come rappresentante di questo dataset. cioè mi porto dietro, anziché i grandi punti, due punti, che sono i vari centri dei due cluster. E questo in qualche applicazione può avere senso, per cui è un'altra trasformazione che può essere fatta e può dirci qualcosa di più per eventuali processing che stanno avanti. Va bene? Ok. Allora, ci sono domande? Andiamo avanti. Allora, un'altra tecnica piuttosto generale che viene, che rientra tra appunto le tecniche di feature engineering è quella della costruzione degli histogrammi, delle feature histogramma. Una feature histogramma è appunto un uno strumento di ingegnerizzazione che diciamo è abbastanza trasversale, quindi esattamente come vi dicevo prima nello standard scalar, si applica a diversi domini applicativi e quindi lo possiamo descrivere perché poi vi do adesso un paio di esempi, vi faccio vedere come può essere applicato per esempio al testo o in generale alle categorie. Partiamo dai dati di tipo categorico. Dati di tipo categorico li abbiamo già incontrati quando abbiamo fatto nei classificatori e ci siamo posti la domanda ma se l'etichetta che ci deve fornire il nostro classificatore, anziché essere 0-1 un'etichetta categorica, un cane o un gatto, abbiamo detto possiamo associare 0 al cane, 1 al gatto, magari non è così strettamente necessario, diventa questo ancora più, quindi diciamo nel momento in cui associamo un numero ci abbiamo implicitamente un ordine, questo diventa ancora più in qualche modo pressante come problema da risolvere quando i dati sono di input, sono di tipo categorico. Supponiamo di avere, dover costruire un sistema di machine learning che lavora su delle analisi tipo medico, quindi per esempio dei dati che possono essere la pressione diastolica e sistolica del sangue di un paziente e quello è un dato, sono dei dati numerici per cui uno ci mette dentro 130 e 90, la massima e la minima, nessun problema. poi ci puoi mettere un qualche tasso di glicemia, ok, altro numero, non c'è nessun problema. Poi ci dobbiamo mettere dentro il gruppo sanguigno. Il gruppo sanguigno o è 0, o è A, o è B, o è B. E allora qui abbiamo la necessità di codificare in formato numerico questo tipo di informazione, perché questi sono dei dati di tipo categorico. Come facciamo a rappresentarli? Abbiamo visto che i nostri modelli di machine learning ricevono come input dei vettori numerici. Come facciamo a rappresentare questi dati di tipo categorico come vettore numerico? Beh, un modo potrebbe essere, no, dire questa è una feature aggiuntiva e io associo arbitrariamente un numero a ognuno delle quattro categorie. Quindi comincio a ragionare in questi termini. Dico, ok, se il paziente ha gruppo 0, io ci associo un'etichetta numerica 0, se il paziente ha gruppo A, appartiene al gruppo A, ci associo l'1, al gruppo B2, e al gruppo AB3. Ora, questo è perfettamente legittimo. Il punto è che bisogna stare attenti perché noi nel fare questo, per il fatto che in realtà lavoriamo qui sotto con dei numeri e i numeri hanno un ordinamento, quindi c'è associato una relazione d'ordine, stiamo implicitamente iniettando, facendo delle assunzioni, perché iniettiamo implicitamente una conoscenza, un'assunzione che facciamo noi dentro i dati e quindi poi dentro il modello. E l'assunzione che stiamo iniettando è che il gruppo sanguigno B e il gruppo sanguigno AB sono abbastanza vicini, sicuramente di più di quanto non lo siano A e AB. Ora, questo da un punto di vista biologico può avere senso oppure no? Però è una domanda che ci dobbiamo porre, probabilmente non lo è. Uno potrebbe dire, ok, allora è più sensato che AB stia magari tra A e B e potrebbe dire, ok, vediamo, se codifichiamo in questo modo, 0 il gruppo 0, 1 il gruppo A, 2 il gruppo AB e 3 il gruppo B. Ok? Però in questo caso io sto implicitamente dicendo che B è molto più lontano da 0 di quanto non lo sia. È corretto questo di nuovo? Non lo sappiamo. Forse non lo è. Probabilmente se lo chiedete a un medico, a un biologo, vi dice no, non lo è. E allora bisogna cercare di uscire da questa rappresentazione dei dati categorici con dato puramente numerico e quello che si fa è si estrae dall'informazione della categoria un vettore. Un vettore che è appunto poi rappresentabile anche come un Instagram, ma adesso ci arriviamo tra poco, che abbiamo già incontrato. Abbiamo già incontrato a proposito dell'output. Lì lavoravamo sull'output. Questa è una rappresentazione dell'input categorico che viene chiamata One-hot encoding. One-hot encoding, di nuovo, come quando facevamo con le etichette, semplicemente rappresenta un dato di tipo categorico come un vettore in cui ci sono tutte le possibili alternative. Quindi qui vado a mettere 0, A, B e AB, le quattro possibili alternative. Quindi ho tanti contenitori, tanti bin, quanti sono le possibili categorie. E vado a mettere a 0 tutte le categorie, tranne una sola che è quella di appartenenza. Quindi se sto elaborando il dato di un paziente che appartiene al gruppo 0, metterò a 1 la prima entry e a 0 tutte le altre. Se il paziente ha gruppo sanguigno A, avrà ad esempio un 1 sulla seconda entry, se del gruppo B avrà un 1 nella terza entry, se del gruppo AB avrà un 1 nella quarta entry. Il che significa che io vado a codificare in questo modo l'informazione per cui in questo modo avrò che, ripeto, quattro possibili alternative che rappresentano queste quattro tipologie. In questo modo, se voi andate a prendere una qualunque coppia di questi e non c'è una relazione d'ordine, non potete dire che questo e questo sono più vicini o più lontani di questo e questo. Perché non c'è più una relazione d'ordine tra quei vettori. E quindi bypassiamo questo problema. Quindi quello che si fa quando ci sono dei dati categorici è codificarli con la tecnica del one-hot encoding. One-hot encoding vi prende un dato di tipo categorico, vi assegna il vettore corrispondente e una volta che avete il vettore corrispondente quello entra a far parte delle feature assieme alle altre feature numeriche. ok? E questa è una tecnica che si fa comunemente. Anche qui se voi andate su scikit-learn avete la possibilità già in automatico lui vi riesce a caricare i dati in memoria e vi fa direttamente la codifica one-hot encoding però è una cosa che ovviamente è una procedura abbastanza semplice da un punto di vista della programmazione. Insomma, chiaramente non rappresenta un problema. però dal punto di vista concettuale è un qualcosa di non banale che ha dei riflessi importanti. Allora, perché l'abbiamo introdotto anzitutto perché è importante in sé. L'altra cosa è perché vedete questi li potete leggere poi come degli histogrammi chiaramente e l'altra cosa che vi volevo far vedere è l'applicazione appunto di questi queste feature a histogramma a dati di tipo testuale. Allora, il creare delle feature basate su histogrammi come ad esempio appunto nel one-hot encoding e in questo caso vedete l'applichiamo a un appunto dati di tipo testuale è una cosa anche questa abbastanza importante abbastanza utile per cui ve lo faccio vedere io vi faccio vedere questa applicazione sui dati tipo testuali cose analoge anche se partono da presupposti chiaramente molto diversi si hanno nel dominio per esempio computer vision cioè nel computer vision vengono create degli histogrammi a partire quindi le feature a histogramma a partire dai dati grezzi con delle tecniche di analisi dell'immagine in realtà questo era quello che si faceva fino a qualche anno fa poi adesso per l'avvento delle reti neurali tutto è molto cambiato qui bisogna fare un piccolo inciso le reti neurali che vedremo verso la fine del corso vedremo un'introduzione a questo mondo delle reti neurali hanno per diversi motivi scardinato diverse cose diversi diciamo diversi concetti e soprattutto diversi modi di lavorare e di fruire di questi sistemi di apprendimento ma uno di questi quindi lungo diverse dimensioni sicuramente le prestazioni la curatezza ma uno di questi è il fatto di poter lavorare con dati di tipo grezzo cioè una rete neurale se voi costruite un classificatore per immagine una rete neurale gli potete dare in pasto tipicamente una griglia di pixel lui prende quel dato raw e lo lo digerisce e vi dice fuori questa è un'immagine con un cane con un gatto un tavolino una bottiglia eccetera prima dell'avvento del deep learning tutto questo non funzionava così quello che si faceva è si erano ingegneri di machine learning o sviluppatori software di machine learning che studiavano hanno studiato per anni come trasformare come ingegnerizzare le feature quindi prendere un'immagine estrarre con una serie di tecniche tutta una serie di feature per esempio venivano chiamate degli histogrammi venivano estrate degli histogrammi dei gradienti orientati quindi si costruiva quello che veniva chiamato un rilevatore per esempio dei contorni quindi veniva scomposta questa immagine in modo da andare a rilevare quali pixel di quell'immagine contenevano i contorni ad esempio di una persona di un tavolo di un banco eccetera e quindi venivano applicate queste tecniche di feature engineering che erano molto sofisticate ma molto dipendenti dal dominio il deep learning questo lo ha molto modificato la domanda che uno vi potrebbe fare a questo punto è dire ma bene allora perché andiamo a studiare le feature Instagram ma se abbiamo le reti neurali possiamo dare in pasto qualunque cosa è una domanda sensata però diciamo dobbiamo ancora aspettare a dare per qualche modo per defunte tutte le tecniche del machine learning più tradizionale pre deep learning perché ancora su queste queste hanno un loro motivo di esistere in certi contesti uno dei quali è quando i dati di input non sono immagini suoni sequenze di lettere cioè dati che hanno una struttura ben precisa ma quando avete per esempio degli input in forma tabulare allora sugli input in forma tabulare quindi ad esempio l'esempio che facevo prima dal mondo della medicina quindi avete la cartella clinica del paziente quindi analisi del sangue pressione eccetera tutti dati di questo tipo oppure analizzate dei dati che vengono da non so cose tipo istat fate conto il censo quindi la composizione di un nucleo familiare dove avete delle grandi matrici delle tabelle con diversi dati che caratterizzano lungo diverse dimensioni una certa popolazione ebbene lì ancora al momento ad oggi 2024 le prestazioni dei metodi cosiddetti tradizionali di machine learning quindi anche alcuni di quelli lineari di cui vi ho parlato oppure le support vector machine la regressione logistica sono ancora competitivi in termini di accuratezza con il deep learning da diversi punti di vista o quantomeno non è chiaro se c'è una soprano mazzia dell'uno dell'altro e lì di fatto il dato di input è informato è quello non è che c'è un input raw come nell'immagine griglia di pixel che deve essere trasformato ma se io ho già il dato che mi dice la pressione sanguigna del paziente il il gruppo sanguigno e così via quello è il mio dato è la mia tabella di input quindi lì diciamo il vantaggio di usare le reti neurali è diciamo quantomeno oggetto di dibattito quindi sopravvive tutto un mondo diciamo del machine learning chiamiamolo tradizionale tra virgolette che utilizza ancora tutta una serie di tecniche come la trasformazione delle feature o di ingegnerizzazione delle feature ecco perché ha senso comunque farle vedere e comunque questa cosa di cui vi parlo è una cosa di cui si avvale anche qualunque rete neurale che va a analizzare testo perché qui stiamo parlando come campo di applicazione l'analisi di dati in tipo testuale quindi voi avete dei dati che provengono da qualunque fonte documenti social media quindi flusso di testo che ha la provenienza la più svariata e volete analizzarlo per fare per esempio sentiment analysis quindi avete delle recensioni volete capire se queste recensioni di un prodotto sono positive oppure negative volete fare rilevazione di spam perché l'email che vi arriva nella casella di posta volete automaticamente buttarla nella casella spam e invece quella che non lo è volete che vi arrivi nella vostra inbox volete fare categorizzazione automatica dei documenti quindi andarle a classificare automaticamente in base al contenuto perché questo può servire in applicazioni di altro tipo tutto questo quindi ha a che fare con del testo da sorgenti le più svariate social media email news online chi più ne ha più ne metta allora ovviamente anche qui c'è un mondo giusto per darvi un piccolo assaggio di questo mondo vi dico che una delle trasformazioni quindi feature transformation stiamo parlando delle feature più comune è creare un histogramma che viene chiamato histogramma cosiddetto bag of words bag of words sta per borsa delle parole per fare cosa? per rappresentare un documento cioè il nostro obiettivo è sempre trasformare il nostro input in modo che il risultato sia un vettore di numeri io come faccio a trasformare un documento in un vettore di numeri l'obiettivo è questo ci sono varie tecniche il bag of words è una di queste e per farvi capire molto sinteticamente come funziona prendiamo questi due esempi proponiamo di avere questi due documenti ovviamente lavoriamo in maniera iper semplificata per far capire di cosa stiamo parlando abbiamo due documenti uno e due nel primo c'è appunto un testo in cui c'è scritto i cani che sono i migliori i dogs are the best nell'altro c'è scritto cats are the worst quindi i gatti sono i peggiori allora indipendentemente da come la pensiate adesso cerchiamo di ragionare su questo ovviamente qui abbiamo già un esempio di polarizzazione del mondo dei social media ovviamente al di là della battuta questo è un problema chiaramente serio allora che cosa si fa si vanno a identificare le parole si vanno a cercare la frequenza di occorrenza delle varie parole quindi si sceglie un dizionario che contiene tutte le possibili parole ok in questo caso un dizionario molto piccolo ma tipicamente prendete un dizionario con 20.000 30.000 parole quello che volete e poi andate a contare di quel dizionario quante sono le occorrenze di ogni parola all'interno del vostro testo in questo caso vedete che io vado ad identificare quattro parole possibili e in realtà le parole sono di più perché ci sono anche il verbo essere e l'articolo ok il verbo essere e l'articolo perché non le andiamo a prendere in considerazione ma andiamo a prendere un dizionario di sole quattro parole perché il verbo essere e l'articolo sono qualcosa che non ci l'obiettivo è cercare di costruire dei vettori che siano rappresentativi del documento articoli verbo essere e altre e altre e altre parole che svolgono comunque hanno una funzione nella sintassi ma da un punto di vista della semantica non aggiungono molto non tolgono molto vengono non prese in considerazione ok se io voglio distinguere il documento 1 dal documento 2 perché voglio fare una classificazione automatica questi non mi dicono molto ok allo stesso modo se io quindi vengono eliminate come primo step io faccio il primo step di questi sistemi è costruire un parser ok il parser è un software che prende il vostro documento e ne fa la scansione sistematica va bene dopodiché quindi vi identifica tutte le parole all'interno del documento dopodiché lo passate a un altro modulo e vi toglie di mezzo le stop words quindi punto punte virgola per esempio tutti i segni di punteggiatura e vi toglie anche queste parole particolari quindi il verbo essere articoli eccetera con cosa rimanete? beh rimanete con tutta una serie di parole che però tipicamente vengono ridotte con un processo che viene chiamato stemming alla radice stem in inglese significa radice ok quindi lo stemming è qualcosa che vi riporta alla radice comune tutte le parole che possono avere una variante invece in quella che non è la radice ma appunto è magari il suffisso e quindi per esempio se voi avete learn learnable in italiano imparare imparato imparo vengono tutti ridotti alla stessa radice imparato ok il risultato alla fine è che il documento viene in qualche modo elaborato rimangono solamente queste parole in questa forma e a quel punto possiamo andare a raccontare le occorrenze delle singole parole se lo faccio in questi due documenti vado a vedere che cosa che ad esempio nel primo documento la parola best ha un'occorrenza e la parola dog ha un'occorrenza nel secondo documento abbiamo un'occorrenza della parola cat e un'occorrenza della parola worst giusto? ok allora quello che si fa è rappresentare quindi con un vettore di quattro parole dove quattro è la dimensione del mio vocabolario rispetto a questi due documenti il singolo documento quindi questo vettore è rappresentativo del documento 1 e questo vettore è rappresentativo del documento 2 va bene? lo vedete dal fatto che c'è insomma delle barre colorate che rappresentano appunto visivamente il fatto che questo è un histogram ma chiaramente quello che voi ottenete a seguito di questo processo è un vettore numerico non è un un'elaborazione grafica avete degli 0 laddove non ci sono le occorrenze di quelle parole nel documento 1 non c'è l'occorrenza della parola cat non c'è l'occorrenza della parola worst di là invece non c'è l'occorrenza chiaramente di best e di dog la domanda che mi potreste fare è perché anziché esserci uno un'occorrenza abbiamo detto che contiamo le occorrenze c'è uno su radice di due qualcuno mi sa dire perché no però ha che fare con siamo da quelle parti la metà la parola è la parola è dog e best però perché c'è un uno su radice di due e non solamente uno se abbiamo detto che controlliamo l'occorrenza perché è quello che si fa ve lo dico io è normalizzare normalizzare questi vettori in modo che ogni entri ovviamente sia compresa tra zero e uno e un modo di normalizzare è dividere per la norma del vettore e la norma L2 di questo vettore che cos'è? la norma Euclidea è la radice di uno più uno che è uguale alla radice di due quindi se voi andate adesso a calcolare la norma di tipo L2 di x1 ottenete uno e la norma di tipo L2 di x2 analogamente è uno quindi sono tutti vettori che hanno norma unitaria sono tutti confrontabili in questo modo e adesso la cosa bella è che in questo modo abbiamo tutti i vettori confrontabili tutti i vettori che quindi hanno norma compresa tra zero e uno a questo punto e la cosa interessante è che possiamo utilizzare degli strumenti dell'algebra lineare molto semplici per fare delle cose abbastanza carine abbastanza interessanti perché quello che vediamo qua è che questi due vettori sono chiaramente non correlati tra di loro non correlati cosa vuol dire? che laddove il primo vettore ha delle entri non nulle l'altro ce l'ha nulle e viceversa allora in algebra lineare uno strumento per vedere se i due vettori sono o meno correlati quale potrebbe essere? andare a vedere se sono ortogonali oppure paralleli paralleli massima correlazione ortogonali massima non correlazione e per fare questo c'è uno strumento ben preciso che è il prodotto scalare lo abbiamo utilizzato più volte all'interno del corso se io faccio il prodotto scalare tra x1 e x2 che cosa ottengo? quanto vale? quindi il prodotto scalare è la somma di esattamente questo per questo che fa 0 questo per questo che fa 0 questo per questo che fa 0 questo per questo che fa 0 lo sommo risultato 0 e quindi laddove io ho una massima dove sono massimamente scorrellati vedete già con un semplice prodotto scalare riesco a scoprire che insomma x1 e x2 sono molto diversi come documenti da un punto di vista semantico questo ha un suo il suo perché in effetti allora uno dice abbiamo trovato la soluzione di tutti i problemi così riesco qualunque documento immediatamente a dire se è molto scorrellato da un altro la risposta è no perché in realtà diciamo potremmo fare dei controesempi in cui questo non è più non è più vero io potrei dire potrei prendere un documento in cui dico pets are worst dogs are best potrei scrivere una cosa del tipo lo scrivo qua e poi potrei avere un altro documento in cui dico dogs are worst and cats ops scusate are best chiaramente sono due cose due frasi che sono ovviamente diciamo ortogonali da un punto di vista del significato giusto? ok però se io vado a costruire il bag of words per ognuna di queste è esattamente lo stesso perché cats worst dogs best hanno tutte un'occorrenza e quindi sono due vettori che sono identici quindi in quel caso se vado a fare il prodotto scalare otterrei uno quindi diciamo che non è la soluzione di tutti i problemi però è comunque uno dei modi con cui si rappresentano dei documenti si trasformano i documenti dal formato testuale in vettori e quindi il documento viene rappresentato come un vettore e una volta che ce l'abbiamo in forma di vettore possiamo farci tante cose e qui c'è un'applicazione un pochino un esempio un pochino più elaborato ma è fondamentalmente lo stesso concetto quindi abbiamo sempre questa costruzione di questo histogramma questa feature basata sull'histogramma Bag of Words applicata a un esempio di sentiment analysis quindi cercare di catturare quello che la sensazione il livello aggregato di opinione da un insieme di clienti che esprimono delle recensioni su un prodotto e volete capire in maniera automatica qual è l'opinione su quel prodotto e che cosa potete fare potete intanto costruire un classificatore però prima di costruire un classificatore quindi prima di intanto la prima cosa che dovete fare è elaborare questo testo e quindi andare a fare parser puoi eliminare il stop words lo standing e poi e poi quello che potete costruire il vostro dizionario e qui va bene andiamo a vedere due ipotetici documenti in cui c'è una recensione da parte di un primo utente questa prima recensione da parte di questo utente e lo leggo perché forse non si riesce a leggere bene this is simply one of the finest feeling of all the time you are guaranteed to laugh every minute of it cioè è semplicemente uno dei film più divertenti di tutti i tempi è garantito che arriverete dal primo all'ultimo minuto il secondo utente la pensa in maniera un pochino diversa dice this is the worst movie ever made and people responsible for it are incredibly immature quindi è il peggior film mai fatto e le persone che sono responsabili per averlo fatto sono incredibilmente immature e chiaramente la pensa in maniera molto diversa sulla stessa cosa e se voi applicate tutte le tecniche che vi dicevo prima quindi togliete il verbo essere fate lo stemming eccetera eccetera rimanete con un vocabolario che è questo questo è il vostro vocabolario questo vocabolario ha 3, 6, 9, 12, 14 parole in questo caso su questi due documenti chiaramente un esempio estremamente limitato per far capire come funziona poi vi andate a costruire il bag of words cioè quell'istogramma come abbiamo detto prima quindi e vedete che cosa che nel primo nel primo dei due nel primo dei due documenti quello del reviewer 1 avete l'occorrenza di tutte parole vedete che sono state messe qui per prime sono quelle diciamo che hanno diciamo qui possiamo associare un'opinione positiva se vogliamo qua abbiamo messo tutte quelle invece associate alla seconda reviewer che chiaramente sono negative e poi viene normalizzato vedete uno su radice di 7 perché ci sono 7 occorrenze qui sono queste 6 di queste prime 7 parole e 7 occorrenze qua ok ok quello che vedete qui si vede molto bene che è la 2 di nuovo come nel caso prima del cane e del gatto dove abbiamo la barra blu la barra gialla è 0 e dove abbiamo la barra gialla la barra blu è 0 quindi sono ortogonali ovviamente questo se aumentate il numero di documenti non è ovviamente sempre così non avete diciamo sempre questa polarizzazione appunto ma quello che vedreste è una cosa di questo genere un andamento di questo tipo in cui avete diciamo delle parole da una parte a cui associate un sentimento positivo avreste un qualcosa un istogramma che è fatto così per cui sfuma verso le parole con sentimento negativo viceversa per recensioni di tipo negativo però avreste comunque vedete modo di di catturare il livello di correlazione tra queste va bene e questo si fa il prerequisito è di poter fare questi costruire questi histogrammi bag of words ok ultimo esempio sul bag of words prima di procedere oltre andare avanti abbiamo parlato prima rilevazione dello spam oggi abbastanza standard qualunque provider che vi mette a disposizione appunto una casella di posta vi mette a disposizione ogni giorno un sistema di filtraggio automatico per l'archivazione della posta indesiderata e quello funziona siccome messaggi di post elettronica sono messaggi di testo è basato esattamente su tecniche come il bag of words o analoghe quindi vengono costruiti dei vettori di histogrammi questi vettori di queste feature diciamo questi histogrammi feature come vengono costruiti esattamente come vi ho detto qui in questo caso è stato costruito un esempio a partire da un dataset in cui viene fatto un costruito un classificatore binario quindi che riconosce le email in prata e le classifica come spam oppure non spam il classificatore binario è un classificatore lineare costruito con funzione di costo softmax e noi l'abbiamo visto che cos'è ok è ottimizzata quindi è una regressione logistica con softmax e ottimizzata con il metodo di Newton metodo del seconde è stato fatto un run questo è il numero di il tasso d'errore rispetto all' all'evoluzione durante il processo di ottimizzazione di apprendimento quindi si parte da un numero elevato di errori e si arriva qui ok quindi dopo 500 interazioni nella discesa si arriva qui e questo è quello che succede la curva azzurra quando codificate ognuno di quei documenti con il bag of words ok poi anche qui ovviamente ci si può sbizzarrire perché uno potrebbe cominciare a dire ok siccome nelle mail di spam ci sono un sacco veramente di punti esclamativi asterischi cose di questo genere se io all'istogramma quindi alla feature oristogramma aggiungo un'ulteriore informazione che mi dice per esempio il numero di asterischi di punti esclamativi che sono presenti in quel documento cosa succede? aggiungete delle informazioni aggiungete delle feature aggiungete delle feature e potete costruire una cosa che è questa curva arancione che chiaramente vi abbassa il tasso di errore ulteriori cose che potete aggiungere è quelle che vengono chiamate spam feature delle feature ulteriori che identificano potenzialmente la presentazione di spam è il numero per esempio di lettere maiuscole se ci aggiungete anche quello siete sulla curva verde e così via però il punto di partenza è sempre qui ok quindi gli histogrammi le feature costruite sugli histogrammi sono nell'ambito sicuramente dell'elaborazione del testo ma come vi dicevo prima non lo vedremo perché non abbiamo tempo di vederlo anche nell'ambito audio anche nell'ambito computer vision sono trasformazioni molto utilizzate lo erano fino a un po' di diciamo fino a 5 anni fa adesso un po' di meno perché ripeto con l'avvento delle reti neurali lì soprattutto è un contesto applicativo dove queste tecniche di ingegnerizzazione delle feature diciamo hanno visto la loro importanza diminuire però per esempio in ambito testo sono molto usate e quindi questa era diciamo una premessa che era doveroso fare e ok ecco qui c'è un po' delle cose che vi stavo dicendo cioè che tecniche analoghe possono essere applicate all'immagine o all'audio per esempio in ambito computer vision venivano sarebbe più corretto dire perché qui effettivamente è dove il machine learning tradizionale ha ceduto passo al deep learning però per esempio fino a qualche anno fa venivano chiamate costruite delle feature basate su degli histogrammi che facevano la rilevazione dei contorni oppure c'erano quelli che venivano chiamati histogrammi dei gradienti orientati vabbè tutte cose che adesso non andiamo a vedere nell'ambito audio veniva veniva estratto lo spettro del segnale questo ancora in alcune applicazioni viene fatto viene costruito un histogramma delle frequenze cioè in quella porzione di segnale quali sono le frequenze che diciamo compaiono più frequentemente quindi è un segnale in cui c'è un tono a tot hertz poi altri tot hertz e così via allora in generale ci sono altre tecniche che possono essere applicate e che possono in qualche modo condizionare in maniera positiva o qualche volta anche negativa se non le fate bene le fasi successive della quella che viene chiamata la pipeline di apprendimento ok quindi di tutto il flusso di lavoro e alcune ve le ho già menzionate il feature scaling con la normalizzazione standard quindi prendete la media e dividete per la direzione standard da ogni punto e quello che viene chiamato PCA whitening o PCA sparing cioè fate la trasformazione trasformate il dato con l'analisi dei componenti principali l'obiettivo di queste ultime due tecniche è trasformare il dato renderlo più compatto e in generale come vi dicevo prima le funzioni di costo risultano più circolari non sono più schiacciate laddove erano delle lissi magari prima non sempre sono delle lissi le funzioni di costo poi questo l'abbiamo detto le linee di contorno delle funzioni di costo ok quindi questo diciamo è un po' la panoramica per quanto riguarda la trasformazione dell'efficio quindi l'ingegnerizzazione dell'efficio ci sarebbe molto da dire ma bisognerebbe poi addentrarsi dopo in ogni specifico dominio applicativo che ognuno si porta dietro le sue quindi diciamo noi dobbiamo rimanere sul generale perché ovviamente non facciamo un corso di machine learning per computer vision o per audio processing quindi rimaniamo oppure per elaborazione del testo quindi ve ne ho fatta una panoramica ed è sufficiente diciamo credo fermarci qui quello che invece andiamo a discutere adesso nella restante parte della lezione sono alcune questioni riguardanti la selezione delle feature che è quella tecnica che vi avevo detto prima che permette come vi ho riportato qui di selezionare un sotto insieme delle feature di partenza per migliorare ad esempio la velocità io se riesco a costruire un classificatore con 100 feature mi accorgo che anziché quelle 100 feature riesco a selezionare come vi dicevo prima le 10 migliori che mi danno un'accuratezza che è quasi quella che otterrei con tutte 100 o magari quella che otterrei con tutte 100 non ha senso che io mi porti dietro un classificatore a 100 feature ha senso che me ne porto uno a 10 feature perché ogni volta che vado a fare l'inferenza uso quelle 10 e vado più veloce spendo meno tempo o una latenza inferiore magari anche l'accuratezza è migliore qui introduciamo per la prima volta un concetto che poi vedremo nelle prossime lezioni che è estremamente importante l'ambito del machine learning è quello dell'overfitting ok l'overfitting è quando voi costruite un modello che è troppo lo spiegheremo meglio più avanti però è quando costruite un modello troppo tra virgolette complicato sarebbe meglio dire molto complesso rispetto ai dati che volete andare a modellizzare che volete andare a utilizzare con quel modello cosa significa? significa che voi a quel punto cercate di riprodurre ogni caratteristica del dataset di addestramento riprodurre all'interno del modello ma questo va a scapito della capacità di generalizzare cioè a quel punto andate a modellizzare vedremo anche del rumore fondamentalmente e il modello non generalizza più bene generalizzare vuol dire quando gli date come input qualcosa che lui ovviamente su cui non è stato addestrato quindi lo mettete nel mondo reale e lui a quel punto sbaglia sbaglia perché è prono ad overfitting cioè soggetto ad overfitting questo ripeto lo chiariremo meglio nelle prossime lezioni per cui adesso rimane un po' sul vago mi rendo conto però tanto tenetelo lì come segna posto per tornarci sopra se io rinuncio a un po' di feature tipicamente riduco l'overfitting perché il modello si semplifica modelli più semplici sono meno soggetti a overfitting di modelli con più parametri con più feature eccetera eccetera questa è la regola generale l'altro motivo per cui voglio poter fare feature selection è per avere una interpretabilità come vi dicevo prima un conto è che il modello lavori su 100 feature un conto è che lavori su 10 o su 5 il risultato io riesco forse a spiegarlo meglio se dico sono arrivato il modello mi propone questo tipo di classificazione sulla base di queste 5 feature piuttosto che sulla base di 100 perché ho più possibilità di spiegare perché ha fatto quello che ha fatto il mio modello questa è quella che viene chiamata interpretabilità e sull'interpretabilità adesso ci sono tantissimi lavori anche proprio a livello di ricerca un problema aperto un problema molto sentito specialmente anche qui ci ricolleghiamo nell'ambito deep learning perché le reti neurali vengono per definizione dette essere dei black box delle scatole nere perché perché hanno migliaia milioni di questi parametri ok lavorano su input ad altissima dimensionalità e andare a capire perché effettivamente hanno fatto quella scelta che hanno fatto non è banale però diciamo l'interpretabilità del modello c'è non solamente come problema nell'ambito delle reti neurali c'è anche nei modelli di machine learning più tradizionali anche qui ci sono tonnellate di tecniche per fare selezioni delle feature non abbiamo assolutamente il tempo per poterle per poterle analizzare però se qualcuno di voi è interessato anche qui ovviamente possiamo possiamo posso darvi delle indicazioni per approfondire o può autonomamente cercare materiale per approfondire che potrebbe essere anche un esempio di qualcosa su cui lavorare per un progetto per esempio in vista dell'esame quindi applicare delle tecniche di feature selection a un dato problema ma la butto là non è che lo dovete fare giusto così sono tanti gli spunti che poi quando dovremo parlare del progetto avremo modo magari di riparlare però insomma adesso mi è venuto in mente che potrebbe essere uno di questi perché anche qui volendo c'è da approfondire e da lavorare vi dico molto brevemente alcune delle delle tecniche più utilizzate ripeto non ci entriamo nel dettaglio giusto sull'ultima diremo qualcosa in più ecco sulle prime due vi dico solo queste poche parole noi possiamo fare feature selection in molti modi una prima categoria di tecniche è quella cosiddetta basata su filtro su filtrabio cos'è la tecnica basata su filtro ma io vado a togliere delle feature che non rispondono a dei requisiti specifici cosa vuol dire vuol dire che per esempio io posso andare a vedere la feature mi accorgo vado a vedere tutti i dati del mio dataset di addestramento mi accorgo che la feature numero 5 ok ha una varianza quindi varia molto poco da campione a campione ora questo è indicativo di che cosa ma potenzialmente del fatto che quella feature può non essere molto utile perché spondiamo per per assurdo che io abbia una feature che è costante su tutti i campioni del mio dataset cioè ha sempre lo stesso valore quello è perfettamente inutile non mi aiuta a discriminare un campione da un altro e quindi io la posso togliere di mezzo quindi io potrei decidere una soglia sulla varianza sulla variabilità di una certa feature e dire ok se questa feature ha una soglia di variabilità una varianza inferiore a un valore th di soglia io la tolgo di mezzo e questo è un metodo molto se vogliamo anche grossolano però che in certi casi funziona e sono i metodi di filtraggio altri metodi un pochino più rifiniti sono i metodi cosiddetti di selezione sequenziale delle feature la selezione sequenziale delle feature consiste nel mettere in piedi degli algoritmi tipo gridi avido che procedo in modo iterativo cercando la miglior nuova feature da aggiungere partiamo da quelli ce ne sono due tipologie forward feature selection cioè in avanti e backward indietro quelli forward cosa fanno partono dall'insieme vuoto e cominciano a metterci dentro la feature che vi dà maggior contributo a una certa metrica ad esempio se state costruendo un classificatore dite ok qual è tra le feature di input quella che da sola mi dà il minor tasso di errore la feature 10 ok la mettiamo dentro dopodiché me ne rimangono altre supponiamo 19 che erano 20 all'inizio e dico tra le 19 che mi sono rimaste qual è che mi dà il contributo marginale maggiore e mi permette quindi di avere il tasso di errore più basso cioè se aggiungo la 1 la 2 la 3 la 4 la 9 la 11 perché la 10 l'ho già inserita fino alla 20 chi è che mi dà il contributo maggiore la 17 ok 10 e 17 vanno nel pool di quelle che ho già sistemato e vado avanti così a questo punto arrivo in fondo con un ranking una classifica delle feature in base al loro contributo lo stesso procedimento lo potrei fare all'indietro partendo da tutte le 20 feature ho fatto l'esempio con 20 feature chiaramente posso partire da tutto il numero di feature che volete e posso rimuoverle una alla volta quindi vado a rimuovere dall'insieme delle feature quella che mi dà il contributo più ampio e rimango le ultime che vado a togliere sono quelle ovviamente che contano di meno quindi di nuovo lì ho un ranking devo definire una meta che può essere il tasso di classificazione se stiamo parlando di un classificatore se stiamo facendo regressione può essere l'errore quadratico medio il min square error ok avete ci siamo fin qui il senso riuscite a seguirlo ok l'ultima cosa è l'ultima grande categoria in realtà poi qui in mezzo ci sono altre altre cose ogni tanto anche qui qualcuno se ne viene fuori con qualche nuovo metodo per fare feature selection quindi diciamo queste sono categorie più più più ampie più che trovate in letteratura però no ce ne sono altre anche in realtà si può fare feature selection anche tramite altre tecniche adesso vabbè le lasciamo qua adesso queste diciamo che queste coprono un'ampia fetta del mondo ok della selezione del feature e un'ultima di questa ampia fetta è la regolarizzazione la regolarizzazione è l'abbiamo già incontrata e la incontreremo è quella tecnica che vi modifica il la funzione di costo con un termine aggiuntivo e in questo caso questo termine aggiuntivo che viene chiamato appunto regolarizzatore è un termine che tende a penalizzare tutti i pesi che diciamo in modo da forzare il modello a ridurre i pesi delle feature meno importanti e adesso cerco di spiegarvelo meglio con l'ausilio della slide successiva ok allora vi dicevo prima cosa regolarizzazione cosa significa significa che noi abbiamo la nostra funzione di costo la nostra funzione di costo originale era questa era la g di w avete il vostro modello con il vostro insieme dei pesi e volete minimizzare la funzione di costo in modo da trovare appunto volete trovare il vettore dei pesi che minimizza quella funzione di costo va bene costruite una nuova funzione di costo aggiungendoci questo termine questo termine viene chiamato termine appunto di regolarizzazione questo è il regolarizzatore lo moltiplicate per un fattore lambda questo fattore lambda vi permette di fare è la manopola che vi permette di agire su quanto è importante questo secondo termine rispetto al primo e chiaramente se lambda è uguale a 0 torniamo al caso base che è il caso da cui siamo partiti quando invece lambda comincia a crescere di valore vuol dire che voi cercate di minimizzare non più g ma minimizzate f che è la composizione di due termini la somma di due termini g più il termine di regolarizzazione allora più è alto lambda più il termine di regolarizzazione comincia ad avere importanza e quindi se lambda è molto piccolo è ovvio che io il mio ottimizzatore nel mio processo di apprendimento cercherà di andare a minimizzare soprattutto il contributo di g man mano che comincia a crescere l'importanza di lambda questi due termini uno e due cominciano a competere quindi io devo il mio ottimizzatore va a minimizzare tutti e due insieme magari quando lambda prevale va a ottimizzare questo ovviamente viene moltiplicato per un fattore numerico più alto e lui non gli importa più niente di minimizzare quello va a minimizzare solamente questo termine questo va a scapito chiaramente magari del del tasso di classificazione se è un classificatore quindi diciamo bisogna trovare un valore giusto poi di lambda per diciamo avere il giusto compromesso tra la correttezza del mio modello e questo termine che adesso andiamo a vedere che cosa fa poi quali sono le possibili suggerimenti le possibili alternative che abbiamo a disposizione però bisogna trovare anche il valore di lambda giusto e quello è quello di nuovo lambda è quello che abbiamo chiamato già in passato un iperparametro gli iperparametri giocano un ruolo importante nel machine learning perché sono tutti quei parametri che non rientrano qui dentro quindi non sono parametri del modello ma sono parametri del algoritmo di apprendimento quindi per esempio il learning rate oppure della regolarizzazione della funzione ciò detto cosa si può fare si può fare una cosa di questo tipo si può andare a costruire hdv come la norma di tipo L0 del vettore dei pesi e quindi costruire una funzione che è la funzione originale di costo più lambda per la norma L0 del vettore dei pesi allora la norma L0 se vi ricordate quando all'inizio le abbiamo introdotte le norme dei vettori fa che cosa ve l'ho riportato qua conta il numero di entri non nulle di quel vettore ok quindi qui cosa stiamo facendo stiamo dicendo che la nostra nuova funzione di costo è la funzione di costo originale più lambda a volte il numero di entri non nulle di v doppio e che significa che cosa che quando io vado ad ottimizzare una funzione di questo tipo competono due fattori il primo è la funzione originale e l'altro è un fattore che se io comincio a incrementare lambda va a pesare sempre di più e rappresenta che cosa il numero di entri non nulle dei pesi ma questo significa che io come il mio ottimizzatore comincio a cercare di trovare una configurazione di pesi v doppio che abbia quale caratteristica secondo voi provate a ragionarci voi siete un ottimizzatore di questa funzione scrivete l'algoritmo oppure siete l'algoritmo stesso che ottimizza e ci avete due termini uno è questo quindi qui andate a minimizzare la funzione di per esempio la loss function di cross entropia che vi permette di minimizzare in qualche modo anche il numero di errori di classificazione e va bene però ci avete anche l'altro termine da soddisfare che vi dice cerca di ridurre che cosa il numero di elementi non nulli del vettore di pesi cioè il vettore ideale da quel punto di vista è il vettore che ha tutte le entri nulle quindi vuol dire che cercate il più possibile di trovare un vettore v doppio che si minimizzi questa ma che contenga pochi valori che contenga molti zero pochi valori non nulli ma avere un vettore v doppio che ha fatto così sponiamo di avere quattro componenti quindi 1.2 0 0 0.7 allora la norma di questo vettore la norma di tipo L0 quant'è? E' 2 giusto? Perché ho 2 entri allora questo significa che cosa che nel momento in cui avete il vostro modello questo è un modello lineare vi ricordate nel modello lineare noi cosa facciamo x trasposto per v doppio x trasposto per v doppio con il cerchietto sopra perché ci mettiamo v significa che quando fate x ci sono le vostre brave feature quando le andate a moltiplicare per questo vettore dei pesi dove avete queste entri nulle quelle feature non contribuiscono per niente quindi qui equivale a dire che il vostro modello non va mai a considerare che cosa la feature 2 la feature 3 quindi avete implicitamente selezionato la 1 e la 4 come le feature che contano per voi e più date peso aumentate lambda più lui vi troverà dei vettori w in cui mette tanti più 0 è ovvio che questo lo comincerà a un certo punto a fare a scapito del termine g perché il termine g magari permette da solo il valore ottimale è 1.5 2 0.2 0 meno 0.7 e 0.9 noi cominciamo a forzare questo a 0 questo a 0 poi se mettiamo anche questo a 0 il tasso di errore diventa probabilmente elevato sto iper semplificando però la logica è questa ci siamo? qual è la strada che questa tecnica intraprende? poi c'è un esempio si capisce meglio ve lo faccio vedere adesso vi dico soltanto che questo è chiaramente un obiettivo che possiamo perseguire in questo modo ma il problema è di andare a minimizzare una cosa di questo genere qual è? che queste sono delle funzioni che sono un pochino più delicate da minimizzare perché hanno problemi di non essere derivabili a diversi punti se non sono derivabili quindi differenziabili fate fatica a costruire algoritmi che insomma far lavorare algoritmi di discesa del gradiente oppure algoritmi di di Newton quindi metodi del primo e del secondo ordine per cui cosa si fa quello che si fa è utilizzare ad esempio una norma di tipo diverso cioè dire anziché forzare alcune entri a 1 io posso anche accontentarmi provare a dire ma anziché la norma L0 metto la norma L1 la norma L1 è la somma dei valori assoluti ve l'ho riportato qua quindi cerco di trovare un vettore dei pesi che al contempo sia buono per le performance del classificatore del regressore del mio modello allo stesso tempo abbia la somma dei valori assoluti sia bassa ora questo è collegato a quello che dicevo prima cioè se la somma dei valori assoluti è bassa insomma alcune entri le forzerò a dei valori più bassi per cui diciamo anche qui implicitamente vado nella direzione irradettata da questa logica cioè vado a selezionare mi dà uno strumento per selezionare alcune di quelle il vantaggio allora è chiaro che qui è un po' meno diciamo pulito il discorso cioè qui era estremamente chiaro nel momento in cui forzo a zero alcune entri non vado a prendere in considerazione le feature però se queste hanno anziché valore zero del valore molto basso comunque il discorso regge ancora il guadagno è che questo tipo di termine mi crea molto meno problemi rispetto a questo perché la funzione risultante questa è una funzione differenziabile quasi ovunque quindi potete usare tranquillamente il metodo di scesa del gradino in qualche caso si va addirittura a usare la norma di tipo L2 però probabilmente questa è la scelta di compromesso migliore perché qui ci allontaniamo un po' da questo schema qui però vengono usate tutte e tre adesso vi faccio vedere un esempio in cui questa logica viene messa all'opera scusate sono andato troppo avanti ok qui c'erano un paio di informazioni di riepilogo quindi la regolarizzazione è una tecnica in cui aggiungete un termine di regolarizzazione in modo da penalizzare i pesi e forzare il modello a ridurre i pesi delle feature meno importanti norme L0 e L1 sono le più utilizzate ok l'abbiamo detto perché forzano il modello i pesi del modello a essere insomma sparsi però L0 ha il problema di essere una funzione di costo discontinuo L1 conduce a dei pattern meno sparsi ma è con vesse differenziabile quasi ovunque quindi molto migliore da ottimizzare il problema è come settare lambda lambda si provano diversi tentativi e si va a lavorare o su quella che viene chiamata validazione incrociata sulla validazione incrociata dovremo tornare anche qui però praticamente voi avete il vostro dataset di addestramento poi avete un dataset di testing e andate a vedere su quel dataset di testing in diversi modi si può fare che cosa qual è la prestazione del vostro modello e andate a vederlo con diversi valori di lambda e scegliete quello migliore oppure lavorate sull'interpretazione umana se di quelle 20 feature a me quelle 5 che ho selezionato mi soddisfano perché mi permettono di capire bene come funziona il modello vado con quel valore adesso vi faccio vedere quello che vi dicevo che mi avevo promesso questo è un esempio in cui quello che è stato fatto è prendere il dataset che è detto Boston Housing è un dataset pubblico che è esattamente una tipologia di dataset del problema di regressione da cui siamo partiti quando abbiamo introdotto le regressioni lineari quello sugli affitti delle case che vi menzionavo anche all'inizio della lezione e viene minimizzato la funzione di costo ai minimi quadrati ok quindi viene costruito un regressore su quel dataset quel dataset ha se non ricordo 13 features se non sbaglio e queste dovrebbero essere queste qui riportate qui ognuna di quelle feature rappresenta una caratteristica diversa quindi perché dicevamo prima che possono essere il numero di metri quadri distanza dal centro distanza dall'ospedale più vicino così viene costruito viene aggiunto al termine di minimizzazione ai minimi quadrati un termine di regolarizzazione di tipo L1 quindi quello intermedio che abbiamo visto tra cui 3 e sono stati a questo punto provati in questo intervallo tra 0 e 130 questo intervallo è stato suddiviso in una griglia con 50 punti intermedi in questo intervallo e sono stati diciamo equispaziati quindi sono stati provati tutti i valori tra 0 e 130 tutti i 50 valori che suddividono questo intervallo in parti uguali ognuno di questi è un valore di lambda quindi si parte da lambda uguale a 0 che corrisponde ovviamente al metodo di minimi quadrati e poi tutti gli altri questo allora il metodo di minimi quadrati e adesso prima vi dico che cosa fa ok fino ad arrivare al valore di lambda quindi progressivamente incrementa per ognuno quindi ognuno di questi valori di lambda produce un modello giusto? perché io addestro tanti modelli quante sono le funzioni di post giusto? ok e su per ognuno di questi modelli il modello corrispondente è stato addestrato con un run di scelta del gradiente con un numero prefissato di passi sempre lo stesso e con lo stesso learning rate ok quindi questo è l'esperimento che è stato fatto i risultati sono questi in realtà anzi che riportare 50 di questi di questi run quello che sono riportati qui sono il valore dei pesi ok è un Instagram in cui avete il valore dei pesi per ognuna delle 13 feature di input ok questo significa che quando lambda è uguale a 0 che riferisce alla parte superiore del del del grafico io sto ragionando con un puro list square senza il termine di regolarizzazione ok quindi da qui vedete che cosa succede vedete il peso è molto basso per la feature 3 però tutti gli altri hanno alcuni pesi positivi altri negativi quindi questo è il vostro modello quello che abbiamo chiamato quando abbiamo fatto la lezione di teoria corrispondente W star ok e anche quando abbiamo fatto la lezione di 4 poi cosa succede aumentate lambda lambda fino ad arrivare a lambda uguale a 100 quindi questo W star di lambda uguale a 0 e questo è il W star di lambda uguale a 100 e vedete le cose come stanno quando lambda è 100 chiaramente comincia a prevalere il termine di regolarizzazione cioè lui nella discesa del gradiente minimizza in modo da schiacciare comprimere il più possibile i pesi e vedete che l'effetto di questo è che molti diventano quasi nulli sarebbero esattamente nulli se io avessi normalizzato con la norma L0 ma mi sovrappravvivono tre barre che sono questa questa e questa che sono relative alla feature 7 alla feature 11 e alla feature 13 alla feature 6 scusatemi non 7 questo ci dice che cosa ci dice che per il modello le tre feature più importanti sono queste quindi volendo io potrei selezionarle avrei chiaramente portandomi dietro questo magari rinuncio ad avere la stessa accuratezza del mio modello rispetto al fatto di portarmi a dietro tutte e 13 però guardate cosa succede succede che io comincio a dire va su 13 possibili feature che sono tutte quelle elencate quali sono quelle che hanno più peso nel prendere una decisione nel dire il prezzo di questo immobile sarà tot sono la 6 la 11 e la 13 se andate a vedere se andate a vedere nel dataset sono rispettivamente il numero medio di stanze per ogni immobile ed è molto ragionevole che il prezzo sia determinato dal numero medio di stanze per ogni immobile il rapporto tra il numero di allunni e il numero di insegnanti nella città perché questo è su città diverse e anche qui è un fattore che può influenzare il prezzo di un immobile se voi avete e vedete che tra l'altro lo influenza con un peso negativo perché all'aumentare di questo rapporto vuol dire che fissato il numero degli insegnanti aumenta il numero degli studenti e questo rende i valori degli immobili più bassi tipicamente almeno questo da passione e qui c'è anche qualcosa che è quantificato come un lower status del population cioè diciamo il fatto che livelli di occupabilità cose di tipo demografico di questo genere anche qui ovviamente se andate a prendere un appartamento che sta in una zona ovviamente dove la popolazione sta meglio costa di più chiaramente se andate a prendere volete comprare un edificio in una zona piuttosto degradata dove ci sono delle difficoltà a livello socio-economico ovviamente questo avrà un valore inferiore questo è di fatto quello che vi dicevo prima riguardo l'interpretabilità nel modello quindi se il modello vi dice ah il prezzo di questo immobile è 100.000 euro perché? ebbè perché vado a vedere queste tre picio bene questo è un altro esempio no questo è sempre lo stesso ce n'era un altro ah sì c'è un altro esempio adesso non c'è la figura che è un esempio questo è un esempio di regressione questo è un altro esempio da un dataset pubblico sempre preso diciamo da un abito in tipo economico se vogliamo su un però è un classificatore classificatore su un dataset tedesco basato sul credito cioè per la concessione del credito praticamente è un classificatore costruito per valutare se concedere o meno il credito per esempio per un mutuo quindi cose che fanno le banche oggigiorno abbastanza comunemente e lì dietro ci sono sempre dei sistemi di questo tipo insomma che ragionano in questo modo qui è stato fatto di nuovo una regolarizzazione di tipo L1 è stato costruito se non sbaglio un regressore logistico con la funzione di costo softmax binario quindi ti concedo il credito oppure non ti concedo il credito con con un certo numero di feature di input non mi ricordo quali fossero adesso vi dico quelle che in qualche modo sono tra virgolette sopravvissute a questo processo di selezione però lì dentro ce ne sono diverse che riguardano chiaramente le caratteristiche del singolo richiedente del prestito e è stato fatto di nuovo il metodo di scelta del gradiente con stesso numero di pass stesso learning rate e il valore di lambda crescente a partire da zero quando ci si ferma lambda a 40 ecco le feature di input erano in totale 20 ok quando ci si ferma questo ce l'avete scritto qua quando ci si ferma a 40 vedete che le ne emergono 4 come quelle principali che sono la feature 1, 2, 3 e 6 nel dataset e che sono qui ha riportato il l'equilibrio del conto corrente quindi praticamente quanto nel vostro conto corrente avete diciamo di sbalzi avete fatto dei mesi con molti debiti oppure dei mesi in cui avete avuto diciamo dei picchi insomma quanto è più è irregolare diciamo il livello del vostro conto corrente qual è qual è la durata in mesi la feature numero 2 di un credito precedente con la banca cioè avete avuto un credito precedente vi hanno già erogato un mutuo quanti mesi inoltre lo status di pagamento lo stato del pagamento di ogni credito anteriore con la banca cioè siete già stati in grado di ripagare un eventuale altro prestito a che punto siete ovviamente più avete debiti con la banca più tipicamente la risposta potrebbe essere negativa e ovviamente anche il valore corrente dei risparmi più risparmi avete da parte più chiaramente avete possibilità che questo vi venga erogato questo di nuovo vi fa emergere come la selezione delle feature vi catturi alcuni aspetti fondamentali cioè il classificatore lo potete costruire con tutti i eventi però feature selection vi dice beh quali sono quelli che pesano di più nel processo di classificazione va bene direi che qui per quanto riguarda il pre-processing quindi l'ingegnerizzazione delle feature e la selezione delle feature è tutto quello che per il momento ci serve e che in generale diciamo ci servirà ci sarebbe anche qui molto altro da dire ma dobbiamo anche qui dire altre cose e in particolare sull'ingegnerizzazione delle feature andremo a parlare della non linearità ma questo lo facciamo la prossima volta perché con questo introduciamo il capitolo del machine learning non linear quindi questo dovremmo un po' ragionare un po' ci staremo un po' per cercare di capire che cosa è ma direi che cominciamo la prossima volta domande? Profio qualcosa sul progetto se fosse possibile intanto magari faccio una cosa intanto blocchiamo intanto la registrazione poi sicuramente posso rispondere a queste domande grazie prego facciamo così intanto fermiamo qui la lezione a queste domande una domande