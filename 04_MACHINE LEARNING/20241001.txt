allora benissimo intanto bentornati alla seconda lezione oggi oggi proseguiamo proseguiamo con le definizioni introduttive e andiamo a cercare di capire che cosa vuol dire costruire un modello di apprendimento automatico e in particolare cominciamo a proseguiamo su quello che avevamo cominciamo a delineare quella che è una pipeline tipica di costruzione di modelli machine learning cioè un flusso di lavoro che permette di costruire appunto quelle che poi sono delle applicazioni di machine learning e io avevamo cominciato a prendere in considerazione questo semplicissimo esempio che ci serve però per dare qualche intuizione pur essendo estremamente appunto se vogliamo anche idealizzato nel senso che abbiamo detto che assumiamo di poter acquisire delle caratteristiche che non è così neanche scontato che uno riesca acquisire cioè andare in automatico a definire una quantità numerica che ci permetta di dire se le orecchie sono più o meno appuntite o arrotondate non è così poi banale e lo stesso modo vale per il naso però ammesso che si possa fare è una cosa che ci permetterebbe dicevamo ieri di fare che cosa di prendere i dati che abbiamo quindi il nostro input e trasformare il nostro input in base alle caratteristiche che abbiamo scelto le caratteristiche che abbiamo scelto in questo caso sono due e sono la punto in questo caso la forma delle orecchie o la dimensione del dato ovviamente nulla dieta di anziché di sceglierne due di feature di sceglierne 3 5 10 20 100 quelle che vogliamo tipicamente se scegliamo più feature ci possiamo aspettare di definire in maniera un po più accurata quello che effettivamente il nostro il nostro input e questo può avere dei riflessi positivi non sempre così però in termini per esempio dell'appuratezza del task che vogliamo andare a risolvere in questo caso continuiamo a ragionare appunto con queste due feature con queste due caratteristiche il concetto di feature è un concetto estremamente importante quindi presente in maniera in maniera direi ubiqua o ubiquitaria nell'ambito del machine learning e rappresenta proprio la caratteristica in questo caso abbiamo due feature quindi due caratteristiche e queste feature ci permettono di trasformare il nostro input in uno spazio geometrico che è lo spazio delle feature lo spazio delle feature appunto uno spazio geometrico vero e proprio in cui noi andiamo a a rappresentare il nostro input in questo caso il nostro input sono delle immagini noi le andiamo a in qualche modo a estrarne un riassunto una sintesi se vogliamo tramite le feature e ognuna di queste immagini non è più rappresentata tramite ad esempio la griglia di valori rgb red green blu che potremmo prendere di pixel ma tramite in questo caso due valori numerici è chiaro che in questo noi andiamo a condensare in maniera radicale quello che il contenuto informativo dell'immagine però stiamo ragionando intanto in maniera astratta questa è un'operazione che si fa di routine cioè abbiamo un input e cerchiamo di rappresentarlo da un punto di vista matematico come un insieme di numeri l'input di ogni sistema di machine learning che voi trovate in circolazione è un insieme di numeri un vettore di numeri e parte del processo di costruzione di un sistema di machine learning è la trasformazione dell'input in vettori numerici una volta che abbiamo questi vettori numerici beh lì si apre tutto un mondo che permette di andare a confrontarli manipolarli eccetera quindi dall'input noi passiamo allo spazio delle feature questo spazio delle feature è uno spazio geometrico lo vediamo qui possiamo rappresentare i nostri i nostri input in uno spazio bidimensionale nel tramite gli assi cartesiani è chiaro che la rappresentazione grafica ce l'avremo in due tre dimensioni questo lo ripeteremo più volte ovvio che se ho più di tre feature quindi se ho diciamo due feature riesco riesco a rappresentarlo se ne ho tre riesco ancora a rappresentarlo se ho un numero maggiore di tre ovviamente non ho più la possibilità di rappresentarle graficamente ma non importa riusciremo a fare comunque delle cose interessanti vedrete ok che cosa significa quindi andare a rappresentare l'input in uno spazio geometrico di questo tipo significa che a questo punto io ho i miei input che vedete sono esattamente collocati in questo piano e io posso cominciare a fare delle ipotesi che sono le ipotesi di lavoro del mio modello abbiamo detto che noi dobbiamo costruire un modello matematico che permetta di risolvere il problema ma supponiamo che il problema sia un problema di questo tipo che ci parla quindi una classificazione di mali in cani e gatti bene a questo punto cerchiamo di capire meglio che cosa significa costruire questo modello matematico modello matematico è semplicemente nulla di più che appunto un modello matematico quindi un qualcosa che ha che lega gli input a un output e tramite un'espressione matematica e questa espressione matematica ci permette di passare dal mondo dell'input al mondo dell'output e a dei paranti qui si capisce molto bene a livello grafico cosa può essere un ipotetico modello per questo problema ipotetico modello per questo problema potrebbe essere ad esempio questa retta nera questa retta nera che cosa fa vedete è una retta una retta sappiamo bene appunto che è un modello matematico specificato da da quanti parametri due che sono la pendenza e l'intercetta tramite questi due parametri noi riusciamo a definire una retta come ad esempio questa retta nera e questa retta vedete che divide lo spazio delle feature in due in due semispazi uno che è stato colorato di azzurro uno che è stato colorato di rosso rosa diciamo meglio che sono gli spazi dove secondo quel modello matematico stanno gli esempi del nostro data set questi esempi del nostro data set sono gli esempi che noi possiamo utilizzare per costruire il modello matematico nel senso che noi formuliamo come progettisti un'ipotesi di quello che può essere il modello matematico in questo caso è una retta quindi stiamo parlando di un modello di tipo lineare abbiamo detto che un modello lineare specificato da due parametri la pendenza e l'intercetta che significa che io in linea ipotetica avrei potuto avere non solo questa retta ma ad esempio avrei potuto avere questa che passa per l'origine e ha un'altra pendenza avrei potuto avere quest'altra possiamo fare anche di colori diversi faremo di colori diversi così vediamo che sono avrei potuto avere quest'altra avrei avuto avrei avuto avrei avuto avrei avuto così è così chiaramente sono tutti modelli tutti questi modelli che cosa fanno mi definiscono un una suddivisione del mio spazio geometrico in due semispazie da una parte staranno alcuni esempi dall'altra E' chiaro che non tutti questi modelli corrispondono a una separazione corretta di quegli esempi. Perché se prendo questa verde vedete che mi sta a dire che cosa se considero questa verde. I due semispazio sono questo, scusatemi, sono questa verde, questo semispazio e quest'altro. E vedete che secondo questo modello, se prendo il semispazio sinistra, questi tre esempi appartengono alla stessa categoria di questi. Quindi questi tre cani in realtà sono equiparati a dei gatti. Allora questo ci dice che cosa? Che nel momento in cui noi cominciamo a ragionare in questo modo abbiamo fatto prima un'ipotesi, che è di costruire un modello. E abbiamo preso un esempio, ecco, abbiamo detto partiamo dal modello lineare perché è il modello più semplice che ci può venire in mente. Una relazione lineare è la relazione più semplice che c'è in questa maniera. Ok? Quindi il modello più semplice è il modello lineare. E infatti noi partiremo dallo studio dei problemi lineari, dei modelli, scusatemi, lineari. Poi si applicano anche problemi che hanno caratteristiche di non linearità. Vedremo con quali performance, cercheremo di capire. Però intanto partiremo da questi. E risolvono peraltro dei problemi anche abbastanza complessi. Quindi nonostante la semplicità sono modelli che hanno un loro valore. Un loro valore e per cui hanno il loro motivo di essere. L'altra cosa che vi volevo dire, ecco, pur partendo comunque da un modello molto semplice, è un modello che ha a disposizione, vedete, diverse alternative. E la scelta di questi parametri, come viene a questo punto effettuata? Viene effettuata tramite il processo di addestramento. Vi ricordate ieri nella lezione introduttiva abbiamo cominciato a definire il concetto di supervisore, nel caso dell'addestramento supervisionato. Quindi il genitore che dice al figlio questo è un cane e questo è un gatto. Qui questo ruolo viene giocato dalle etichette. Queste immagini sono state selezionate, sono state etichettate. Alcune sono cani, alcune sono gatti. E allora l'obiettivo, arrivo subito un attimo, l'obiettivo è addestrare il modello, cioè fare che cosa? Scegliere quei parametri, in questo caso sono solo due parametri, l'intercetta e la pendenza, in modo tale da fare che cosa? Da minimizzare, ad esempio, l'errore di classificazione sul dataset di addestramento. Per cui la retta verde compie degli errori, perché sicuramente mi dice che questi tre sono dei gatti, ma non sono dei gatti. E allora tra tutte queste infinite alternative, che sono le infinite rette, io andrò a scegliere nel mio processo di addestramento quella o quelle, ce ne possono essere chiaramente più di una, noi ci accontenteremo di una, quell'insieme di parametri, in questo caso due, quella coppia di parametri, che mi minimizza un certo, una certa metrica, diciamo così, poi cercheremo di capire meglio quali sono. Dimmi pure. No, no, we can, we can use the linear model also, if we have not only binary outputs, but also multi-class output, and we will see how to build them, ok, later during the course. Allora traduco per quelli che, in italiano, la domanda era, il modello lineare può essere utilizzato solamente nel caso di output tipo binario? La risposta è no, vedremo modelli lineari più avanti nel corso, anche per problemi di classificazione multi-class. Benissimo, allora, fino a qui ci siamo, quindi, ok, tutto posto, andiamo avanti, quindi addestrare il modello che cosa significa? Significa trovare la configurazione di parametri migliore per il nostro task. Il nostro task è, in questo caso, classificare correttamente questi esempi. Quindi significa identificare, ad esempio, questa retta nera, ma sarebbe andata bene anche un'altra retta di altro tipo, che anche questa retta, per esempio, di questo colore, sarebbe stata corretta rispetto al nostro esempio, va bene? Come facciamo? Lo vedremo, questo piano piano impareremo a farlo. Intanto, prima di vedere come fare a trovare queste soluzioni, quindi questi modelli, quindi i parametri che ci portano a costruire un modello che ha una sua accuratezza, dovremmo definire anche l'accuratezza, eccetera, eccetera. Quello che vi posso dire, l'accuratezza in questo caso è definibile in maniera abbastanza intuitiva, quanti esempi riesco a classificare correttamente con il mio sistema automatico. Nel tipico flusso di lavoro della costruzione di un sistema di machine learning, abbiamo detto, si raccolgono quindi dei dati, dataset, in questo caso deve essere etichettato perché devo risolvere un problema di classificazione, quindi devo avere degli esempi, devo avere una forma di supervisione, e poi addestro il mio modello. Faccio un'ipotesi di lavoro, quindi dico il mio modello è lineare, lo addestro, quindi trovo la configurazione di parametri, a quel punto il mio modello è quello, può essere congelato e possiamo a quel punto mandarlo se vogliamo in produzione, ma prima di mandarlo in produzione quello che facciamo è testarlo o validarlo. Però per testarlo o validarlo come facciamo? Abbiamo bisogno di altri dati, perché prima di mandarlo in produzione vogliamo sapere come si comporta, perché se c'è qualche problema siamo ancora appunto in tempo per correggere eventuali problemi, allora prendiamo altri esempi, altri esempi che sono detti test di val... set insieme di validazione o insieme di test, quindi test set. La validazione o test, quindi questo diciamo solitamente viene chiamato insieme di test, anche di validazione, ma vedremo che di validazione poi più avanti ne parleremo su altro significato. Comunque, che cosa significa? Significa che noi prendiamo ulteriori immagini, attenzione, che il sistema non ha visto durante la fase di addestramento. Questo è cruciale perché altrimenti significa che staremo barando, facciamo una cosa che non va bene. Se noi facciamo il test su delle immagini che lui ha già visto, lui è stato addestrato, quindi il suo compito è facilitato, chiaramente. Questo non va mai fatto, quindi ho un principio di funzionamento corretto. E... E supponiamo, vedete, di... Queste sono le immagini che lui non ha visto, sono quelle dell'insieme di test che sono state messe di nuovo in questo spazio bidimensionale e in questo spazio bidimensionale abbiamo tracciato la retta nera che rappresenta il nostro modello. In particolare del nostro modello rappresenta quello che viene chiamato confine decisionale di un classificatore. Anche questo è un termine che poi più avanti pareremo a conoscere meglio. E però è chiaro, direi, è abbastanza intuitivo che cosa rappresenta. Il confine decisionale rappresenta quel luogo dei punti che separa quei due semispazi, per cui al di sopra saranno tutti i punti che il mio classificatore, il mio modello classificherà come gatti, al di sotto come cani. In particolare, questi esempi, vedete, ricadono in questo caso qua. Questo è un gatto che ha orecchie particolarmente appuntite e naso piccolo. Questo è un cane che ha orecchie particolarmente rotonde ma il naso grande. Questo è un cane che ha naso grande però ha orecchie abbastanza appuntite. e qui vedete che succede una cosa interessante perché succede che questo esempio, che è un cane, ok, non mi ricordo di che razza sia, in realtà, è scritto da qualche parte, sì, sì, si affomiglia un po', però non so se viene chiamato in maniera diversa nell'esempio che ho preso, forse equivalente a quello che noi chiamiamo nel carico. In ogni caso, quello che è, è un cane che ha una caratteristica di avere le orecchie appuntite e il naso abbastanza piccolo. Quindi, che cosa succede? Che va a cascare qui nel mio semispazio. E qui, ovviamente, secondo il mio modello, quello è un cane, cioè il mio sistema automatico dirà, attenzione, questa è la foto di un cane. E questo è un modo tipico di procedere. Cioè, noi abbiamo costruito a partire da un dataset e attenzione, la costruzione del dataset è parte integrante del processo di costruzione di un sistema di machine learning ed è fondamentale. Avere un dataset accurato, avere un dataset con tanti esempi è cruciale per poter costruire dei sistemi che funzionano bene. E, tanto è che, appunto, i dati sono oggigiorno nella società in cui viviamo sempre più importanti. Sempre più importanti da un punto di vista veramente tecnico, scientifico, ma anche economico. Una volta che ho costruito i dati e ho addestrato il sistema, lo vado a testare e mi accorgo che ha delle determinate performance che tipicamente possono essere non sempre allineate con quelle del mio dataset di addestramento. perché mi comincio ad accorgere che qualcosa può cambiare e allora ho una performance o delle prestazioni diverse. Comincio magari ad accorgermi che alcuni casi li sbaglio e allora che cosa posso fare? Posso eseguire un feedback, quindi una retroazione in cui vado a dire cerco di migliorare il mio modello. come posso fare a migliorare le prestazioni? Beh, posso farlo fondamentalmente in tre modi e dire vado a risolvere questo tipo di problematica. Uno, potrei raccogliere più dati. Comincio a raccogliere più dati perché non erano sufficienti a costruire un'idea diciamo del mondo, magari il mondo che io sto cercando di andare a in qualche modo a costruire con questo modello è un mondo in cui qui ci sono tanti cani. tanti altri esempi di cani che io non mi ero accorto che ci fossero magari e per cui aggiungendo tutti gli esempi di cani qui magari ci stanno altri esempi invece di gatti beh, arrivado a fare l'addestramento e mi accorgo che il mio modello lineare più accurato avrebbe potuto essere questo e a quel punto con quest'altro modello ho sul test set delle prestazioni. L'altra cosa che potrei fare è progettare delle feature magari migliori che mi aiutano a discriminare meglio. Io qui ci siamo concentrati su queste due feature ma magari c'è qualche feature più interessante potrebbe essere il colore del manto diciamo che in realtà non ci aiuta più di tanto nel problema di risolvere cani e gatti magari ci aiuterebbe se dovessi distinguere un orso polare da un grizzly ok? uno è bianco e uno è marrone chiaramente è una feature molto discriminante qui avrei potuto introdurre che ne so la coda non so qualcosa l'altra cosa che posso fare e questo è un altro processo tipico che si fa per cui abbiamo la scusa per vedere che cosa si fa tipicamente quando si costruiscono dei sistemi di questo tipo si cerca di raccogliere più data se vogliamo migliorare le performance si cercano di progettare delle feature migliori e ultimo ma non da ultimo prima si provano le altre cose poi eventualmente si comincia a ragionare su altri modelli su altri modelli che possono essere ad esempio dei modelli di tipo non lineare per cui mi accorgo che magari può essere utile una cosa che fa così ok? e questo è il tipico modo di procedere su diversi di questi aspetti torneremo pian piano insomma cercheremo di costruire un po' di tutto il bagaglio di conoscenza che è all'interno del nostro corso fino a qui tutto chiaro? ci siamo? ok qui abbiamo un riepilogo delle cose che abbiamo detto finora questa slide vi sintetizza quello che è proprio quello che vi dicevo quello che vi dicevo è il tipico flusso di lavoro la tipica pipeline di un sistema di machine learning e la pipeline è primo step raccolta dati i dati li suddivido tipicamente in dataset di addestramento e dataset di test o valida quindi training set e test set progetto delle feature il processo di progettazione delle feature è un processo che è estremamente complesso e complicato ed è molto dipendente dal dominio dominio intendo dominio applicativo devo analizzare se devo risolvere un problema di computer vision devo estrarre delle feature da delle immagini e queste saranno diverse dalle feature che posso estrarre se devo risolvere un problema di riconoscimento del parlato oppure se devo risolvere un problema che classifica dei testi e quindi è un processo non banale che spesso e volentieri ha rappresentato e rappresenta una parte non trascurabile dello sforzo di chi ingegnerizza questi sistemi questa cosa è cambiata diciamo con l'introduzione dei sistemi basati su reti neurali perché le reti neurali hanno la caratteristica di possono lavorare ovviamente su feature che costruiamo noi ma hanno la caratteristica non sono le uniche diciamo gli unici modelli di questo tipo ma sono dei modelli tra quelli sicuramente più più famosi più utilizzati per cercare di scoprire tra virgolette in maniera autonoma le feature migliori cioè riescono a estrarre da dei dati grezzi che possono essere l'immagine come griglia di pixel quelle che sono le caratteristiche principali per poter arrivare a una classificazione e questo è uno dei punti di forza delle reti neurali sì la differenza è che il bambino lo riesce a fare con pochissimi esempi le reti neurali hanno bisogno di tantissimi dati per poterlo fare e qui ancora ci fa capire qual è il gap tra l'intelligenza artificiale e quella ovviamente naturale però la capacità di estrarre in maniera autonoma queste feature è un qualcosa che effettivamente è una caratteristica estremamente importante di questi sistemi però c'è anche tutto ancora un machine learning basato su feature che vengono invece ingegnerizzate a mano da chi conosce bene quel problema e dice ok no per fare una classificazione corretta delle immagini sulle immagini ormai computer vision è quasi tutto nelle reti neurali però diciamo fino a un po' di tempo fa comunque si costruivano si estraevano come l'istogramma dei gradienti l'istogramma delle varie caratteristiche dell'immagine che si andava ad analizzare come anche nel parlato venivano estratte vengono estratte possono essere ancora costruiti dei sistemi che prendono estraggono le componenti in frequenza del segnale eccetera eccetera ok passo numero 3 addestramento del modello direi che qui tra il 2 e il 3 potremmo aggiungere un qualcosa cioè qui è incapsulato anche la scelta del modello stesso ok ma potrebbe essere aggiunto tra il 2 e il 3 cioè io scelgo la mia ipotesi di lavoro il mio modello e poi cerco di validarlo o di testarlo adesso adesso diamo un po' quindi abbiamo fatto questo esempio questo esempio molto molto semplificato ci permette intanto di cominciare a mettere l'acqua a qualche punto perché poi ci aiuta a a mettere le basi per diverse cose che cominceremo a vedere più con più con maggiore rigore e adesso quello che facciamo è anche sulla base di questo esempio e di altri che pian piano introduciamo arricchiamo un pochino la la terminologia è quella che viene chiamata appunto una tassonomia di base cioè suddivisione dei vari tipi di di di apprendimento che nell'ambito del machine learning possiamo incontrare allora l'apprendimento supervisionato l'abbiamo già detto è fondamentalmente che cos'è è l'apprendimento con supervisione è fondamentalmente che cosa l'apprendimento automatico di regole computazionali con delle relazioni imputate io il l'esempio dei cani dei gatti voglio che la macchina impari a rispondermi cane quando ho un'immagine di un cane gatto quando ho un'immagine di un gatto e con supervisione perché il mio dataset ha degli input ma anche degli output il dataset di addestramento cioè io dico al sistema qual è il comportamento corretto che lui deve imparare qui c'è la supervisione però non è l'unico diciamo non è l'unica modalità adesso vedremo ne vedremo parleremo anche del non supervisionato prima di passare al non supervisionato vi faccio notare come finora abbiamo ragionato con problemi che sono problemi di classificazione il cane il gatto ok oppure l'esempio di riconoscimento oppure no di keyword di parole nel frammento audio sonoro che sono che rappresentano appunto le cosiddette wake up word quelle che risvegliano un assistente vocale e lì c'è o non c'è la parola che deve risvegliare l'assistente vocale però non sono gli unici problemi che si possono trovare ad affrontare perché ci sono problemi per esempio di regressione la regressione è un'altra comune problema che si trova nell'ambito del learning dell'apprendimento con supervisione essenzialmente sono lo stesso tipo se vogliamo di problema ma la differenza principale è che mentre nei problemi di classificazione l'output è discreto quindi cane gatto questa parola c'è oppure no binario in questo caso oppure ho più classi magari ne ho dieci ma ho uno due tre quattro cinque sei sette otto nove dieci un insieme discreto di possibili categorie di possibili etichette e questo è un problema di classificazione l'output è discreto il problema di regressione è molto simile ma l'output non è più discreto può variare con continuità e quindi assumere qualunque valuta allora per capire di cosa stiamo parlando c'è questo esempio che secondo me è abbastanza esemplificativo supponiamo di voler calcolare di voler di disporre di una serie di dati anche qui bisogna sempre partire dal dato che rappresentano come varia questo è preso da un esempio del punto di economia quindi dal mondo diciamo dell'economia supponiamo di avere una serie di aziende di cui conosciamo i guadagni annui quindi sappiamo quali sono i guadagni che l'azienda il fatturato dell'azienda e supponiamo che queste aziende sono quotate in borsa e di ogni azienda conosciamo anche il valore delle azioni quindi la quotazione in borsa quindi per questa azienda che è l'azienda A1 noi conosciamo qual è il fatturato e sappiamo qual è il valore delle soluzioni poi per l'azienda A2 conosciamo il fatturato e il valore delle azioni per l'azienda A3 conosciamo il fatturato e il valore delle azioni e così via ok a questo punto io ho dei dati in cui ho un input che è il fatturato è una singola feature quindi un esempio molto semplice con una sola feature e ho un output che è il valore delle azioni vedete che l'output è un valore numerico in questo caso che può variare con continuità può essere 10 dollari 5,5 75 euro 4,2 eccetera non è più un output discreto è un output che può variare con continuità però io ho costruito comunque un dataset in cui ho degli input e ho anche l'output solo che l'output non è più discreto ma è continuo ma posso comunque pormi il problema di dire ok supponiamo di conoscere l'incasso i guadagni il fatturato annuo di una di una di un'azienda e supponiamo che questa azienda si voglia quotare in borsa io voglio fare delle previsioni su quali possono essere i prezzi il valore delle azioni con questo tipo di fatturato posso farlo? sì posso farlo risolvendo un problema di regressione il problema di regressione è un problema di nuovo di tipo supervisionato in cui io colleziono dei dati di input e lo schema è esattamente quello che abbiamo visto prima per la classificazione perché io colleziono dei dati di input di cui conosco anche l'output e faccio un'ipotesi su quello che è il modello di apprendimento il mio modello di apprendimento è un modello matematico che se vogliamo partire dall'esempio più semplice è di nuovo una retta modello lineare guardate che cosa succede che con un semplice modello lineare di questo tipo io posso andare a dire ok secondo me il modello lineare al crescere ragionevolmente del fatturato dell'azienda cresce il valore delle sue aziende linearmente linearmente in realtà questo è quello che in matematici chiamano un modello a fine cioè è una retta ma c'è anche una sua brava intercetta qui non passa per l'origine quindi l'equazione di una retta e una volta che io ho definito il mio modello beh posso sfruttare il mio dataset di addestramento per fare che cosa per cercare di fare quel che viene chiamato nella regressione un fitting dei dati cioè adattare capire bene tra le infinite alternative perché di nuovo quante una volta che ho scelto il modello lineare ho comunque infinite possibilità perché io posso selezionare di nuovo questa retta questa quest'altra quale tra queste è la retta tra virgolette migliore il modello migliore beh intuitivamente tra questa che ho disegno che c'è disegnata in azzurro e ad esempio questa in giallo quale scegliereste voi quella azzurra risposta corretta perché ok da un punto di vista qualitativo è questo i punti del nostro dataset di addestramento si avvicinano di più allora dovremo imparare a quantificare che cosa vuol dire si avvicinano di più lo faremo però io posso tradurre tutto questo in quella che viene chiamata una metrica cioè un modo di misurare quanto si avvicina quell'insieme di punti al mio modello cioè quanto il mio modello spiega potremmo dire in altro modo quel tipo di insieme di punti ok e questo impareremo a farlo intuitivamente quei punti stanno più vicini alla retta azzurra di quanto non lo siano alla retta gialla e quindi facendo così io comincio a dire ok a questo modello azzurro corrisponde un insieme di parametri di nuovo pendenza e intercetta della mia retta a quello giallo corrisponde un'altra coppia di parametri a questi due tipo a questi due modelli corrispondono delle prestazioni diverse che impareremo a quantificare per esempio dire ok quanto sono vicino con i miei punti alla retta gialla quanto sono vicino alla retta blu in realtà la retta gialla è più lontana lo quantificheremo e quindi impareremo a far sì che il nostro modello il processo di addestramento sia tale da portarci a una scelta che è quella plausibilmente auspicabilmente della retta azzurra che è più vicina una volta che io ho questo modello posso dire ok ho una nuova azienda di cui conosco il il fatturato mi posso domandare qual è un'ipotetica quotazione in borsa beh che cosa faccio semplicemente prendo il mio modello che è questo vado a dire ok questo è il fatturato vado a vedere sulla retta che rappresenta la mia quella che viene chiamata retta di regressione dove viene intersecato appunto in corrispondenza di quel valore dell'X vado a vedere qual è il valore dell'Y e ho una previsione di quello che può essere un ipotetico valore questo è di nuovo una tipologia di problema di tipo supervisionale però è quello che viene chiamato problema di regressione perché perché l'output può variare con continuità qui il valore dell'Y può essere questo questo questo quello che vuole mentre nel caso della classificazione cani e gatti era zero oppure uno cambio in base certo ovviamente quelle che sono più alte avranno un valore più alto delle azioni si presuppone che Apple che incassa tantissimi soldi abbia ragionevolmente delle azioni di valore più alto rispetto a un'azienda più piccola che ovviamente ha minori possibilità ecco sul mercato di ha il suo valore di azione ma magari inferiore ecco semplicemente riepilogavo il fatto che l'output è discreto per i problemi di classificazione mentre è continuo per i problemi di regressione e poi i problemi di regressione prendono valore di uscita nei numeri reali mentre per i problemi di classificazione il valore di uscita è uno tra possibili alternative in un insieme discreto benissimo direi che possiamo senz'altro andare avanti e scusate sono tornati indietro anziché andare avanti eccoci qua e qui facciamo un altro esempio giusto per far capire meglio di cosa parliamo quando parliamo di regressione perché la classificazione è più intuitiva forse la regressione è un po' di meno ma alla fine vedete che sono due problemi che sono chiaramente estremamente intercorrelati questi sono dati per quanto ne so io veri in particolare sono il il debito che le famiglie americane in Stati Uniti hanno accumulato nel corso degli anni per far studiare i figli i figli all'università quindi è un è la somma perché vedete sono trilioni trilioni significa migliaia di miliardi di dollari di no scusate sono miliardi no perché billion sono milioni e i miliardi di dollari il debito di diciamo delle famiglie americane a partire dal 2006 poi 2008 2010 vedete ci sono anche tanti punti intermedi che sono stati misurati nei singoli anni e vedete che questi punti questo debito è progressivamente chiaramente cresciuto tra il 2006 e il 2014 che significa che le famiglie si sono indebitate sempre di più per far studiare i loro figli all'università questo non è procapite chiaramente è aggregato su tutto il paese e da qui si vede bene che in funzione dell'anno semplicemente è una tendenza vedete particolarmente schiacciata su quella retta questo è proprio il tipo di fitting che visivamente vedete è estremamente immediato dire che è lineare quindi con questo se fa l'addestramento di questo modello lineare riesce a fare una previsione ecco la regressione serve anche a fare delle previsioni sul futuro che è una cosa non banale e spesso si sbaglia anche perché poi avvengono tanti fattori quindi anche qui bisogna sempre avere una una cautela ovviamente quando si utilizzano questi modelli perché hanno chiaramente i loro limiti che bisogna sempre tenere presente però diciamo supponendo che le condizioni al contorno che hanno determinato questa crescita rimangano le stesse è chiaro che io posso andare a fare delle previsioni quanto sarà il debito nel 2000 adesso oramai siamo nel 2024 quindi potevo farlo e controllare cosa succedeva nel 2016 2018 eccetera eccetera e posso andare a fare delle stime anche per l'anno prossimo il 2025 e così via e anche qui lo faccio accumulando dei dati per ogni anno vado a misurare il debito ipotizzando un modello facendo l'addestramento quindi scegliendo da tutti gli infiniti modelli in questo caso lineari qual è quello migliore secondo dei criteri che abbiamo detto impareremo a definire e provando ad andare a vedere su un dataset su alcuni punti di test come il mio modello risponde quanto siamo stati bravi a costruire un modello che è in grado di effettuare delle predizioni il più possibile corrette giusto per dirvi una stima secondo questo modello è che nel 2026 ci sia un debito pari a 2 per 10 alla 12 ok per 10 alla 12 sono tanti dollari benissimo allora un altro esempio è preso dal mondo della biologia ed è questo qui abbiamo un in questo caso la feature è sempre una ok quindi riusciamo a visualizzarlo comodamente in un piano cartesiano abbiamo questo è un grafico log log cioè logaritmo delle quantità in questo grafico abbiamo sull'asse x il logaritmo della massa del peso di diversi animali nel mondo animale sono pesi medi e sull'asse dell'y quello che è il tasso metabolico quindi diciamo quanta energia serve a quel determinato animale per tenere il suo organismo a riposo funzionante comunque senza svolgere particolari attività stando fermi qual è il il rate metabolico e andando a misurare i biologi insomma i biologi o zoologi chi si occupa di queste cose hanno fatto per vari organismi diverse diverse misure e viene fuori una cosa di questo genere vedete che sulla scala logaritmica c'è una relazione lineare tra tra queste due grandezze per cui animali tipicamente più grossi e più pesanti hanno un tasso metabolico tipicamente più alto ovviamente anche se la relazione lineare su scala log log non lo è poi nella grandezza originale perché la pendenza è circa tre quarti di questa retta quindi in realtà significa che poi alla fine la relazione è una relazione di questo tipo cioè che il tasso metabolico giusto adesso è per interesse non è un qualcosa che riguarda il corso di machine learning però giusto per darvi questa curiosità il metabolic rate chiamiamolo MR alla fine risulta essere siccome la pendenza è 0,75 diciamo proporzionale facciamo circa meglio proporzionale alla massa elevata a 3 su 4 giusto 0,75 e questo significa che in realtà è vero che crescendo il peso cresce chiaramente la quantità di energia che deve spendere quel determinato organismo per stare in per resistere però il fatto che ci sia questa è una crescita un po' meno che lineare cioè è lineare nella scala log log non lo è nella scala originale e questo significa che in realtà proporzionalmente fa meno fatica il tricheco rispetto al colibri stare a riposo e questo spiega anche perché animali più grossi vivono anche di più spiega è correlato al fatto che animali più piccoli vivo di meno perché hanno un tasso metabolico sempre abbastanza elevato quindi con tutto quello che le conseguono va bene giusto per dirvi io qui potrei mettere una massa di un animale incognito e domandarmi qual è il suo tasso metabolico senza andarlo a misurare avrei comunque una possibilità tramite la regressione di rispondere a una domanda di questo tipo più o meno correttamente e di esempi di questo tipo potremmo farne da quante ne vogliamo presi dal mondo dell'industria dal mondo dell'economia eccetera eccetera ok allora qui abbiamo semplicemente un riepilogo invece di quelle che sono le che è il problema di classificazione l'abbiamo visto con i cani e i gatti essenzialmente un problema in cui noi andiamo a costruire uno spazio delle feature e in questo spazio delle feature abbiamo un dataset che è etichettato e quindi abbiamo di cui abbiamo la supervisione quindi rosso sarà cani blu sarà gatti oppure se stiamo cercando di classificare un un'immagine in cui il nostro telefonino cerca di capire se quel riquadro contiene la faccia oppure no di una persona sarà appunto faccia o non faccia e qualunque cosa vi possa venire in mente una volta che abbiamo fatto questo possiamo scegliere un modello in questo caso è un modello lineare definisce un confine decisionale che è una retta e suddivide lo spazio in due semispazio e una volta che ho fatto questo mi posso domandare ok che cosa succede se io ho un punto incognito beh io di questo punto incognito so tutto perché conosco la feature 1 e la feature 2 e me lo collocano in questo semispazio quindi la conclusione sarà è un cane oppure un gatto è azzurro oppure rosso quindi questo chiaramente è quello che prima abbiamo chiamato appunto del test set o del validation set è un qualcosa che lui non ha mai visto durante l'addestramento questo mi raccomando è fondamentale i sistemi machine learning si addestrano e poi si testano su esempi che non ha il sistema mai visto durante l'addestramento e problemi di classificazione ce ne sono tantissimi dalla rilevazione degli oggetti alla rilevazione degli oggetti significa che voi avete un'immagine e all'interno di quell'immagine andate a selezionare il riquadro che contiene un dato oggetto quindi per esempio delle persone e allora questo può essere fatto sia su un'immagine statica ma anche su un serie di immagini per esempio in un video in cui andate frame per frame a identificare all'interno di quel frame c'è una persona e sta qui e questo è importante capite bene in tutti i sistemi per esempio che sono che vanno nella direzione dei sistemi di guida autonoma in cui c'è l'identificazione per esempio del pedone che deve attraversare la strada oppure tutti i sistemi di monitoraggio sistemi di antiintrusione eccetera riconoscimento di oggetti è importante anche sempre tornando ai sistemi di guida non autonoma che ancora diciamo non vogliamo andare fino così avanti perché ancora non ci sono ma già sulle macchine quelle un po' più recenti ci sono i sistemi di guida assistita in cui ci sono i sensori le telecamere che puntano la strada e non so se non è mai capitato di vedere riescono a identificare i segnali stradali per cui vi dicono su questa strada c'è il limite di velocità dei 50 dei 70 oppure dei 90 come fanno? la telecamera riprende il segnale lo inquadra quel segnale viene passato a un sistema di machine learning che è stato addestrato su tanti esempi di segnali che stanno nell'immagine in punti magari anche diversi perché chiaramente l'inquadratura può essere differente e viene risolto un problema di rilevazione dell'oggetto l'oggetto in quel caso è il segnale quindi viene risolto un problema di rilevazione dell'oggetto cioè l'oggetto viene identificato non solo ma poi viene classificato tra tante possibili alternative che sono 50 70 90 110 120 tutti i possibili limiti di velocità ma un problema di classificazione è un problema anche quello quella che viene chiamata sentiment analysis che cos'è è semplice non semplicemente è tutto quell'insieme di tecniche che cercano di inferire a partire da diciamo dei testi quelle che sono delle opinioni quindi un stato di sentimento nei confronti per esempio di un prodotto quindi che ne so prendiamo un esempio di recensioni su un prodotto un disco un film un ristorante eccetera si raccolgono in automatico quelle che sono eventuali recensioni su quel prodotto e queste recensioni vengono prese per esempio dai social media e si cerca di capire in automatico qual è il sentiment cioè l'opinione su quel nuovo prodotto nuovo per esempio perché è uscito sul mercato su quel prodotto e si cerca di fare una previsione a partire appunto da quella che è un'analisi a partire da quello che appunto sono le opinioni che le persone hanno espresso scrivendo dei testi quindi lì i testi vengono trasformati l'input di quei testi di nuovo in valori numerici in vettori numerici quei vettori numerici vengono utilizzati per classificare quel determinato testo come positivo o negativo cioè è una recensione positiva oppure negativa chiaramente su scale che possono essere quelle ecco parliamo di produzioni che vengono lanciate a livello mondiale un nuovo prodotto che ha migliaia migliaia milioni di clienti e quindi migliaia migliaia potenzialmente migliaia di recensioni non vogliamo avere le aziende non vogliono andarle a vedere uno per uno ma vogliono un sistema automatico e dice ok ma questa che cos'è è una recensione positiva oppure no e quindi di nuovo un problema di classificazione le diagnosi mediche sono chiaramente un altro l'ambito biomedico è un dominio applicativo estremamente importante per il machine learning direi anche estremamente utile per noi e ovviamente insomma i sistemi di supporto alla diagnosi sono estremamente importanti quindi la diagnosi medica pensate appunto sulla base di quello che è una serie per esempio di analisi oppure una radiografia una scansione di una risonanza magnetica cercare di fare la diagnosi su una tipologia di malattia in supporto chiaramente all'essere umano al medico ma con un sistema tra virgolette intelligente che ha avuto la possibilità di essere addestrato su magari tanti esempi simili quindi riesce a cogliere anche delle sottigliezze spesso e volentieri che possono essere di aiuto e di supporto abbiamo parlato ieri vabbè ogni giorno i filtri anti-spam nei sistemi di email sono ovviamente all'ordine del giorno vi permettono e lì ci sono dei classificatori che vi dicono spam o non spam quindi un problema di classificazione binaria e poi ci sono molte applicazioni nell'ambito della finanza rilevazione delle frodi quando voi fate una transazione con una carta di credito e comprate qualcosa su un sito di commercio elettronico oppure anche semplicemente non solo di commercio elettronico ma andate a comprare la benzina e pagate con la vostra carta di credito ovviamente quella transazione viene subito fatta passare dai sistemi della vostra banca a un sistema di rilevazione che sulla base di tanti esempi ha costruito una conoscenza in automatico che è in grado di fare una previsione del fatto che la transazione che è stata fatta in quel momento associata alla vostra carta possa essere o meno una transazione fraudulenta oppure e quindi restituisce 1 oppure 0 nel momento in cui c'è un 1 potenzialmente c'è un allarme e magari venite contattati e dici guarda è stata effettuata questa transazione sei sicuro è a nome tuo e sulla tua carta di credito verifica e contattaci eventualmente qui ci sono un po' di problemi legati al fatto che anche questo in qualche modo avremo modo più avanti di riparlarne però alcune di queste applicazioni siccome vi dicevo raccogliere i dati è estremamente importante se pensate a un'applicazione di questo genere ma anche ad applicazioni mediche e qui c'è un problema che la raccolta dati che non è mai banale oltretutto qui è complicata da che cosa dal fatto che tipicamente se pensiamo a un'applicazione come ci sono anche problemi di privacy sicuramente non solo ma se pensate alle applicazioni mediche beh grazie a Dio sono molti più le persone sane di quelle malate di una determinata malattia quindi non è così semplice raccogliere altrettanti dati riguardo ai casi rispetto a quelli dei controlli sani e quindi tipicamente voi finite con un dataset che ha una popolazione molto più ampia di esempi di un certo tipo rispetto a un altro è come se nell'esempio dei cani e gatti ci fossero molti più cani oppure molti più gatti e questo crea dei problemi la stessa cosa avviene nella rilevazione delle frodi le transazioni per fortuna che ci sono su 100 transazioni ce ne sarà una che potentemente una frodi quindi voi riuscite ad accumulare nel corso del tempo un dataset una knowledge base una base di conoscenza che è quello che si dice un dataset sbilanciato i dataset sbilanciati sono un problema perché i classificatori chiaramente imparano che cosa che se nel 99% dei casi ci sono delle persone sane la cosa più semplice che può fare un classificatore è dire ok sei una persona sana comunque sbaglierò l'un per cento ma quello non lo vogliamo sbagliare perché lì il classificatore più semplice che possiamo costruire è anche più corretto sul training set e dire rispondi sempre uno ok su questo torneremo quando parleremo delle metriche per valutare i sistemi quando avremo fatto un po' un po' di strada va bene questo è un esempio semplicemente di riepilogo le cose le abbiamo detto questo è appunto l'esempio di identificazione delle facce una foto con i fratelli Wright pionieri del volo e ovviamente che cosa fanno questi sistemi in automatico per la riepilograzione delle facce creano dei bounding box quindi questi riquadri che vengono fatti scorrere sull'immagine e per ogni posizione del bounding box viene invocato il classificatore c'è una faccia oppure no c'è una faccia oppure no se siamo qui non c'è una faccia se siamo qui non c'è una faccia se siamo qui e qui c'è una faccia quindi ricado nel spazio delle feature dalla parte appunto da una parte del classificatore piuttosto che dall'altra questa è una visione ovviamente iper semplificata che è il problema ma fondamentalmente stiamo parlando di questo e niente questo poi il sistema per esempio sui smartphone serve per la messa a fuoco ad esempio del volto ok questo è un esempio sempre di classificazione su queste sono delle immagini di risonanza magnetica e qui ci sono degli esempi appunto di quelli che sono considerati casi quindi sono persone con problemi neurologici e poi c'erano persone invece non affette da problemi neurologici quindi anche qui lo scopo era costruire un classificatore che permettesse di distinguere in automatico una scansione di una risonanza magnetica di una persona affetta oppure no e fare una previsione una previsione del fatto che fosse o meno affetta da quel determinato disturbo neurologico come vedete è una quantità infinita di possibili applicazioni e questo anche questo vi dà un'idea dell'importanza di questa disciplina ogni giorno perché poi si può applicare a tantissime cose questo è un altro esempio classico con questo concludiamo gli esempi della classificazione ve lo faccio vedere perché è un esempio tra l'altro su cui torneremo che utilizzeremo anche noi ci costruiremo sopra un classificatore per fare per risolvere questo problema quando faremo un'esercitazione di laboratorio ed è il problema di riconoscimento delle cifre scritte a mano abbiamo delle cifre scritte a mano su cui vengono fatte delle delle foto create delle immagini quindi si parte dalle immagini che sono delle immagini di queste cifre scritte a mano e si vuole riconoscere se che cosa rappresenta ad esempio questo che cosa rappresenta questa immagine c'è un 3 ovviamente il nostro occhio allenato e riesce a rispondere subito così non è una cosa banale oggigiorno lo è diventato ma fino a 25 anni fa non era affatto banale costruire un sistema che potesse fare questo e e questo è un problema interessante al di là dell'applicazione che oggigiorno comunque continua in qualche caso esercizioni sono sistemi per esempio quelli che riescono in automatico a permettere il deposito degli assegni bancari scrive a mano oppure anche si usano sempre di meno di giorno ma qualcuno ancora scrive posta tradizionale quindi nel sistema di smistamento automatico della posta quindi se devi riconoscere il codice di avviamento postale in automatico ci sono i sistemi con videocamera che puntano rilevano l'immagine e lo passano a un sistema di questo genere però non è per questo che è interessante lo è diciamo oggigiorno è diventato uno strumento su cui fare esercitazioni quindi diciamo pedagogico didattico ma soprattutto il dataset un primo dataset che è stato costruito primissimi anni 90 di problemi di tipo classificazione supervisionata era un problema di questo tipo ed è un dataset che è diventato abbastanza famoso perché poi su quello è stato reso pubblico e su quello si sono cimentati un sacco di ricercatori di gruppi in giro per il mondo per costruire sistemi in grado di riconoscere in automatico questa cosa un problema di computer vision che è stato risolto un problema di classificazione risolto ma non subito ma nel corso degli anni tant'è che questo dataset viene chiamato il la il moscerino della frutta del del del computer vision il moscerino della frutta è la drosofila melanogaster viene molto utilizzato dai biologi per farci un sacco di studi sulla genetica su tante cose perché per tanti motivi e questo è un dataset che è stato molto utilizzato per lo sviluppo delle prime reti neurali negli anni 90 o meglio le reti neurali esistono dagli anni 50 60 per le prime diciamo della nuova della nuova mandata quella che ci ha portato fino ad oggi diciamo ok a partire degli anni 90 hanno avuto grande successo ok su questo torneremo qui il problema di classificazione è un problema in quante classi l'output non è più binario 10 tra 10 possibili alternative deve scegliere il classificatore ok qui ve l'avevo riassunto ve l'ho anticipato il task è dato un'immagine di cifra iscritta a mano a fare la predizione della classe un problema multiclasse in cui l'input è un'immagine una scala in un'immagine in scala di grigio viene convertita vedremo in pixel a ogni pixel associato un'intensità nella scala di grigio per cui vanno da 0 fino a un valore a 255 tutto bianco o tutto nero e nel mezzo ci sono i vari grigi e l'output è una classe tra 0 e 9 questo dataset che poi appunto ha preso il nome di MNIST dataset NIST è il National Institute credo for Standard Technologies che insomma presso cui era stato depositato credo negli Stati Uniti è costituito da 70.000 immagini e come vi ho detto è noto anche come il moscerino della frutta della ricerca nell'ambito della rete innovativa e questo lo faremo questo esercizio di suddividere il training set 60.000 immagini tipicamente i training set sono più ricchi belli ampi perché vogliamo addestrare bene il nostro sistema e il nostro classificatore per esempio il test set sono le altre 10.000 immagini su cui andiamo a vedere non sono disponibili chiaramente al sistema durante il training e vengono utilizzate per valutare la prestazione del sistema che abbiamo addestrato già 20 anni fa su questo data set le reti neurali ottenevano il 99% di ancora è stato un primo passo verso poi il boom che è avvenuto però intorno al 2015 2016 su altri data set che erano quelli magari avevano modo di parlare proprio di riconoscimento di un'immagine a quel punto data set più ricchi rispetto a questo insomma ne parliamo magari più avanti va bene io direi che per oggi vediamo un po' no ancora abbiamo tempo per cui possiamo andare avanti molto bene un attimo vogliamo fermarci beh adesso direi che facciamo così andiamo un po' avanti magari vi lascio un pochino prima ok qualche volta magari faremo una pausa in mezzo 5-10 minuti qualche volta invece cerchiamo di finire magari insomma qualche minuto prima in funzione anche di quello che facciamo di quanto vi vedo stanchi domande su questo no tutte cose abbastanza semplici molto introduttive però aiutano a costruire un pochino il quadro a delimitare insomma un po' il perimetro in cui ci muoveremo ecco sì qui c'è questo dataset ecco qui interessante era quello che non mi ricordavo di averlo messo qui ve ne avrei parlato più avanti ma allora ve lo dico adesso era quel dataset di cui dicevo prima intorno a metà degli anni insomma da 2015 mi pare è stato introdotto questo dataset si chiama ImageNet è stato sviluppato dall'università di Stanford che era un dataset con tantissimi esempi di foto che servivano appunto poi per essere catalogati in maniera automatica cioè il sistema doveva riconoscere in questa foto c'è una tazza in questo altro c'è un tavolino in questo altro c'è una sedia c'è un una bottiglia un bicchiere quindi le etichette lì erano migliaia migliaia di possibili categorie quindi è un problema di classificazione ma con migliaia di possibili alternative chiaramente più difficile che dire 0-1 e con diciamo milioni di etichette scusate di immagini etichettate quindi un dataset di notevole valore perché è stato curato e costruito tra l'altro immagini prese in condizioni variabili dal punto di vista dell'inquadratura dal punto di vista dell'illuminazione eccetera e su quello anche quello è stato un passo fondamentale perché su quel dataset poi si sono di nuovo cimentati tanti gruppi di ricerca sia in ambito accademico dell'università ma poi anche gruppi di ricerca quelle che sono le aziende aziende più grosse quindi da Google Facebook hanno sviluppato chiaramente hanno cominciato a costruire divisioni interi di ricerca questo volentieri anche sottraendo alle università persone estremamente in gamba pagandole ovviamente profumatamente questo è in realtà proprio un problema che c'è stato una tendenza che c'è cioè è stato uno svuotamento di un sacco di persone che lavoravano facciano ricerca all'interno delle università in giro per il mondo in particolare in Nord America ma in giro per il mondo da parte appunto di aziende che hanno cominciato nel momento in cui hanno cominciato a prendere piede certe tecnologie hanno visto chiaramente che era necessario costruire delle divisioni di ricerca all'interno delle loro aziende che potessero sviluppare determinati determinati prodotti e vabbè tornando a questo dataset anche qui è stato un punto fondamentale perché ha permesso a molti di avere a disposizione un dataset per provare i propri sistemi quindi per costruire dei modelli delle reti neurali oppure altri modelli non di reti neurali che provassero a migliorare le prestazioni e in pochi anni il tasso d'errore è passato dal 25,7% al 5,7% grazie anche alla disponibilità di questo dataset perché la gente ha potuto cominciare a sviluppare sistemi e testarli ad addestrarli e poi a testarli quindi di nuovo l'importanza di disporre di dati è un concetto fondamentale l'apprendimento automatico è apprendimento come vi dicevo ieri data driven e se voi disponete di dati potete fare qualcosa se non li avete è un grosso problema non solo se li avete devono essere anche buoni perché un'altra regola non scritta nell'ambito del machine learning e anche qui prima la dico in inglese perché di solito si dice in inglese si potrebbe dire direttamente in italiano è garbage in garbage out cosa vuol dire che se noi abbiamo dei dataset che sono dataset poco curati sporchi cosa vuol dire contenenti errori e quindi addestriamo un sistema su un dataset che non è affidabile quello che otteniamo è un sistema non affidabile che è un concetto molto banale ma tanto banale quanto vero e quanto importante da tenere presente e qui era un dataset curato contenente un sacco è costato fatica lavoro è costato soldi e però ha permesso poi di non solo il dataset perché poi c'è tanto lavoro anche dopo però è stato il presupposto per passare per abbattere dal 25% al 5% il tasso del ruolo qui c'è stato il balzo delle reti neurali come modello vincente nell'ambito della computer di vita questo è giusto dal punto di vista tra virgolette storico che parliamo di 10 anni fa non di più altro problema è quello della generazione di caption l'immagine cioè che cosa c'è qui dentro c'è l'immagine che dicevo anziché scegliere genera automaticamente una caption un testo che dice si tratta di un cestino dell'immondizia si tratta di un banco dell'università e poi si potrebbe andare avanti ulteriormente però anziché andare avanti con questi esempi adesso parliamo dell'altra grande sottocategoria del machine learning che può essere non è suddiviso solo in supervisionato e non supervisionato ma quello è diciamo con questi due già copriamo un'ampia fetta di tutte le possibili applicazioni però qui abbiamo dopo diremo qualcosa anche su altre tipologie di apprendimento che però ve lo dico dopo prima intanto ragioniamo su introduciamo ecco diciamo meglio quello che è appunto la tipologia di problemi di apprendimento non supervisionato quello con supervisione è abbastanza intuitivo abbiamo l'input e abbiamo l'output e quello viene dato al sistema per apprendere sia l'input che l'output nell'apprendimento non supervisionato il sistema ha a disposizione solamente l'input su cui fare apprendimento e questo chiaramente capite bene già da qui la definizione è abbastanza sfumata e è un problema se vogliamo più difficile non banale cosa vuol dire come fa un sistema ad apprendere l'output però qui diciamo ci viene in aiuto adesso poi torno un attimo su no partiamo da questo esempio diciamo questi esempi di nuovo molto semplici ci vengono in aiuto cioè il punto chiave è questo che vi ho riportato qua l'apprendimento scusate l'apprendimento automatico di nuovo di regole computazionali che ci aiutano a descrivere solamente l'input cioè noi vogliamo sulla base dell'input imparare che caratteristiche ha noi abbiamo a disposizione dell'input il sistema viene addestrato per cercare di capire che caratteristiche ha e impara a spiegare qual è la struttura di quegli dati qual è la struttura di quei dati e cosa vuol dire beh facciamo un esempio noi ne vedremo due di tipologia che sono anche qui coprono tantissime gran parte la riduzione della dimensionalità e il clustering qui c'è un esempio di riduzione della dimensionalità cosa intendiamo per riduzione della dimensionalità ma i dati abbiamo detto sono rappresentabili mediante dei vettori numerici quindi noi li possiamo vedere come insieme di una questo vedrete sarà vero sempre con tecniche diverse di preprocessing ma questo è vero la riduzione della dimensionalità e questi questi dati vivono in uno spazio ambiente nativo che è loro cosa intendo con questo ritorniamo all'esempio del cane del gatto lo spazio delle feature era bidimensionale quindi ognuno di quei punti era rappresentato tramite due punti due valori numerici scusate ogni punto era rappresentato tramite una coppia di valori ok se io ho un'immagine bianco e nero quella è un'immagine supponiamo di 100% pixel ok quel 100% pixel sono 10.000 valori che posso organizzare in un vettore numerico ok ognuno di quei valori a quel punto mi può rappresentare una feature in realtà quelle non sono feature estratte ma è proprio il dato grezzo così come l'abbiamo però ognuno di quei valori mi rappresenta il valore di un pixel dell'intensità numerica di un pixel da 0 bianca a 255 o meglio sarebbe il contrario 0 nera e 255 bianca direi e tutto in mezzo ci sono le scale di un'immagine una semplice immagine 100% pixel quindi a quel punto è rappresentabile come un vettore di 10.000 punti ok capite bene che se ho anche un'immagine a media risoluzione con un milione di pixel un megapixel che è un'immagine che è un'immagine che è un'immagine matrimonio di giorno neanche già i telefonini di ultima generazione le fanno molto più con una più alta risoluzione se le voglio rappresentare come dato grezzo sono degli input di quanto un milione di valori quindi vettori con un milione di entri giusto? ok per vari motivi può essere comodo anziché lavorare con vettori di un milione di entri in questo caso ridurre la dimensione dell'inni perché ci fa comodo per motivi di visualizzazione ci può far comodo per motivi di efficienza computazionale un po' ne parleremo più avanti quando cominceremo a studiare i problemi sempre superviventi quindi di fatto io posso avere interesse a fare cosa? a dire ho dei punti in questo caso sono dei punti che vivono uno spazio a due dimensioni e li vado ad esempio a proiettare su questa retta che è uno spazio a una dimensione li vado a proiettare e queste sono queste qui le rappresentazioni di questo punto e questa di questo punto e quest'altra di questo punto e questa sullo spazio a una dimensione quindi a questo punto la mia nuvola di punti il mio insieme di punti non vive più in uno spazio di due dimensioni ma una allo stesso modo qui ho dei punti che stanno in uno spazio tridimensionale e vengono proiettati su questo piano azzurro è chiaro che nel momento in cui io effetto questa proiezione questa riduzione della dimensionalità mi gioco qualcosa in termini di perdita di informazione però il punto è proprio costruire un sistema che da solo sulla base solo dei punti sia in grado di effettuare una cosa di questo tipo e lui come esempio ha solo i dati i dati stessi e cerca di costruire una rappresentazione dei dati a più bassa dimensionalità questo ci arriveremo dopo che avremo fatto problemi di classificazione di regressione lo faremo più avanti però ci torneremo su un'altra tipologia classica di problemi sono i problemi quindi quella che viene chiamata cluster analysis quindi costruire il clustering qui si capisce un po' meglio che cosa significa e questo è un apprendimento senza supervisione se io vi faccio vedere solamente questo questo esempio quindi non guardate a destra ma guardate solamente a sinistra e vi dico quanti raggruppamenti di punti potete riuscire a vedere l'occhio umano che cosa fa? subito dice attento ci sono due gruppi perché siamo immediatamente il nostro cervello abituato a identificare dei pattern non sempre così banali in qualche caso ci sarebbero dei raggruppamenti su cui avremo delle opinioni diverse però perché il nostro cervello ragione di venire diversa delle volte l'uno dall'altro però in questo caso direi che siamo tutti d'accordo che a partire da questo esempio avendo nulla di più che i punti a disposizione il nostro cervello è in grado di fare che cosa? di costruirsi una rappresentazione per cui attenzione dici io identifico un primo raggruppamento che è questo dei punti colorati in blu e un altro dei punti colorati in blu che cosa fa il nostro cervello? sta facendo una cluster analysis identifica dei raggruppamenti come fa? è difficile dirlo dal punto di vista del ci vorrebbe appunto un neuroscienziato però quello che fa è identifica presumibilmente e i sistemi poi che fanno questo in automatico che sono quelli che noi vogliamo costruire riescono a identificare delle caratteristiche comuni tra i punti di un raggruppamento per esempio qui cosa direste? questi punti cosa hanno in comune? sono tra loro ravvicinati quindi hanno delle caratteristiche delle feature che li rendono simili questi altri punti tra di loro hanno delle caratteristiche delle feature che li rende simili che li rendono perché il plurale simili e allo stesso modo se passiamo all'esempio qui sotto qui vedete che quello che possiamo dire è che qui identifichiamo un cluster un raggruppamento perché sono punti abbastanza densi abbastanza ravvicinati e questi altri presumibilmente sono un altro raggruppamento perché hanno delle caratteristiche tali e qui non sono tanto diciamo questo punto non è tanto ravvicinato a questo quindi è un problema meno semplice per l'univinente da risolvere però anche qui il nostro cervello è abbastanza abituato a identificare questo come qualcosa in cui c'è un pattern corrente è come se questi punti stessero su un'ideale curva costruire un sistema che faccia questo in automatico è uno degli obiettivi dell'apprendimento senza supervisione qui non c'è nessuna etichetta non c'è nessuno che va a dire guarda che questi sono esempi che devono stare tutti su questo in questo aggruppamento è il sistema che in automatico riesce a partire dai dati di input a costruire un una sola a identificare una struttura all'interno del dataset e questo si fa per diversi motivi perché poi ne parleremo con il mio perché va bene ok e adesso arriviamo all'introduzione delle prossime lezioni con questo poi oggi vi lascerò l'ultima cosa che voglio cominciare a dirvi è l'introduzione al il problema dell'ottimizzazione perché fino adesso abbiamo detto per scontato tante cose tra cui ok facciamo l'ipotesi di costruire un modello lineare ritorniamo all'esempietto del classificatore cani e gatti abbiamo detto che anche lì ci sono infinite rette che rappresentano un potenziale modello come faccio a scegliere quella di più beh quello che fa oppure partiamo qui c'è l'esempio della regressione ma la stessa cosa poi c'è anche quello della classificazione torniamo all'esempio della regressione ecco che era nelle slide allora supponiamo di avere il problema della regressione quello per il calcolo del prezzo delle azioni no e come faccio a dire che questa retta è migliore quella a sinistra scusatemi e questa è migliore di questa qualcuno di voi correttamente diceva perché questa retta in qualche modo i punti sono più vicini ed è questo il concetto questo concetto verrà espresso matematicamente deve essere espresso matematicamente e impareremo a esprimerlo mediante quella che vengono chiamate funzioni di costo o funzioni di perdita in inglese cost function o più spesso si parla di loss function ecco cosa sono le funzioni di costo o funzioni di perdita sono delle funzioni molto importanti perché sono delle funzioni che vi permettono una volta che avete un esempio di dire quanto il vostro modello è in grado di dare una risposta corretta rispetto a quell'esempio quindi le funzioni di costo vi dicono per ogni esempio quanto siete più o meno vicini o più o meno lontani dalla risposta corretta questo è importante perché quantificare questo in un numero vi permette a questo punto di costruire un qualcosa che è quello che sta qui di sopra nella parte superiore di questi grafici perché esprimere una funzione di costo significa dire che cosa che qui abbiamo detto il nostro modello ha due semplici parametri che sono l'intercetta e la pendenza a quel punto io ho infinite alternative tra tutte le possibili coppie di parametri giusto? se però per ogni copia di parametri mi corrisponde a una retta ok? quindi qui abbiamo una copia di parametri e qui ne abbiamo un'altra allora rispetto a questa copia di parametri io posso costruire quella che è una funzione di costo funzione di costo che mi dice ma guarda su tutti i punti del tuo dataset tu mediamente fai questo errore vedremo come però intanto cominciamo a provare a intuirlo e allora io una volta che dispongo di questa funzione di costo questa funzione di costo è una funzione che dipende dai parametri del mio sistema giusto? cioè io posso dire per ogni possibile coppia intercetta e pendenta dove mi colloco rispetto alla funzione di costo facciamo questo esempio qui ho un'intercetta che vale questo una pendenza che vale quest'altro saranno dei valori numerici e qui avrò questo vedete questa superficie rappresenta un'ipotetica funzione di costo cioè è un qualcosa che vi dice guarda che se sei qui ti collochi in questo punto della funzione di costo se ti sposti in questa altra coppia intercetta quest'altra coppia di parametri che intercetta tendenza vai a finire qua su un altro punto della funzione di costo e attenzione perché qui sei su un valore della funzione di costo che può essere non so butto là un numero 3 3.2 qui sei su un altro punto che vale 10.5 magari è chiaro che questa è una funzione di costo più basso è un valore della funzione di costo più basso e questo più alto perché perché qui io rispetto la mia funzione di costo che rappresenta facciamo un esempio giusto per capirci meglio io potrei andare a prendere la distanza di ognuno di questi punti da questa retta ok vado a fare la media mi sommo divido per il numero di punti e ottengo un valore numero in realtà non prenderemo la distanza per vari motivi prenderemo al limite la distanza in valore assoluto al quadrato questo lo ve lo spiego più avanti però sommandoli ho una misura di quanto stiamo vicini a questa retta o no che era quello che dice questa sta più lontana cioè quello stesso valore sarà più ed è questa qui un qualcosa che ha un'intercetta diversa una pendenza questa in particolare ha un'intercetta vedete più alta e ha una pendenza che è un po' più bassa questa ha una pendenza un po' più alta e un'intercetta invece vicino allo giro e rispetto a questa funzione di costo che non è un caso che abbia questa forma scodella perché in realtà se vado a prendere la somma proprio dei valori della distanza di quei punti al quadrato andremo a prendere otteniamo proprio una funzione di costo ci dice tanto perché ci dice a quel punto che a questo modello corrisponde una certa prestazione una certa performance e a quest'altro moderno corrisponde una prestazione peggiore ma allora a questo punto io dispongo di uno strumento fondamentale e formidabile perché perché ho fatto un'ipotesi ho raccolto dei dati ho fatto un'ipotesi su un modello un costituzione ad esempio un modello lineare ho definito i parametri che sono in questo modello ma ho costruito anche una funzione di costo che mi dice quanto quel modello risponde alle mie esigenze a questo punto mi rimane di fare una cosa come scelgo tra tutte le infinite alternative tra tutte le possibili coppie di parametri quello giusto vado a prendere vado a cercare che cosa il minimo di quella funzione di costo cioè la combinazione di parametri che mi minimizza la funzione di costo cioè mi muovo lungo questa superficie al variare delle combinazioni dei parametri e qui entrano in gioco gli algoritmi di minimizzazione in caso di problemi di minimizzazione o di ottimizzazione quindi quello che è appunto l'ottimizzazione matematica che è la cornice nella quale ci muoviamo partiamo dalle definizioni matematiche ma poi il nostro obiettivo è farlo in automatico su un calcolatore quindi ci preoccuperemo di definire degli algoritmi che fanno questo ma gli algoritmi di ottimizzazione sono il il il il il motore del sistema di machine learning e il motivo sta qui cioè noi tra tutte le possibili alternative vogliamo scegliere quella migliore e lo vogliamo fare mediante un algoritmo a partire da funzione di costo e dato ecco perché spenderemo un po' di tempo nelle prossime lezioni dedicheremo alcune lezioni proprio a cercare di capire meglio che cosa vuol dire ottimizzare una funzione e come farlo da un punto di vista algoritmico ok ci siamo? dimmi pure perché se tu assumi che questa retta dice è migliore di quest'altra è perché in qualche modo sei riuscito a quantificarla questa cosa e poniamo di quantificarlo dicendo ipoteticamente se fossero tutti allineati qui sopra esatto esatto esatto e questi avrebbero zero se fossero tutti allineati no? in realtà non sono tutti allineati però sono abbastanza vicini posso andare a prendere le varie distanze al quadrato far la media e quella è la mia funzione di costo è una possibile poi vedremo che al variare delle funzioni di costo cambiano anche tante cose però ne avremo modo di parlare diciamo perfetto questo per quanto riguarda la regressione stessa cosa per la classificazione se vogliessimo tornare all'esempio del cani e gatti ma anche lì il punto è sempre lo stesso cioè io ho una coppia di feature in questo caso e un modello che mi taglia in due questo spazio delle feature questo spazio geometrico e questo è un altro modello vedete anche questi modelli a due parametri vi faccio notare che riusciamo a rappresentarli poi in questi grafici perché due parametri una funzione la funzione di costo dipende da due parametri quindi va nella terza dimensione se fossero tre parametri già non ci riusciamo quindi gli esempi che faremo per visualizzarli ovviamente partiremo da sistemi a dimensione molto bassa due o tre parametri è ovvio che sistemi di machine learning moderni non hanno due parametri non ne hanno tre tranne nei casi molto semplici appunto di rette di regressione molto semplici ma anche sulla regressione già si va su un numero di feature che nei casi migliori 10 15 20 non riusciamo neanche a immaginarli ma non cambia nulla da un punto di vista concettuale è sempre una minimizzazione solo che qui andiamo a minimizzare vedete questa è una retta che corrisponde a questo punto questa combinazione pendenza intercetta questa è l'altra combinazione aumentiamo la pendenza qui e diminuiamo l'intercetta ok o meglio no non credo che si venga a diminuzione rispetto a questa è un po' più alta però disegno probabilmente in scala però il concetto è questo qui mi vado a posizionare in questo punto della funzione di costo qui in quest'altro punto della funzione di costo questo modello concludo che è migliore di quest'altro e lo concludo perché la mia funzione di costo qui è più bassa e allora a quel punto io posso andare a analizzare tutto il profilo di questa funzione di costo in cerca della configurazione di parametri migliori vi dicevo prima del numero di parametri i sistemi moderni di machine learning hanno migliaia di parametri quando non milioni di parametri le reti neurali più recenti più moderni sono reti che hanno milioni miliardi addirittura di parametri quindi si tratta di ottimizzare delle funzioni di costo in uno spazio enorme ed è per quello che l'addestramento di questi sistemi è un addestramento che è tutt'altro che banale cioè perché vengono addestrati su tanti dati tutt'altro che è banale anche dal punto di vista computazionale stiamo parlando di tanti dati e tanti parametri hanno bisogno di tanti dati perché hanno tanti parametri che hanno bisogno di tanto tempo di addestramento perché lavorano su tanti dati e devono trovare la configurazione migliore per quel numero di parametri però andiamo per ordine il punto sarà costruire quella conoscenza di costo che nel caso della classificazione vedremo insomma faremo un po' di ragionamenti però il punto di partenza sarà dire ok in questo caso nel mio training set questo sistema quante ne classifica correttamente quante ne sbaglia in quest'altro caso quante ne classifica correttamente sbaglia vedremo che in realtà questo ci dà un numero è una funzione di costo però è una funzione di costo che ha dei problemi proprio perché gli algoritmi di ottimizzazione ottimizzare questo tipo di funzione di costo lavorano appunto su certe ipotesi più facilmente che su altre e modificheremo a quel punto la funzione di costo però di questo ne parliamo pian piano quando ci arriviamo però il punto chiave da cui usciamo oggi va bene detto questo direi che possiamo concludere qui la lezione e interrompere quindi anche la registrazione se a-