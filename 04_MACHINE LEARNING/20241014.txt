Allora, intanto buongiorno e bentornati. Partiamo questa lezione con una breve riepilogo di algebra lineare, richiamiamo qualche nozione riguardante gli autovalori e gli autovettori, qualche definizione proprio di base, giusto quello che ci serve perché poi ritorneremo alle tecniche di ottimizzazione del secondo ordine, siccome tra le condizioni di ottimizzazione del secondo ordine vedremo che per le funzioni di più variabili sarà necessario chiamare in causa il concetto di autovalore e di autovettore, a questo punto è il caso di riandare a prendere qualche definizione appunto in quella che è l'ambito dell'algebra lineare, ma giusto proprio per permettere di riallinearci con le cose che stavamo dicendo e rinfrescare alcuni concetti. Allora, data una matrice quadrata, possiamo chiamare A, matrice che quindi è definita su un insieme di numeri reali, quindi in R, N per N, una matrice N riga nelle colonne, lambda, che è un numero che appartiene in generale, può essere un numero complesso, quindi nel campo dei complessi, in particolare quindi come sotto insieme anche quello dei numeri reali ovviamente, lambda è bene appunto, quindi qualunque numero complesso in generale o reale, lambda, viene detto autovalore, di A e X che è un vettore anche qui che può avere e A, N elementi e questi possono appartenere a si stessi numeri complessi, diciamo noi per semplicità ragioneremo sui numeri reali e diciamo le applicazioni che ci riguardano sono reali, però nella maggior parte dei casi, però in generale si parla di autovalori appunto che appartengono e auto vettori al campo dei numeri complessi e quindi X viene detto il corrispondente auto vettore se vale la seguente equazione, che è un'equazione, intanto scrivo poi analizziamo insieme, un'equazione matriciale, vettoriale, diciamo che coinvolge matrici e vettori, l'incognita in realtà è il vettore X ed è un'equazione che ci dice che cosa? Che le soluzioni appunto dell'equazione, data la matrice A quadrata moltiplicata per X, la soluzione di questa equazione AX uguale lambda X è la soluzione che ci fornisce appunto gli autovalori X e gli autovettori lambda che soddisfano questa equazione. Allora questo è un prodotto vettore per matrice, quindi, scusate, matrice per vettore, quindi è un n per n per n per n per 1 che fa n per 1. Questo invece è il prodotto di uno scalare per un vettore, quindi ogni elemento del vettore viene moltiplicato per quello scalare. Quindi il primo produttore eseguisce un vettore. Un vettore e il secondo lo stesso. Quindi, esatto, ci deve essere chiaramente una coerenza dimensionale. Allora, la soluzione di questa equazione è la soluzione che ci permette di fornire appunto l'insieme degli autovalori e degli autovettori corrispondenti. Quindi chiaramente abbiamo che lambda è l'autovalore e X è l'autovettore corrispondente. Allora, questa qui la possiamo riscrivere in formato che è questo, introducendo la matrice identità, ma rimane sempre la stessa. La matrice identità è quella matrice n per n quadrata che ha tutti uno sulla diagonale principale e 0 come nelle altre entri. Se moltiplichiamo lambda per i e ci sottraiamo a moltiplicato per X, mettiamo uguale a 0, abbiamo appunto che è un formato equivalente della notazione che c'è sopra. Allora, una volta che abbiamo definito l'autovalore e il concetto di autovalore di autovettore, quello che risulta vero è che... Scusate... Allora, volevo semplicemente fare così. Allora, una volta che abbiamo evidenziato questa definizione, quelle che possiamo dire si possono ricavare sul concetto di autovalore e autovettore e su questa equazione potremmo stare, vengono costruiti appunto i corsi, interi corsi di algebra lineare, quindi ovviamente non è questo il nostro scopo. Come vi dicevo, vi riepilogo brevemente alcune proprietà interessanti, però ecco, degli autovalori e degli autovettori. In particolare, chiaramente, diciamo, sono alcune proprietà che hanno a che fare con... Magari li mettiamo in una nuova lavagna... che hanno a che fare con le matrici, con i vettori, con le loro strutture. In particolare, una volta che abbiamo gli autovalori e gli autovettori associati a una matrice A, quello che si può verificare è che valgono, seguenti ai risultati matematici che vi riporto, cioè... La soluzione di quell'equazione vettoriale possiamo avere per una matrice N, N per N, N autovalori ed N autovettori, e in particolare, quello che si può verificare, e questo è un teorema matematico, che la somma degli N autovalori è pari alla traccia della matrice A, dove la traccia della matrice A è la somma degli elementi che stanno lungo la diagonale. Ma un'altra proprietà interessante è che quella matrice ammette un determinante che può essere calcolato come il prodotto degli N autovalori. Il prodotto per i che va da 1N di λI dal determinante della matrice. Infine, un'ultima proprietà interessante, quindi un'ulteriore teorema matematico che è possibile dimostrare è che il rango della matrice che corrisponde al numero di righe oppure di colonne linearmente indipendenti è pari al numero di autovalori non nulli di A. Quindi io vado a contare il numero di autovalori non nulli di A e ottengo il rango della matrice. È un prodotto di tutti gli autovalori. Ho N autovalori, vado a fare il prodotto e vado a determinante. Vi faccio notare che qui ho scritto nella slide precedente, nella lavagna precedente, x diverso da zero, cioè scartiamo la soluzione banale in cui l'autovettore è nullo, a tutte le entri è nulla, perché quella ovviamente è una soluzione banale di quella equazione. Allora, altra proprietà per tutte le matrici simmetriche reali, quindi A matrice simmetrica e reale e a valori reali, i lambda i appartengono tutti a R, cioè sono tutti numeri reali. Questo sarà il caso nostro, diciamo prevalentemente, a cui ci riferiremo, e quindi A simmetrica e reale troveremo spesso e volentieri matrici di questo tipo e lambda i, quindi potremo concludere che è reale, sono tutti reali gli autovettori, scusatemi gli autovalori. Poi ci sono altre proprietà che, diciamo, potrebbe valer la pena introdurre, un'ultima dei quali vi dico, poi magari altre che ci serviranno, se ci serviranno più avanti le richiameremo. Un'ultima che vi riporto è che se A di nuovo è simmetrica, gli autovettori di matrici simmetriche sono ortogonali l'uno all'altra, quindi gli autovettori, quindi i valori X, chiamiamo l'X con I, sono gli autovettori della matrice di una matrice reale simmetrica e una matrice simmetrica sono ortogonali tra di loro. Cosa vuol dire? vuol dire sono dei vettori, per cui io prendo il primo autovettore, ad esempio il quinto autovettore, se faccio il prodotto scalare tra questi autovettori, ottengo un numero scalare che è pari a zero. Sono tutti i teoremi matematici che è possibile dimostrare. nel discorso del coseno. Esattamente, certo, è legato al coseno dell'angolo tra i vettori. Questo è quello che si può dimostrare che se la matrice è una matrice simmetrica, gli autovettori sono ortogonali tra di loro. Queste sono proprietà che ci torneranno utili, alcuni adesso, insomma il concetto di autovalore e autovettore ci tornerà utile anche più avanti quando parleremo di apprendimento non supervisionato, perché vedremo alcune cose che hanno a che fare di nuovo con questi concetti. Richiamato questo, questo è tutto quello che ci serve per il momento, giusto per rinfrescare appunto le idee, la memoria su quello che è il concetto di autovalore e autovettore, quindi data una matrice noi sappiamo che esistono degli oggetti chiamati autovettori e degli oggetti chiamati autovalori che soddisfano l'equazione che abbiamo prima definito e per il momento è tutto quello che ci serve. Torniamo invece ora a quella che è la nostra lezione, termini di slide che avevamo incominciato a vedere l'ultima lezione che erano le slide relative alle condizioni di ottimizzazione del secondo ordine, perché di quello stavamo parlando. Allora, avevamo visto che richiamato anche qui alla memoria alcuni concetti base di matematica per cui i punti stazionari sono collegati a derivate seconde, oltre che alle derivate prime. In particolare eravamo arrivati a vedere questo, cioè che se abbiamo un punto di minimo locale o globale, sicuramente a maggior ragione globale se lo è anche locale, se questo minimo ha la derivata seconda, quindi se la funzione è differenziabile almeno due volte, se la derivata seconda è positiva nel punto stazionario, allora quello è un punto di minimo perché è un punto di stazionalità che si verifica in porzioni complesse della funzione. Viceversa, se un massimo locale ha una derivata seconda minore di zero, se un punto stazionario v ha la derivata seconda minore di zero, allora è un minimo locale. Perché? Perché la sua occorrenza avviene in punti della funzione concava. Viceversa, se la derivata seconda è uguale a zero e cambia di segno, allora si tratta di un punto di sella. Queste sono delle condizioni appunto che possono essere formalizzate, appunto sono anche questi dei teoremi matematici che derivano appunto dall'analisi matematica e che possono essere anche estesi al caso multidimensionale, cioè di una funzione di più variabili che va da Rn in R che come vi ho già detto più volte è di nostro interesse perché nel machine learning i modelli sono modelli multidimensionali, quindi sono spesso e volentieri appunto funzioni che prendono valori in input a molte dimensioni restituiscono degli scalari. In qualche caso abbiamo anche addirittura più valori in output, quindi funzioni reali, di più variabili reali che vanno da Rn in Rm, ma in generale quanto meno Rn in R. Allora, un punto stazionario, quindi la matematica ci dice di una funzione di più variabili, G di W, è un minimo locale o un minimo globale se tutti gli autovalori di questa matrice che abbiamo richiamato l'altra volta che è la matrice siana, che è quella che contiene tutte le derivate seconde parziali, sono positive. Cioè, e qui ecco il motivo per cui abbiamo richiamato il concetto di autovalore, perché è importante, perché se io vado a prendere la matrice siana di una funzione che va da Rn in Rn, la funzione, abbiamo detto, F, o meglio qui la chiamiamo G, quindi è corretto chiamarla G. Allora, se questa funzione G va da Rn in R, chiaramente la matrice siana, l'abbiamo detto l'altra volta, sarà una matrice che appartiene a Rn per n, cioè sarà una matrice n righe ed n colonne, giusto? Questa matrice n righe ed n colonne ammette n autovalori ed n autovettori, che sono soluzioni dell'equazione agli autovalori e agli autovettori che abbiamo visto prima, questi autovalori possono essere studiati e se sono positivi, allora possiamo concludere certe cose, se sono negativi, altre cose, in particolare l'occorrenza di autovalori tutti positivi della matrice siana è collegata a un minimo, a un punto di... ovviamente stiamo parlando di un punto stazionario, quindi abbiamo verificato prima che la condizione del primo ordine è verificata, cioè la derivata prima si deve annullare, quindi il gradiente deve essere nullo in caso di funzioni più variabili, se il gradiente è nullo e l'essiano ammette autovalori positivi, allora quello che succede è che possiamo concludere che quello è un minimo locale oppure un minimo globale se siamo in condizioni di minimo globale. Viceversa, se gli autovalori della matrice siana sono positivi, quello che possiamo concludere è che il punto stazionario V, scusatemi, è un punto di massimo locale o eventualmente globale. Perché? Perché di nuovo abbiamo che l'occorrenza è in punti, in porzioni, diciamo, o convesse o concave della funzione. Infine, se gli autovalori sono mescolati, quindi sono alcuni positivi e alcuni negativi, allora siamo in un punto di flesso che generano un punto di sella della funzione. Quindi abbiamo concavità e convessità che si alternano. Quindi questa è l'estensione del concetto di funzione di una variabile, di condizione del secondo ordine dalla funzione di una variabile a estensione a funzione di più variabile. Di fatto, quello che ci dicono questi, che sono poi dei teoremi veri e propri, nel senso che io potrei formularlo appunto in termini di teorema è dire che ho una condizione necessaria e sufficiente, quindi se fx è differenziabile e x è un punto di stazionarietà, allora quello che ne consegue è che sicuramente deve valere che il gradiente deve essere nullo e sicuramente la matrice essiana che indichiamo con nabla quadro di x deve essere, come si dice, positiva semidefinita. Positiva semidefinita è semplicemente un sinonimo di tutti gli autovalori sono positivi. Ma vale anche diciamo una condizione inversa, cioè se la funzione è una funzione differenziabile e se valgono le due condizioni, cioè matrice il gradiente è zero e la matrice essiana ha autovalori solamente positivi quindi positiva semidefinita, allora è vero anche l'inverso, cioè possiamo concludere che x è un punto di minimo. Quindi x è un punto di minimo. Allora, questo è il teorema che l'analisi matematica ci dice che vale e da questo teorema derivano queste condizioni di ottimaleità che sono riportate in questa slide. Quindi, in sintesi, noi abbiamo un punto, abbiamo un punto di stazionalità e in base, se riusciamo a studiare la derivata seconda o più in generale quella che viene chiamata la curvatura della funzione possiamo concludere se si tratta di un minimo o di un massimo locale. Allora, una volta che abbiamo richiamato tutti questi concetti alla mente, quello che possiamo fare è cominciare ad andare nella direzione di domandarci ma possiamo per caso a questo punto sfruttare il concetto di curvatura, quindi la nozione di derivata seconda, per inventarci un algoritmo che sfrutti appunto le condizioni di secondo ordine esattamente come abbiamo costruito l'algoritmo di discesa del gradiente che sfruttava quello del primo ordine e in caso questo sia possibile quali proprietà ha quest'algoritmo eventuale? Allora, la risposta alla prima domanda è affermativa perché è possibile e cercheremo anche di capire quali sono le caratteristiche di questo algoritmo e per diciamo andare nella direzione di capire come funziona quest'algoritmo quindi di fatto di progettarlo è utile anche qui diciamo fare un parallelo con quello che abbiamo visto per l'algoritmo di discesa del gradiente lì se vi ricordate per costruirlo abbiamo sfruttato l'approssimazione di una funzione tramite le serie di Taylor vi ricordate? Serie di Taylor del primo ordine avevamo un termine che era quello della derivata prima però vi ho detto guardate che la serie di Taylor se vi ricordate contiene infiniti termini e quindi l'approssimazione la potete rendere via via più fine più precisa man mano che includete più termini per cui ci sarà un termine di secondo ordine che è collegato alla derivata seconda uno di terzo ordine è collegato alla derivata terza e così via allora qui quello che si fa è si ripercorre quel quel tracciato in analogia andando a estendere il concetto includendo i termini di secondo ordine nell'espansione di Taylor quindi si approssima la funzione con una funzione quadratica che include un termine del secondo ordine e poi con quello vedrete che faremo qualcosa di diverso o meglio qualcosa di analogo alla discesa del gradiente un po' di diverso perché in realtà su quell'approssimazione vedrete ci baseremo per costruire l'algoritmo iterativo allora torniamo un attimo quindi a una rappresentazione che può essere una rappresentazione di una funzione di una singola variabile quindi una funzione quadratica noi andremo a studiare che avrà una forma funzionale di questo tipo quindi termine noto un termine lineare e un termine quadratico dove a b e c sono degli scalari arbitrari questa è la forma più generica di una funzione quadratica in una dimensione ritrovate l'equazione della parabola no anche con questa e è interessante notare come se voi andate a fare uno studio no cosa avete fatto nel studio di funzioni se volete studiare le caratteristiche di questa funzione potete studiare la derivata prima e la derivata prima di questa funzione che cos'è? allora la derivata rispetto al termine noto si annulla rimane b più due volte c per v e quindi la derivata seconda di g rispetto a v è chiaramente è semplicemente due volte c ok e due volte c quindi da questo vediamo che cosa è quello che c'è scritto qui che c controlla la cavità o la convessità della funzione perché a seconda che c sia positivo o negativo la funzione risulta concava o convessa per la derivata seconda sarà positiva o negativa ora ragionamenti analoghi valgono anche per funzioni che sono di fatto l'estensione di questo concetto ma ha più variabili se ho una funzione di più variabili io la posso scrivere esattamente in questo modo la posso scrivere come un termine noto più un termine diciamo che è l'equivalente del termine lineare solo che in questo caso ho il mio vettore che rappresenta le mie variabili che viene moltiplicato a sinistra per un vettore che chiamiamo b b trasposto perché per noi i vettori sono colonna quindi lo devo lo devo rendere vettore a riga per moltiplicarlo per il vettore colonna v doppio e far sì che restituiscano uno scalare e più ho il termine che diciamo è l'analogo del termine quadratico che è dato dal prodotto tra v doppio trasposto c e v doppio ora esattamente dove c che cosa ecco perché si ha una funzione quadratica ha la caratteristica di essere una matrice simmetrica prima vi ho parlato delle proprietà delle matrici simmetriche no? se sono numeri reali e di quella simmetrica sappiamo che gli autovalori sono tutti reali ad esempio per un risultato che abbiamo richiamato prima a questo punto c esattamente come nel caso di funzioni di una variabile è quello che ci permette di controllare alcune proprietà di questa funzione quadratica in particolare nel caso di una variabile guardavamo c se c era positiva concludevamo che la funzione era convessa se c è negativa che la funzione era concala qui andiamo a vedere gli autovalori cioè quello che si può vedere è che se gli autovalori sono tutti positivi quindi la funzione maggiore o uguale di zero quindi la funzione positiva semidefinita allora si può concludere che g è convessa se sono tutti minori o uguali di zero si può concludere che g è concava se sono mescolati quindi alcuni positivi alcuni negativi la funzione non sarà né concava né convessa vi ricordo che questi autovalori sono reali perché quella è una matrice reale simmetrica per la proprietà che abbiamo detto perché la scegliamo sì perché perché le funzioni quadratiche che esprimono le appunto le relazioni che esprimono le espressioni che esprimono le funzioni quadratiche di di più variabili hanno la struttura per cui c deve essere simmetrica allora volendo introdurre appunto un concetto di un punto di vista geometrico possiamo come vi dicevo prima fare un'operazione di questo genere cioè andare a prendere un'approssimazione in serie di Taylor della funzione abbiamo visto che se la vediamo da un punto di vista geometrico e parliamo ad esempio di funzioni di una singola variabile partiamo sempre da una sola variabile l'approssimazione di serie di Taylor del primordine che cosa rappresentava rappresentava l'approssimazione della funzione come un termine che era il valore della funzione in questo punto più un altro termine che era w-w moltiplicato per la pendenza della funzione in quel punto quindi era equivalente ad approssimare la funzione con la retta che è tangente in quel punto ok estendere al secondo ordine significa includere il secondo termine dell'espansione della serie di Taylor che è un mezzo derivata seconda della funzione nel punto per lo scarto w-w elevato al quadrato poi qui potrei avere tutti gli altri termini se ammettessero se ammettesse la funzione derivata terza eccetera quarta se fosse infinitamente differenziale avrei potenzialmente infiniti termini di quella sommatoria di quella che diventa appunto una serie che è la serie di Taylor però noi ci fermiamo al secondo ordine il che significa che approssimiamo la funzione del punto w come che cosa guardate qui qui abbiamo questo termine che esattamente gioca il ruolo di questo b e abbiamo qui la derivata prima e il termine lineare e poi abbiamo la derivata seconda in realtà poi abbiamo anche i termini derivata prima di g meno v che entrano poi in gioco nel definire quelli che sono questi scalari ma comunque la struttura qui è chiara abbiamo un termine di primo ordine un termine noto e un termine di secondo ordine va bene quindi significa che cosa anche qui l'analogia con il caso unidimensionale di approssimazione del primo ordine qual è? che nel primo ordine approssimavamo la funzione con la retta tangente al punto qui approssimiamo la funzione con che cosa? con una funzione quadratica quindi con di fatto in una dimensione con una parabola cioè cerchiamo un'approssimazione che è una parabola della funzione è un'approssimazione del secondo ordine questa approssimazione analogamente all'approssimazione del primo ordine è tanto più giusta quindi tanto ha un termine di errore tanto più piccolo quanto più sono vicino al punto in questione man mano che mi allontano è ovvio che le due funzioni la funzione in questione è la parabola che è la prossima diciamo saranno diverse quindi divergeranno e l'errore aumenterà da notare che se la funzione è convessa nel punto v la funzione quadratica che è la prossima quindi l'approssimazione serie di Taylor sarà convessa dovunque siccome è una funzione quadratica quella viceversa se la funzione è concava l'approssimazione la parabola con cui vado ad approssimare la funzione è concava dovunque questo si vede bene quindi vado avanti un attimo nella slide successiva per farvi vedere questo concetto a livello visivo non è quella successiva saltiamo un po' avanti però dopo torniamo alle slide indietro qui si vede bene quello che stavo dicendo vedete in questo punto se io vado vedete abbiamo la curva nera è quella che dobbiamo approssimare quindi la nostra g la curva azzurra è la quadratica approssimante quindi se vado in questo punto la funzione che approssima la mia funzione originale è questa funzione quadratica questa bella parabola vedete che tanto più mi allontano da questo punto quindi vado verso di qua e fin qui l'errore è poco qui diventa sempre più grande chiaramente però la cosa interessante è che in questo punto la funzione è conversa e ovviamente anche la funzione approssimante è conversa se io mi fossi messo in quest'altro punto la funzione che va a approssimare è concava e lo è anche la funzione approssimata ho tutto questo finché siamo in una variabile ma anche in questo caso il discorso può essere esteso a funzioni più variabili più variabili analogamente a quanto abbiamo visto per l'approssimazione del primo ordine e qui ritroviamo pari pari che cosa il fatto che se ho funzioni più variabili allora posso approssimare la nostra funzione g tramite una una funzione che chiamo h che costruisco in questo modo un termine noto che è il valore della funzione g nel punto più un termine del primo ordine che è il gradiente della funzione nel punto v moltiplicato per lo scarto w meno v più e qui se mi fossi fermato qui sarebbe quello che abbiamo utilizzato per la costruzione dell'algoritmo di discesa del gradiente qui aggiungiamo un termine di secondo ordine che è un mezzo v meno v doppio trasposto che moltiplica l'essiano della funzione nel punto v per v doppio meno v quindi vedete qui la prossima azione del secondo ordine ci porta a chiamare in causa l'essiano della funzione che vi ricordo è quella matrice che contiene tutte le derivate parziali di secondo ordine o anche qui ci sono una serie di proprietà che diciamo in matematica possono essere dimostrate rigorosamente e che ci limitiamo a richiamare tra le quali che se la funzione anche in questo caso g è convessa in un punto v la funzione quadratica che è la prossima che è quella che abbiamo costruito che abbiamo chiamato h in questa in questa in questa slide ebbene quella funzione la funzione h è convessa dovunque è una funzione quadratica quindi possiamo concludere che è convessa dovunque viceversa se la funzione è concava la funzione g che vogliamo approssimare è concava in v magari non è detta che lo sia dovunque ma in quel punto è concava allora la funzione quadratica che è la prossima è concava dovunque dovunque ok va bene ci siamo qui perfetto a questo punto ecco introdiciamo la domanda che prima avevo anticipato poiché l'approssimazione in serie di Taylor del primo ordine ci ha portato poi a definire un algoritmo di ottimizzazione locale che è quello di discesa del gradiente riusciamo a costruire un algoritmo di discesa basato su serie di Taylor di ordine più alto la risposta è affermativa sì e in particolare quello che andremo a discutere adesso nella parte restante di questa lezione è l'approssimazione in serie di Taylor del secondo ordine e l'algoritmo che ne consegue che è noto come metodo di Newton metodo di Newton questo ci dice che è collegato ovviamente a Newton che nel settecento è stato tra sicuramente gli scienziati protagonisti di tutta una serie di cose tra cui appunto alcuni aspetti fondazionali dell'analisi matematica e in particolare ha ricavato un metodo per trovare le radici di una funzione cioè dove una funzione generica interseca l'asse delle x per esempio se parliamo di una funzione di una variabile cioè la ricerca dei valori di x che annullano una determinata funzione questo metodo viene chiamato appunto metodo di Newton ed è collegato strettamente alle cose che vi starò dicendo è il motivo per cui appunto questo algoritmo prende il nome di metodo di Newton un algoritmo di Newton allora come si fa a costruire un algoritmo che ci permetta di minimizzare una funzione di una o di più variabili sfruttando appunto le condizioni del secondo ordine allora vi anticipo una cosa che questo algoritmo è particolarmente utile per funzioni che hanno un numero di input moderati questo le rende perché ci sono un po' di criticità di cui parleremo ma queste sono delle caratteristiche che analizzeremo dopo criticità che riguardano appunto la dimensione dell'input però intanto prima di arrivare a questo vediamo un po' cosa come viene costruito questo questo algoritmo allora anzitutto direi che vale la pena fare una una breve digressione prima di di andare avanti con questa slide quindi quello che direi che possiamo fare è condividere la lavagna ok ritorniamo alla nostra lavagna andiamo avanti ecco e quello che volevo richiamare volevo no volevo scrivere non richiamare perché non l'abbiamo visto è questo scrivervi intanto ragionare in una sola variabile e andare a riscrivere qui il valore scusatemi l'approssimazione di Taylor del secondo ordine che riporto qui per comodità allora allora no non sono diciamo god il caught Allora, questa è esattamente l'approssimazione al secondo ordine in serie di Taylor della funzione g, e chiamiamo questa approssimazione h, va bene? Quindi abbiamo il nostro termine del primo ordine che è g, scusate, di ordine costante, il nostro termine del secondo ordine, e questo è, di nuovo, termine costante, termine del primo ordine e termine del secondo ordine, scusate, li avevo mescolati. Allora, qui facciamo un po' di considerazioni. Cosa succede anzitutto se impongo la verifica di una condizione del primo ordine? Cioè vogliamo andare a vedere se quello è un punto stazionario e allora dobbiamo imporre che cosa? Andiamo in cerca, questo ci è utile perché poi vedrete ci è utile nella costruzione dell'algoritmo. Allora, se io vado in cerca dei punti di stazionarietà, allora questa è la funzione che approssima la mia funzione, quindi una bella parabola. L'algoritmo funziona andando a cercare un punto di stazionarietà di questa parabola, cioè va in cerca del minimo di questa parabola, ok? Quindi per andare in cerca del minimo di questa parabola noi dobbiamo andare a vedere dove si annulla la derivata di h rispetto a w, giusto? Quindi vado a cercare dove la derivata prima si annulla. Quindi vado a scrivere a questo punto la derivata prima di h, che sarà una cosa di questo tipo, sarà derivata di g rispetto a w, che è la... Allora, questo primo termine è un termine che si annulla perché è costante, è un numero, g di w, ok? Quindi quando vado a derivare g rispetto a w, chiaramente, scusatemi, h rispetto a w, questo è un termine che si annulla. Mi rimane invece questo termine di g di w, questo è un altro numero, questo che vi vado ad evidenziare adesso. Questo è un altro numero, quindi vedetelo come un numero moltiplicato per w meno un altro numero. Quindi quando vado a fare la derivata prima, questo è pari a 1 e mi sembravvive solamente il termine moltiplicativo. Poi che cosa abbiamo? Abbiamo i termini che derivano dal termine del secondo ordine e qui abbiamo due volte, moltiplicato un mezzo, la derivata seconda di g rispetto a v due volte, che moltiplica w meno un u. Questa come l'abbiamo costruita? L'abbiamo costruita semplicemente andando a fare la derivata di questa cosa qua. Derivata delle funzioni composte, il 2 va fuori e poi si ride con l'altro 2, e rimane, questo è un termine moltiplicativo costante, vado a prendere la derivata rispetto a w di questo termine, rimane solo, chiaramente, w meno v, perché ho la derivata delle funzioni composte. E quindi rimango con questa cosa qua. Questa è proprio applicazione di regole di derivazione. Allora, la cosa interessante è che questa, che è esattamente di h rispetto a w, noi la vogliamo porre uguale a zero. Cioè vogliamo andare a vedere un punto stazionario di quella quadratica h. Va bene, approssimante. E cosa succede? Succede che se lo mettiamo uguale a zero, abbiamo che la derivata seconda di g rispetto a w due volte nel punto w moltiplicata per w doppio deve essere uguale a dg due volte nel punto w per v meno dg rispetto a w doppio nel punto w. Allora, se impongo che questa equazione valga, quindi tutto questo sia uguale a zero, allora che cosa posso semplicemente andare a portare di là, vedete mi rimane questo termine, mi rimane qui, e invece porto di là questo, scusate, non volevo in realtà utilizzare, ah no, ci picchia, vediamo un po' se riesco ad annullare questa cosa, ecco, no, no, voglio andare. Questa, esatto, ok, era questa, grazie. Allora, facciamo un annullamento selettivo, ok, utilizzare questa penna, scusate, allora, vi dicevo, questo termine è quello che finisce a destra, questo termine è quello che sopravvive a sinistra, e quest'altro termine è quello che col segno meno moltiplicato per di secondo va nella parte destra, ok, quindi da questa discende immediatamente questa, va bene? basta considerare che questo è un numero, questo è un numero, e quindi, questo è un numero, è un numero colorato, giusto? non volevo colorare, nessun colore, perché ho tralasciato questo, Termine. Termine. Va bene? Bene, allora una volta che abbiamo fatto questo possiamo andare a fare la seguente cosa, e andare a risolvere, quindi questa è un'equazione in cui abbiamo un numero per v doppio uguale a un altro numero per v, eccetera eccetera. Risolviamo rispetto a v doppio, chiamiamo la soluzione di questa equazione perché noi cerchiamo il punto v doppio che rende vera questa. Quindi cerchiamo il punto v doppio che rende vera questa. Chiamiamo questo v doppio soluzione di questa equazione v doppio asterisco, e v doppio asterisco lo possiamo a questo punto ricavare molto semplicemente come v meno derivata di g rispetto a v doppio nel punto v fratto derivata seconda di g rispetto a v doppio due volte nel punto v. Questa è esattamente questa qui in cui semplicemente vado a dividere per questo termine ambo i membri. Se divido ambo i membri di questa equazione per il termine della derivata seconda, questo mi si semplifica, mi rimane il termine v, e poi qui ho il termine di derivata prima fratto la derivata seconda. Ok? Quindi questo punto che ho chiamato v doppio asterisco è il punto... stazionario di che cosa? Della funzione quadratica che approssima la mia funzione. Ci siamo? Sì, ci sono. Questi calpe di... Però capite il quale c'è. Ok, quindi... Perché noi vogliamo andare a vedere qual è quel punto stazionario? Adesso si chiarirà tra poco, ma ve lo anticipo. Questo è il punto successivo nella discesa, praticamente. Cioè noi costruiremo un algoritmo di discesa in cui il punto successivo sarà quello, cioè noi andiamo ad approssimare la funzione con una quadratica. Dopodiché andiamo in cerca di quel punto di stazionalità, che è il minimo della parabola, se siamo in una variabile, con cui andiamo ad approssimare la funzione. E da lì ripartiamo. Quindi faccio che darci una direzione, in questo caso ci dà proprio... Sì, ci dà anche la direzione, perché di fatto poi se andiamo in più variabili è un vettore, quindi ci dice la direzione. Infatti adesso quello che vi faccio vedere è che cosa succede quando siamo in più variabili. Ma questo lo vediamo direttamente nella slide. Allora, questa adesso tenetela presente perché l'abbiamo derivata e adesso quello che facciamo è andare direttamente a vedere nella slide da cui eravamo partiti che cosa abbiamo. E nella slide avevamo esattamente questo. Guardate, questa è esattamente la... Quello che vi ho fatto vedere adesso, cioè qui è riportata, cioè che cosa? Utilizzando la condizione del primordine, cioè imponendo che il gradiente sia uguale a zero, ancora siamo in una sola variabile, che la derivata prima sia pari a zero, e andando a cercare il punto W star che rappresenta il punto stazionario della quadratica che approssima al secondo ordine della nostra funzione, si ottiene questo. Cioè questo è il punto W star che si ottiene a partire da cosa? Al punto V in cui noi siamo, che è quello a partire dal quale facciamo l'approssimazione, a cui togliamo derivata prima fratto derivata seconda della funzione in quel punto. E W star, vi ricordo, è quindi il punto stazionario, è il minimo o eventualmente in qualche caso potrà essere il massimo, della parabola, perché siamo in una variabile, della funzione quadratica, diciamo più in generale, che approssima la nostra funzione nel punto. Ma se la parabola viene quella concavità rivolta verso il basso, di come è la spazio a chiarire? Eh, questo, hai toccato subito un punto, che è un punto dolente di questo algoritmo, perché il problema è appunto questa parabola dove rivolge la sua concavità. Infatti uno dei problemi che hanno gli algoritmi di questo tipo, i metodi Newton, è proprio che non abbiamo nessuna garanzia del fatto che in certi punti della funzione non si vada a scalare la funzione anziché scendere. E quindi andiamo verso punti di massimo. Poi lo vedremo bene con l'esempio. Intanto andiamo avanti con le nostre considerazioni e a questo punto, avendolo derivato per le funzioni di una variabile, spero che, diciamo, se facciamo un piccolo salto in avanti su quelle di più variabili, non ci siano, diciamo, eccessivi smarrimenti, perché dovrebbe essere a tutti chiaro che cosa? Che se andiamo in più variabili possiamo fare la stessa cosa, cioè siamo in un punto, andiamo ad approssimare la funzione con l'approssimazione di Taylor a secondo ordine e andiamo a cercare il punto stazionario di quella funzione approssimante, che è una funzione quadratica. Quindi mettendo a zero il gradiente di quella funzione, quello che troviamo è un'equazione, andandola a risolvere troviamo un punto w asterisco, che questa volta è chiaramente un vettore, non è più un punto in una variabile, no? e si ricava esattamente come abbiamo ricavato questo, cioè abbiamo il vettore v a cui togliamo che cosa? Vedete, a numeratore abbiamo, dove prima avevamo in una variabile la derivata prima, qui abbiamo il vettore gradiente. E l'equivalente del denominatore non è la divisione di un vettore per una matrice, che è un'operazione che non esiste, ma è una moltiplicazione per quella che è la matrice inversa della matrice siana, che è esattamente l'analogo di 1 fratto la derivata seconda, se vogliamo, in una variabile. Vedete che c'è una certa analogia tra queste due equazioni. e questo, ripeto, si riesce a dimostrare abbastanza bene, insomma, però una volta che abbiamo fatto già tutti i passaggi per quella di una variabile, direi che andare su due variabili è abbastanza evidente, che ci conduce a una scrittura di questo genere, quindi c'è un bel risparmio come passaggio, come dimostrazione. Però vedete che la struttura è esattamente la stessa. Questo è il termine di derivata che c'è al gradiente. Al posto di 1 diviso abbiamo l'inverso della matrice. Vi ricordo che l'inverso di una matrice A è quella matrice che chiamiamo A-1, tale che A per A-1 è uguale alla matrice identità. Ok? A questo punto, e qui ci dice che noi siamo andati a cercare questo punto V, asterisco, V star, V stella, potete chiamarlo, che ci dice che cosa? Ci dice un nuovo punto di approdo in un ipotetico algoritmo interativo di scelta del gradiente. Questo punto di approdo viene calcolato a partire dal punto V, dove eravamo all'istante precedente, in una direzione che è definita da chi? Da questo termine qua. E in questo caso questo termine, esattamente come in una variabile, vedete la differenza con l'algoritmo di scelta del gradiente, che già si comincia a intravedere qua, questo sarà il valore a una certa iterazione, calcolato dal valore, partire dal valore all'iterazione precedente, nel caso dell'algoritmo del primordine avevamo semplicemente derivata prima moltiplicata eventualmente per alfa. Qui invece viene premoltiplicata per un qualcosa che tiene conto della derivata seconda. In più variabili avevamo il punto precedente, meno alfa gradiente. Qui abbiamo questo termine, che è di fatto una matrice, che va a premoltiplicare il nostro vettore gradiente, che è una matrice che ci tiene conto della curvatura della funzione. A questo punto è lecito domandarsi, ok, se io volessi ragionare esattamente come stavamo ragionando, cioè fare di questo meccanismo un algoritmo interativo di discesa, la discesa che viene dettata da questo vettore, che è il vettore gradiente premoltiplicato per il vettore, l'inversa della, scusatemi, della matrice essiana. È una valida direzione di discesa? La risposta è quella che avevi per esempio intuito prima, non sempre, nel senso che se la funzione che vado a minimizzare è convessa dovunque, l'approssimazione quadratica non solo ha la stessa curvatura della funzione che sto approssimando, ma è anche essa stessa sempre convessa, e il punto stazionario che io vado ad identificare in questo modo è un punto di minimo globale. Cioè quello che si può vedere è che se stiamo cercando una funzione convessa e l'approssimiamo con una funzione quadratica al secondo ordine di Taylor, siamo sicuri che andiamo a pescare sempre il minimo globale. Tra l'altro la cosa interessante è che lo pescheremo in una sola iterazione in questo caso, perché vado proprio a cercare quel punto di stazionarietà e quindi se la funzione è una quadratica, lo pesco in una sola iterazione, cioè quella di cui vado a calcolare. La cosa interessante è che il minimo w asterisco, in questo caso, della funzione quadratica è di fatto un valido punto di discesa, cioè se la funzione che sto approssimando è una funzione convessa, questo algoritmo ci fornisce sempre delle direzioni di discesa valide. E quindi l'intuizione dietro a questo metodo, che è il metodo di Newton, è che possiamo in maniera ripetuta effettuare dei progressivi passaggi verso punti che sono definiti da punti stazionari dell'approssimazione del secondo ordine. E per funzione convesse questo ci porta alla minimizzazione della funzione. è sempre così? No, perché se la funzione non è convessa non è così, perché appunto posso ritrovarmi in una situazione di questo tipo. E adesso qui proviamo un attimo a vedere alcune, cioè le figure che ci sono in questa slide, partendo da, ecco, alcune di queste le avevamo viste prima, che erano queste in alto, qui la nostra funzione, in nero, e quella in azzurro è la quadratica che approssima questa funzione di una variabile, qui chiaramente siamo in una variabile, però diciamo ci serve per capire bene i contorni del problema, e qui vedete che questa, la funzione g, è una funzione che chiaramente non è convessa, ok? Ha dei punti di convessità, delle porzioni in cui è concava, eccetera. E allora qui vedete che la funzione approssimante è una funzione, chiaramente che è una funzione convessa, lo è anche qui, non lo è più qui. Allora significa questo che cosa? Che se io vado in cerca da qui, e questo lo vedete bene nella parte, scusatemi, in questa porzione, nelle ultime tre in basso, come funziona l'algoritmo di Newton? Funziona che io parto da un punto, ad esempio da questo, approssimo la funzione con la sua quadratica, che è questa in azzurro, che è una bella parabola, e vado a cercare il minimo di quella parabola, il punto di stazionarietà, perché ho cercato i punti dove la derivata è zero dell'approssimazione quadratica, giusto? Allora, qual è il minimo di questa parabola? Questo punto qui corrisponde a questo, o meglio, diciamo, questo è il punto che vado a cercare, che corrisponde a questo valore sulla funzione quadratica approssimante. Questo è il punto da cui in realtà io riparto per il punto successivo nell'algoritmo di discesa dell'approssimante quadratica, esattamente. Da qui io riparto e ho il valore della funzione è questo e di nuovo rifaccio la stessa cosa. Ora è chiaro che mentre mi muovo da questo punto rosso a questo punto verde, la funzione, vedete, vado in una direzione di discesa valida, perché sono una porzione di una funzione, della funzione convessa, ma siccome la funzione non è convessa ovunque, nel momento in cui io dovessi partire da questo punto in rosso, cioè sono su questo punto di approssimazione, mi costruisco sempre la mia bella curva blu che è la quadratica approssimante. La quadratica approssimante vado a cercare il punto di stazionarietà che è questo a cui corrisponde il massimo di quella parabola. Mi ritrovo con questo valore e vedete, nel passare da qui a qui la funzione è aumentata. Quindi questo algoritmo, questo semplice, scusatemi, esempio, ci dice che l'algoritmo non trova sempre una direzione di discesa valida, la trova solamente se le funzioni sono convesse, altrimenti no. E quindi questa è una cosa alla quale bisogna stare attenti. Vedete che questa invece è una bella funzione convessa e qui è interessante vedere come funziona l'algoritmo perché partendo da questo punto, questo è il valore corrispondente sulla parabola, quello che succede è che il punto successivo sarà il punto che ho appena evidenziato in verde che corrisponde al punto di stazionalità al minimo della quadratica approssimante. Questo punto ha questo valore sulla funzione che abbiamo appena approssimato, ok? Da qui io posso ripartire e cercare nella figura di destra un nuovo punto, una nuova funzione quadratica approssimante che è questa azzurra che adesso ho ripassato in arancione evanescente appena sopra. di nuovo vado in cerca del suo punto di minimo e avrò il punto successivo da cui riparto e vedete che ho trovato il minimo in questo caso perché questa è una funzione tutta convessa quindi ho la garanzia che vado sempre in discesa. Però anche con l'algoritmo di discesa del gradiente non avevamo la garanzia di andare sempre in realtà in discesa perché a seconda del valore di alfa abbiamo visto che potevamo cominciare anche a rimbalzare in punti della funzione in salita e quindi sì si fa anche quello si può fare come non fare adesso c'è una slide proprio in cui richiamo questo punto quindi riepilogando allora fin qui siamo in una abbiamo visto l'esempio in una variabile però già avevamo anche parlato di come l'equazione si modifica per funzioni nuova di due variabili scusatemi di più variabili e l'algoritmo quindi noto come metodo di Newton che cosa fa? Fa dei passi successivi in maniera iterativa che vanno verso i punti di stazionarietà dell'approssimazione in serie di Taylor del secondo ordine quindi questo equivale per funzioni di una sola variabile al cappesimo passo a calcolare è l'approssimazione di Taylor secondo ordine centrata nel punto vk-1 cioè io vado a calcolare questa funzione che è la funzione originale che voglio approssimare nel punto wk-1 che è quello dell'iterazione precedente poi c'è il termine legato alla derivata prima il termine legato alla derivata seconda risolviamo rispetto al punto di stazionarietà quindi significa che troviamo quel valore che prima avevamo chiamato w star che questo lo avevamo chiamato w che è esattamente ricavato come w meno derivata prima diviso derivata seconda per funzioni più variabili facciamo la stessa cosa richiamiamo questa approssimazione in questo modo vedete abbiamo il termine del primo ordine e questo è il termine del secondo ordine di nuovo risolviamo rispetto al punto di stazionarietà quindi significa che andiamo a cercare un punto che chiameremo wk che sarà ricavato a partire scusatemi da wk meno 1 e poi ci sarà un termine che sarà meno gradiente premoltiplicato per l'inversa della matrice siana quindi andiamo a vedere e vedete che troviamo proprio quello che dicevamo wk è uguale a wk meno 1 meno il gradiente premoltiplicato per l'inversa della matrice siana o tutto questo esattamente rientra nello schema degli algoritmi di ottimizzazione locale che abbiamo visto quando li abbiamo cominciati a introdurre perché guardate che vi ricordo che l'algoritmo di ottimizzazione locale ha una struttura di questo genere wk è uguale a wk meno 1 più un vettore di discesa all'interazione k moltiplicato eventualmente per uno scalare alfa che è il learning rate e nel nostro caso il vettore d è meno inversa della siana per il vettore gradiente e in questo caso implicitamente alfa è uguale a 1 ciò non toglie che volendo io posso impostare anche un learning rate e imporre anche lì un valore di alfa diverso però implicitamente la derivazione dell'algoritmo in base è quella con learning rate unitario però potrei tranquillamente andare a moltiplicare questo vettore per uno scalare alfa e ottenere un learning rate diverso se noi andiamo ad analizzare questa funzione scusatemi questo algoritmo vediamo che la generica iterazione in cosa consiste la generica iterazione consiste nel calcolo del gradiente e questo abbiamo detto che da un punto di vista algoritmico è un problema risolto si riesce a fare scala bene con la dimensione e non ci sono problemi l'abbiamo visto per l'algoritmo di discesa del gradiente quindi diciamo fino a qui la stessa complessità è quella dell'algoritmo di discesa del gradiente in più però se vogliamo applicare questo metodo oltre al calcolo del gradiente che cosa abbiamo? il problema dell'essiano ok allora non solo il problema di calcolare l'essiano ma c'è un problema anche di calcolare l'inversa di quella matrice allora mentre il calcolo del gradiente vi ho detto si fa in maniera efficiente anche se non abbiamo avuto modo di entrarci nel dettaglio vi ho fatto vedere che ci sono librerie che lo fanno vi ricordate l'ho fatto la volta scorsa diciamo fidatevi del fatto non facciamo l'analisi della complessità dell'algoritmo per il calcolo come si fa però diciamo ci sono degli strumenti che noi prendiamo e che lo fanno in maniera grigia sul calcolo del siano ma soprattutto sul sull'inversione della matrice n per n e cominciamo ad avere dei problemi perché la matrice l'inversione di una matrice è qualcosa che non è banale come algoritmo in termini di numero di operazioni in realtà quello che si può dimostrare e anche qui si vede abbastanza semplice che questa è nel tutto equivalente a questa cioè io posso andare se voi premoltiplicate ambo i membri per l'essiano ottenete essiano per w uguale a essiano per w k meno 1 meno essiano per la sua inversa per il gradiente essiano per la sua inversa fa matrice di identità e sopravvive il solo gradiente ok questa non lo vedete immediatamente ma di fatto è un qualcosa nell'incognita w ed è una struttura questa è una matrice che moltiplica w e questo è un vettore noto che è calcolato in questo modo e questo è di fatto la struttura di un sistema di equazioni lineare del tipo a per x uguale a b adesso questo l'ho scritto male a per x uguale a b ok dove x è la vostra incognita quindi quello che si può dimostrare è che calcolare questo da un punto di vista dell'algoritmo per risolverlo è equivalente a risolvere un sistema lineare di equazioni di quale dimensione n equazioni nelle incognite ora per la risoluzione dei sistemi lineari ci sono degli algoritmi specializzati che fanno che riescono a fare questo lo fanno in tempo polinomiale ma diciamo l'algoritmo in base n al cubo poi in realtà c'è qualcosa che fa un po' meglio ma non si riesce attualmente ad arrivare neanche n quadro ora questo chiaramente può cominciare a rappresentare un po' un problema quando n cresce ma non è questo il punto principale che rappresenta il punto debole di quest'algoritmo il punto principale è la memoria l'occupazione di memoria perché tenere memorizzato in memoria una matrice la matrice Siana è di fatto un punto critico perché come vi ho detto n cresce n può essere avere un valore estremamente elevato nei modelli di machine learning quelli che si usano oggi se anche prendete un modello con 10.000 variabili e ce ne sono tranquillamente la matrice Siana quante entri avrà 10 alla quarta per 10 alla quarta cioè 10 alla 8 cioè 100 milioni di entri e comincia a essere dovete memorizzare 100 milioni di double 10 alla 8 cominciate a andare insomma entrate nell'ordine di gigabyte già se salite chiaramente questo può diventare un problema oltretutto comincia a diventare un problema anche il calcolo diciamo la risoluzione stessa perché avete un termine cubico quindi la singola iterazione può diventare onerosa da un punto di vista computazionale però la cosa bella è che il vantaggio qual è? che se la singola iterazione può essere pesante da un punto di vista computazionale se confrontata con l'algoritmo di discesa del gradiente è anche vero che in certe condizioni qui marciamo a passo molto più spedito cioè si converge in molti meno passi verso il minimo e questo diciamo si vede bene per esempio con le funzioni quadratiche adesso poi se non ricordo male c'è una slide che esemplifica questo concetto questo invece è un esempio riassuntivo di diverse delle cose che abbiamo detto finora che vi fa vedere come funziona l'algoritmo nel momento in cui lavora e quindi di nuovo abbiamo la funzione g di w che è la nostra funzione che vogliamo minimizzare che è questa in nero e partiamo da un punto w0 che è un punto inizialmente scelto per esempio scelto a caso anche se non necessariamente e partendo da qui costruiamo l'approssimazione di questa funzione quadratica alla funzione g nel punto da qui andiamo in cerca del punto di stazionarietà di questa approssimazione quadratica che come vi dicevo prima è il minimo di questa parabola che corrisponde a questo punto w1 qui di nuovo riparto e approssimo la funzione in questo punto con questa funzione quadratica vado in cerca del suo punto di minimo che è esattamente questo che mi porta al punto w2 il valore della funzione in w2 è questo che ho appena evidenziato da qui vado in cerca del minimo il minimo è questo e così via e questo chiaramente qui nella parte invece inferiore abbiamo una questa era chiaramente una funzione convessa questa non lo è più e ovviamente se parto da w0 vado in discesa come dicevamo prima se parto da w0 in questo caso e questo è w0 e mi ritrovo in w1 qui ho trovato una direzione valida di salita quindi il metodo ovviamente fallisce nel momento in cui abbiamo una funzione non convessa quindi generalmente quello che si può vedere è che come vi dicevo prima il metodo di Newton richiede meno passi per la convergenza rispetto al metodo di discesa del gradiente anche se il singolo passo può essere oneroso se n comincia a crescere da un punto di vista computazionale però è più difficile da usare con funzioni non convesse ok perché? perché nelle parti concave può risalire verso un massimo anziché scendere ok e qui abbiamo un esempio che ci fa capire perché andiamo a convergenza più rapidamente in meno passi più rapidamente allora abbiamo una funzione di due variabili che vedete è una funzione quadratica nei termini w1 w2 al quadrato poi abbiamo il doppio prodotto ok questa è una funzione quadratica del tipo che abbiamo visto anche nella lezione scorsa quindi una certa struttura ha le sue curve di livello che sono queste che sono quelle lì si concentri che possono essere più o meno schiacciate a seconda del valore di quei parametri no? quello 026 e quello 048 a sinistra avete un esempio di algoritmo di discesa del gradiente in cui partite da un certo punto che è questo che è il punto iniziale e poi vedete lui si muove ortogonale di livello si porta in questa in questa a partire da questo punto a quest'altra e poi comincia la discesa verso il minimo che trova dopo un certo numero di interazioni vedete in questo caso sono abbastanza e a destra avete la stessa cosa con un algoritmo invece basato su metodo di newton in cui partite esattamente dallo stesso punto iniziale quindi dallo stesso w0 iniziale che è questo e vedete la cosa bella è che si va in una sola iterazione verso il minimo perché si va in una iterazione sola verso il minimo perché quella che sto minimizzando è una funzione quadratica io la minimizzo con la prossimo con una funzione quadratica del secondo ordine quell'approssimazione è perfetta cioè se vado in cerca del punto di stazionalità della funzione approssimante di fatto è anche il minimo della funzione che ho appena approssimato e quindi in una sola iterazione arrivo a convergenza in generale questo è chiaramente un caso particolare per le funzioni quadratiche ma se ho altre funzioni in generale quello che si verifica è il metodo di newton funziona bene in termini di numero di iterazioni che deve fare cioè il fatto di sfruttare l'informazione sulla curvatura ci porta in pochi passi ad identificare un minimo il problema è che ognuno di questi pochi passi può risultare parecchio onoroso da calcolare perché significa invertire una matrice n per n significa comunque o comunque alternativamente risolvere un non alternativamente significa risolvere poi un sistema di equazioni perché perché vi ho detto prima che torniamo un attimo alla slide precedente ecco esatto questa qui vi ho detto prima che l'iterazione quella che è scritta in alto può essere equivalente a quella che abbiamo scritto in basso e quella che abbiamo scritto in basso non c'è più l'inversione della matrice ma di fatto c'è la risoluzione di un sistema di equazioni lineari che comunque è cubico in generale poi ci sono dei trucchi per migliorarle queste cose non è non è così semplice quantomeno più che quadratico ok questo è l'esempio che abbiamo visto direi che abbiamo quasi terminato i metodi del secondo ordine rimangono due slide quindi vale la pena provare a a finirle vediamo un po' di ulteriori aspetti che caratterizzano questo questo metodo appunto che è noto come metodo di newton allora qui vediamo dalla sopra c'è la generica iterazione nel caso di una variabile cosa succede che anche se siamo in una variabile se siamo in porzioni quasi piatte della funzione g quindi laddove la funzione g spiana numeratore e denominatore hanno valori vicini scusate non non nulli ma hanno valori proprio vicini allo zero ok è chiaro che io se sono vicino a un punto di stazionarietà il gradiente si annulla e se la porzione è abbastanza piatta la funzione quindi ha anche una curvatura che diventa piatta e quindi denominatore e numeratore sono entrambi nulli da un punto di vista numerico quindi dell'algoritmo che andiamo a implementare questo è un problema perché c'ho zero su zero un'instabilità numerica quindi questi sono algoritmi in cui bisogna porre attenzione a questo tipo di problema evitare le divisioni per zero allora per evitare questo problema questo quello che si fa si fa quello che viene chiamato passo di newton regolarizzato ok regolarizzato cioè si va a fare che cosa un piccolo trucco diciamo numerico per cui si va a prendere l'iterazione a esattamente la stessa struttura che abbiamo visto finora però al denominatore ci aggiungiamo un piccolo valore epsilon che fa sì che se anche la derivata seconda si annulla ci sommo un valore abbastanza piccolo che non perturba e non modifica più di tanto la dinamica dell'algoritmo ma mi aiuta a evitare situazioni di divisione per zero poi ho sempre il problema che se in realtà dove mi si annulla il gradiente in realtà avanzerò poco però anche questo può essere opportunamente risolto mediante normalizzazione del vettore gradiente quindi diciamo questo è un problema che si risolga i valori di epsilon tipici sono molto piccoli cioè basta qualcosa dell'ordine di 10 alla meno 7 per evitare di finire sotto la precisione numerica diciamo della macchina la stessa cosa può essere fatta cioè può essere data una versione del singolo passo di Newton regolarizzata per funzioni multi input in cui avete al posto del questa è la solita iterazione wk uguale a wk-1 meno essiano essiano vi ricordo alla meno 1 per gradiente al posto di essiano alla meno 1 qui si introduce un termine che è epsilon per i i è la matrice identità n per n cioè una matrice che ha tutti zero tranne zero zero tranne gli 1 sulla diagonale io lo vado a premoltiplicare per un valore pari a 10 alla meno 7 quindi ho 10 alla meno 7 10 alla meno 7 10 alla meno 7 sulla diagonale e quelli li vado a sommare alla matrice siana ecco l'inversa di questo è un qualcosa che numericamente non crea problemi laddove questa comincia ad avere tutti i valori nulli perché ci vado ad aggiungere dei valori per cui non ho più problema diciamo di inversione di una matrice nulla che è numericamente instabile anche lì e poi l'ultima cosa che c'è scritta qui era quella che mi stavi dicendo prima tu correttamente ed era il fatto della scelta del passo cioè l'algoritmo di Newton standard così come l'abbiamo derivato si riferisce a un valore di alfa uguale a 1 implicitamente però nulla vieta come anche vi dicevo prima che noi possiamo introdurre un valore alfa e andare a pesare per alfa il gradiente moltiplicato a sinistra per l'inversa della matrice siana in modo da andare a condizionare il nostro algoritmo a quel passo di avanzamento come nota storica il metodo di Newton ve l'ho detto prima è stato inventato di fatto come un algoritmo qui viene chiamato zero finding cioè va a ricercare le radici di una funzione quindi non con un algoritmo di ottimizzazione locale ma è strettamente collegato a questo che abbiamo visto oggi pro e contro allora sicuramente dato che utilizziamo sia le condizioni del primo ordine che anche quella del secondo ordine quindi la curvatura e utilizziamo più informazioni rispetto all'algoritmo di scesa del gradiente questo fa sì che questi algoritmi per esempio non soffrono del problema del che abbiamo chiamato dello zigzag il metodo di scesa del gradiente rischia di rimbalzare da una parte all'altra nel profilo della funzione gli algoritmi del secondo ordine non hanno questo tipo di problema questo metodo di newton sfruttando l'informazione della curvatura evita questo tipo di problema perché sa esattamente dove andare a muoversi fatto salvo il discorso di quando interpreta una salita per una discesa che è un problema che sempre si può avere il fatto di approssimare con una quadratica implica una convergenza più veloce specialmente vicino al minimo si va estremamente veloci però il problema di cui tenere presente è che il singolo passo è più dispendioso rispetto al singolo passo di un metodo del primo ordine puro come la discesa del gradiente e tenere in memoria la matrice siana può essere può essere un problema per dati ad alta dimensionalità perché scala quadraticamente l'occupazione di memoria quindi se ho un milione di variabili devo tenere una matrice con mille miliardi di entri altro problema l'abbiamo visto l'abbiamo detto se lo applichiamo a funzioni non converse in linea di principio può anche andare in salita anziché in discesa e per cui insomma ci sono delle contromisure in realtà che possono essere applicate per migliorare in particolare questi ultimi due problemi quindi ci sono in realtà anche delle soluzioni sia al problema dell'essiano che al problema delle funzioni non converse che sono metodi che non avremo modo di vedere ma che in letteratura sono stati sviluppati nel corso degli ultimi anni e che fanno sì che comunque anche se il metodo del primordio il metodo di discesa del gradiente è la scelta standard per la minimizzazione di un sacco di modelli di machine learning per esempio le reti neurali il deep learning è quasi totalmente dominato da algoritmi di discesa del gradiente però c'è spazio anche per algoritmi del secondo ordine e questi insomma stanno cominciando ad apparire sempre di più anche questi alla ribalta per alcuni tipi di applicazioni con questo concludiamo la parte del programma relativa abbiamo concluso la parte del programma relativa ai metodi di ottimizzazione e la prossima lezione cominceremo a entrare finalmente nel vivo dei dettagli perché questi modelli cominceremo a costruirli quindi cominceremo a trattare problemi di regressione in particolare regressione lineare e andremo e cominceremo a dare per scontato il fatto di disporre di un metodo che permetta l'ottimizzazione di questi modelli però direi che lo iniziamo la prossima volta direttamente quindi invece oggi se ci sono domande può essere un momento questo per approfondire queste cose se no fermiamo qui la registrazione ok va bene allora direi che intanto fermiamo la registrazione poi eventualmente le domande le potete comunque sempre fare l'וא� non se ne Grazie a tutti.