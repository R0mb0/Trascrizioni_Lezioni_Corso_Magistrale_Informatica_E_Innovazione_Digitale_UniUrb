Benissimo, allora riprendiamo le ultime cose che abbiamo visto la settimana scorsa nell'ultima lezione di teoria. Abbiamo iniziato a parlare di feature learning, quindi la possibilità di costruire un sistema che riesca a essere un modello per risolvere un problema di apprendimento, quindi un problema di classificazione o regressione, ma abbiamo detto più in generale le considerazioni che faremo valgono anche per altri tipi di problemi di apprendimento automatico. Nel quale il modello stesso riesce ad estrarre delle feature significative senza la necessità di andare noi manualmente a costruire delle trasformazioni non lineari. Quindi siamo partiti da un'ipotesi di modello che era, vi ricordo, il modello di questo tipo. Ok. Era questa l'ipotesi di modello che noi facevamo e ci siamo domandati come facciamo a scegliere F1? F1, F2, Fb e anche quanti di questi contributi per arrivare a una cosa di questo genere, quindi una figura che ci permetta di approssimare, questa serie di punti tramite una funzione altamente non lineare o classificare una distribuzione di punti non lineari. Questa era la domanda da cui siamo partiti e abbiamo fatto un po' di considerazioni riguardanti la possibilità di... abbiamo introdotto, diciamo, questo concetto di complessità del modello su cui oggi diremo qualcosa in più, di cui diremo qualcosa in più, su cui ci soffermeremo. L'altra cosa di cui abbiamo cominciato a parlare, abbiamo parlato di dati perfetti e dati reali e abbiamo mantenuto questa analogia tra il mondo dell'algebra lineare, in cui approssimiamo in maniera arbitrariamente precisa qualunque vettore, anzi lo esprimiamo esattamente come combinazione lineare di vettori opportuni che definiscono quella che abbiamo chiamato una base per il nostro sistema. Vi dicevo, abbiamo fatto un'analogia con il mondo delle funzioni e abbiamo visto che ci sono parecchie cose in comune tra queste due rappresentazioni, per cui anche una qualunque funzione y abbiamo visto che in linea teorica può essere espressa come combinazione di più funzioni di un certo tipo, su cui abbiamo fatto anche qualche considerazione preliminare. Abbiamo cominciato a dire che affinché questa funzione possa essere espressa in maniera arbitrariamente precisa, si devono verificare delle condizioni, queste condizioni sono la diversità delle funzioni, quindi questo concetto, quanto più sono varie le funzioni che andiamo a considerare, tanto più abbiamo possibilità di costruire un'approssimazione efficace e tanti più sono quei termini b, quanto più questa approssimazione risulta essere accurata. La differenza rispetto ai vettori nella rappresentazione è che questo b può diventare potenzialmente infinito se voglio raggiungere una precisione arbitraria. Quindi eravamo arrivati a fare questi tipi di considerazione e oggi andiamo avanti, nel senso che portiamo alcuni di questi argomenti alle loro naturali conseguenze. Cosa voglio dire? Nel momento in cui abbiamo uno spazio di vettori e il numero di termini, abbiamo detto, è maggiore o uguale di n, noi sappiamo che quelli, se sono tutti linearmente indipendenti, rappresentano un approssimatore universale. Cosa vuol dire? Vuol dire che noi riusciamo ad approssimare, a descrivere un qualunque vettore in termini di quella base. Ok? Bene. Data ogni funzione, e vi ricordo che una funzione, diciamo, y di x, è quella che noi vogliamo approssimare, per esempio, in un problema di regressione, ma è anche la base per costruire un classificatore non lineare, perché poi facciamo, se quella rappresenta, diciamo, un'opportuna combinazione delle feature di input, la facciamo passare, ad esempio, attraverso la funzione segni, abbiamo visto che questo è un modo per costruire un classificatore non lineare, e il profilo, diciamo, non lineare del confine decisionale rappresenta, appunto, quello che noi vogliamo di fatto riuscire a catturare in maniera arbitrariamente precisa. Quindi l'obiettivo è dire, cercare di rispondere a una domanda, se ogni funzione y di x può essere, in linea di principio, approssimata con un livello di precisione che noi decidiamo, da un insieme ricoprente di funzioni, quindi da una base di funzioni, che abbia una certa capacità, la massima possibile. Noi vogliamo identificare questo spanning set, se esiste, chiaramente ce ne esistono una varietà infinita, vedremo, perché ovviamente le funzioni, pensate, cioè questo è intuito, anche se non è un'affermazione che motiviamo rigorosamente, però pensate, avete una qualunque funzione, pensate di poterla descrivere in termini di altre funzioni, abbiamo un'infinita varietà a disposizione di funzioni, quindi ne abbiamo infinite. L'esistenza, ecco, di questo insieme di funzioni che definiscono questo spanning set, a partire dal quale voi potete approssimare qualunque funzione, viene chiamata anche qui approssimatore universale. Chiaramente, come vi dicevo prima, per ottenere una precisione arbitraria potenzialmente potremmo avere bisogno di infiniti di questi termini, mentre per i vettori ne bastano almeno n. Allora, il fatto che esistano questi approssimatori universali è, intanto la risposta è affermativa, perché la domanda che uno si può fare è ma ne esistono? Sì, ne esistono. Cioè, tra queste infinite combinazioni di funzioni a cui potete pensare, esistono delle categorie di funzioni che sono appunto degli approssimatori universali. E questo ce lo dice la matematica, ci sono dei teoremi, che appunto sono teoremi, che vengono chiamati di approssimazione universale, che ci dicono che è possibile fare questo. Ma questo ce lo dice la matematica da prima del machine learning. Se voi pensate, pensate che ne so, l'abbiamo introdotto, la serie di furie, la serie di furie di fatto, l'abbiamo richiamata a memoria nell'ultima lezione, la funzione rappresenta questo, rappresenta la possibilità di approssimare una qualunque funzione con una somma potenzialmente infinita di termini di oscillazioni sinusoidali, se la funzione è una funzione periodica. Se la funzione non è periodica non avremo una sommatoria, ma avremo, quindi non avremo più la serie di furie, ma abbiamo quella che si chiama trasformata di furie, ma di fatto è l'estensione di questo concetto. Ma se pensate anche allo sviluppo in serie di Taylor di una funzione continua, lì rappresentate la funzione in un singolo punto, e lo potete fare per ogni punto dove la funzione è continua, come la somma di tanti termini. E più termini aggiungete, più siete precisi, perché l'errore si riduce. Aggiungete un termine lineare, uno quadratico, uno cubico, eccetera. e infatti questo è un qualcosa che teorema di approssimazione con il quale i matematici hanno avuto a che fare da diverso tempo. Adesso vi dirò tra poco uno dei primi risultati, infatti che abbiamo a disposizione, proprio per una categoria di approssimatori di cui vi parlo tra poco, è un risultato noto in matematica dalla seconda metà dell'Ottocento. Detto questo, nel machine learning, le categorie di approssimatori universali che noi andremo a prendere in esame, in realtà poi ne approfondiremo uno solo, però intanto su cui diremo qualcosa, per tutta una serie di motivi, perché poi fare machine learning significa andare a prendere queste funzioni e andare a ricavare i parametri che queste funzioni utilizzano. Quindi diciamo ci sono anche delle problematiche tecniche, algoritmiche per cui non ci si occupa in generale di tutte le possibili funzioni che rappresentano gli approssimatori universali, ma ci si occupa di quelli che poi hanno una ricaduta in termini pratici sui modelli che possiamo costruire. E le grandi categorie comunque che hanno significato il machine learning per costruire questi approssimatori universali sono tre fondamentalmente. E vengono suddivise appunto, secondo questa diciamo ampia categorizzazione, in approssimatori cosiddetti in forma fissa, adesso poi vi dirò qualcosa di più di che cosa sono. Per intenderci qua dentro ci sono polinomi, sviluppi in serie di furie e cose di questo genere, reti neurali e alberi. Alberi di decisione, se qualcuno li ha mai sentiti nominati. Alberi di decisione sono degli approssimatori universali. Ok. Per ognuna di queste tre categorie, quello che si può dimostrare sono dei teoremi matematici, per cui questi sono approssimatori universali, cioè potenzialmente vi permettono di esprimere una qualunque funzione arbitraria, non lineare, tramite una combinazione di termini, di unità, che appartengono a una di queste categorie. Va bene? Ok. Partiamo dai primi. Allora, qui abbiamo un esempio di quelli che vengono chiamati appunto anche approssimatori fixed shape, vuol dire forma fissa, perché si chiamano fixed shape, perché hanno, diciamo, non hanno parametri interni, ok? Quindi la loro forma è sempre questa. Se voi prendete, questo è l'esempio in una dimensione, ok? Unidimensionale, qui poi andiamo a vedere in due dimensioni, quindi qui significa abbiamo una feature sola, qui due feature, ok? Come sempre la visualizzazione in bassa dimensionalità ci aiuta a capire di cosa stiamo parlando, però ovviamente l'estensione a più dimensioni, quella poi spesso e volentieri più significativa nella pratica, come vi ho detto più volte, machine learning lavora ad alta dimensionalità. Allora, se vi dicevo, si chiamano in forma fissa, perché di fatto non avendo parametri interni, se io prendo, ecco, f di x uguale a x, che è una prima possibile unità, questi sono tanti potenziali blocchetti, no? Di quelle funzioni che potete andare a combinare, no? Avete w1, f1 di x, più w2, f2 di x, no? Vi ricordate quella combinazione lineare di tante unità? Ognuna di quelle unità può avere questo, questo, questo, oppure questo. In questo caso ci siamo fermati a un grado d, che è uguale a 4, ma d può essere chiaramente arbitrario. E questo vale per una dimensione. quello che ottenete è ovviamente, quindi in generale, f di x in una dimensione è uguale a x elevato alla d. Ok? Mettendo insieme tante funzioni di questo tipo, quante ne dobbiamo mettere insieme? Non lo sappiamo, però potenzialmente riuscite ad approssimare, dipende dalla funzione, una funzione arbitraria. La generalizzazione di questo concetto, questo è un polinomio chiaramente, questa è una tipologia, sono i polinomi. I polinomi sono chiaramente una tipologia di approssimatori in forma fisso, sono questi che stiamo descrivendo, non sono gli unici. Come vi dicevo prima, anche i componenti delle serie di Fourier, in cui il singolo elemento è il seno di x, è un'oscillazione sinusoidale, anche quelli possono essere interpretati alla luce degli approssimatori, cosiddetti in forma fissa. Però noi adesso vediamo solamente i polinomi. L'estensione a due, nel caso bidimensionale, è questa. Qui abbiamo un esempio di una funzione di questo tipo, in cui prendiamo in considerazione solamente una delle due variabili, questo chiaramente è un piano. Se abbiamo la prima variabile moltiplicata per l'altra al quadrato, abbiamo una superficie di questo genere. Questi sono i termini di primo grado che vengono combinati tra di loro. Vedete, sono alcune delle combinazioni che voi potete ammettere. Qui abbiamo x1 al quadrato per x2 alla terza. In generale, se voi avete due variabili, potete concepire ognuno di quei singoli blocchi. Qui scrivo f con indice b per indicare che questa è una di quelle b grandi, no? Vi ricordate che abbiamo uno, due, fino a b funzioni che andiamo a combinare. Ognuna di queste, qui andiamo a prendere la b esima, ok? La b esima e la andiamo a scrivere qui dentro. Va bene? In generale, se abbiamo un'unità di tipo polinomiale, come queste, un'unità di tipo polinomiale, in due variabili, può essere descritta in questo modo. x1 elevato alla f per x2 elevato alla q. Quindi qualunque combinazione valida con p e q tali che maggiori o uguali di 0 e tali che p più q è minore o uguale di d. Cioè, voi fissate il grado del polinomio e qualunque combinazione la cui summa è minore o uguale del grado del polinomio di queste due variabili è un possibile termine. Ovviamente, se aumenta la dimensionalità, aumenta il numero dei possibili termini, quindi avete chiaramente una, in realtà poi un'esplosione combinatoria di possibili di possibili unità che potete andare a scegliere. come? È il grado del polinomio. Voi potete fissarlo ed è un'altra, è un'altra, un altro ulteriore parametro che avete a disposizione. La cosa interessante è che questi, i polinomi sono stati dimostrati essere approssimatori universali. Nel 1885 c'è un teorema si chiama, in matematica, il teorema di Ston-Weierstrass. Il teorema di Ston-Weierstrass dice che voi potete prendere una combinazione di polinomi costruiti così e ci potete approssimare qualunque funzione che abbia ovviamente certe caratteristiche di regolarità, però insomma, di fatto è questo. E non sono gli unici. Ripeto, sono anche per esempio le serie di Fourier possono rientrare in questa categoria di approssimatori e i polinomi e anche le serie di Fourier danno luogo poi a un ramo del machine learning, forse qualcuno l'ha sentito nominare qualche volta, sono le cosiddette kernel machine. Tutta la teoria delle macchine kernel che diciamo negli anni soprattutto intorno ai primi anni 2000 rappresentavano un po' uno stato dell'arte in diverse applicazioni di machine learning derivano da qui fondamentalmente. Cioè utilizzano questo concetto di approssimazione per costruire dei classificatori o regressioni che regressori che lavorano con questi tipi di di unità. Cioè ognuno di questi è una unità di questo tipo, ognuno di questi contributi di combinazione, di questi blocchi perché qui ragioniamo andando a identificare ognuna di queste, ok? E ognuna di queste in questo caso ha una struttura di questo tipo, se siamo in due dimensioni, se siamo in tre o in quattro o in cinque c'è la generalizzazione di questo concetto. Noi, diciamo, adesso faremo qualche esempio in cui vedremo qualcosa di questo, ma non non entreremo poi nel dettaglio di queste unità. Cioè tra quelle, vi anticipo, che tra quelle tre tipologie che vi ho detto prima, che sono tre categorie, macro-categorie di approssimatori universali che sono rispettivamente questi approssimatori in forma fissa, alberi e reti neurali, diciamo, vedremo degli esempi nelle cose che diremo riguardanti tutti e tre, ma poi cercheremo di approfondire solamente uno di questi tre che sono le reti neurali. Per motivi di tempo, anche se sarebbe interessante, ripeto, dire cose su, per esempio, sui kernel, dire anche gli alberi e sono tuttora usati e sono tutti modelli usati, anche qui di nuovo vi invito a non confondere il machine learning con le reti neurali di learning e basta, cioè a non appiattire questa visione. Ci sono delle cose anche al di fuori del di learning che però ovviamente chiaramente ha avuto il successo che ha avuto e quindi ovviamente ha una sua importanza al momento, però non è l'unico, ecco, non è solo quello, c'è anche altro e questi risultati, per esempio, o queste tipologie di approssimatori universali vi rendono conto di questa affermazione. Ci siamo fin qui? Ok. Benissimo, adesso andiamo a vedere cosa succede. invece, cominciamo a dire qualcosa sulle reti neurali che di nuovo sono una famiglia di funzioni che sono parametrizzate in questo caso, cioè hanno dei parametri interni, mentre quelle altre avete visto, abbiamo x, x1, il quadrato, x2, non hanno dei parametri interni, le unità delle reti neurali hanno dei parametri interni, quindi questo che vedete qui, qui cominciamo a parlare, vi ricordo, delle unità, cioè ognuna di queste, di cui adesso cerchiamo di dire qualcosa in più, rappresenta il grafico di una funzione prodotta da una singola unità di rete neurale. Poi dovete sempre tenere presente che voi avete una combinazione lineare di più di queste unità. Ok? Quindi questo di fatto è quello che viene, è il prodotto di quello che chiameremo un singolo neurone, va bene? Una singola unità. Quindi, ve lo scrivo qua. è una di quelle singole unità B, dove B va da 1 fino a B grande. Va bene? Ok. Allora, anche, queste sono funzioni parametrizzate, adesso vi dico qualcosa in più di come possono, qual è la descrizione matematica. Vi anticipo che anche per unità di questo tipo ci sono due teoremi matematici, sono più recenti, non si va più nel 1800, ma si arriva, diciamo, agli anni 80 e 90, ok? Il secolo scorso, quindi stiamo parlando di qualcosa relativamente recente, 40-30 anni fa, ci sono i primi risultati di approssimazione sulle reti neurali, quindi è una garanzia matematica che combinando unità di questo tipo voi riuscite ad approssimare qualunque funzione, non lineare. qui ne abbiamo un esempio in una dimensione e questo in due dimensioni, ma chiaramente c'è poi la generalizzazione a n dimensioni, non dobbiamo dimenticare sempre questa possibilità. In una dimensione, se volessimo scrivere la funzione espressa dall'unità singola biesima, ok, la potremmo scrivere in questa forma, ad esempio, questa è una possibile, ne esistono tante, ok? Tangente iperbolica di wb0 wb0 più wb1 lx, dove wb0 e wb1 sono due parametri. Vi ho detto che sono parametrizzati internamente, questi sono dei parametri interni. Questo è il primo dei due parametri interni, e lo chiamo wb0 perché è relativo a questo valore di b. ognuna di quelle funzioni può avere un differente valore di questo parametro o di quest'altro. Ognuna di queste b. Ecco perché è indicizzato con b. E questi sono due parametri indicizzati in 0 e 1 perché sono due. Ok? E quello che vedete rappresentato qui è il risultato di che cosa? Di quattro possibili istanze, 1, 2, 3 e 4, in cui quello che si è preso si è... Allora, la tangente iperbolica l'abbiamo vista a suo tempo a lezione, no? È una generalizzazione della sigmoide tra meno 1 e più 1 della funzione sigmoidale. E se voi prendete e generate quattro valori casuali, esercizio che potete fare in Python, anche come volete, agevolmente, e prendete dei valori di x e poi andate a costruire, generando a caso questi due parametri, andate a costruire questa espressione, questa combinazione lineare, la fate passare attraverso la tangente iperbolica per diversi valori di x, quello che ottenete è una curva di questo genere. Chiaramente, cambiando questi parametri, in questo caso queste quattro istanze corrispondono a quattro combinazioni diverse di WB0 e WB1. In quest'altro caso è venuta una cosa di questo genere, questa è un'altra opzione, questa è un'altra opzione possibile. Sono stati generati a caso quattro coppie di parametri e i risultati sono questi. Vedete come già l'aggiungere questa parametrizzazione interna cambia tantissimo quello che fa la singola unità. Poi tenete sempre conto che queste le andate a combinare con altre. Quindi è come se io volessi combinare queste quattro, ok, ho a disposizione una certa varietà. che è data proprio dal fatto che i parametri interni cambiano. E vi ricordo che la varietà quanto sono diverse vi permette di aggiungere alla vostra funzione da approssimare delle possibilità di farlo con minori anche termini magari. Vi ricordate quando vi ho detto da cosa dipende? Dipende dal numero di termini e dalla diversità di ogni singolo termine. il fatto di avere questi parametri interni aggiunge molta diversità. Lo vedete qui, questi sono stati generati a caso scegliendo a caso questi quattro parametri. Figuratevi, ne avrei potuti generare a caso altri mille e avrei avuto tante funzioni diverse. Mi siamo? Mi riuscite a seguire? Ok? La tangente iperbolica, sapete cos'è? o tenete presente che poi ne riparleremo quando vedremo le reti neurali, in realtà questa funzione non è l'unica che potete utilizzare. Ci sono altre funzioni, questa è una funzione di attivazione, verrà chiamata, ne riparleremo quando studieremo proprio le reti neurali in maniera un po' più sistematica, un'introduzione, diciamo faremo le reti neurali un po' più sistematica, ma non è l'unica che avete a disposizione. È una funzione non lineare e qui avete una combinazione lineare. la generalizzazione di questo caso a due dimensioni è una funzione che potete scrivere in questo modo. Diciamo, non a due dimensioni, ma questo è uno dimensionale, questo è due dimensionale, adesso vi scrivo n dimensioni la generalizzazione. è sempre la tangente iperbolica di Wb, 0 più Wb, 1 per x1 più Wb 2 per x2 più Wb n per x con n. Vedete che questa che ho appena scritto, scusate, vediamo se riusciamo a tornare. Vi dicevo, se prendiamo, prendiamo questa così si vede meglio. Allora, questa qui è in una dimensione, questa è la generalizzazione a n dimensioni, va bene? Allora, questa generalizzazione vedete che abbiamo che cosa? Abbiamo una combinazione lineare di tutte le fiche per dei pesi opportuni. Ognuna viene combinata con un parametro che è un parametro interno a questa funzione. In più ci aggiungete il termine di bias, questa è una trasformazione affine, questa è una combinazione lineare. I matematici distinguono tra trasformazione affine e combinazione lineare. Rimane il fatto che qui prendete le vostre feature, ok? Le moltiplicate per dei coefficienti e li sommate. Dopodiché fate passare il tutto attraverso una funzione come ad esempio la tangente iperbolica. Questa è la generalizzazione di questo, a n dimensioni e quello che ottenete è appunto come vi dicevo di nuovo un risultato che è un singolo elemento di una più ampia possibilità di combinare tante unità di questo tipo e questo vi permette di costruire un approssimatore universale. In due dimensioni se voi rifate lo stesso gioco che vi ho descritto prima di andare a generare a caso quattro combinazioni diverse di parametri in questo caso in due dimensioni avete da generare non più delle coppie ma una tripla di parametri e cominciate a fare il plot delle superfici che ottenete otterreste ad esempio cose di questo genere. Di nuovo abbiamo una varietà e da questa una varietà che è determinata dalla parametrizzazione interna. Questa varietà è alla base proprio del fatto di poter approssimare qualunque funzione con un opportuno numero di unità di quel tipo. Poi dipende più cresce la dimensione più ovviamente il discorso dell'approssimazione diventa meno semplice legato al fatto che ne dovete mettere di più però insomma chiaramente la strada è questa per comprendere che cosa vi permettono di fare questi strumenti. ci siamo? Questo è abbastanza importante perché ripeto noi non vedremo i risultati di approssimazione universale sono dei teorimi matematici non banali però sono la base teorica dietro la quale si spiega il funzionamento di molte di queste cose perché poi è vero che questi strumenti vengono detti anche spesso e volentieri una sorta di scatola nera un black box ma diciamo non è perché funzionano per magia in realtà c'è dietro una teoria rigorosa c'è tanto ancora anche da capire per spiegare diversi comportamenti di questi di questi sistemi però diciamo una base rigorosa di partenza c'è e soprattutto il black box deriva dal fatto che quando voi avete un input che data una rete neurale e questa ha migliaia milioni di parametri è molto difficile capire perché effettivamente vi dà quella risposta perché quando dovete descrivere il perché con quel dato input avete ottenuto quel dato output con milioni di parametri capite bene che è lì la complessità del problema però in linea di principio se no non ci sarebbe nulla di diciamo di così di oscuro non comprensibile uno descrive una una funzione con un certo numero di parametri il problema è che sono milioni e ha delle garanzie anche di poter approssimare con quel numero di parametri potenzialmente un comportamento arbitrario in termini di qualunque funzione che descrive il vostro problema di regressione o il confine decisionale del vostro classificatore quindi in linea di principio è abbastanza rigoroso il tutto il problema è che quando avete a disposizione tanti di quei parametri non riuscite più perdete di vista quello che effettivamente poi è il comportamento meccanicistico dello strumento che avete costruito però questo diciamo ci porterebbe lontano adesso è un excursus che potete lasciare per il momento da parte abbiamo detto tre categorie vediamo la terza categoria che sono gli alberi gli alberi alberi di decisione anche vengono chiamati gli alberi sono anche qui ci sono dei risultati i primi risultati sulla possibilità di approssimare qualunque funzione con degli alberi che adesso vi dirò che cosa sono comunque risalgono circa gli anni 60 quindi una sessantina di anni fa ok quindi i risultati di approssimazione universale va bene universal all'approximamento allora come sono fatti beh anche qui si fa la prima cosa che si fa presto a descrivere è proprio la singola unità vi ricordo che lo schema è sempre quello noi abbiamo componiamo tante di queste unità di nuovo ne abbiamo una due fino a b grande e la b esima unità ha una struttura che in una dimensione è una cosa di questo genere v1 se x è minore o uguale di un valore di soglia e v2 se x è maggiore dello stesso valore di soglia questa è una funzione molto semplice una funzione da gardino se x è maggiore o uguale di quella di quella soglia allora abbiamo un certo valore se x è minore un altro valore se qualcuno ha un'idea di come funzionano gli alberi di decisione sono proprio fanno una cosa di questo genere prendono una feature guardano se qual è il suo valore e a seconda del valore dicono ok vai a sinistra oppure a destra e scendi nell'albero vengono costruiti in questo modo a partire da queste unità e chiaramente diciamo voi avete ognuno di questi allora vi dico un po' di nomenclatura ve la scrivo qua allora s viene chiamato split point cioè il punto di suddivisione v1 e v2 toglie ok perché è una suddivisione una ramificazione quindi si va verso un po' allora anche qui vedete che è una parametrizzazione interna perché ogni unità può avere un valore di split point diverso e anche dei valori di v1 e v2 diversi e in questo caso abbiamo un esempio di nuovo di quattro unità che sono state generate mantenendo dei valori direi di v1 e v2 che sono gli stessi ma variando gli split point casualmente e vedete avete anche qui diversi profili di comportamento potete generalizzare questa cosa a due dimensioni e diciamo come viene fatto viene scelto uno split point lungo ogni dimensione quindi ogni dimensione ha il suo split point diverso quindi se x1 è minore di s1 allora vale e poi si va a vedere quanto vale x2 se x2 è minore di s2 allora vale quest'altro e così via quindi lo componete in n dimensioni in due e poi anche potete estenderlo a n e qui avete un esempio di cosa succede se cominciate a mescolare queste cose generando a caso i valori appunto di s e in questo caso di s1 e di s2 e anche volendo potete generare a caso v1 e v2 quindi avete diciamo varie opzioni a disposizione di nuovo componendo tante unità di questo genere si può dimostrare che qualunque funzione può essere approssimata in maniera arbitrariamente precisa a patto di disporre di un numero dopo dipende da quale è l'accuratezza che volete raggiungere se volete essere più precisi dovete aggiungere termini unità di questo tipo più unità di questo tipo avete più riuscite a essere precisi gli alberi sono un altro ramo diciamo vi dicevo prima del machine learning in cui si è lavorato si lavora tuttora tantissimo hanno ancora moltissime applicazioni pratiche e per motivi di tempo non possiamo addentrarci però sono uno strumento tuttora molto usato in alcune applicazioni quindi fondamentalmente sì tra l'altro nella loro versione diciamo base nel senso sono proprio alberi di decisione nel senso che vi dicono che ne so diciamo una cosa semplice io debba classificare distinguere in base ai tratti caratteristici non so vediamo cosa ci può venire in mente abbiamo fatto l'esempio dei fiori di iris no nel quando abbiamo fatto l'esercitazione e lì potrebbe essere un primo punto di split e andare a vedere la lunghezza del petalo se la lunghezza del petalo è maggiore di facciamo facciamo un esempio ancora più se la lunghezza adesso vediamo se se mi viene di ricostruirlo diciamo una cosa semplificata se la lunghezza adesso non mi ricordo i valori ma se la lunghezza del petalo è maggiore di so quanto quanto può essere adesso non mi ricordo i valori 0.5 allora vai di qua altrimenti vai di là dopodiché vai qui e vai a vedere dopo che hai visto la lunghezza del petalo prima feature vai a vedere la larghezza del petalo seconda feature se è minore di 0.2 vai a sinistra altrimenti vai a destra e dopodiché vai a vedere magari altre caratteristiche la cosa interessante di un sistema di questo genere è che appunto nel momento in cui si presenta un nuovo input ok qui un input nuovo e seguirà una tra queste possibili strade e quindi avete anche una giustificazione del perché alla fine arrivate a una delle possibili opzioni che sono le foglie di questo albero diciamo si dice anche che è più interpretabile ovviamente rispetto a un'altra un'altra opzione non è l'unica queste sono gli alberi di decisione nella loro variante nella loro scusatemi versione base ci sono anche tante varianti in cui per vari motivi si prendono più di per esempio più di questi alberi messi insieme allora si parla di foreste di alberi quindi se qualcuno ha mai sentito nominare per esempio i random forest trees che sono uno strumento molto utilizzato ha delle prestazioni in termini di classificazione e regressione diciamo confrontabili per esempio con le reti neurali in tutta una serie di applicazioni che sono input di tipo tabulare per esempio cioè laddove avete dei dati con delle feature ben precise per esempio non so se devo elaborare delle caratteristiche in ambito pazienti per cui ho pressione sanguigna diciamo ho preso il mondo biomedicale ma non solo potrei prendere il mondo per esempio dei cosiddetti dati del censo quindi cose in cui avete delle tabelle altezza peso età avete la pressione sanguigna avete il reddito medio annuo avete il numero di metri quadri della superficie di un appartamento questi sono i dati di tipo tabulare che vengono distinti dai dati che sono dati tipo per esempio una sequenza un segnale un'immagine che hanno una struttura molto diversa non sono tabulari ecco nei dati di tipo non tabulare sequenziali quindi audio immagini testo le reti neurali hanno delle performance che sono difficilmente battibili oggigiorno ma in dati di tipo tabulare gli alberi per esempio rappresentano in qualcosa che se la gioca la pare con vantaggi in altri termini vabbè adesso ci porterebbe lontano questo discorso di tutto questo il messaggio è di nuovo sì molto molto bello oggigiorno sentire parlare di queste cose mirabolanti che fanno queste reti neurali non sono l'unico strumento a disposizione di un progettista di sistemi di machine learning come sempre bisogna sapere adattare l'obiettivo l'applicazione o meglio data l'obiettivo l'applicazione bisogna sapere adattare quelli che sono gli strumenti a disposizione e scegliere quelli giusti ok quindi allora noi dobbiamo costruire un modello di approssimatore universale che ha una struttura di questo tipo ok la struttura è questa combinazione di tanti blocchi di questo tipo ognuno di questi è un'unità di quelle che abbiamo descritto prima quindi se sono reti neurali ognuna di queste è tangente iperbolica della combinazione lineare delle feature di input se è un albero ognuna di queste è un'unità di quella che vi ho appena descritto con lo split e le foglie se è un polinomio è un polinomio di grado di un certo grado allora uno potrebbe dire ok potremmo anche mescolare le cose cioè potrei prendere questa è un'unità di rete neurale questa è un'unità di albero questa è un'unità di polinomio sì volendo in teoria uno lo potrebbe fare però solitamente non si fa ne si mescola per motivi di che cosa beh per il fatto che comunque già sono strumenti che diventano complessi perlomeno in questo modo tenendoli diciamo omogenei si riesce a tenere sotto controllo tutta una serie di questioni tecniche che ognuno di questi si porta dietro in particolare qui vi ho riassunto alcune caratteristiche generali se voi avete a che fare con gli approssimatori in forma fissa lì avete dei problemi cosiddetti di scala cosa vuol dire vi ho detto prima se fissate il grado del polinomio al crescere della dimensione che cosa succede avete tantissime possibilità no quali polinomi potete avete visto già solo in due dimensioni se fissate un certo grado avete tante possibili unità e questo quindi rappresenta un problema il problema man mano che cresce la dimensione il grado del polinomio è l'esplosione del numero di potenziali termini che avete a disposizione e tenendoli tutti uguali perlomeno riusciamo un po' a circoscrivere il problema ci sono problemi di ottimizzazione problemi di ottimizzazione soprattutto ecco le reti neurali hanno una caratteristica questo ne riparleremo le funzioni di costo che voi riuscite a costruire su una qualunque rete neurale perché poi l'obiettivo cos'è una volta che avete il vostro modello cosa dovete fare come sempre costruire la funzione di costo per andare a ricavare i parametri di quel modello che sono parametri questi esterni ma anche tutti quelli interni a ogni singola unità allora le funzioni di costo delle reti neurali sono praticamente sempre non convesse ma non convesse con una varietà enorme quindi diciamo questo è un problema non da poco se voi cominciate a mescolare un po' di reti neurali con qualche unità di albero con qualche unità non se ne esce è un ginepraio che decisamente vorremmo evitare per cui solitamente diciamo ha senso studiare termini omogenei di questo questo tipo perlomeno fino ad oggi poi magari qualcuno se ne esce domani con una qualche invenzione di altro tipo e va bene prenderemo studieremo anche quella eventualmente gli alberi hanno il problema che cosa avete visto quei profili di diciamo che descrivono la la funzione che definisce una singola unità no sono dei profili a scalino e quindi danno luogo a delle funzioni di tipo discreto le funzioni di discreto anche lì si portano dietro un po' di problematiche nella gestione quindi sempre meglio tenerle tutte omogenee quindi questo per dirvi che il modello che costruiamo è un modello in cui non mescoliamo tipicamente unità di diverso tipo bene ci sono domande? ok allora se vi ricordate siamo partiti nell'ultima lezione a parlare appunto di non linearità e di apprendimento introducendo quella metafora no di quel disco di quel selettore di complessità del modello vi ricordate dicendo che il modello la complessità di un modello è qualcosa che ci dà una misura della sua capacità di adattarsi e di approssimare meglio adesso possiamo affinare un po' il discorso quelle che sono appunto le caratteristiche di una funzione che descrive appunto ad esempio il confine decisionale oppure che descrive un profilo di regressione ecco adesso andiamo a per motivi che pian piano spero si chiariscano a descrivere meglio questo questo selettore della complessità questa sorta di ideale disco no il quale noi andiamo via via a costruire potenzialmente dei modelli via via più complessi e lo suddividiamo in due aspetti perché in realtà la cosa che vedrete che sarà sarà chiara dagli esempi che cominciamo sui quali cominciamo a ragionare è che abbiamo due leve fondamentalmente per giocare sulla complessità e queste due leve sono una quella che viene chiamata capacità del modello che cos'è la capacità del modello ma la capacità del modello effettivamente è legata per esempio al numero di parametri quanti quante unità io vado a mettere ad esempio di reti neurali o di alberi quel valore di b b grande ok e più alto e b più grande la capacità del modello come yes sure esattamente questo e più però non è l'unica leva che avete a disposizione se ci pensate bene abbiamo a disposizione anche l'ottimizzazione cioè io ho fissato il modello posso decidere di fermarmi per esempio se uso la discesa del gradiente a 100 iterazioni epoche ad esempio se a destra invece una rete neurale 200 500 o 1000 e più vado avanti più vado auspicabilmente a minimizzare la mia funzione di costo e produco un modello più accurato allora queste due leve insieme determinano la complessità come l'abbiamo detta del nostro modello quindi capacità più ottimizzazione ok adesso qui cominciamo tenendo presente questo a fare qualche ragionamento in cui andiamo a vedere che cosa succede quando andiamo a variare la complessità del modello ok con degli esempi partendo da quello che era il caso diciamo un'approssimazione di quello che è il dataset perfetto il dataset perfetto abbiamo detto è infiniti punti e privo di rumore quindi no rumore e tantissimi punti ovviamente tanti punti non li possiamo presuppongono infinite capacità computazionali quindi qui cominciamo ad andare verso il mondo reale quindi questi vengono chiamati anche dati near perfect quasi perfetti quindi diciamo prendiamo tanti punti in questo caso 10.000 punti un problema di regressione prendiamo 10.000 punti generati supponiamo che siano il processo che ha generato questi punti sia un'onda sinusoidale e quindi noi andiamo a prendere tantissimi punti secondo questo andamento che è quello di un'onda sinusoidale va bene dopodiché andiamo a costruire una serie di modelli e andiamo a vedere come quei modelli riescono ad approssimare questo andamento sinusoidale in questo caso avete lungo le colonne vedete tanti possibili modelli quindi il primo è costruito con unità di tipo polinomiale la seconda colonna sono unità di tipo rete neurale e il terzo sono alberi va bene? che cosa differenzia le righe? è che qui avete scelto una sola unità di tipo polinomiale, rete, neurale o albero qui ne scegliete tre e qui ne scegliete cento dopodiché quello che viene fatto è fare l'addestramento alla massima alla massima possibilità cioè diciamo supponiamo di andare a ottimizzare ad esempio con un metodo come può essere una discesa del gradiente meglio che possiamo ok? fino a un numero sufficiente di di iterazioni che diciamo che ci permette di andare a trovare il minimo di quella funzione quindi ipotizziamo di fare una cosa del genere e se cominciate a costruire una cosa di questo genere dei grafici di questo genere vi accorgete che ad esempio se vedete qui un'unità di tipo polinomiale se avete una sola unità qui siamo in una dimensione il problema è unidimensionale e avete il modello più semplice che è chiaramente un modello lineare che è quello che abbiamo studiato partendo dal problema di regressione che chiaramente si adatta male a un andamento di tipo sinusidale ma vedete che anche una rete di tipo rete neurale non si adatta molto bene e chiaramente anche l'albero ha un andamento di questo tipo scalino non spiega molto bene se però cominciate ad aumentare il numero di di unità quindi diciamo l'ottimizzazione la teniamo fissata la facciamo meglio che possiamo cominciamo ad aumentare il numero di unità a disposizione che significa che giochiamo sul prima di quelle due di quelle due leve che vi dicevo prima e aumentiamo la complessità del nostro modello aumentando la capacità vedete che già con tre solo unità tipo colinomiali guardate che cosa questa curva che qui è disegnata in blu che vi ho appena ripassato sopra in arancione è il risultato del fitting e già è un fitting pressoché perfetto se vedete gli alberi fanno un po' più fatica ancora perché hanno questi andamenti scalini di scritto per cui dobbiamo andare a aumentare il numero di unità ma se arrivate a 100 unità vedete che tutti e tre riescono ad approssimare perfettamente questo andamento a partire da un dataset che tenete a mente questa cosa quasi perfetta cioè avete molti punti e non sono rumorosi ok allora I'll I'll repeat for the recording and then I will also translate allora la domanda è dove prendiamo il dataset è un dataset come facciamo a pulirlo la risposta I'll say before in Italian and then I'll answer in English la risposta è ovviamente qui stiamo ipotizzando di avere un dataset pulito chiaramente non ce l'abbiamo sempre quindi o lo riusciamo a pulire noi però questo è per farvi capire che cosa che cosa succede quando passeremo da near perfect o perfect a reale so the answer is of course we assume here the existence of a dataset which is perfect or near perfect we don't have of course in real world the possibility of having a dataset perfect or near perfect but this is to introduce what will happen and we will see in the next slide when we move from this case of near perfect data to a real data ok so just wait for a few slides and you'll have the complete answer ci siamo fin qui ok ok qui è la stessa cosa con un problema di classificazione un problema di classificazione bidimensionale in cui abbiamo questi sono i dati vi ricordate perfetti no in cui abbiamo infiniti punti senza errore e e qui abbiamo quattro tipologie allora sulle varie colonne qui adesso vi dico che cosa c'è allora queste sono unità di tipo polinomiale ok e abbiamo chiaramente quattro ogni colonna corrisponde a un problema di classificazione diverso questo è sono linearmente separabili se partiamo dalla prima colonna abbiamo che questi sono un dataset perfetto vedete infiniti punti separabile linearmente questo è un dataset diciamo perfetto in cui avete un profilo non lineare un altro profilo non lineare di diverso tipo fino alla quarta colonna qui che cosa succede qui avete la rappresentazione questa è la vista diciamo che abbiamo chiamato di percetrone questa è la vista cosiddetta di regressione in cui avete l'approssimazione a 10.000 punti di questo problema qua per cui avete una cosa di questo tipo qui avete lo scalino in due dimensioni questi sono tutti i punti della classe meno uno tutti i punti della classe più quindi è l'approssimazione al caso di nuovo del dato perfetto e questo rappresenta l'equivalente di questo in cui avete questi 10.000 punti e qui abbiamo questo che rappresenta questo visto da sopra e qui abbiamo questo e questo qui visto sempre questo qui è la vista da sopra di questo ok ci siamo fin qui se io guardo da sopra ottengo queste qui va bene ok qui è che cosa è che cosa è rappresentato nella terza e nella quarta riga nella terza e nella quarta riga sono rappresentate il fitting con 30 unità di tipo polinomiale ok b grande quindi è 30 che cosa cambia da qui a qua è semplicemente che qua avete una funzione di tipo ls e qua least square quindi minimi quadrati e qua di tipo softmax l'abbiamo visto abbiamo visto che cosa sono quindi cambia solo la funzione di costo e con 30 unità di tipo polinomiale vedete che riuscite a fare un ottimo lavoro perché guardate questo qui in particolare con il softmax vedete come riesce ad approssimare bene questo tipo di profilo ma anche questo guardate qui si vede un po' meno bene questo decisamente fa un ottimo lavoro ma anche questo e anche questo quindi con sole 30 unità di tipo polinomiale riuscite a riuscite a ottenere quel tipo di comportamento ok adesso fin qui tutto bene se i dati sono perfetti o quasi ne abbiamo tanti e non c'è rumore riusciamo qual è la caratteristica torno un attimo indietro alla quella al grafico precedente che si vedeva ancora meglio qual è la caratteristica il messaggio è che se ho dati perfetti o quasi perfetti aumentando la capacità del modello quindi la sua complessità aumento la precisione della prossima attesa però i dati sono perfetti o quasi perfetti se sono reali rimane vero sì ma emerge un altro fenomeno quindi la risposta in realtà è sì ma dipende da cosa stiamo guardando adesso cerchiamo di capirlo meglio ok come facciamo a vedere ricondurci a un caso reale beh facciamo un esperimento controllato supponiamo di avere la nostra silusoide che rappresenta la distribuzione dei dati quindi conosciamo il fenomeno che ha generato questi dati è una silusoide cominciamo a anziché averne a disposizione diecimila ne estraiamo un certo numero quindi per esempio in questo caso ne sono estratti ne sono stati estratti 99 ne seleziono 99 e poi ognuno di quelli ci aggiungo del rumore casuale quindi ci sommo un valore numerico casuale quindi per esempio una gaussiana una gaussiana ok quindi ne prendo 99 ci aggiungo del rumore e ottengo questa cosa qua a ognuno di quei punti ho aggiunto un po' di rumore quindi questo viene spostato magari più in su questo viene spostato più in giù questo viene spostato in qua eccetera bene ci siamo adesso vediamo cosa succede quando lavoriamo con questo problema quindi partiamo da casi questo rappresenta un case study un un un un diciamo di esempio per un problema di regressione andiamo a vedere cosa succede se cominciamo ad approssimare questo dataset con i nostri strumenti abbiamo capito che ci sono questi approssimatori universali uno dice bene bene anche questo cominciamo a vedere se comincio cosa succede se comincio ad aumentare la capacità allora qui vedete si legge avete sulla prima riga il grafico diciamo ci sono si legge abbastanza bene perché c'è scritto tra l'altro cosa rappresentano le varie righe unità di tipo polinomiale reti neurali o alberi e qui quello che cambia è la complessità del modello tre unità partiamo da un'unità tre unità dieci unità venti unità allora cominciamo ad analizzarli uno per uno e vediamo che cosa succede allora supponiamo di partire dalle unità di tipo polinomiale se ho una sola unità questa è la nostra solita retta poi se ne metto tre vedete che quello che ottengo è una cosa di questo tipo un andamento di questo tipo che diciamo è quello che effettivamente noi sappiamo corrisponde alla distribuzione che ha generato quel tipo di dati poi cosa succede se aumento il valore di b cioè aumento la complessità del modello vedete che ottengo delle curve che chiaramente vanno a rappresentare un miglior fitting del dataset di addestramento però vedete cominciano a oscillare in maniera direi estrema come nel caso di ugualamente e la stessa cosa accade se voi andate a prendere delle unità di reti neurali partite da un modello molto semplice che non rappresenta il fitting corretto arrivate a un punto in cui avete un buon fitting aumentate ancora la capacità e a differenza del caso dei dati perfetti o quasi perfetti e qui succede che comincia a oscillare stessa cosa se fate con con gli alberi si vede un po' di meno qui vedete comincia perché ha delle caratteristiche diverse qui in realtà è un punto in cui riesce bene a a descrivere questo dataset ma se voi andaste avanti se voi aveste qui per esempio b uguale a 30 50 vedreste che anche per il caso degli alberi oltre una certa soglia lui comincia ad avere questo comportamento oscillante che cosa significa questo secondo voi qualcun altro esatto lo abbiamo anticipato in qualche modo un paio di lezioni fa il modello è forzato proprio a seguire che cosa aumentando la capacità lui cerca di minimizzare che cosa l'errore che commette nella ricostruzione e quindi è forzato a fare un fitting perfetto di questi punti ma nel fare il fitting perfetto lui lo fa non sapendo che lì cede il rumore questo significa che per seguire esattamente il profilo della funzione lui comincia ad oscillare e quindi va in quel regime che viene chiamato di overfitting l'overfitting significa proprio questo cioè che se io vado poniamo di avere questo modello e questo sia il nostro modello a questo punto che mandiamo in produzione se gli arriva un punto ad esempio qui ok questo punto qua x e gli domandiamo qual è il valore secondo il nostro modello di x lui va qui e dice ah beh siamo magari con un cino e sbaglia sbaglia perché perché abbiamo costruito un modello troppo complesso rispetto a quelle che sono i dati a nostra disposizione che sono finiti e rumorosi e questa è una tendenza che c'è indipendentemente dal tipo di approssimatore usato cioè tutte le le tipologie di approssimatori che sono approssimatori universali quando hanno a che fare con dati di tipo reale hanno questo problema finché stiamo al di sotto di una certa zona siamo in underfitting cioè la complessità del modello è troppo poca cioè qui un modello lineare non spiega bene questo andamento sinusoidale poi arriviamo a un punto in cui siamo nella regione giusta di fitting corretto ma se continuiamo ad aumentare la complessità la complessità aumenta oltre a un certo valore si va in un regime di overfitting cioè cominciamo a pretendere di schiacciare il nostro modello di approssimare con il nostro modello qualunque minima variazione del dataset che è un dataset reale che contiene del rumore che ha un numero di punti discreto finito e questo fa sì che io vado a perdere che cosa di vista quello che è il comportamento reale e se gli do un punto nuovo che lui non ha mai visto probabilmente mi spara un qualche valore di uscita che è totalmente slegato da quello che è l'effettivo l'effettiva dinamica che ha generato il processo che ha generato quella distribuzione di punti questo significa che il modello che è in overfitting non ha come anche quello che è in underfitting capacità di generalizzazione la generalizzazione è la capacità di avere un punto una volta che avete addestrato il modello che lui non ha mai visto e di rispondere in maniera corretta rispetto a quel punto la risposta allora è chiaro che se io usassi questo modello qui lui mi direbbe questo valore questo sarebbe corretto ma se io uso quest'altro lui mi dice questo o quest'altro qua su in cima non generalizza bene qui qui si perché è proprio questo il punto se io vado allora finché sono qui io ho un certo un certo errore che è l'errore ad esempio errore quadratico quindi vado a vedere qual è la distanza dei punti dalla curva verde e li sommo questa è la mia funzione di costo se io passo da 3 a 10 la mia funzione di costo è più bassa perché lui comincia a fare queste oscillazioni per assecondare fino a che non arriva addirittura a D uguale a 20 vedi passa proprio per i punti quindi lui qui riduce di più l'errore che è la funzione determinata dalla funzione di costo ma per fare questo ovviamente comincia a passare qui qui questo significa che tipicamente nel passare qui lui che cosa succede lui vede questo tipo di scarto per andare da qua a qua e lui non sa che in realtà questo è quello che c'è sotto e comincia a costruire un modello che magari invece fa così per passare da questo punto a questo a quest'altro e quindi tipicamente comincia ad oscillare così ma questo è una perdita di generalizzazione perché qua non ci sarà mai nulla che arriva qua giù in fondo è questo il punto per cui se voi andate a fare dei test di questo genere vi accorgete che proprio quei modelli sono molto corretti nell'approssimare vedete questi punti però per fare questo dicono ok questo punto qui l'ho preso ma se tu me ne dai uno che sta appena di qua allora vuol dire che sta qua giù secondo me ma non è così e questo lo vedete sicuramente a un certo punto magari qualcuno ce l'avrà più di qua più in là cioè per valori diversi di B come per esempio ecco gli alberi ma anche all'interno delle reti neurali o secondo il problema arriva a un punto oltrepassato il quale voi questo fenomeno si innesca quasi inevitabilmente ed è legato al fatto quindi la risposta alla domanda è perché sorge perché lui cerca di approssimare perfettamente e nell'approssimazione perfetta siccome il dato è rumoroso lui comincia a saltare da un punto all'altro e quindi costruisce queste curve che non solo è rumoroso ma ce n'ha un numero discreto non ce n'ha tantissimi qui in mezzo per cui lui può a quel punto assecondarli in maniera molto più fine deriva da quello dal fatto che abbiamo una discretizzazione lungo quest'asse il numero di punti di addestramento è finito meno ne ho più problemi di overfitting posso avere o meglio si presentano prima rispetto a quando vado a girare la manopola della complessità e più rumore ho stessa cosa peggio sono messo la stessa cosa ce l'abbiamo nel caso del qui abbiamo scusate prima di vedere la classificazione vi faccio vedere cosa succede sempre con questi esempi con la regressione tornate qui vi dicevo torniamo all'esempio sempre della regressione vi faccio vedere qualcosa che è legato forse anche all'ultima domanda che è stata fatta allora qui abbiamo l'unità di tipo polinomiale quindi si va a vedere che cosa succede con quel dataset quando abbiamo una, tre, dieci o venti ok? e questo è quello che abbiamo sostanzialmente visto prima vi faccio vedere una cosa interessante che è questa se voi andate a fare il plot della funzione di costo nel primo caso vi fermate qua cioè ha un certo valore la minimizzate supponiamo di ottimizzare il modello quello che ottenete è questo se voi aumentate il numero di unità polinomiali la funzione di costo diventa più bassa quello che vi dicevo prima cioè lui riesce a avvicinarsi meglio a quell'insieme di punti se voi aumentate ancora lui continua ad abbassare ulteriormente la funzione di costo cioè voi la ottimizzate meglio che potete e aumentando il numero di unità chiaramente abbasserete sempre la funzione di costo cioè riuscite a fare un fitting praticamente perfetto il problema è che arrivare a fare un fitting praticamente perfetto non significa necessariamente avere la capacità di generalizzare correttamente perché? perché appunto nel momento in cui io produco un modello di questo tipo arrivando a un modello di questo tipo che cosa succede? succede che nel momento in cui mi viene chiesto viene chiesto al modello quanto vale questo l'output del modello ad esempio in questo valore di x con 0 lui mi dirà se va bene questo ma questo è molto lontano dal modello reale perché mi dice questo? perché ha costruito una sua idea della funzione anche se la funzione non la restituisce esplicitamente che è questa che avete che è visualizzata in verde e questa idea della funzione produce sì una funzione di costo minore ma di fatto produce anche una perdita della capacità di generalizzazione quindi la prima conclusione a cui arriviamo è 1 l'overfitting significa ridotta generalizzazione 2 la funzione di costo non rappresenta il problema in questione cosa vuol dire che non rappresenta il problema cioè voi monitorando la funzione di costo o monitorando il numero di errori che fate se parliamo di classificazione non vi accorgete di essere andati in overfitting ecco perché forse non è così neanche banale capire perché anche voi non ve ne accorgete voi avete sotto controllo la funzione che dovete minimizzare o andate a vedere il numero di errori non ve ne accorgete perché voi avete il vostro dataset di addestramento e gli state dicendo al sistema cerca di fare meglio che puoi su quel dataset ci siamo? stessa cosa accade se avete un problema di classificazione supponiamo di partire da questo dataset e da questo dataset estraiamo un certo numero di punti che sono prima quanto vi avevo detto che erano? no questi quanti avevo detto che erano? in realtà vi ho detto 99 o 21? 21 questi erano non cambia niente giusto per farvi capire perché sono non mi ricordo questi erano 21 punti che erano stati estratti non so se prima abbiamo detto 21 99 perché 99 è giusto per dirvi come è stato costruito l'esempio perché un esempio reale sono questi invece questi sono 99 ok l'esempio di prima erano 21 punti di regressione questi sono praticamente quasi 100 punti che vengono costruiti a partire da questo dataset ok e poi su questi 21 che avete selezionato scusate su questi 99 di nuovo sto scambiando ma capite bene che non cambia nulla su questi 99 ce ne sono 5 su 99 5 su 99 a cui è stato invertito il la label l'etichetta quindi questi erano di classe blu gliel'abbiamo invertito sono diventati di classe rossa e diciamo solo questi forse ce n'era qui uno che era di classe rossa e diventato blu ok quindi anzi no c'è anche questo che era di classe blu diventato no questi sono di classe blu sono diventati rossi invece che erano rossi sono diventati blu c'è questo qui ecco va bene quindi abbiamo introdotto del rumore e abbiamo un qualcosa ne abbiamo selezionato una manciata di questi punti quindi molti di meno di quelli che avevamo a disposizione quindi dataset reale cosa succede nella classificazione una cosa del tutto analoga al problema della regressione proviamo a vedere perché e poi qui ci fermiamo questo è interessante perché se voi vedete quello che succede è allora abbiamo prima riga le unità di tipo polinomiale seconda riga di questo di questa figura unità di tipo rete neurale terza riga alberi ok allora prendiamo le unità di tipo polinomiale abbiamo nel primo nel primo esempio il risultato è una sola classe praticamente la risposta del classificatore è tutti di classe blu e quindi è un disastro stessa cosa con due unità di rete neurale stessa cosa con due alberi se passiamo a cinque unità di rete di tipo polinomiale vedete che quello che riusci a fare è riesci a costruire questo bel profilo di di confine decisionale e anche le tre reti neurali o le cinque unità di alberi vedete che riescono a isolare abbastanza bene una classe dall'altra ok quindi cominciano a costruire un un classificatore che è in grado bene di spiegare quella che è la distribuzione dei punti che c'è dietro e a classificare correttamente diciamo sbaglieranno alcuni punti che sono questi ad esempio ok ma ma questi li sbagliano perché perché in realtà sono punti che sono affetti da rumore cioè l'errore nel dataset non è nel modello esattamente che cosa succede se aumento la capacità del modello e quindi la complessità succede che nei dati perfetti io riesco ad approssimare meglio il e quindi quella che la distribuzione il processo che ha generato quell'insieme di punti del mio dataset lo riesco a catturare meglio come idea ma il problema nei dataset reali è che ce ne ho un numero discreto e sono affetti da rumore è aumentato la complessità quindi portandola a 9 unità polinomiale o 5 di rete in orale o 9 guardate che cosa succede succede una cosa di questo tipo compaiono vedete queste isole diciamo qui in questo caso gli alberi sono un pochino più resistenti però già quando passate a 17 vedete questa forma particolare qui compaiono delle isole che sono questa questa questa qui e che cosa significa significa che vedete lui per prendiamo ad esempio il caso delle reti neurali vedete che lui per cercare di inglobare questo punto rosso lui cerca comincia a costruire un confine decisionale che è questo quindi lui dirà a questo punto tutti i punti che cadono qui sono rossi allo stesso modo se aumentate la complessità cercherà di fare la stessa cosa con questo punto qua tutti i punti che cadono qui sono rossi perché perché la sua idea tra virgolette del mondo è questa lì ci sono dei punti rossi e con gli strumenti che a disposizione comincia a cercare di approssimare questo concetto la stessa cosa vedete con i polinomi o anche con gli alberi ma questo corrisponde al vero no noi lo sappiamo qual è il vero perché l'abbiamo costruito noi l'esempio il vero è questo perché perché qui c'è del rumore ed è quel rumore e il fatto anche che ne abbiamo pochi di questi punti perché se ne avessimo molti e qui intorno fossero tutti blu lui non costruirebbe questa il problema è che ne ha pochi e sono rumorosi e allora comincia a fare una cosa di questo genere e prima o poi quindi compare il fenomeno dell'overseed lui comincia a minimizzare che cosa il tasso d'errore lui vuole abbassare il tasso d'errore vuole abbassare quella funzione di costo che è un proxy del tasso d'errore e comincia a costruire queste cose strambe strane che in realtà non non ci fanno gioco perché non ci fanno gioco perché si arriva un punto incognito ok lo coloriamo di nero che sta qui su questo modello ok e il nostro classificatore al nostro classificatore scusate non lo facciamo e al nostro classificatore domandiamo qual è la classe di questo modello lui risponderà rossa ma sbaglia sbaglia perché non sa generalizzare qui invece se gli facciamo la stessa domanda lui cosa dice blu riesce a generalizzare più correttamente esatto quindi di nuovo underfitting a sinistra overfitting a destra nel mezzo c'è il giusto e questo vale per ogni tipologia di approssimatore universale che voi potete andare a considerare a prendere in considerazione è trasversale e è legata al fatto di avere del rumore e pochi punti quindi è un problema che potete mitigare se aumentate il numero di punti del vostro dataset ecco perché è importante avere molti punti nel dataset per esempio le reti neurali tipicamente hanno bisogno di tantissimi punti dataset molto pesanti per addestrare perché vanno molto facilmente in overfitting mediamente e voi la necessità ecco quindi di avere dataset sempre più sempre più ricchi ma possibilmente anche non rumorosi e comunque da un certo punto in avanti il problema dell'overfitting ce l'avete se voi avete molti parametri rispetto al numero di punti che avete andate in overfitting qualunque sia il modello bene allora da qui ripartiamo domani perché la domanda ah no vi faccio vedere anche questo poi da domani partiamo da qua vi faccio vedere quest'ultima slide allora qui abbiamo la stessa cosa lo stesso concetto in cui riprendiamo le modello basato su rete neurale con una tre oppure sei unità e quindi passiamo da un modello molto molto semplice in cui la funzione di costo si posizionerà qui a un modello un po' più complesso in cui abbassiamo la funzione di costo e abbassiamo il numero di errori che commette sul dataset di addestramento qui ne commette ancora di meno il problema è che ne commette pochi ma sono di nuovo nella ragione di overfitting e da qui ripartiamo domani perché la domanda con cui vi lascio è come facciamo ad accorgerci che siamo entrati in overfitting perché qui lo sappiamo perché sappiamo che il modello giusto è una cosa fatta così nel confine decisionale ma se io lo conoscessi non avrei bisogno di addestrare un modello e allora la domanda è come facciamo a trovare il livello di complessità giusta e ad accorgerci che siamo andati in overfitting oppure che siamo in underfitting insomma come facciamo a capire in quale in quale regime stiamo lavorando potete accorgersi anche facendo i test sui set di test se facciamo lo splitting del dataset si fa tipicamente così dopo il problema è che non c'hai più il dataset di test perché l'hai usato per addestrare il modello pensateci un attimo domani domani cioè se tu hai usato il tuo dataset di test per accorgerci se sei andato in overfitting non hai più dataset di test perché l'hai usato per fare addestramento per scegliere il modello migliore quindi il gioco è abbastanza semplice perché lo che si fa poi è ulteriormente fare un ulteriore split ma di questo parliamo domani va bene ok dopo la registrazione bene