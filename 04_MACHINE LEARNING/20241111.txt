Allora, intanto buongiorno a tutti. Riprendiamo le nostre lezioni sull'apprendimento non supervisionato, tornando alla... No, vi dicevo, tornando all'analisi dei componenti principali, che avevamo, era l'ultima cosa che avevamo iniziato nella scorsa lezione, nell'ultima lezione di teoria che abbiamo fatto la settimana passata. Abbiamo, vi ricordo, derivato un po' di formule, in particolare abbiamo parlato degli autoencoder, gli autoencoder lineari, quindi tecniche di riduzione della dimensionalità come proiezione lineare su uno spazio più bassa dimensionalità. E abbiamo visto che la minimizzazione, diciamo, di questa formula era quella che abbiamo chiamato appunto funzione di costo associata all'autoencoder lineare. E abbiamo detto, beh, la soluzione, quindi la matrice C che ci dice quali sono le basi dello spazio su cui andiamo a proiettare i nostri punti, possono essere diverse. Cioè questa formula, questa funzione obiettivo può avere più minimizzatori, quindi più punti di minimo. E abbiamo detto, ce n'è uno in particolare che riveste particolare interesse nell'analisi dei dati, quella che viene chiamata anche analisi esplorativa dei dati, che è quella delle componenti principali. Le componenti principali sono delle direzioni lungo le quali i dati hanno la massima, sono delle direzioni ortogonali, lungo le quali i dati hanno la massima varianza. E l'esempio che avete qui è appunto un'esemplificazione di quello che significa questa affermazione. Voi avete una distribuzione di dati, in questo caso è una distribuzione di dati che vive in uno spazio a due dimensioni. In questo spazio a due dimensioni, queste coordinate, i dati hanno questa distribuzione. Se voi andate, e vedremo adesso tra poco vi dico come può essere fatto, a ricavare l'insieme delle componenti principali, cioè andare a trovare tra i minimizzatori di questa funzione di costo, le due direzioni, vedete, trovate queste due, che sono due direzioni ortogonali tra di loro. Prima cosa che notate. L'altra cosa che notate è che vedete che queste due sono effettivamente, tra tutte le possibili direzioni su cui potete andare, in cui potete andare a trasformare il vostro dataset, i punti d'ingresso, quindi dallo spazio originale allo spazio trasformato, queste sono delle direzioni ortogonali e vedete che hanno la massima varianza, cioè preservano quella che è la variabilità del dato. Quindi da questo punto di vista, concettualmente vi permettono di mantenere maggiore informazione possibile, quindi in linea di principio vi permettono di preservare nella trasformazione l'informazione che avete associata. La cosa interessante è che nel momento in cui utilizzate appunto questo strumento come riduzione della dimensionità, potete andare a proiettare questo input su uno spazio più bassa dimensione, cercando di preservare il più possibile la variabilità del dato. Allora, non vedremo i passaggi matematici in dettaglio di come si arriva e il perché, diciamo come si arriva al risultato e il perché, poi soprattutto quello che si trova a questa proprietà, cioè di avere delle direzioni ortogonali di varianze, lungo le quali la varianza è massima. Se qualcuno fosse interessato non è una cosa particolarmente difficile, posso darvi qualche ulteriore, diciamo, o materiale da approfondire, da vedere, quindi suggerirvi qualche lettura in più, possiamo sicuramente approfondire, a parte, diciamo, le lezioni, come sapete, hanno, abbiamo un certo numero di ore a disposizione, quindi dobbiamo anche andare avanti, l'abbiamo fatto altre volte, per cui quello che faremo è prendere questi risultati e darli, e darli, diciamo, come buoni. Sono dei teoremi matematici, rigorosi. Ci sono diversi modi per ricavare, tra l'altro, le componenti principali a partire da un dataset. Quello che vi faccio vedere è basato sulla diagonalizzazione delle matrici simmetriche, cioè una scomposizione delle matrici simmetriche tramite autovalori e autovettori, e anche qui, diciamo, non vi do, appunto, poi il risultato a cui si arriva è quello delle componenti principali. Quello che non lo vediamo, quello che vediamo, un attimo, è un breve, diciamo, una breve traccia di come si costruisce il tutto. Allora, per fare questo, vi condivido un attimo la lavagna, perché riprendiamo qualche breve definizione riguardante autovalori e autovettori. per fare questo, allora, condividiamo. Allora. Ciao. Allora, siamo qui. Vi richiamo brevemente alcune notazioni. Allora, stiamo parlando, quindi intanto scriviamo di cosa stiamo parlando. Allora, lo strumento, il tool che viene utilizzato più frequentemente è quello che passa attraverso la diagonalizzazione delle matrici simmetrici. Cosa è la diagonalizzazione di una matrice simmetrica? È un strumento che è collegato alla teoria riguardante gli autovalori e autovettori di una matrice, che abbiamo già introdotto quando abbiamo parlato della matrice siana a proposito del metodo di minimizzazione del secondo ordine. Quindi abbiamo richiamato il concetto di autovalore e autovettore. Comunque, molto brevemente, dato una matrice x, l'auto vettore associato alla matrice x, v, è tale che è soluzione dell'equazione matriciale x per v uguale a lambda v, dove lambda è invece l'autovalore, sono gli autovalori associati alla matrice. Ok, quindi sono le soluzioni di x, v uguale a lambda v. Se la matrice è simmetrica, questa è la condizione di matrice simmetrica, gli autovettori, quello che si può dimostrare è che gli autovettori sono ortogonali, uno con l'altro, e gli autovalori sono numeri reali. Ok? Questo ce lo dice l'algebra lineare, la matematica. A questo punto, questi autovettori e autovalori, quindi una volta che abbiamo una matrice simmetrica, possiamo fare una cosa di questo tipo, costruire una matrice v a partire dalla nostra matrice x, che mette nella prima colonna il primo autovettore, nella seconda colonna il secondo autovettore, nell'ennesima colonna l'ennesimo autovettore. Quindi costruiamo una matrice v fatta così, e poi costruiamo una matrice diagonale, invece, che è una matrice che ha chiaramente zero su tutti gli elementi che non stanno sulla diagonale, e invece lungo la diagonale ha gli autovalori associati, che sono reali perché la matrice simmetrica. a questo punto quello che si fa è poter scrivere x, v, vi ricordo, deve essere uguale a lambda v, ma quello che si può fare è, per tutti gli autovalori e tutti gli autovettori, posso scrivere in maniera compatta a questo punto la matrice x, v uguale v per v. Ok? Questi sono vettori, questo v grande è una matrice. Quindi in maniera compatta per tutti gli autovalori e autovettori io riesco a scrivere questa cosa. Ok? Quindi la matrice x moltiplicata per la matrice degli autovettori è uguale alla matrice degli autovettori moltiplicata per la matrice diagonale. Questi sono tutti i risultati che in matematica sono veri e che si dimostrano in maniera abbastanza agevole. Posso andare avanti? La cosa interessante è che siccome vale la proprietà che v per vt è uguale alla matrice identità, la proprietà, perché è interessante che si, è un'altra proprietà che va dimostrata, si può dimostrare, ma gli autovettori di una matrice reale sono tra di loro ortogonali, in particolare il prodotto di quella matrice con la matrice trasposta, della matrice che abbiamo chiamato v, è la matrice identità, nel nostro caso, rimane vero quindi che cosa? Che siccome abbiamo detto prima x v è uguale a v per d, allora x, scusatevi, per v per v trasposto sarà uguale a v per d per v trasposto. Questo è la matrice identità, quindi il risultato di tutto questo è che x è uguale a v per d per v trasposto. Questo è il risultato di cui vi parlavo, è quello appunto che viene chiamata diagonalizzazione di una matrice simmetrica. Cioè noi abbiamo una matrice simmetrica, la possiamo scrivere come il prodotto di tre matrici, prodotto di tre matrici in cui la prima matrice è una matrice in cui mettete nelle varie colonne gli autovalori, gli autovettori, scusate, di quella matrice, poi avete una matrice diagonale che contiene lungo la diagonale tutti gli autovalori e poi avete la matrice v trasposta. Quindi ogni matrice simmetrica può essere espressa come il prodotto di queste tre matrici che sono definite da autovalori e autovettori. Questo è un risultato fondamentale dell'algebra lineare ed è un risultato che viene utilizzato, come vi dicevo, può essere utilizzato per ricavare le componenti principali. Adesso vi dirò come. Fino a qui abbiamo parlato di matrici simmetriche, semplicemente quello che viene fatto è andare a costruire una matrice simmetrica a partire dal dataset che noi abbiamo. Non è l'unico modo di fare analisi dei componenti principali, quindi ricavare le componenti principali, però è uno sicuramente dei più utilizzati e sfrutta appunto i teoremi di diagonalizzazione. Questo punto è noto come teorema di diagonalizzazione. Va bene? E adesso direi, diciamo, se avete finito di copiare, possiamo tornare alle slide e vi faccio vedere come questo può essere utilizzato. e vi faccio vedere come questo può essere utilizzato. Ok. Cosa si fa? Allora qui... C'è un po' riepilogo di quello che... Nel primo i due punti, quello che abbiamo visto finora. Quello che abbiamo visto finora è questo. La PCA, vi ho detto, può essere derivata come... Può essere derivata come... Ops, no, mi sa che era meglio in alto. Ok. Minimizzatore, vi dicevo, della formula dell'autoencoder, quindi una base ortonormale, ma si può dimostrare, e qui quello che vi dicevo, il risultato non è particolarmente, come vi dicevo, complicato, però insomma non lo facciamo, può essere calcolata a partire dagli autovettori di quella che viene chiamata matrice di covarianza dei dati. Cos'è la matrice di covarianza dei dati? È una matrice, allora, che viene costruita a partire da quella che è la data matrix. La data matrix è una matrice che raggruppa tutti i vostri dati. È una matrice che ha dimensione n per p, dove n è la dimensione dello spazio da cui partiamo, quindi è il numero delle feature. Ok? Mi ricordo, quindi lo scriviamo qua. e p è il numero dei punti del dataset. Allora, l'unica cosa che bisogna ricordarsi è che questi punti, perché torni tutto, e questo deriva da poi quello che è sotto il funzionamento della matematica che c'è sotto, devono essere centrati rispetto alla media. Cosa vuol dire centrati rispetto alla media? Dobbiamo fare una normalizzazione, per cui abbiamo i nostri punti e lungo ogni dimensione, quindi per ogni feature, andiamo a vedere qual è il valore medio. Quindi se ho 100 punti vado a vedere la prima feature lungo la prima dimensione e qual è il valore medio. Poi lo guardo per la seconda, per la terza, per la quarta, faccio la media su tutti i 100 punti e sottraggo questa da ogni vettore. I vettori così, appunto, che si dicono centrati rispetto alla media, possono essere organizzati poi, messi in colonna, in questo modo. Quindi ho x1, x2, fino a xp vettori, ognuno ha n componenti, quindi ognuno di questi chiaramente è n, mentre qui abbiamo p. E questa è la nostra, quella che viene chiamata matrice dei dati, cioè è semplicemente un modo di organizzare tutti i punti che abbiamo a nostra disposizione, nel nostro dataset di addestramento. Dopodiché quello che facciamo, costruiamo una matrice che, in analisi statistiche, è abbastanza nota perché vi dice un po' di cose su quelle che sono le caratteristiche del vostro dataset. E questa matrice viene costruita in questo modo, viene moltiplicata per se stessa trasposta e diviso ogni entry, ogni entry della matrice risultante, viene divisa per il numero di punti. E questa viene chiamata matrice di covarianza. Ok? O, per definizione, siccome io ho moltiplicato una matrice per la sua trasposta, quello che ottengo è una matrice quadrata ed è una matrice simmetrica. Se ci pensate un attimo. Quindi, siccome è una matrice simmetrica, che cosa possiamo fare? Utilizzare il teorema di diagonalizzazione che abbiamo prima riportato e quindi fare quella che viene chiamata con la diagonalizzazione una, in inglese, eigen decomposition, eigen values, eigen vector sono gli autovalori e gli autovettori, quindi la autoscomposizione, la potremmo chiamare, della matrice di covarianza. Molto semplicemente abbiamo la nostra matrice di covarianza, che è questa, che andiamo, siccome è reale e simmetrica, a scomporre nel prodotto di tre matrici. Quelle tre matrici sono, di nuovo, la matrice degli autovettori e la matrice diagonale con tutti gli autovalori sulla diagonale. Ci siamo? Ok. V per D per V trasposto. I vettori in V sono dei vettori che abbiamo detto hanno la proprietà di essere ortonormali. Ortonormali vuol dire che se io prendo un autovettore e lo moltiplico scalarmente per un altro autovettore, il risultato sarà zero, perché sono ortogonali. Se lo moltiplico per se stesso il risultato sarà uno, perché sono appunto ortonormali. Ok. Quello che si può dimostrare, e qui appunto bisogna andare a vedere la dimostrazione se uno vuole capire bene effettivamente come stanno le cose, però il risultato a cui si arriva è che, ed è un risultato, diciamo, secondo me di una certa eleganza, tutt'altro che scontato, solo a vedere appunto che cos'è, ma quando si va dietro a vedere come funziona effettivamente la dimostrazione, ci si convince che effettivamente funziona così. Quello che si può vedere è che quelle che sono le componenti principali sono effettivamente l'insieme dei vettori v. Cioè, questa matrice v contiene v1, v2, vi ricordo, fino a vn. Ok? E' la vostra matrice v. Tutte queste insieme vengono chiamate componenti principali e la cosa veramente interessante è che la varianza, cioè ognuna di queste definisce una direzione, chiaramente, della nostra trasformazione. Noi prendiamo i nostri dati di input e troviamo delle direzioni. Ok? Quante ne troviamo? n. Quindi, fino a qui non facciamo neanche riduzione della dimensionalità. Trasformiamo i dati da uno spazio da n dimensioni in uno spazio da n dimensioni se le usiamo tutte. La cosa interessante è che la varianza lungo ognuna di queste direzioni, ognuna di queste direzioni viene chiamata componente principale. Questa varianza ha una magnitudo, ci saranno alcune direzioni a cui è associata una maggiore varianza del dato nello spazio originale e altre che sono più alle quali è associata una varianza minore. Vi ricordate la prima, la slide da cui siamo partiti? Lì ci sono due direzioni, quelle con le frecce rosse, no? Una è associata alla dispersione, c'era un'ellisse di distribuzione dei dati. Lungo un asse l'ellisse aveva una maggiore dispersione, lungo l'altra ce l'aveva più piccolo. Ecco, quelle due direzioni sono identificate da due autovettori della matrice di covarianza associata a quei dati. Quanto quelle direzioni identificano la dispersione ce lo dicono gli autovalori, cioè gli autovalori non negativi. Se voi andate a prendere gli autovalori non negativi e andate a vedere qual è la magnitudo, l'intensità di quegli autovettori, quello vi dice qual è la direzione, scusate, qual è l'ampiezza della varianza associata. Quindi per sapere quali sono le direzioni principali io basta che vada a vedere qual è l'autovalore con il valore assoluto più grande. Scusate, con il valore positivo più grande. ok, qui c'è un esempio ed è un esempio in cui abbiamo una nuvola di punti a sinistra, un esempio analogo a quello che avevamo visto nella slide introduttiva. In questa nuvola di punti vedete, questi vivono in uno spazio che è quello delle feature x1 x2 quindi è uno spazio di due feature quindi giusto per fissare le idee in questo caso abbiamo un certo numero di punti p che non so quanto è però la matrice dei dati che n per p sarà in questo caso 2 per p che non so quanto valga uno dovrebbe contarli saranno occhio e croce una cinquantina di punti forse un po' di meno ok poco importa quello che importa è che rispetto a questa distribuzione vedete che voi a occhio riuscite a identificare una direzione un asse lungo il quale i punti variano molto e un altro asse ortogonale lungo il quale avete l'altra dispersione se voi applicate la cercate costruite la matrice di covarianza e poi cercate di diagonalizzarla quello che ottenete sono quindi due autovalori e due autovettori questi autovalori e autovettori rappresentano c1 e c2 le due basi scusate questi autovettori rappresentano c1 e c2 che sono le basi del nuovo sistema di riferimento nel quale potete trasformare ognuno di questi punti e portarlo di qua e e vedere quello che potete vedere è che se voi andate a vedere l'autovalore associato a questa direzione adesso non si capisce più quindi devo cancellare se voi andate a vedere l'autovalore che è associato a questa componente è quello di massima variabilità quindi sarà maggiore dell'autovalore associato a questa altra direzione e potete andare a identificare c1 e c2 come le due direzioni nelle quali codificare i dati come abbiamo fatto con la di fatto continua ad essere un autoencoder lineare l'autoencoder lineare è un metodo di trasformazione proiettate il dato in un altro in un altro sistema di riferimento in questo caso non avete riduzione della dimensionalità perché non c'è perdita di informazione partivamo da uno spazio bidimensionale siamo in uno spazio bidimensionale ma nulla vieta di andare a utilizzare la pca come strumento di riduzione della dimensionalità perché io che cosa posso fare partire da uno spazio a n dimensioni identificare un certo numero di componenti principali ridotto rispetto a n e dire vado a proiettare i miei dati in quello spazio che ha una dimensione ridotta e spero di portarmi dietro la maggiore informazione possibile la varianza è massima quindi forse ho buone probabilità di portarmi dietro maggiore informazione possibile e questo è un ragionamento che si fa è una delle tecniche per ridurre la dimensionalità la pca allora in linea di principio questo aiuta il processo poi perché perché facciamo questo perché poi quando abbiamo rappresentato i dati in uno spazio più bassa dimensionalità possiamo ad esempio costruirci un classificatore ok oppure fare altre operazioni un modello di machine learning fare tante cose e le possiamo fare con un tempo di elaborazione più basso perché non abbiamo dei vettori con 10.000 componenti ma abbiamo dei vettori magari con 5 10 componenti possiamo anche provare a visualizzare l'oggetto di cui stiamo parlando l'analisi di componenti principali ha molti utilizzi oltre a questo che vi stavo dicendo per esempio viene utilizzata appunto quando si fanno analisi statistiche dei dati per cercare di capire in un dataset quali sono le direzioni appunto proprio di massima variabilità perché questo vi dà una un'idea significativa di quali sono delle componenti che non riuscite a visualizzarle perché noi qui partiamo da esempi che sono bidimensionali ma se avete esempi ad alta dimensionalità e volete sapere quali sono le direzioni lungo le quali il dato varia allora si fa l'analisi con la PCA si prendono le prime componenti principali è ovvio che sono delle trasformazioni lo dicevamo la volta scorsa no? cioè andiamo in uno spazio che non è più quello originale ma è uno spazio trasformato però è uno strumento molto utile spesso e volentieri ma non è come tutti gli strumenti un qualcosa che può essere utilizzato sempre indiscriminatamente e conduce ovviamente a risultati sempre ottimali perché qualche volta per esempio se io faccio PCA prima di andare a fare una classificazione questo può creare dei problemi qui c'è un piccolo esempio diciamo giocattolo da tenere presente che ci esemplifica questa cosa è un esempio particolare in cui abbiamo una distribuzione di punti bidimensionali che appartengono a due classi diverse quindi abbiamo sopra i punti della classe blu sopra i punti della classe rossa ok se io andassi a costruire un classificatore a partire da questa da questa rappresentazione dei miei dati beh con un classificatore lineare li riuscirei a separare tranquillamente ok se prima di fare questo facessi applicassi l'analisi dei componenti principali e proiettassi tutto su una dimensione cosa troverei troverei che questi punti e anche questi ovviamente hanno questa direzione come componente principale perché è l'asse lungo le quali la direzione lungo le quali abbiamo la massima varianza del dato ok quindi se io vado a fare l'analisi della componente principale e proietto questi dati in questo caso con riduzione della dimensionalità quindi passo da uno spazio a due dimensioni a uno spazio a una dimensione che cosa succede? vado a proiettare tutti questi punti linearmente lungo quest'asse lungo questa direzione ok e mi ritrovo in questa situazione qua ora in questa situazione qua è molto più difficile andare a separare i punti quindi se voglio costruire un classificatore innanzitutto con un classificatore lineare non faccio più niente dovrei costruire un classificatore non lineare che ha un confine di eccesione che è una cosa micidiale ma poi ci sono comunque dei punti che non riesco ad andare più a distinguere quindi questo è un diciamo un warning che ci dice attenzione molto spesso si fa di fare PCA riduzione della dimensionalità costruire il classificatore però attenzione perché non sempre funziona bene e dipende da come sono distribuiti i punti ok domande? benissimo allora ci sono domande andiamo avanti e vediamo una variazione sul tema diciamo della PCA perché poi è qualcosa che è collegato alle cose che stiamo dicendo ed è un problema di tipo non supervisionato che si incontra frequentemente nell'oggigiorno nella in molte applicazioni di commercio elettronico quindi dietro c'è un qualcosa che a grandi linee è riconducibile alla cosa che vi sto dicendo che sono i cosiddetti qui c'è il titolo recommender system cioè sistemi di raccomandazione e cos'è un sistema di raccomandazione? è un sistema automatico che supponiamo di avere una piattaforma di commercio elettronico in cui diciamo viene utilizzato un metodo di machine learning che è di fatto un metodo di tipo non supervisionato per dare dei suggerimenti e fare delle raccomandazioni personalizzate sull'acquisto di ulteriori prodotti quindi l'esempio tipico che si fa è una piattaforma come diciamo di contenuti come Netflix ma potrebbe essere appunto legata alla musica o a altri prodotti quindi cose simili le avete sulle maggiori piattaforme di commercio elettronico appunto da Amazon Spotify quello che volete tutti dietro hanno dei sistemi per cui vi suggeriscono ogni volta che li utilizzate qualche nuovo prodotto che vi potrebbe secondo loro piacere allora come fanno vediamo vediamo un esempio ovviamente come fanno a grandi linee ecco diciamo e questo è uno dei modi in cui si può risolvere questo problema non è chiaramente però diciamo è un'esemplificazione che è utile per capire di cosa stiamo parlando allora sistema di questo tipo per esempio Netflix gli utenti cosa fanno chi vuole può fornire delle recensioni sui prodotti ok quindi dare una recensione quindi parliamo di film abbiamo una scala da 1 a 5 ok una scala da 1 a 5 quindi una stelletta fino a 5 stellette a seconda del del fatto che all'utente sia più o meno piaciuto quel dato film ok questa tutta questa serie di informazioni viene organizzata in matrice matrice in cui avete ad esempio associate alle righe il film e alle colonne gli utenti o viceversa poco poco importa e quindi abbiamo un certo numero di utenti P che se parliamo di queste grandi piattaforme è dell'ordine dei milioni utenti e poi abbiamo n che è il numero di film che è dell'ordine di migliaia va bene ordine di grandezza allora la caratteristica che ha una matrice di questo tipo che appunto vi dice all'incrocio tra riga I e colonna J vedete per esempio l'utente 7 è film 1 associato una valutazione di 3 l'utente 4 al film 2 una valutazione di 3 l'utente 5 al film 4 una valutazione di 2 e così via qual è la caratteristica che salta immediatamente all'occhio è che se voi guardate il singolo utente quindi ragionate su una singola colonna chiaramente quella singola colonna sarà una colonna molto poco popolata perché ogni utente ovviamente avrà il tempo per guardarsi 200.000 film ok quindi ne avrà guardati qualcuno per quanto possa essere un assiduo appassionato di cinema avrà un certo numero di di recensioni per tipicamente una frazione molto molto bassa di tutte quelle possibili e e allora il punto è che la matrice risultato è una matrice che si dice sparsa ok da un punto di vista matematico informatico le matrici sparse sono matrici cui abbiamo solamente alcune entri e con alcune entri intendo che il 99% tipicamente di quella matrice non lo conosciamo non sappiamo qual è l'obiettivo però è andare a riempire queste caselle magari perché se io so che l'utente capire diciamo quali sono degli ipotetici valori da assegnare a queste caselle in modo da poter suggerire all'utente in questo caso quattro dei prodotti che potenzialmente possono essere per lui di interesse che quindi lui potrebbe andare a recensire favorevolmente per fare questo questo significa fare raccomandazioni personalizzate e viene utilizzato vengono utilizzate tecniche di apprendimento non supervisionato che sono una variazione sul tema dell'analisi dei componenti principali adesso cerchiamo di capire qualcosa di più molto molto rapidamente senza entrare troppo nel dettaglio però del come questo funziona prima faccio un inciso siccome poi l'ho saltato ma ve lo faccio vedere poi quando ho terminato adesso il discorso sui sistemi di raccomandazione torniamo sulla PCA dopo perché vi voglio dare una traccia veloce di un pseudocodice da utilizzare ad esempio in Python per poter fare per poter fare l'analisi dei componenti principali ok invece adesso torniamo al al nostro problema che è quello di abbiamo un certo numero di di vettori x1 x2 fino a x con P e ognuno di questi vettori identifica quello che è il vettore diciamo delle recensioni dell'utente P ok quindi è un vettore in cui lui avrà le sue recensioni quindi 3 5 4 eccetera che sono i voti che lui dà da 1 a 5 ma chiaramente non ce li ha per tutti per tutte le entry possibili quindi ci sarà un insieme degli indici dell'utente P che è il che sono tutte le coppie J virgola P per cui esiste una recensione cioè vuol dire che l'utente P ha dato una recensione per il prodotto J ok l'obiettivo qual è del sistema di recommender è riempire le entry che mancano allora per fare questo vi dicevo esistono diversi diversi modi una soluzione possibile è basata su questa assunzione fondamentalmente quello che il ragionamento che si fa è abbastanza semplice si dice ok supponiamo che io possa descrivere i gusti di un determinato utente come una combinazione lineare una piccola combinazione lineare di una serie di pochi profili base cioè io posso dire che c'è l'utente che è appassionato di commedie posso definire un utente che è appassionato di film d'azione posso dire che c'è un utente appassionato di film comici e così via definisco una manciata pochi di questi profili base e poi definisco il cerco diciamo di identificare quello che è l'effettivo profilo di un utente tramite una combinazione lineare di questi profili base il che significa che se ho un utente che ha un certo tipo di magari di gusti di profilo che è un appassionato magari di film di commedie e magari ha anche un componente non so di appassionato di film western ok sarà una combinazione lineare in cui avrò i due coefficienti di queste tipologie più elevati e magari gli altri vicini allo zero ok e così via quindi questo equivale da un punto di vista matematico a dire che cosa che ho assumo che ci sia che esista un insieme di un certo numero di vettori di base di gusto ok k k dove k è tipicamente più piccolo ben più piccolo di p ok che formano di fatto una base un insieme ricoprente quindi questo equivale a dire che cosa equivale a dire che io sto formalmente assumendo che il vettore xp che rappresenta vi ricordo il quello che viene chiamato il rating vector cioè il vettore delle recensioni del mio del mio utente p esimo lo posso scrivere circa esprimere circa uguale come una combinazione lineare dove i coefficienti della combinazione lineare sono questi p riferito al piesimo utente e ne ho di questi coefficienti è k con n piccolo che va da 1k e ognuno viene moltiplicato per un vettore cn che cos'è cn è l'ennesimo supponiamo di avere 5 k uguale valga 5 ok se k vale 5 vuol dire che ho 5 gusti base ok ognuno di questi gusti base è espresso da un vettore c1 c2 c3 c4 c5 ok e ognuno di quelli lo vado a moltiplicare per un coefficiente e vado il mio obiettivo è cercare che cosa sia w ma anche c perché io c non li conosco e questo è esattamente il problema da cui siamo partiti quando abbiamo affrontato il problema di scrivere un autoencoder solo che passiamo da uno la differenza lì qual è allora vediamo intanto l'ulteriore analogia l'analogia è che partiamo da uno spazio che ha dimensionalità n ok a uno spazio andiamo che ha dimensionalità in questo caso 5 la differenza dove sta beh la differenza il problema è che non conosciamo non conosciamo tutto cioè la nostra matrice dei dati quando parlavamo dell'autoencoder la conoscevamo tutta qui non la conosciamo tutta ci sono ci sono dei buchi ma adesso su questo ragioniamo tra poco in ogni caso questo l'abbiamo scritto esattamente come abbiamo scritto l'autoencoder abbiamo detto ok in maniera compatta vuol dire che io sto costruendo una matrice C che contiene tutti i 5 C con N tale che è moltiplicata per WP dove in WP vado a mettere i pesi W mi restituisce più o meno XP e a quel punto se io conosco XP vuol dire che ho risolto il problema di andare a riempire quei buchi che non avevo quindi l'obiettivo a questo punto diventa che cosa imparare far apprendere al sistema la matrice C e i vettori di peso W ma questo lo avevamo visto si fa lo possiamo fare perché quello che possiamo fare è minimizzare una funzione di costo una funzione di costo ai minimi quadrati come quella che abbiamo visto quindi vado a mettere dentro questa funzione è una funzione che dipende dal vettore dei pesi W1 W2 fino a WP dove P è il numero degli utenti dipende da quella matrice C che la devo andare a scoprire che è la matrice in cui per ognuno di questi io ho un no un profilo base di gusti e questa è la solita funzione ai minimi quadrati list square io voglio rendere il più possibile vicino a zero questa differenza la differenza rispetto al problema dell'autoencoder è che adesso io ho a disposizione solamente ve lo accennavo prima le entri della mia matrice X che sono indicizzate da quell'insieme Omega P che sono le uniche entri diverse da zero che conosco quindi quello che devo fare è andare a minimizzare questa quantità qui cosa vuol dire quando sto ragionando su quell'insieme di indici ok perché gli altri non li conosco quindi devo ragionare solo su quelli che conosco e questa è la differenza con l'autoencoder se no è fondamentalmente un'applicazione di questo tipo di logica e se vogliamo anche della della PCA poi ok comunque stiamo girando intorno a a questo problema ora la differenza rispetto all'autoencoder lineare è che nell'autoencoder lineare non avevo questa restrizione qui sì e il fatto di ragionare su un sotto insieme degli indici implica che non possiamo arrivare dalle soluzioni ortonormali ecco perché è diverso da autoencoder lineare dalla PCA da quello che volete però io è una funzione obiettivo la posso minimizzare per esempio con metodi di discesa del gradiente coordinate discesa delle coordinate eccetera e posso una volta che ho utilizzato questi strumenti andare a ricavare dei valori di WP e di C e una volta che ho WP e C ho tutto cioè ho risolto il mio problema di andare a riempire le entry di quella matrice perché posso ricostruire in maniera approssimata XP e quindi nel momento in cui ho ricostruito in maniera approssimata XP vuol dire che ho fatto delle previsioni su quali sono i gusti dell'utente in tutte quelle caselle che l'utente non aveva espresso e quindi a quel punto potrò dire questo film l'utente non l'ha visto ma forse gli potrebbe piacere oppure questo film l'utente non l'ha visto ma sicuramente secondo me il sistema non gli può piacere tutto questo chiaramente capite bene che ha tantissime variazioni sul tema ovviamente non è l'unico modo in cui questi sistemi vengono costruiti ci sono altre tecniche però questa è una tecnica di base che c'è dietro spesso e volentieri e capite bene che anche dietro a questi strumenti girano ovviamente un sacco di soldi per cui c'è gente che fa questo di lavoro esclusivamente e a proposito ho menzionato i soldi perché qualche anno fa c'era Netflix aveva sponsorizzato proprio un premio si chiamava Netflix Prize proprio in cui era stato rilasciato un dataset pubblico e c'era un premio in denaro ed era un premio a cui partecipavano gruppi di ricerca di aziende di industrie private ma anche di università in giro per il mondo una volta all'anno c'era questa gara questo contest in cui appunto si cercava veniva data questa matrice molto grande molto molto sparsa e venivano chiaramente mascherate le entry e quindi resa sparsa ma si sapeva come funzionava perché ovviamente era invece stata costruita a partire da dati reali che aveva ovviamente la piattaforma e tutto questo diciamo ha permesso di sviluppare questi sistemi condietro spesso e volentieri però questo tipo di approccio ecco abbiamo semplificato molto ma penso diciamo potesse valerla appena menzionare perché appunto vi dicevo un settore che a livello industriale ha il suo peso una cosa che non capita bene è quando abbiamo detto che dobbiamo capire un certo numero di profili di base sì noi decidiamo soltanto il numero dei profili di base però la semantica di profilo di base tipo amante dell'equilibrio dell'equilibrio eccetera eccetera come facciamo a capire che un certo profilo di base corrisponde a un certo tipo di ok sì questo diciamo perché tu dici poi comunque la c la ricavo e quindi non ce l'ho associata ok certo no infatti questo è per dire è l'assunto di partenza poi è ovvio che questi qui non corrispondono a quel tipo non hanno quel tipo di semantica questi c vettori nel senso che difficilmente però in realtà se io quando tu vai a vedere c'è in realtà una corrispondenza perché se tu costruisci queste direzioni e ritrovi appunto che l'utente P è un utente che ha messo 5 a film di quella categoria non sempre così semplice però e tu riesci a vedere che effettivamente il peso associato è quello allora puoi dire a quel punto fare una sorta di reverse engineering e dire a questa direzione effettivamente quella esatto esatto bravo sì sì però partiamo dall'assunto che esistano ok però non glieli imponiamo prima però è un punto sicuramente interessante questo che solledi allora torno un attimo brevemente invece adesso prima di iniziare l'ultimo argomento sull'unsupervised per il momento su questo blocco di slide torno a ricondividere la lavagna perché faccio quel passo indietro ritorno un attimo sulla pca vi do un attimo brevemente uno schema di di funzionamento di come può essere implementato quindi il pseudocodice per la pca alla luce di quello che abbiamo visto allora questo era quello che avevamo scritto e alla luce di quello che avevamo scritto mi volevo semplicemente dare questo e non ve l'ho dato prima siccome poi ci torneremo su credo anche se non ricordo male nell'esercitazione avevo preparato qualcosa dedicato a questo allora primo step per fare analisi dei componenti principali è centrare x centrare x vuol dire sottrare la media da unificio ok secondo step creare la matrice di covarianza a questa matrice di covarianza se parliamo di implementazione semplicamente si aggiunge una cosa di questo genere si aggiunge un termine che è la matrice identità n per n moltiplicata per un fattore lambda piccolino questo lambda non è lambda degli autovalori è un numero 10 alla meno 3 ok secco perché viene utilizzato questo è un termine di nuovo viene chiamato di regolarizzazione questa regolarizzazione viene introdotta ve l'ho detto in più punti del nostro corso anche con semantiche differenti cosa vuol dire aggiungere questa cosa qui vuol dire che io vado a costruire la mia matrice di covarianza in maniera aggiungendoci un termine in cui lungo la diagonale di questa matrice vado ad aggiungere quel 10 alla meno 3 perché faccio questo questo per evitare possibili problemi questi sono algoritmi numerici gli algoritmi numerici possono diventare instabili quindi aggiungere un piccolo termine di rumore rende l'algoritmo stabile da un punto di vista numerico quindi è consigliato fare questa operazione quindi andiamo a perturbare in maniera molto piccola la matrice di covarianza vera terzo passo calcolo degli autovalori e degli autovettori come si fa anche qui non è banale ci sono dietro degli algoritmi che fanno questo algoritmi di calcolo numerico a partire dall'algebra lineare vi permettono di fare questo strumenti come NumPy hanno a disposizione il calcolo degli autovalori degli autovettori quindi sono già implementati sono algoritmi ingegnerizzati funzionano bene oramai c'è una base di codice che ha diversi anni che è stabile su questo e viene continuamente migliorata ma il il nocciolo diciamo è sicuramente consolidato nella letteratura scientifica e questo è qualcosa che sicuramente possiamo utilizzare ultimo punto seleziona i k auto vettori corrispondenti agli autovalori più alti gli autovalori più alti sono quelli ricordate che agli autovalori è associata la varianza quindi se io seleziono i 5 auto vettori che corrispondono ai 5 autovalori più alti sto selezionando le direzioni 5 direzioni con le 5 valori di varianza più alte ok questo è sorta di pseudo codice che riassume gli step per implementare la pca e ovviamente qui dietro ci vuole un sistema che faccia in maniera questo ve lo potete implementare un prodotto matrici per vettore però se lo fate utilizzando un unpy che ha appunto la vettorizzazione di quelle operazioni abbiamo visto molto più efficiente calcolo autovalori e auto vettori non è banalissimo farselo da soli quindi anche lì utilizzare una libreria di calcolo scientifico che lo fa per voi in maniera efficiente è sempre una buona cosa poi potete selezionare i k auto vettori che corrispondono agli autovalori più alti e siete passati da uno spazio a dimensione n a uno spazio a dimensione k dove avete selezionato le k direzioni ortogonali di massima varianza del dato a va bene ok questo era l'inciso che volevo farvi diciamo che prima in realtà mi ero scorato di fare quindi l'ho posticipato e qui sui recommender system abbiamo visto direi tutto c'è tutto abbiamo visto abbiamo fatto un rapidissimo excursus in realtà perché adesso dobbiamo dare spazio a un'altra problematica fino adesso abbiamo parlato un'altra problematica di unsupervised learning fino adesso abbiamo parlato di riduzione della dimensionalità adesso parliamo di clustering clustering o cluster analysis è un altro ramo un'altra tecnica di apprendimento non supervisionato molto utilizzata per diversi motivi cluster significa raggruppamento in inglese ok e il problema è proprio quello di raggruppare i punti di un dataset raggrupparlo con quale criterio ma in modo che se io vi faccio vedere questo insieme di punti che sta qui a sinistra un attimo che cerchiamo di rilanciare scusate forse non si vede un po' ricondividendo questo dovrebbe esserci ok mi dicevo la se io vi facessi vedere questo insieme di punti che avete qui a sinistra questo insieme di tutti e vi dicessi quali sono quanti sono i raggruppamenti in questo insieme di punti probabilmente ognuno di voi direbbe questo è un primo raggruppamento questo un altro questo un altro perché perché il vostro cervello è abituato a riconoscere immediatamente questi pattern geometrici e parlo non a caso i pattern geometrici perché il criterio è tipicamente quello appunto di dire raggruppo i punti in modo che quelli che stanno geometricamente più uno vicino all'altro cadono nello stesso nello stesso e in questo caso stiamo facendo una e a destra di fatto c'è il risultato di questa di questo raggruppamento non è sempre così banale anche visivamente perché ci sono dei casi in cui uno diciamo ci potrebbero essere delle difformità per cui uno vede magari un raggruppamento un altro ne vede un altro tipo eccetera però in questo caso li identifichiamo direi in maniera abbastanza uniforme tutti come tre tre cluster con questi raggruppamenti che avete in destra quindi il problema è quello di raggruppare dei punti in modo che siano il più possibile vicini l'uno all'altro da un punto dove questo vicino è un concetto di vicinanza geometrica ok che andrà opportunamente specificato problema fino adesso però un problema un po sfumato dobbiamo cercare di di arrivarci per ordine la prima cosa prima prima di entrare un po più nel dettaglio che volevo su cui volevo farvi riflettere a perché dobbiamo occuparci di questa di questo tema beh una cosa interessante che emerge subito nel momento in cui noi riusciamo a fare un'analisi di questo tipo è che se io riesco a dire che questo dataset è costituito da questi tre cluster io posso per ognuno di questi tre cluster questo rosso questo verde o questo blu andare a calcolare un baricentro baricentro viene chiamato nel gergo del machine learning centroide anche ok ed è esattamente proprio il concetto di baricentro cioè faccio la media dei punti ognuno di questi è un punto che è rappresentato da da un vettore numerico vado a fare la media lungo ogni dimensione di quei vettori e ottengo questi che sono evidenti qui queste stelline qua che rappresentano il baricentro di quei punti il centroide ora il centroide a questo punto io posso prendere il centroide del cluster rosso e andarlo a utilizzare come rappresentante di tutti i punti del cluster cioè questo può essere una sorta di prototipo medio che mi rappresenta tutti i punti di quel cluster la stessa cosa posso fare per il cluster verde e per il cluster blu e quindi mi ritrovo con tre punti anziché 5 8 12 punti in questo caso che mi rappresentano il mio dataset quindi un modo di condensare l'informazione che è associata a quel dataset e questo molto importante tutta una serie di applicazioni dove posso voler ragionare su un certo numero ridotto di punti che sono però ugualmente rappresentativi di un dataset questo è il motivo per cui l'analisi di cluster è estremamente importante perché vi permette di ottenere una sintesi di quello che è un insieme di dati ok allora adesso cerchiamo un po di capire meglio come portare avanti la risoluzione del problema di cluster in cui vedremo un esempio un algoritmo che si chiama algoritmo k-means che permette di risolvere questo questo tipo di problema o di affrontarlo diciamo e di risolverlo in diversi casi allora noi abbiamo partiamo anche qui dalla formalizzazione del problema come al solito abbiamo un dataset con p punti x1 x2 xp ognuno di questi è un vettore ognuno di questi un vettore a n componenti e nel nostro punto di partenza dopodiché questi punti sono suddivisi ipotizziamo in k raggruppamenti k cluster e quindi ognuno ha c1 c2 fino a ck centroidi ok andiamo a definire un insieme sk come l'insieme dei punti p tali che se xp appartiene al k esimo cluster allora lo vado a mettere dentro questo insieme quindi è l'insieme dei punti p tali che p xp appartiene al k esimo cluster quindi avrò un supponiamo in questo caso andiamo a vedere il nostro esempio in questo caso abbiamo tre cluster quindi avrò s1 s2 s3 in s1 vado a mettere tutti i punti che sono che appartengono al cluster 1 quindi supponiamo che questi siano numerati in questo modo 1 2 3 4 5 6 7 8 9 10 11 e 12 è chiaro che in s1 ci sono tutti i punti del cluster 1 quindi ci sono 1 2 5 7 e 8 in s2 ci saranno 3 4 e 6 e in s3 ci sono tutti i restanti ok va bene a questo punto io posso andare a costruire il centroide esattamente vi dicevo come baricentro cioè vado a prendere tutti i centroide per esempio c1 lo costruisco come la somma di tutti i punti che appartengono a s1 in questo caso sono 5 ok dei vari vettori li vado a sommare e lo vado a dividere devo far la media lungo ogni dimensione lo vado a dividere in questo caso per 5 il centroide c2 sarà 1 diviso 3 perché sono tre punti e sarà x3 x4 più x6 mentre questo sarà 1 diviso 5 per x1 più x2 scusate vado a scrivere qua più x5 x7 più x8 e c3 sarà la stessa cosa per l'ultimo raggruppamento va bene ok quindi abbiamo definito il centroide abbiamo definito l'insieme sk e questo ci aiuta adesso come premessa ad andare un po' più avanti nel nel problema nella definizione del problema andiamo ci siamo andiamo avanti cancello eh ok qual è il problema il problema è che ogni generico punto xp deve essere assegnato al cluster ecco come posso formalizzarlo il problema cioè come posso dire voglio che il raggruppamento sia tale che in quel raggruppamento finiscono punti che sono geometricamente vicini tra di loro lo posso formalizzare in questo modo cioè io voglio che ogni punto xp venga assegnato un certo cluster la cui distanza dal centroide che è questa è minima da un punto di vista geometrico vuol dire che io prendo xp lo vado a confrontare nel caso precedente con c1 c2 c3 vado a vedere la distanza euclidea di xp da c1 poi la vado a vedere da c2 poi la vado a vedere da c3 e prendo quella minima e dico ok questo punto lo assegno a questo centroide ok quindi questo problema viene chiamato problema del cluster assignment cioè dell'assegnazione del cluster ed è uno step fondamentale dell'algoritmo di clustering il punto xp verrà assegnato quindi si dirà che apparterà al cluster k star quindi al cluster 1 al cluster 2 oppure al cluster 3 se ci riferiamo all'esempio di prima se l'assegnazione di quel punto è il risultato di che cosa vado a prendere quando k va da 1 fino al numero di cluster la distanza dal centroide e prendo l'argmin se per esempio nell'esempio di prima avevo k grande uguale a 3 possibili raggruppamenti andavo a vedere e avevo un punto generico a che ne so torniamo indietro non ce l'ho più come li avevo suddivisi però abbiamo a 10 ok cioè il punto vado a considerare xp uguale x10 vado a vedere qual è l'assegnazione di a 10 voglio capire a 10 a chi lo assegno a 10 sarà l'argmin quando k piccolo varia da 1 fino a 3 di che cosa di x10 meno ck distanza euclidea cioè vado a vedere il punto x10 se è più vicino a centroide 1 centroide 2 centroide 3 il minimo se supponiamo che il minimo fosse 2 allora a 10 è uguale a 2 ok questo è il problema del cluster assignment che capite bene è il nucleo fondamentale del problema del clustering perché adesso lo metteremo all'interno di un largo ritmo che poi vedrete itera e fa questa operazione ripetuta una serie di volte e vedrete che metteremo insieme tutto allora il primo problema è come facciamo a automatizzare il cluster assignment perché qui una cosa che non abbiamo notato è un problema fondamentale che abbiamo detto supponiamo che k sia uguale a 3 noi abbiamo visto prima abbiamo detto ok qui siamo tutti d'accordo che ci sono 3 raggruppamenti ma come facciamo a sapere che ci sono 3 raggruppamenti nella realtà delle cose non lo sappiamo è quello che vogliamo scoprire quindi la cosa che si fa è ok partiamo dal dall'ipotizzare di conoscere k in realtà non lo facciamo non lo sappiamo quindi dopo dobbiamo risolvere questa cosa ma se noi conoscessimo k cioè il numero di cluster allora io a quel punto potrei mettere in piedi un meccanismo di questo genere comincio a cercare di indovinare a caso k posizione dei centroidi cioè comincio ad assegnare k posizione dei centroidi a caso alla prima iterazione perché è un algoritmo iterativo quello che stiamo costruendo questo algoritmo prende il nome di k-means ok le medie sui k sui k raggruppamenti e il primo step è ok se mi dici quanto vale k faccio una generazione casuale di k centroidi una volta che ho fatto questa generazione casuale di k centroidi ok non funziona vediamo un attimo ok una volta che ho fatto questa generazione a caso quello che facciamo è torniamo allo step sopra abbiamo visto come viene svolto e effettuiamo quello che viene chiamato appunto il cluster assignment facciamo un loop su tutti i punti per trovare esattamente questo cioè assegniamo ad ogni punto il proprio cluster cioè io ho sono partito da tre no? supponiamo di fissare k uguale a tre tre centroidi che ho generato a caso e vado ad assegnare tutti i punti del dataset rispetto a questi centroidi e questo è la prima il primo round di iterazioni che andiamo a fare dopodiché questo round di iterazioni assegna dei cluster ma a questo punto io che cosa posso fare? per ognuno di questi cluster posso andare a calcolare il centroide il primo centroide l'ho generato a caso adesso comincio a calcolarlo in base alle assegnazioni che ho fatto e e come faccio? beh, semplicemente vado a fare esattamente quello che avevamo detto prima cioè vado a calcolare il centroide che è questo l'abbiamo visto prima quindi fino adesso sono due le operazioni che facciamo cluster assignment e calcolo del centroide questo è quello che ci serve le metto in una serie di iterazioni e vado a ripetere questo schema cluster assignment calcolo del baricentro del centroide e lo faccio un certo numero di volte tipicamente non c'è garanzia su questo ma tipicamente arrivo a converge cioè o meglio ho la garanzia che l'algoritmo converge tipicamente arrivo a convergenza su un raggruppamento che è ragionevole non è detta di fatto noi stiamo minimizzando una funzione qui dietro che è la distanza media dei punti all'interno di ogni cluster e quello che facciamo tipicamente è andare in cerca del minimo di questa funzione con questo meccanismo quindi questo è un algoritmo iterativo che ripeto non vi dà garanzia di trovare sempre il minimo perché queste sono anche queste funzioni di costo che non sono convesse hanno un solo minimo globale quindi potremmo rimanere intrappolati in qualche minimo locale però generalmente funziona abbastanza bene e vi dicevo ripetete questo meccanismo che è cluster assignment e calcolo del centroide cluster assignment e calcolo del centroide un certo numero di volte 10 100 quello che volete oppure vi accorgete a un certo punto che non cambia più cioè non riuscite più ad assegnare nessun cluster nuovo i baricentri rimangono sempre gli stessi a quel punto da per un certo numero di iterazioni 5 o 10 non cambia più niente e vi fermate questo è l'algoritmo K- all'opera un algoritmo molto semplice dal punto di vista concettuale ma funziona bene e ci sono anche qui delle varianti che vi permettono ad esempio di una potenziale debolezza è questo random guessing iniziale che vi permettono di fare un random guessing un po' intelligente quindi di minimizzare un po' di problemi associati a questo però dietro poi rimane lo stesso ok quindi e questo è un esempio di come funziona questo questo algoritmo qui ci sono le prime due iterazioni possiamo andarle a vedere insieme allora qui abbiamo questi punti quindi partiamo da qua e la prima cosa facciamo vi dicevo è assegniamo dei valori a caso dei centroidi quindi supponiamo che un centroide sia questo un centroide sia questo un centroide sia quest'altro ok quindi ho definito dei vettori a caso a due componenti e ho ottenuto quei centroidi allora in base a questi centroidi io faccio cluster assignment cioè vado a dire ok ho questi punti questi li vado ad associare al cluster rosso perché perché questi punti hanno lui come punto più vicino di centroide questo punto invece vede lui come suo centroide questo vede lui come suo centroide e così via quindi questi faranno parte di un altro raggruppamento tutti questi punti colorati in blu vengono invece assegnati a quest'altro centroide che è quello della stella colorata in blu ok questo è il primo step ho generato a caso e ho fatto i centroidi ho fatto cluster assignment a questo punto dallo step 1 andiamo allo step 2 che è questa è la prima iterazione quindi sarebbe meglio chiamarla 1a che è calcolo del centroide e 1b che è cluster assignment e a questo punto quindi 1a è calcolo del centroide e cluster assignment l'1b è il ricalcolo del centroide perché? perché io ho assegnato questi quattro punti a questo cluster a questo punto posso andare a calcolare l'effettivo centroide di questi quattro punti e l'effettivo centroide si sposta da qui verso qua come anche per i punti verdi ovviamente ricalcolo il centroide e ottengo questo e questo è lo spostamento per il terzo cluster questa è la prima completa la prima iterazione seconda iterazione ricomincio ho dei nuovi centroidi rifaccio cluster assignment faccio cluster assignment e quello che succede è che questa volta rispetto a questo centroide ok che era questo vedete che cambiano un po' le cose perché questo punto che qui era all'iterazione precedente assegnato al cluster rosso verrà assegnato al cluster verde quest'altro punto che era nel cluster verde verrà assegnato al cluster blu e questi due del cluster blu verranno assegnati al cluster rosso e vedete come cambiano i raggruppamenti però vedete che se vado avanti e ricalcolo di nuovo per ognuno di questi raggruppamenti il il il centroide vedete che i centroidi progressivamente si spostano sempre di meno l'algoritmo sta andando a convergenza e soprattutto i cluster assignment produrranno meno effetti qui di nuovo ricalcolo il centroide ricalcolo il centroide me lo sposta qua me lo sposta qua e se rifate un'altra iterazione vi accorgete che qui non cominciate a spostare più niente non cominciate a spostare più niente e il centroide rimane sempre lo stesso quel punto quindi arrivate a convergenza del del metodo l'ipotesi di partenza da cui siamo partiti è che conosciamo k e qui abbiamo detto k uguale a 3 rimane in sospeso come risolvere questo problema e adesso ne parliamo tra poco ok ok questo è un altro esempio dell'algoritmo al lavoro e vi fa vedere l'effetto dell'inizializzazione cioè l'inizializzazione è cruciale nella riuscita del dell'algoritmo in questo caso fissiamo k uguale a 2 quindi ipotizziamo di dargli k uguale a 2 ripeto lasciando in sospeso un attimo il problema del fatto che non sappiamo se k uguale a 2 o quanto può valere e e guardiamo come funziona questi questi due questo grafico che è suddiviso in due parti la prima sopra e la seconda sotto ok allora quello che cambia dalla prima alla seconda è l'inizializzazione cioè supponiamo che i punti che gli diamo all'inizio siano questi questi due questa croce blu e questa croce rossa su queste due su questa distribuzione di punti di questi due raggruppamenti allora cosa succede se io gli do questo punto e questo lui quello che fa è chiaramente assegnare a questa serie di punti qui il cluster chiamiamolo blu ok e li coloriamo il patin blu per farlo vedere mentre tutti questi che stanno sotto vedono lui come centroide più vicino ok questo significa nel momento in cui io rivado a fare il calcolo del centroide questo mi si sposta verso qui questo mi si sposta verso qua ok a questo punto che cosa faccio di nuovo rispetto a questo punto e a questo vado a fare il cluster assignment ma il cluster assignment non cambia nulla rimango intrappolato qui questo da qui in avanti la soluzione è sempre questa posso andare avanti quante iterazioni voglio ma non mi muovo quindi per lui l'assegnazione sarà questa quindi il risultato del clustering sarà questo e capite bene che rispetto a quello che noi vediamo in realtà avrebbe naturale di dire questo è un cluster e questo è un cluster quindi lui è arrivato a convergenza ben presto ma ha prodotto un risultato insoddisfacente il problema è l'inizializzazione iniziale perché basta che io sposto uno di questi due valori di centroide qui all'inizio e guardate che cosa succede succede che alla prima iterazione ho un cluster assignment in cui quasi tutti i punti vengono assegnati chiaramente come più vicini a questa croce rossa e vengono assegnati al cluster rosso e solamente questi tre al cluster blu poi però succede una cosa interessante perché comincio a fare l'aggiornamento del del bari centro quindi lui si sposta verso qua lui si sposta verso qui e se rifaccio cluster assignment a questo punto tutti questi punti che stanno su questo lato verranno assegnati al blu e tutti questi altri al rosso e il risultato sarà questo di nuovo questo porta a un piccolo spostamento del scusatemi di nuovo a questo punto arrivado vado qua pardon sono qua arrivado a fare ho fatto i primi due calcoli dei primi due centroidi vado a calcolare il terzo centroide che vedete mi si proietta dentro il raggruppamento corrispondente e a questo punto il risultato è consolidato ed è un risultato decisamente più corretto rispetto rispetto al primo la chiave del successo o del fallimento dipende quindi dall'inizializzazione da un da un'inizializzazione che però noi abbiamo detto essere casuale allora come facciamo a sapere a trovare un modo di evitare questo è di fatto equivale a rimanere intrappolati in un minimo locale mentre qua c'è una soluzione che nella funzione di costo che c'è dietro a questo algoritmo è migliore come faccio? come faccio a risolvere questo problema? ma in realtà la soluzione è abbastanza semplice posso posso provare più iterazioni con punti iniziali diversi generati a caso e poi vado a prendere la migliore il problema è dire qual è la migliore mi devo dotare di una di una metrica di un modo per quantificare una soluzione rispetto a un'altra soluzione allora anche qui ci sono diverse diverse modalità quindi quello che si fa è fare più run con differenti inizializzazioni e si prende la miglior soluzione secondo quella che è una certa metrica di qualità del cluster le metriche di qualità del cluster ce ne sono tante che rispondono a esigenze diverse io ve ne menziono una che è quella più immediata che forse ci può venire in mente se ragioniamo sul problema che è la cosiddetta distanza media intracluster che è la distanza media di ogni punto dal suo centroide cioè io vado a prendere ogni punto ho il mio risultato vado a vedere ok quanto dista questo punto dal suo centroide quest'altro punto dal suo centroide quest'altro punto dal suo centroide e faccio la media più è bassa meglio è a questo punto io posso fare 5 run vado a prendere quello con la distanza intracluster media più bassa e quella sarà la mia soluzione in questo modo mi accorgo che se al primo run ho fatto un'inizializzazione che era stata particolarmente sfortunata come quella dell'esempio precedente al secondo run magari ne prendo una migliore e ho sotto controllo il tutto perché voglio trovare la soluzione migliore secondo questa metrica ok e adesso rimane da capire abbiamo risolto parzialmente diciamo il problema della dell'inizializzazione facciamo più run rimane da capire come facciamo a scegliere k perché fino adesso abbiamo detto k ipotizziamo cioè l'assunto fondamentale del k- è che l'input è k oltre ai dati come faccio a scegliere k in realtà anche qui non lo scelgo ma molto semplicemente faccio una cosa che è forse anche qui una cosa abbastanza se vogliamo brute force di tipo forza brutta cioè provo diversi valori definisco un range e comincio a provare diversi valori dopodiché di nuovo confronto i risultati confronto in base a una metrica la metrica può essere quella che vi ho detto prima la distanza intracluster e vado a fare una cosa di questo tipo supponiamo di avere qui questi dati ok io quello che faccio è provo parto con un solo raggruppamento e ovviamente un solo raggruppamento gli do k uguale a 1 faccio run avrò un solo centroide vado a calcolare la distanza media intracluster e avrà un certo valore che sarà questo dopodiché vado avanti con k uguale a 2 ripeto la stessa cosa e dico cosa succede all'algoritmo k-means che probabilmente mi trova questi due raggruppamenti a questi due raggruppamenti sarà associata una distanza media intracluster che sarà ad esempio 2 virgola qualcosa ripeto la cosa con 3 con 4 con 5 eccetera eccetera eccetera e qui però mi accorgo subito anche qui di un problema nel senso che qui allora il risultato sarebbe la funzione di costo è questa io vado a scegliere il minimo in base a quello che ho detto prima però c'è da stare attenti a una cosa che questa distanza intracluster è monotonicamente decrescente cioè la soluzione migliore è quella con tanti cluster quanti sono il numero dei punti è ovvio che questo non ci va bene e allora che cosa si fa si fa una si sceglie una soluzione approssimata si va a vedere tipicamente queste curve hanno quello che viene chiamato un gomito un elbow un gomito nella curva e si va a prendere quello dove c'è più pronunciato dove c'è proprio una sorta di spigolo ok che in questo caso vedete non è non è sempre anche qui così immediato perché qui potremmo stare a discutere è due oppure tre probabilmente due sì però potrebbe essere anche tre uno due e tre quindi questo ci fa capire anche che anche qui non è così semplice anche se abbiamo automatizzato quasi tutto ecco c'è un quasi i problemi di tipo non supervisionato come vi dicevo sono sempre un po' sfumati un po' difficili da caratterizzare ma fa parte della natura stessa del problema io non ho delle etichette che mi dicono ok devi fare così e quindi devi riprodurre questo comportamento e qui devo cercare di fare ordine nei dati e non è sempre così scontato però ecco insomma con tutte queste con tutti questi diciamo accorgimenti si riesce genericamente a fare una cosa una cosa ragionevole quindi generalmente si prende il valore diciamo al gomito allo spigolo a partire dal quale poi la distanza totale non decresce troppo a partire da qui o da qua da qui in avanti no in realtà no però per completezza cioè tu potresti partire direttamente da due o meglio diciamo potrebbe esserci anche una situazione in cui di fatto esatto effettivamente ne hai uno su è un caso molto particolare va bene ok questo diciamo è completa un po' la panoramica per il momento sul learning non supervisionato torneremo un po' sopra più avanti anche su qualcosa di learning non supervisionato però diciamo questo è quanto abbiamo visto per ora quindi vi ricordo quindi riduzione della dimensionalità e clustering questo con tecniche lineari poi ci sono anche tecniche non lineari per questi problemi ma queste sono un po' fuori per diciamo del qualcosa avremo modo di dire magari più avanti ma insomma sono un po' fuori del programma però per il momento insomma questo è quanto invece la prossima volta andiamo avanti con un nuovo blocco di slide nuovo argomento se non ci sono domande anche da remoto direi che intanto posso fermare la condivisione e anche la registrazione spartere e l'ansioni di cont Aware